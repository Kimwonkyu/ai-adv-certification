
chapter_name = "LLM 기본"

questions = []

# --- 100 MCQs ---
# Unique conceptual and practical questions based on 3.md

mcq_data = [
    # 1. LLM과 트랜스포머 (1-25)
    ("전통적인 RNN/LSTM 모델이 긴 문장을 처리할 때 겪었던 '장기 의존성(Long-term Dependency)' 문제에 대한 설명으로 옳은 것은?", ["문장이 길어질수록 앞부분의 정보를 소실하거나 잊어버리는 현상이다.", "문장의 길이에 상관없이 항상 동일한 성능을 유지하는 특징이다.", "문장 전체의 통계적 빈도만 고려하여 문맥을 파악하지 못하는 것이다.", "컴퓨터의 메모리 용량이 부족하여 프로그램이 꺼지는 현상이다.", "단어를 무작위로 섞어서 학습하기 때문에 발생하는 오류이다."], "문장이 길어질수록 앞부분의 정보를 소실하거나 잊어버리는 현상이다.", "RNN은 순차적으로 데이터를 처리하므로 멀리 떨어진 단어 간의 관계를 학습하기 어려운 한계가 있었습니다.", "RNN 한계", "3001"),
    ("2017년 구글이 발표한 'Attention is All You Need' 논문의 핵심적인 기여는?", ["RNN의 성능을 2배 높이는 새로운 방식을 제안했다.", "이미지 처리를 위한 CNN 아키텍처를 완성했다.", "순차 처리 대신 병렬 처리가 가능한 '트랜스포머' 구조를 제시했다.", "데이터 보안을 위한 새로운 암호화 알고리즘을 발표했다.", "파이썬의 실행 속도를 높이는 인터프리터를 개발했다."], "순차 처리 대신 병렬 처리가 가능한 '트랜스포머' 구조를 제시했다.", "트랜스포머는 어텐션 메커니즘을 전면에 내세워 문맥 파악 능력을 비약적으로 높였습니다.", "트랜스포머 탄생", "3002"),
    ("트랜스포머의 '어텐션(Attention)' 메커니즘이 수행하는 가장 주된 작업은?", ["단어의 글자 수를 세어 가장 긴 단어를 찾는다.", "문맥상 어떤 단어들이 서로 밀접한 관계가 있는지 가중치를 계산한다.", "파일의 용량을 줄이기 위해 텍스트를 압축한다.", "오타를 실시간으로 교정하여 문법을 맞춘다.", "단어를 다른 나라 언어로 즉시 번역한다."], "문맥상 어떤 단어들이 서로 밀접한 관계가 있는지 가중치를 계산한다.", "어텐션은 특정 단어를 이해할 때 문장 내 다른 단어들을 얼마나 참고할지 점수를 매깁니다.", "어텐션 원리", "3003"),
    ("트랜스포머 아키텍처 중 '인코더(Encoder)'의 특징에 대한 설명으로 옳은 것은?", ["주로 문장을 새로 생성(Generate)하는 작업에 최적화되어 있다.", "입력 문장을 수치화하여 그 의미를 압축하고 이해하는 데 강점이 있다.", "GPT 모델의 핵심 구조로 사용된다.", "다음에 올 단어를 하나씩 예측하며 결과물을 내놓는다.", "오직 한국어 분석에만 사용 가능한 특수 구조이다."], "입력 문장을 수치화하여 그 의미를 압축하고 이해하는 데 강점이 있다.", "인코더는 문맥의 상호 의미를 파악하는 데 특화되어 있으며, BERT가 대표적인 인코더 기반 모델입니다.", "인코더 특징", "3004"),
    ("현대 LLM(GPT 등)이 주로 채택하고 있는 '디코더 전용(Decoder-only)' 구조의 특징은?", ["문장의 의미를 이해하기만 할 뿐, 새로운 답변을 만들지는 못한다.", "앞서 생성된 단어들을 바탕으로 다음에 올 단어를 확률적으로 예측한다.", "인코더보다 항상 크기가 작고 성능이 낮다.", "반드시 이미지 데이터와 함께 학습되어야만 동작한다.", "입력 데이터의 순서를 고려하지 않고 무작위로 답변을 내놓는다."], "앞서 생성된 단어들을 바탕으로 다음에 올 단어를 확률적으로 예측한다.", "디코더는 이전 토큰들의 맥락을 유지하며 다음 토큰을 생성하는 생성 작업(Generation)의 표준입니다.", "디코더 특징", "3005"),
    ("GPT 모델이 문장을 생성할 때 단어를 하나씩 내뱉는 방식은?", ["한꺼번에 문장 전체를 사진처럼 찍듯이 생성한다.", "다음 토큰을 예측하며 순차적으로 한 단어씩 생성한다.", "문장의 마지막 단어부터 거꾸로 생성한다.", "사전에 정의된 문장 템플릿에 단어만 끼워 넣는다.", "사용자가 엔터를 칠 때까지 기다렸다가 한 번에 대답한다."], "다음 토큰을 예측하며 순차적으로 한 단어씩 생성한다.", "Auto-regressive(자기 회귀) 방식으로, 이전 결과가 다음 입력이 되어 문장을 완성해 나갑니다.", "생성 메커니즘", "3006"),
    ("트랜스포머에서 단어의 순서(위치) 정보를 모델에게 전달하기 위해 사용하는 기법은?", ["Sequence Count", "Positional Encoding", "Index Mapping", "Word Order Tagging", "Linear Alignment"], "Positional Encoding", "트랜스포머는 데이터를 한꺼번에 처리하므로, 단어의 위치 구분을 위해 인코딩 값을 더해줍니다.", "위치 인코딩", "3007"),
    ("모델 아키텍처 중 BERT는 주로 ( A ) 방식이며, GPT는 주로 ( B ) 방식이다. ( )에 들어갈 적절한 조합은?", ["A: 디코더, B: 인코더", "A: 인코더, B: 디코더", "A: CNN, B: RNN", "A: 임베딩, B: 토크나이징", "A: 지도학습, B: 비지도학습"], "A: 인코더, B: 디코더", "BERT는 문맥 이해(인코더), GPT는 문장 생성(디코더)에 강점이 있는 대표적 모델입니다.", "모델 구분", "3008"),
    ("트랜스포머 구조에서 여러 개의 어텐션을 동시에 수행하여 다양한 관점을 학습하는 기술은?", ["Single-Line Attention", "Parallel Attention", "Multi-Head Attention", "Complex Attention", "Super Attention"], "Multi-Head Attention", "여러 '헤드'를 통해 문장의 다양한 문맥적 특징을 동시에 추출합니다.", "Multi-Head", "3009"),
    ("딥러닝 모델의 층이 깊어질 때 학습이 잘 안 되는 문제를 해결하기 위해, 입력값을 뒤쪽 층에 직접 전달하는 구조는?", ["Skip Layer", "Back Link", "Residual Connection (잔차 연결)", "Fast Track", "Data Tunnel"], "Residual Connection (잔차 연결)", "입력 정보를 결과에 더해주어 기울기 소실(Vanishing Gradient) 문제를 완화합니다.", "잔차 연결", "3010"),
    ("LLM이 처리하는 데이터의 최소 단위인 '토큰(Token)'에 대한 설명으로 틀린 것은?", ["글자 하나일 수도 있고, 단어 하나일 수도 있다.", "모델은 텍스트를 직접 읽는 것이 아니라 토큰화된 숫자를 처리한다.", "영어보다 한글이 토큰 소모량이 보통 더 적다.", "단어의 일부(서브워드) 단위로 쪼개지기도 한다.", "토큰 소모량이 많을수록 API 비용이 더 많이 발생한다."], "영어보다 한글이 토큰 소모량이 보통 더 적다.", "한글은 교착어 특성상 형태소 단위로 쪼개지면 영어보다 토큰을 더 많이 사용하는 경향이 있습니다.", "토큰의 정의", "3011"),
    ("단어의 의미를 고차원 공간상의 좌표(실수 리스트)로 나타내는 과정을 무엇이라 하는가?", ["Vectorization", "Embedding (임베딩)", "Scaling", "Positioning", "Dimensioning"], "Embedding (임베딩)", "임베딩을 통해 컴퓨터는 단어 사이의 의미적 유사도를 계산할 수 있게 됩니다.", "임베딩", "3012"),
    ("유사한 의미를 가진 단어들은 벡터 공간상에서 어떤 특징을 갖는가?", ["서로 멀리 떨어져 있다.", "서로 수직 관계에 있다.", "서로 가까운 거리에 위치한다.", "모두 0에 수렴한다.", "아무런 상관관계가 없다."], "서로 가까운 거리에 위치한다.", "코사인 유사도 등을 통해 벡터 간의 거리가 가까울수록 의미가 유사하다고 판단합니다.", "공간적 의미", "3013"),
    ("서브워드(Subword) 토큰화 기법 중 하나로, 자주 등장하는 문자 쌍을 반복적으로 병합하는 방식은?", ["WordPiece", "SentencePiece", "BPE (Byte Pair Encoding)", "N-gram", "Jamo Splitting"], "BPE (Byte Pair Encoding)", "BPE는 가장 빈번한 조합을 하나의 단어로 묶어 어휘 사전의 효율성을 극대화합니다.", "BPE", "3014"),
    ("LLM이 한 번에 기억하고 처리할 수 있는 입력 데이터의 최대 범위는?", ["Memory Span", "Context Window (문맥 창)", "Token Buffer", "Input Horizon", "Processing Limit"], "Context Window (문맥 창)", "이 범위를 벗어난 이전 대화 내용은 모델이 망각하게 됩니다.", "문맥 창", "3015"),
    ("GPT-3 모델의 매개변수(Parameter) 개수는 약 얼마인가?", ["1.7B (17억 개)", "175B (1,750억 개)", "7B (70억 개)", "1T (1조 개)", "500M (5억 개)"], "175B (1,750억 개)", "GPT-3는 초거대 언어 모델의 시대를 연 상징적인 모델로 1,750억 개의 파라미터를 가집니다.", "GPT-3 규모", "3016"),
    ("별도의 추가 학습 없이 프롬프트에 예시를 몇 개 보여주는 것만으로 모델이 방식을 익히는 현상은?", ["Fine-tuning", "In-Context Learning (Few-shot)", "Hard Coding", "Manual Training", "Meta Learning"], "In-Context Learning (Few-shot)", "모델 가중치를 고정하고 프롬프트 맥락 내에서 지식을 습득하는 능력입니다.", "퓨샷 학습", "3017"),
    ("예시를 전혀 주지 않고 바로 명령만 내리는 방식을 무엇이라 하는가?", ["One-shot", "Zero-shot", "No-shot", "Direct-shot", "Fast-shot"], "Zero-shot", "모델의 사전 지식과 지시 이행 능력에만 의존하는 방식입니다.", "제로샷", "3018"),
    ("OpenAI가 발표한 모델 중 멀티모달 능력을 갖추고 이미지 인식까지 가능해진 유료 모델 버전은?", ["GPT-2", "GPT-3", "GPT-4", "GPT-Neo", "InstructGPT"], "GPT-4", "GPT-4는 텍스트뿐만 아니라 이미지 입력을 이해할 수 있는 강력한 성능을 보여줍니다.", "GPT-4", "3019"),
    ("메타(Meta)가 공개하여 오픈소스 LLM 생태계를 폭발시킨 모델의 이름은?", ["Alpaca", "Claude", "LLaMA (라마)", "Gemini", "Mistral"], "LLaMA (라마)", "라마의 가중치 공개는 개인과 연구자들이 저사양으로도 LLM을 연구하게 만든 전환점이 되었습니다.", "LLaMA", "3020"),
    ("허깅페이스(HuggingFace)에서 모델을 다운로드하여 내 서버에서 직접 구동하는 방식의 장점은?", ["관리 인력이 아예 필요 없다.", "서버 비용이 0원이다.", "데이터 보안이 강력하고 커스텀 학습이 자유롭다.", "메모리(RAM)를 거의 쓰지 않는다.", "인터넷이 끊겨도 전 세계 데이터를 다 안다."], "데이터 보안이 강력하고 커스텀 학습이 자유롭다.", "외부 서버로 데이터를 보내지 않아 보안에 유리하며, 우리 비즈니스에 맞게 수정이 가능합니다.", "오픈 모델 장점", "3021"),
    ("OpenAI API 등을 사용하여 클라우드 기반으로 모델을 쓰는 방식의 장점은?", ["가장 최신/최고 성능의 모델을 인프라 관리 없이 즉시 쓸 수 있다.", "데이터 유출 위험이 절대 없다.", "인터넷이 없어도 작동한다.", "사용료가 평생 무료이다.", "모델의 내부 코드를 마음껏 수정할 수 있다."], "가장 최신/최고 성능의 모델을 인프라 관리 없이 즉시 쓸 수 있다.", "고성능 GPU 서빙 비용과 운영 리스크를 줄이며 최상급 성능을 활용할 수 있습니다.", "상용 API 장점", "3022"),
    ("모델의 답변 스타일 중 '온도(Temperature)'를 낮게 설정하면 나타나는 결과는?", ["답변이 매우 창의적이고 돌발적으로 바뀐다.", "답변이 일관되고 결정론적이며 보수적으로 나온다.", "답변의 길이가 10배 이상 길어진다.", "답변의 속도가 훨씬 느려진다.", "틀린 글자가 더 많이 섞이게 된다."], "답변이 일관되고 결정론적이며 보수적으로 나온다.", "낮은 온도는 가장 확률이 높은 단어 위주로 선택하여 정확성을 높여줍니다.", "온도 낮음", "3023"),
    ("소설이나 창의적인 아이디어를 얻고 싶을 때 권장되는 'Temperature' 범위는?", ["0.0 ~ 0.2", "0.3 ~ 0.5", "0.7 ~ 1.0", "-1.0 ~ 0.0", "오직 0.0 고정"], "0.7 ~ 1.0", "높은 온도는 모델이 다양한 후보 단어를 선택하게 하여 창의적인 결과를 유도합니다.", "온도 높음", "3024"),
    ("LLM이 존재하지 않는 사실을 지어내어 말하는 '환각' 현상의 영문 명칭은?", ["Illusion", "Distortion", "Hallucination", "Confusion", "Deception"], "Hallucination", "학습되지 않은 내용에 대해 그럴싸한 거짓말을 하는 생성 모델의 한계점입니다.", "환각", "3025"),

    # 2. 토큰화 및 모델 특징 (26-50)
    ("한글 텍스트 '안녕하세요'를 GPT 토크나이저로 변환했을 때 예상되는 결과 구조는?", ["한 글자당 토큰 1개씩 총 5개", "전체를 묶어 토큰 1개", "의미와 형태소에 따라 쪼개진 여러 개의 숫자 리스트", "영어 알파벳으로 치환된 텍스트", "바이트 단위의 0과 1"], "의미와 형태소에 따라 쪼개진 여러 개의 숫자 리스트", "토크나이저는 문장을 수치화된 토큰 ID의 시퀀스로 변환합니다.", "한글 토큰화", "3026"),
    ("OpenAI의 'tiktoken'이나 HuggingFace의 'tokenizers' 라이브러리의 역할은?", ["텍스트의 오타를 수정한다.", "텍스트를 토큰으로 분리하거나 토큰을 텍스트로 합친다.", "모델을 직접 학습시킨다.", "강력한 보안 암호화 기능을 제공한다.", "인터넷 검색 속도를 높여준다."], "텍스트를 토큰으로 분리하거나 토큰을 텍스트로 합친다.", "모델 입력 전의 전처리와 모델 출력 후의 후처리를 담당하는 핵심 도구입니다.", "토크나이저 라이브러리", "3027"),
    ("토큰(Token)과 단어(Word)의 관계에 대한 설명으로 옳은 것은?", ["항상 1토큰 = 1단어이다.", "보통 1단어는 1개 이상의 여러 토큰으로 쪼개질 수 있다.", "토큰은 단어보다 항상 긴 텍스트 단위이다.", "단어는 잊어버리고 오직 토큰만 사전에 등록된다.", "영어는 토큰을 쓰고 한글은 단어를 쓴다."], "보통 1단어는 1개 이상의 여러 토큰으로 쪼개질 수 있다.", "공통되지 않은 단어는 서브워드로 쪼개어 효율적으로 처리합니다.", "토큰 vs 단어", "3028"),
    ("모델의 '파라미터(Parameter)'가 많아질수록 나타나는 일반적인 특징은?", ["학습 속도가 빨라진다.", "더 정교한 추론과 지식 습득이 가능하지만 연산 비용이 증가한다.", "저장 용량이 획기적으로 줄어든다.", "모델이 훨씬 멍청해진다.", "인터넷이 없어도 동작하지 않게 된다."], "더 정교한 추론과 지식 습득이 가능하지만 연산 비용이 증가한다.", "규모의 경제(Scaling Law)에 따라 모델이 클수록 더 똑똑해지는 경향이 있습니다.", "파라미터 증량", "3029"),
    ("트랜스포머 아키텍처에서 '병렬 처리'가 가능하다는 말의 의미는?", ["여러 문장을 한 번에 번역한다는 뜻이다.", "문장 내 단어들을 동시에 한 번에 계산할 수 있다는 뜻이다.", "CPU와 GPU를 동시에 쓴다는 뜻이다.", "파이썬과 C언어를 섞어 쓴다는 뜻이다.", "사용자가 여러 명이어도 괜찮다는 뜻이다."], "문장 내 단어들을 동시에 한 번에 계산할 수 있다는 뜻이다.", "RNN처럼 앞 순서를 기다리지 않고 행렬 연산으로 한 번에 처리하여 속도가 빠릅니다.", "병렬 처리", "3030"),
    ("다음 중 '오픈 웨이트(Open Weights)' 모델에 해당하는 것은?", ["GPT-4", "Claude 3.5", "Llama 3", "Gemini 1.5 Pro", "o1-preview"], "Llama 3", "Llama, Mistral 등은 모델의 가중치를 공개하여 로컬 실행이 가능한 오픈 모델입니다.", "오픈 웨이트", "3031"),
    ("상용 LLM API(예: gpt-4o) 호출 시 가장 큰 비용을 차지하는 요소는?", ["사용한 API 키의 개수", "입력 및 출력에 소모된 '토큰'의 양", "접속한 인터넷의 속도", "키보드를 타이핑한 횟수", "모니터의 해상도"], "입력 및 출력에 소모된 '토큰'의 양", "대부분의 LLM 서비스는 토큰 단위로 과금을 진행합니다.", "API 과금", "3032"),
    ("프롬프트에 '너는 친절한 상담원이야'라고 설정하는 가장 윗 단계의 입력창 이름은?", ["User Message", "System Message", "Assistant Message", "Instruction Message", "Base Prompt"], "System Message", "시스템 메시지는 모델의 정체성과 가이드라인을 정하는 최상위 지시문입니다.", "시스템 메시지", "3033"),
    ("이전 대화 내역을 모델에게 전달할 때 쓰는 메시지 유형은?", ["User/Assistant Message", "History Message", "Log Message", "Archive Message", "Backlink Message"], "User/Assistant Message", "이전의 질문과 답변 쌍을 순서대로 전달하여 문맥을 유지합니다.", "대화 내역", "3034"),
    ("HuggingFace 모델 페이지에서 볼 수 있는 'Model Card'의 역할은?", ["모델을 유료로 결제하는 카드", "모델의 용도, 학습 데이터, 제약 사항 등을 적은 설명서", "모델의 성능을 2배 높이는 치트키", "모델의 로고 디자인", "모델 제작자의 명함"], "모델의 용도, 학습 데이터, 제약 사항 등을 적은 설명서", "모델의 윤리적 사용과 기술적 사양을 명시한 문서입니다.", "모델 카드", "3035"),
    ("모델 크기가 커져도 성능이 일정 수준에서 멈추지 않고 계속 좋아진다는 법칙은?", ["Moore's Law", "Scaling Law (척도 법칙)", "Entropy Law", "Zipf's Law", "Efficiency Law"], "Scaling Law (척도 법칙)", "데이터, 파라미터, 연산량이 늘어나면 언어 능력이 향상된다는 관찰 결과입니다.", "스케일링 법칙", "3036"),
    ("특정 규모 이상의 모델에서 갑자기 나타나는 논리 추론 등의 고차원 능력을 일컫는 말은?", ["Hidden Skill", "Emergent Ability (창발적 능력)", "Sudden IQ", "Jump Point", "Super Feature"], "Emergent Ability (창발적 능력)", "작은 모델에서는 불가능하던 작업이 거대 모델에서 가능해지는 현상입니다.", "창발적 능력", "3037"),
    ("임베딩 벡터들 간의 유사도를 측정할 때 가장 표준적으로 사용되는 계산법은?", ["덧셈과 뺄셈", "유클리드 거리", "코사인 유사도 (Cosine Similarity)", "평균값 비교", "글자 수 비교"], "코사인 유사도 (Cosine Similarity)", "방향성을 위주로 측정하여 단어 간의 의미적 유사성을 잘 잡아냅니다.", "코사인 유사도", "3038"),
    ("LLM이 다음에 올 토큰의 확률 분포에서 샘플링을 할 때, 상위 P%의 누적 확률 내 단어들만 고려하는 기법은?", ["Top-K", "Nucleus Sampling (Top-P)", "Random Cut", "Greedy Search", "Softmax Filter"], "Nucleus Sampling (Top-P)", "확률이 낮은 꼬리 부분을 자르고 유의미한 상위 단어들만 후보로 삼습니다.", "Top-P", "3039"),
    ("매번 가장 높은 확률을 가진 단어 하나만 100% 선택하여 생성하는 딱딱한 방식은?", ["Random Search", "Greedy Search (탐욕적 검색)", "Beam Search", "Smart Pick", "Top-N"], "Greedy Search (탐욕적 검색)", "가장 뻔한 답변이 나오기 쉽고 창의성이 낮아집니다.", "그리디 서치", "3040"),
    ("트랜스포머 아키텍처 논문 제목 'Attention is All You Need'가 시사하는 바는?", ["RNN을 더 많이 써야 한다.", "어테션만으로도 충분히 강력한 모델을 만들 수 있다.", "데이터 보안이 가장 중요하다.", "인터넷 속도가 생명이다.", "사람의 관심(Attention)이 모델을 만든다."], "어테션만으로도 충분히 강력한 모델을 만들 수 있다.", "기존의 복잡한 구조를 걷어내고 어텐션이 핵심임을 천명한 제목입니다.", "논문 제목 의미", "3041"),
    ("GPT 시리즈의 역사를 순서대로 나열한 것은?", ["GPT-3 -> GPT-2 -> GPT-1", "GPT-1 -> GPT-2 -> GPT-3", "BERT -> GPT-1 -> T5", "GPT-Open -> GPT-Closed", "Llama -> GPT -> Claude"], "GPT-1 -> GPT-2 -> GPT-3", "버전 번호가 커지며 파라미터 수와 성능이 비약적으로 증가해 왔습니다.", "GPT 역사", "3042"),
    ("라마(LLaMA) 모델이 벤치마크 점수는 높으면서도 크기를 줄일 수 있었던 비결은?", ["학습 데이터를 모두 한글로 해서", "양보다 질 좋은 방대한 양의 데이터를 학습해서", "파라미터를 0으로 만들어서", "이미지만 학습해서", "인공지능을 쓰지 않아서"], "양보다 질 좋은 방대한 양의 데이터를 학습해서", "모델 크기 대비 더 많은 양의 고품질 텍스트를 학습시킨 것이 핵심입니다.", "LLaMA 성공 요인", "3043"),
    ("다음 중 모델 서빙 시 메모리 사용량을 줄이기 위해 가중치의 정밀도를 낮추는 기법은?", ["Normalization", "Quantization (양자화)", "Distillation", "Pruning", "Augmentation"], "Quantization (양자화)", "16비트 모델을 4비트로 낮추면 메모리를 4배 아낄 수 있습니다.", "양자화", "3044"),
    ("큰 모델(Teacher)의 지식을 작은 모델(Student)에게 전수하여 소형화하는 기법은?", ["Teaching", "Knowledge Distillation (지식 증류)", "Inheritance", "Copy-Paste", "Hard Training"], "Knowledge Distillation (지식 증류)", "작지만 똑똑한 모델을 만드는 데 사용되는 최적화 기법입니다.", "지식 증류", "3045"),
    ("데이터 분석과 학습 기록을 위해 코드와 실행 결과를 한 장의 문서로 관리하는 도구는?", ["Excel", "Jupyter Notebook", "Notepad", "PowerPoint", "Slack"], "Jupyter Notebook", "LLM 학습 및 테스트 시 인터랙티브한 코딩 환경을 제공합니다.", "쥬피터 노트북", "3046"),
    ("모델 학습 시 '에포크(Epoch)'의 정의로 옳은 것은?", ["데이터를 한 줄 읽었을 때", "전체 데이터를 모델이 한 번 다 훑었을 때", "1초의 시간이 흘렀을 때", "에러가 한 번 났을 때", "답변을 한 번 생성했을 때"], "전체 데이터를 모델이 한 번 다 훑었을 때", "학습의 반복 단위를 나타내는 기본 용어입니다.", "에포크", "3047"),
    ("LLM이 '이전의 대화 흐름'을 기억하려면 매번 질문할 때 무엇을 같이 보내야 하는가?", ["전체 대화 내역(Chat History)", "내 컴퓨터의 로그인 ID", "내 어제의 일기", "현재 날씨 정보", "인터넷 브라우저 쿠키"], "전체 대화 내역(Chat History)", "모델은 상태를 저장하지 않으므로(Stateless), 이전 내역을 매번 전부 전달해야 합니다.", "대화 기억", "3048"),
    ("HuggingFace에서 모델을 불러올 때 사용되는 파이썬 라이브러리 명칭은?", ["torch", "transformers", "scipy", "django", "requests"], "transformers", "트랜스포머 기반 모델들을 쉽게 다루는 표준 라이브러리입니다.", "transformers Lib", "3049"),
    ("GPT-4o 모델에서 'o'의 의미와 멀티모달의 연결이 적절한 것은?", ["Optimized: 속도가 빠름", "Open: 코드가 공개됨", "Omni: 텍스트/이미지/오디오 통합 처리", "Online: 실시간 검색 가능", "Only: 텍스트만 처리"], "Omni: 텍스트/이미지/오디오 통합 처리", "Omni는 '모든'이라는 뜻으로 다양한 미디어를 입출력함을 의미합니다.", "GPT-4o", "3050"),
    
    # 추가 50문제 (개념 심화 및 실무)
    ("트랜스포머에서 'Self-Attention'과 'Cross-Attention'의 차이점으로 옳은 것은?", ["Self는 자기 자신을, Cross는 다른 모델을 본다.", "Self는 입력 문장 내의 관계를, Cross는 인코더와 디코더 사이의 관계를 본다.", "Self는 영어만, Cross는 번역만 한다.", "둘은 이름만 다를 뿐 100% 동일한 연산이다.", "Self는 CPU에서, Cross는 GPU에서 수행된다."], "Self는 입력 문장 내의 관계를, Cross는 인코더와 디코더 사이의 관계를 본다.", "Cross-Attention은 번역기 등에서 소스 문장을 참고할 때 중요합니다.", "Self vs Cross", "3051"),
    ("생성 제어 파라미터 중 Top-K를 1로 설정하면 어떤 기법과 동일해지는가?", ["Beam Search", "Random Sampling", "Greedy Search", "Nucleus Sampling", "Penalized Search"], "Greedy Search", "가장 확률 높은 1개 단어만 후보로 두므로 탐욕적 검색과 같아집니다.", "Top-K 1", "3052"),
    ("학습 데이터에 편향(Bias)이 섞여 있을 때 발생하는 사회적 위험은?", ["컴퓨터가 고장 난다.", "인종, 성별 등에 대해 차별적인 답변을 내놓을 수 있다.", "전기료가 많이 나온다.", "인터넷 속도가 느려진다.", "모델이 아무 대답도 하지 못한다."], "인종, 성별 등에 대해 차별적인 답변을 내놓을 수 있다.", "공정하고 윤리적인 AI를 위해 데이터 정제가 필수적입니다.", "모델 편향", "3053"),
    ("거대 언어 모델이 추론(Reasoning)을 더 잘하게 만들기 위해 단계별로 생각하게 유도하는 프롬프트 기법은?", ["CoT (Chain-of-Thought)", "Few-shot", "Persona", "Output formatting", "Role playing"], "CoT (Chain-of-Thought)", "풀이 과정을 먼저 적게 함으로써 정확한 정답 도출을 돕습니다.", "CoT", "3054"),
    ("OpenAI API에서 'max_tokens'를 너무 작게 설정하면 발생하는 일은?", ["답변이 나오지 않는다.", "답변이 중간에 뚝 끊긴다.", "답변이 더 정확해진다.", "무료로 전활된다.", "오타가 수정된다."], "답변이 중간에 뚝 끊긴다.", "생성할 수 있는 최대 길이를 넘어서면 끊긴 채로 전달됩니다.", "맥스 토큰", "3055"),
    ("모델의 '가중치(Weights)'란 무엇을 의미하는가?", ["모델 파일의 실제 무게(kg)", "단어 간의 관계 강도를 나타내는 숫자 값들", "모델 개발자의 직급", "데이터베이스의 용량", "서버의 전기 소모량"], "단어 간의 관계 강도를 나타내는 숫자 값들", "학습 과정을 통해 최적화된 수억 개의 수치들을 말합니다.", "가중치", "3056"),
    ("임베딩 벡터의 차원이 보통 수백~수천 차원인 이유는?", ["컴퓨터가 보기에 멋있어 보여서", "단어의 복잡한 의미적 특징을 다각도로 담아내기 위해서", "메모리를 최대한 많이 쓰기 위해서", "해킹을 어렵게 하려고", "숫자가 클수록 무조건 좋아서"], "단어의 복잡한 의미적 특징을 다각도로 담아내기 위해서", "고차원 공간일수록 미세한 의미 차이를 분리하여 표현하기 유리합니다.", "임베딩 차원", "3057"),
    ("다음 중 OpenAI가 제공하는 가장 똑똑하지만 비싼 최상위 모델 라인업은?", ["Mini", "Ada", "Turbo", "GPT-4 / 4o", "Babbage"], "GPT-4 / 4o", "GPT-4 계열은 가장 복잡한 추론 작업을 수행하기 위한 플래그십 모델입니다.", "모델 레벨", "3058"),
    ("실무에서 '토큰화' 비용을 줄이기 위한 가장 효과적인 방법은?", ["영어로만 대화한다.", "프롬프트를 최대한 길게 쓴다.", "질문을 명확히 하고 불필요한 컨텍스트를 제거한다.", "인터넷창을 닫는다.", "회원 가입을 다시 한다."], "질문을 명확히 하고 불필요한 컨텍스트를 제거한다.", "간결하고 핵심적인 프롬프트 구성은 비용 효율적입니다.", "비용 절감", "3059"),
    ("모델이 사용자의 위험한 질문(폭탄 제조 등)을 거부하도록 훈련된 것을 무엇이라 하는가?", ["Safety Alignment (안전 정렬)", "Hard Coding", "Blacklisting", "Firewalling", "Blocking"], "Safety Alignment (안전 정렬)", "RLHF 등을 통해 유해한 출력을 방지하도록 정교하게 조정됩니다.", "안전 정렬", "3060"),
    ("학습에 사용되지 않은 외부 문서를 가져와 답변에 참고하는 기술의 약자는?", ["Fine-tuning", "RAG", "GAN", "RNN", "API"], "RAG", "검색 증강 생성(Retrieval-Augmented Generation)의 약자입니다.", "RAG 약자", "3061"),
    ("모델 서빙 도구 중 'vLLM'이나 'TGI'가 주로 해결하는 문제는?", ["모델을 더 예쁘게 시각화하기 위해", "추론 속도와 처리량(Throughput)을 극대화하기 위해", "오타를 교정하기 위해", "코딩 교육을 하기 위해", "배터리를 절약하기 위해"], "추론 속도와 처리량(Throughput)을 극대화하기 위해", "고성능 추론 엔진을 통해 대규모 동시 접속을 효율적으로 처리합니다.", "서빙 엔진", "3062"),
    ("트랜스포머의 '레이어 정규화(Layer Norm)'가 수행되는 위치는?", ["학습이 다 끝난 후 파일 저장 시", "각 레이어의 연산 과정 중간중간", "사용자가 질문을 날릴 때 딱 한 번", "데이터를 웹에서 가져올 때", "컴퓨터 부팅 시"], "각 레이어의 연산 과정 중간중간", "수치 안정성을 유지하여 깊은 신경망의 학습을 가능하게 합니다.", "레이어 정규화", "3063"),
    ("딥러닝 학습 시 가중치를 업데이트하는 방향을 결정하는 핵심 알고리즘은?", ["Forward Propagation", "Backpropagation (역전파)", "Encryption", "Parsing", "Sorting"], "Backpropagation (역전파)", "오차를 뒤로 전달하며 파라미터를 수정해 나가는 기본 원리입니다.", "역전파", "3064"),
    ("LLM 학습을 위해 인터넷 상의 모든 텍스트를 긁어모으는 행위를 무엇이라 하는가?", ["Mining", "Scraping/Crawling", "Fishing", "Hunting", "Hoarding"], "Scraping/Crawling", "Web 데이터는 LLM 사전 학습의 가장 큰 재료입니다.", "데이터 수집", "3065"),
    ("거대 모델일수록 '환각' 현상이 완전히 사라진다는 주장은?", ["100% 사실이다.", "전혀 사실이 아니며 거대 모델도 환각을 일으킨다.", "이미 2023년에 해결된 문제이다.", "모델 크기와 환각은 상관이 없다.", "환각은 사람이 느끼는 착각일 뿐이다."], "전혀 사실이 아니며 거대 모델도 환각을 일으킨다.", "생성 모델의 본질적 특성상 환각은 완전히 제거하기 매우 어렵습니다.", "환각과 규모", "3066"),
    ("Anthropic의 Claude 모델이 강조하는 'Constitutional AI'의 핵심 요소는?", ["모델에게 수만 권의 법전을 외우게 한다.", "모델이 지켜야 할 원칙(헌법)을 주고 스스로를 정렬하게 한다.", "국가 헌법 기관에 모델을 설치한다.", "모델의 이름을 대통령 이름으로 짓는다.", "오직 법률 상담만 한다."], "모델이 지켜야 할 원칙(헌법)을 주고 스스로를 정렬하게 한다.", "인간의 지속적 피드백 대신 원칙 기반의 자동 정렬을 시도하는 기술입니다.", "Claude 특징", "3067"),
    ("파이썬의 'list'와 'numpy array'의 차이점에 대한 복습: NumPy가 데이터 분석에 유리한 이유는?", ["파이썬 리스트는 숫자를 저장할 수 없어서", "배열 전체에 대한 벡터화 연산이 가능하여 매우 빨라서", "NumPy가 더 최신 라이브러리라서", "NumPy 배열은 크기를 줄일 수 없어서", "NumPy는 유료이기 때문"], "배열 전체에 대한 벡터화 연산이 가능하여 매우 빨라서", "행렬 연산을 순식간에 처리하는 NumPy는 AI 연산의 기초입니다.", "NumPy 복습", "3068"),
    ("HuggingFace 모델 이름이 `meta-llama/Llama-3-8B`일 때 '8B'가 뜻하는 것은?", ["파일 용량이 8기가바이트이다.", "학습 기간이 8개월이다.", "매개변수(Parameter) 개수가 80억 개이다.", "동시 사용자 수가 8명이다.", "데이터 종류가 8가지이다."], "매개변수(Parameter) 개수가 80억 개이다.", "B는 Billion(10억)의 약자로, 모델의 지능 척도를 나타냅니다.", "8B 의미", "3069"),
    ("사용자가 '이 말을 비밀로 해줘'라고 했을 때 모델이 실제로 기억을 삭제하는가?", ["네, 즉시 서버에서 지웁니다.", "아뇨, 모델은 실시간으로 지식을 잊거나 배우는 능력이 기본적으로 없습니다.", "네, 다음 사용자는 그 비밀을 모릅니다.", "사용자가 돈을 내면 지워줍니다.", "모델이 '알겠습니다'라고 하면 진짜 지운 것입니다."], "아뇨, 모델은 실시간으로 지식을 잊거나 배우는 능력이 기본적으로 없습니다.", "모델은 학습된 시점에 고정되어 있으며 대화 내역은 일시적인 데이터일 뿐입니다.", "모델의 기억 실체", "3070"),
    ("GPT-4o가 소리를 실시간으로 듣고 반응할 때 사용하는 기술 흐름은?", ["소리를 텍스트로 바꾸고 답변을 다시 소리로 바꾼다.", "중간 변환 없이 소리 데이터를 직접 처리하는 단일 신경망 모델이다.", "사람이 뒤에서 몰래 타이핑해준다.", "오디오를 이미지로 찍어서 판독한다.", "소리 주파수를 수학적으로 계산만 한다."], "중간 변환 없이 소리 데이터를 직접 처리하는 단일 신경망 모델이다.", "Native Multimodal로, 지연 시간을 최소화하고 감정까지 읽을 수 있습니다.", "오디오 처리", "3071"),
    ("LLM이 특정 전문 분야(의료, 금융 등)의 용어를 더 잘 쓰게 하려면 추천되는 방식은?", ["모델에게 응원 메시지를 보낸다.", "해당 분야 데이터로 파인튜닝(Fine-tuning)을 수행한다.", "질문할 때 '의사라고 생각하고 답해'라고 한 번 말하고 끝낸다.", "컴퓨터를 해당 병원에 비치한다.", "인터넷 게시판에 질문을 올린다."], "해당 분야 데이터로 파인튜닝(Fine-tuning)을 수행한다.", "특화된 데이터셋 학습을 통해 도메인 전문가로 만들 수 있습니다.", "전문성 향상", "3072"),
    ("프롬프트 엔지니어링 팁 중 '구분자(Delimiter)'를 사용하라는 말의 의미는?", ["입력 데이터와 지시문을 ### 등 특수문자로 나누어 모델의 혼란을 방지한다.", "단어마다 띄어쓰기를 3번씩 한다.", "영어와 한글을 절대 섞어 쓰지 않는다.", "질문을 여러 개의 파일로 쪼개어 보낸다.", "정답을 미리 알려주고 모른 척한다."], "입력 데이터와 지시문을 ### 등 특수문자로 나누어 모델의 혼란을 방지한다.", "구조화된 입력은 모델이 작업 범위를 정확히 파악하게 돕습니다.", "구분자 활용", "3073"),
    ("모델 서빙 중 'FP16'에서 'INT8'로 양자화하면 줄어드는 비용은?", ["전기세", "메모리 점유량과 연산 속도", "인터넷 통신료", "사무실 월세", "라이브러리 사용료"], "메모리 점유량과 연산 속도", "수치의 정밀도를 낮추어 성능 하락을 최소화하면서 자원을 아낍니다.", "양자화 효과", "3074"),
    ("다음 중 LLM을 활용한 서비스 개발 시 '할루시네이션(환각)'을 줄이는 가장 실질적인 방법은?", ["모델에게 '거짓말하지 마'라고 계속 입력한다.", "RAG 시스템을 도입하여 근거 문서를 기반으로 답하게 한다.", "온도(Temperature)를 2.0으로 높인다.", "답변의 길이를 최대(Max Tokens)로 설정한다.", "모든 질문을 영어로 번역해서 시킨다."], "RAG 시스템을 도입하여 근거 문서를 기반으로 답하게 한다.", "검색된 사실 정보를 프롬프트에 제공하는 것이 환각 방지의 표준입니다.", "환각 방지 실무", "3075"),
    ("Transformer 블록 내에서 텍스트 데이터가 흐르는 순서는?", ["Embedding -> Attention -> FeedForward", "FeedForward -> Attention -> Embedding", "Attention -> Embedding -> Output", "Output -> Attention -> Embedding", "수시로 바뀐다"], "Embedding -> Attention -> FeedForward", "입력이 수치화된 후 관계를 파악하고 고차원 특징을 추출하는 순서입니다.", "데이터 흐름", "3076"),
    ("LLM이 답변을 생성하다가 갑자기 멈춘 경우, 다시 이어 쓰게 하려면 보통 어떤 명령을 내리는가?", ["처음부터 다시 해", "계속해서(Continue) 설명해줘", "왜 멈췄어?", "돈 줄게", "키보드 엔터 키를 누른다"], "계속해서(Continue) 설명해줘", "모델에게 이전 문맥의 마지막을 보여주며 이어서 생성하도록 유도합니다.", "생성 중단 대처", "3077"),
    ("트랜스포머 아키텍처에서 '병렬성'을 저해하는 요소가 거의 없는 이유는?", ["CPU를 안 쓰기 때문", "단어 간의 순차적 상태 전달(Hidden State)이 없기 때문", "글자 수가 적어서", "프로그램이 간단해서", "구글이 만들었기 때문"], "단어 간의 순차적 상태 전달(Hidden State)이 없기 때문", "RNN과 달리 행렬 연산으로 앞뒤 결과를 한 번에 계산할 수 있는 구조입니다.", "병렬성 극대화", "3078"),
    ("거대 언어 모델이 추론 시 사용하는 GPU의 주요 자원은?", ["코어 클럭 속도", "비디오 메모리 (VRAM)", "RGB 조명", "쿨링 팬 속도", "모니터 연결 단자"], "비디오 메모리 (VRAM)", "수천억 개의 파라미터(가중치)를 메모리에 상주시켜야 하므로 VRAM 용량이 핵심입니다.", "GPU 자원", "3079"),
    ("최근 LLM 동향 중 'Small Language Models (SLM)'이 주목받는 이유는?", ["큰 모델보다 항상 똑똑해서", "특정 도메인에서 저비용/고효율로 동작 가능해서", "이름이 귀여워서", "무료로만 배포되기 때문", "업데이트가 안 되기 때문"], "특정 도메인에서 저비용/고효율로 동작 가능해서", "특정 작업에 최적화된 작은 모델은 실무 적용 시 가성비가 매우 높습니다.", "SLM 주목 이유", "3080"),
    ("`tokenizer.decode([10, 25, 40])`를 실행한 결과물은?", ["숫자 리스트 [10, 25, 40]", "해당 숫자들에 매칭되는 '문자열'", "에러 메시지", "이미지 파일", "오디오 파일"], "해당 숫자들에 매칭되는 '문자열'", "ID 숫자를 다시 사람이 읽을 수 있는 글자로 변환하는 과정입니다.", "디코딩", "3081"),
    ("트랜스포머의 인코더가 출력하는 정보의 형태는?", ["정답 문장 하나", "각 단어의 의미가 담긴 벡터 리스트 (Contextual Embeddings)", "예/아니오 결과", "랜덤한 숫자", "파이썬 코드"], "각 단어의 의미가 담긴 벡터 리스트 (Contextual Embeddings)", "문맥 정보를 가득 담은 임베딩 값을 다음 층이나 디코더로 넘깁니다.", "인코더 출력", "3082"),
    ("GPT의 'Attention Mask' 설정값이 0인 부분의 의미는?", ["모델이 이 부분에 집중해야 함", "모델이 이 부분을 무시(Ignore)해야 함", "오타가 있는 부분임", "정답이 숨겨진 부분임", "가장 중요한 단어임"], "모델이 이 부분을 무시(Ignore)해야 함", "미래의 단어나 불필요한 패딩(Padding) 부분을 보지 못하게 가리는 용도입니다.", "어텐션 마스크", "3083"),
    ("LLM 학습 데이터 전처리 시 중복 제거(Deduplication)를 하는 주된 목적은?", ["데이터 양을 억지로 부풀리기 위해", "모델이 특정 문장을 암기(Memorization)하는 것을 방지하기 위해", "데이터를 모두 지우기 위해", "저작권을 속이기 위해", "파일 개수를 맞추려고"], "모델이 특정 문장을 암기(Memorization)하는 것을 방지하기 위해", "중복이 많으면 모델이 편향되거나 단순 암기를 하게 되어 범용성이 떨어집니다.", "중복 제거 목적", "3084"),
    ("모델 평가 지표 중 'MMLU'는 무엇을 측정하는가?", ["모델의 생성 속도", "다양한 학문 분야에 대한 일반 지식과 문제 풀이 능력", "이미지 생성 퀄리티", "네트워크 지연 시간", "한국어 맞춤법"], "다양한 학문 분야에 대한 일반 지식과 문제 풀이 능력", "대학 수준의 지식을 얼마나 잘 알고 있는지 평가하는 대표 벤치마크입니다.", "MMLU", "3085"),
    ("딥러닝 학습 시 'Overfitting(과적합)'이 발생했다는 것은?", ["학습 데이터는 잘 맞추지만 새로운 데이터에는 멍청해진 상태", "너무 똑똑해져서 사람을 무시하는 상태", "데이터가 너무 적어서 학습이 안 된 상태", "컴퓨터가 과열된 상태", "인터넷이 끊긴 상태"], "학습 데이터는 잘 맞추지만 새로운 데이터에는 멍청해진 상태", "학습 데이터에만 너무 맞춰져 범용적인 추론 능력이 상실된 경우입니다.", "과적합", "3086"),
    ("LLM 서비스 시 답변이 한 글자씩 나오는 'Streaming'의 장점은?", ["최종 답변이 더 정확해진다.", "사용자가 답변이 생성되는 과정을 체감하여 답답함을 줄여준다.", "토큰 비용이 저렴해진다.", "모델 개발이 더 쉬워진다.", "보안이 더 안전해진다."], "사용자가 답변이 생성되는 과정을 체감하여 답답함을 줄여준다.", "전체 생성을 기다리는 지루함을 없애 체감 속도(UX)를 높여줍니다.", "스트리밍 장점", "3087"),
    ("다음 중 '멀티모달' 기능과 가장 무관한 작업은?", ["이미지를 보고 텍스트로 설명하기", "음성 명령을 듣고 그림 그리기", "텍스트를 다른 나라 언어로 번역하기", "동영상을 보고 내용 요약하기", "표를 보고 엑셀로 변환하기 (시각 정보 포함)"], "텍스트를 다른 나라 언어로 번역하기", "단순 텍스트 번역은 단일 모달(Unimodal) 작업에 해당합니다.", "멀티모달 구분", "3088"),
    ("GPT-3와 같은 LLM은 기본적으로 ( ) 방식으로 다음 토큰을 맞히며 학습된다. 빈칸은?", ["지도 학습", "비지도 학습 (Self-supervised)", "강화 학습", "전이 학습", "사후 학습"], "비지도 학습 (Self-supervised)", "별도의 정답 라벨 없이 텍스트 자체에서 다음 단어를 맞히며 스스로 공부합니다.", "학습 방식", "3089"),
    ("거대 언어 모델이 인류의 안전과 이익에 부합하도록 만드는 최종 조율 단계는?", ["Pre-training", "Pre-processing", "Alignment (정렬)", "Parsing", "Compressing"], "Alignment (정렬)", "RLHF 등을 통해 인간의 의도와 윤리에 맞게 맞추는 핵심 단계입니다.", "정렬", "3090"),
    ("LLM이 특정 문법 형식을 지키도록(예: JSON) 시스템 프롬프트에 예시를 넣는 것을 무엇이라 하는가?", ["Strict Mode", "Output Constrainting", "Formatting Guide", "Constraint Prompting", "JSON Enforcement"], "Constraint Prompting", "모델의 자유도를 제한하여 구조화된 데이터를 얻는 기법입니다.", "형식 제약", "3091"),
    ("토크나이저 사전(Vocabulary)에 없는 단어가 들어오면 보통 어떤 토큰으로 처리되는가?", ["<END>", "<UNK> (Unknown)", "<START>", "<PAD>", "에러로 중단됨"], "<UNK> (Unknown)", "모르는 단어는 특수 토큰으로 처리하여 일단 진행합니다.", "UNK 토큰", "3092"),
    ("트랜스포머의 'FeedForward' 층이 수행하는 역할은?", ["단어 간의 관계를 찾는다.", "어텐션 결과를 바탕으로 고차원적인 비선형 특징을 추출한다.", "결과를 화면에 출력한다.", "데이터를 외부로 전송한다.", "가장 높은 확률값을 고른다."], "어텐션 결과를 바탕으로 고차원적인 비선형 특징을 추출한다.", "어텐션이 관계를 본다면, 피드포워드는 각 토큰의 의미를 더 깊게 가공합니다.", "피드포워드 역할", "3093"),
    ("학습 시 데이터셋을 작은 덩어리로 나누어 GPU에 올리는 단위를 무엇이라 하는가?", ["Chunk", "Batch", "Segment", "Particle", "Piece"], "Batch", "한 번의 가중치 업데이트를 위해 묶음으로 처리하는 단위입니다.", "배치", "3094"),
    ("LLM을 사용할 때 '할루시네이션'을 긍정적으로 활용할 수 있는 분야는?", ["의료 진단 보고서", "은행 대출 심사", "소설 창작 및 브레인스토밍", "법률 판례 분석", "정밀 부품 설계"], "소설 창작 및 브레인스토밍", "창의적 영역에서는 때로 사실이 아닌 기발한 상상이 도움이 됩니다.", "환각의 활용", "3095"),
    ("모델의 'Softmax' 함수 출력값의 모든 합은 항상 얼마인가?", ["0", "1", "100", "무한대", "데이터마다 다름"], "1", "출력값을 확률 분포로 바꾸어 전체 합이 1(100%)이 되도록 정규화합니다.", "Softmax 특성", "3096"),
    ("API 호출 시 'stop' 파라미터는 언제 사용하는가?", ["특정 글자가 나오면 생성을 강제로 멈추고 싶을 때", "매달 결제를 멈추고 싶을 때", "키보드 작성을 멈출 때", "인터넷을 끌 때", "모델을 삭제할 때"], "특정 글자가 나오면 생성을 강제로 멈추고 싶을 때", "불필요한 답변 생성을 막아 비용과 시간을 아끼는 용도입니다.", "Stop 옵션", "3097"),
    ("로컬에서 LLM을 돌릴 때 CPU보다 GPU가 권장되는 가장 큰 성능상의 이유는?", ["GPU가 전기료가 싸서", "수천 개의 행렬 연산을 동시에 처리하는 병렬성에 최적화되어 있어서", "GPU가 기억력이 더 좋아서", "CPU는 게임용이기 때문", "GPU가 더 예쁘게 생겨서"], "수천 개의 행렬 연산을 동시에 처리하는 병렬성에 최적화되어 있어서", "행렬 곱셈이 99%인 딥러닝 연산은 GPU의 병렬 구조에서 압도적으로 처리됩니다.", "GPU 필요성", "3098"),
    ("오픈소스 모델 중 벤치마크 1위를 수차례 탈환한 프랑스 기반의 AI 팀 이름은?", ["OpenAI", "Anthropic", "Mistral AI", "DeepMind", "Meta"], "Mistral AI", "Mistral 7B, Mixtral 등 작지만 강력한 오픈 모델로 유명합니다.", "Mistral", "3099"),
    ("교재 3장의 내용을 바탕으로 할 때, 좋은 LLM 활용 능력을 갖추기 위해 가장 중요한 습득 사항은?", ["모델의 모든 수학적 수식을 외우는 것", "프롬프트 원리와 모델별 특징을 알고 적절히 도구화하는 것", "PC의 메모리를 1테라로 늘리는 것", "타이핑 속도를 높이는 것", "인터넷 유료 기사를 많이 읽는 것"], "프롬프트 원리와 모델별 특징을 알고 적절히 도구화하는 것", "기술적 배경을 이해하고 이를 실무에 녹여내는 능력이 인공지능 시대의 핵심 경쟁력입니다.", "학습의 목적", "3100")
]

for q, o, a, w, h, i in mcq_data:
    questions.append({"chapter_name": chapter_name, "type": "객관식", "difficulty": "medium", "id": i, "question": q, "options": o, "answer": a, "why": w, "hint": h})

# --- 20 Code Completion Questions ---
cc_data = [
    ("토큰화기 불러오기", "from transformers import Auto____ # 이 부분을 채우세요", "Tokenizer", "모델의 토큰화기를 불러오는 클래스입니다."),
    ("텍스트를 토큰으로", "input_ids = tokenizer.____(text) # 이 부분을 채우세요", "encode", "텍스트 원문을 숫자 ID 리스트로 변환합니다."),
    ("토큰을 텍스트로", "result = tokenizer.____(output_ids) # 이 부분을 채우세요", "decode", "숫자 리스트를 다시 읽을 수 있는 텍스트로 복원합니다."),
    ("주의 집중 기법", "Self-____: 트랜스포머의 핵심 연산. (이 부분을 채우세요)", "Attention", "문맥 내 가중치를 계산하는 매커니즘입니다."),
    ("GPT 구조", "GPT는 ____ 전용(Decoder-only) 모델입니다. (이 부분을 채우세요)", "디코더", "생성 작업에 특화된 트랜스포머의 한 축입니다."),
    ("BERT 구조", "BERT는 ____ 전용(Encoder-only) 모델입니다. (이 부분을 채우세요)", "인코더", "문맥 이해와 분류에 특화된 트랜스포머의 한 축입니다."),
    ("프롬프트 기법 (무시)", "____-shot: 프롬프트에 예시를 0개 넣음. (이 부분을 채우세요)", "Zero", "아무런 예시 없이 지시만 내리는 방식입니다."),
    ("프롬프트 기법 (전달)", "____-shot: 프롬프트에 예시를 몇 개 넣음. (이 부분을 채우세요)", "Few", "패턴을 익히도록 몇 개의 데이터를 주는 방식입니다."),
    ("단계별 사고", "Chain-of-____ (CoT): 단계별로 생각하라. (이 부분을 채우세요)", "Thought", "추론 성능을 높이는 유명한 프롬프트 기법입니다."),
    ("생성 제어 (온도)", "____: 0에 가까우면 보수적, 1에 가까우면 창의적 결과. (이 부분을 채우세요)", "Temperature", "생성 확률 분포의 무작위성을 제어합니다."),
    ("생성 제어 (길이)", "max_____: 답변의 최대 길이를 제한. (이 부분을 채우세요)", "tokens", "토큰 단위로 생성 한계를 정하는 파라미터입니다."),
    ("환각 현상", "____ (Hallucination): 거짓 정보를 사실처럼 말함. (이 부분을 채우세요)", "환각", "LLM이 가진 태생적이고 확률적인 오류 현상입니다."),
    ("문맥 한도", "____ Window: 한 번에 처리할 수 있는 정보량. (이 부분을 채우세요)", "Context", "모델의 '단기 기억' 범위에 해당합니다."),
    ("임베딩", "Word ____: 단어를 벡터 공간에 매핑함. (이 부분을 채우세요)", "Embedding", "의미적 유사성을 숫자로 표현하는 기술입니다."),
    ("오픈소스 모델", "____ (Meta가 공개한 유명 모델 시리즈 이름)", "Llama", "오픈 웨이트 LLM의 대중화를 이끈 모델입니다."),
    ("허깅페이스 도구", "from transformers import ____ # 가장 쉬운 실행 방법", "pipeline", "모델 태스크를 간단히 수행하는 인터페이스입니다."),
    ("벡터 저장소 연결", "Semantic Search는 문장 간의 ____ 유사도를 계산함. (이 부분을 채우세요)", "코사인", "방향 차이를 이용한 벡터 유사도 측정법입니다."),
    ("파라미터 크기", "7____ (7 Billion 파라미터 모델을 뜻함)", "B", "10억 단위의 규모를 나타내는 약호입니다."),
    ("딥러닝 프레임워크", "import ____ # 파이토치 라이브러리 (이 부분을 채우세요)", "torch", "트랜스포머 라이브러리의 기반이 되는 프레임워크입니다."),
    ("추론 단계", "____ (Inference): 학습된 모델로 답을 내는 단계. (이 부분을 채우세요)", "추론", "서비스 실제 사용 시점에 해당하는 용어입니다.")
]

for i, (title, code, ans, explain) in enumerate(cc_data):
    questions.append({
        "chapter_name": chapter_name, "type": "코드 완성형", "difficulty": "medium", "id": str(3101 + i),
        "question": f"{title} 개념을 완성하세요.\n```text\n{code}\n```",
        "answer": ans,
        "why": explain,
        "hint": title,
    })

def get_questions():
    return questions
