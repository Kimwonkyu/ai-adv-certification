# 📘 [학습 노트] 교재 3. LLM 기본 및 아키텍처 (Comprehensive Deep Dive)

이 교재는 Transformer의 수리적 기초부터 현대 LLM의 추론 최적화와 분산 학습 아키텍처까지 모든 최신 지식을 다룹니다.

---

## 1. Transformer: 병렬 연산의 혁명
Transformer는 기존 RNN의 순차적 연산 한계를 극복하고 모든 토큰을 동시에 처리하는 구조를 채택했습니다.

### 🧠 Self-Attention과 수리적 강점
- **Constant Path Length**: 모든 토큰 쌍 간의 거리가 항상 1로 고정되어 정보 유실 없이 병렬 연산이 가능합니다.
- **연산 복잡도**: $O(n^2 \cdot d)$ (n: 시퀀스 길이, d: 모델 차원). 시퀀스가 길어질수록 메모리 점유율이 제곱으로 늘어나는 결정적 단점이 있습니다.
- **Scaled Dot-Product Attention**: $\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$
    - $\sqrt{d_k}$로 나누는 이유는 내적 값이 커질 때 Softmax 결과가 0이나 1로 쏠려 그라디언트가 소실되는 것을 방지하기 위함입니다.

### 🧩 인과적 어텐션 (Causal Masking)
- **Decoder-only 모델**: 미래 토큰의 정보를 미리 보지 못하도록(Information Leakage 차단) 미래 시점의 가중치를 $-\inf$로 마스킹합니다. 이 자기회귀(Auto-regressive) 구조가 생성 작업의 핵심입니다.

---

## 2. 추론 최적화 (Inference Engineering)
LLM 서빙 레이턴시의 주범은 연산 속도보다 **메모리 대역폭(Memory Bandwidth)**입니다.

### ⚡ KV Caching (키-값 캐싱)
- **원리**: 이전 토큰들의 Key와 Value 행렬을 메모리에 저장하여 매 토큰 생성 시 중복 계산을 피합니다.
- **병목**: 거대한 KV 텐서를 매번 HBM(고대역폭 메모리)에서 읽어와야 하므로 **메모리 대역폭 병목**이 발생합니다.
- **해결책 (MQA/GQA)**:
    - **MQA (Multi-Query Attention)**: 모든 Query 헤드가 1개의 K, V 헤드를 공유. 메모리 사용량은 줄지만 성능 저하 위험.
    - **GQA (Grouped Query Attention)**: Query 헤드들을 그룹화하여 그룹당 1개의 K, V 헤드를 공유. Llama 3의 표준 기술입니다.

### 🚀 Flash Attention (HBM I/O 최적화)
- **핵심**: 어텐션 행렬 전체를 VRAM에 쓰지 않고, **Tiling(타일링)** 기법을 통해 SRAM(빠른 온칩 메모리) 내에서 부분적으로 Softmax를 계산하여 HBM 입출력 횟수를 획기적으로 줄입니다.
- **결과**: 메모리 요구량을 시퀀스 길이 $n$에 대해 선형적으로 줄여 긴 컨텍스트(Long Context) 지원을 가능하게 했습니다.

### 🐕 Speculative Decoding (추측적 디코딩)
- **원리**: 작고 빠른 **Draft Model**이 여러 토큰을 미리 생성하고, 거대한 **Target Model**이 이를 한 번의 Forward Pass로 병렬 검증(Parallel Validation)합니다. 검증 성능이 보장되면서도 생성 속도를 2~3배 높일 수 있습니다.

---

## 3. 대규모 학습과 분산 처리
단일 GPU 메모리를 초과하는 모델을 학습시키기 위한 기술입니다.

### 🛡 DeepSpeed ZeRO (Zero Redundancy Optimizer)
모델 상태를 GPU들 사이에 분할 저장하여 중복을 제거합니다.
- **Stage 1**: Optimizer States 분할.
- **Stage 2**: Gradients(기울기) 분가 분할.
- **Stage 3**: Parameters(가중치)까지 모두 분할. Stage 3를 쓰면 이론상 GPU 개수에 비례하여 모델 크기를 확장할 수 있습니다.

---

## 4. LLM의 특이 현상과 성능 지표
### 🌟 발현 능력 (Emergent Abilities)
- 특정 파라미터 규모(일반적으로 수백억 개)와 학습량(Compute) 임계점을 넘어서면, 이전 단계에서는 불가능하던 복합 추론 능력이 돌발적으로 나타나는 현상입니다.

### 📉 Lost in the Middle
- 컨텍스트 윈도우가 아무리 길어도, 모델은 입력의 시작과 끝부분 정보는 잘 활용하나 중간 부분의 정보는 제대로 인출하지 못하는 경향이 있습니다.

### 🏗 Mixture of Experts (MoE)
- **추론 효율**: 전체 모델이 47B 파라미터라도 추론 시에는 각 토큰별로 2개 정도의 전문가(Active Parameters, 예: 13B)만 활성화하여 연산 비용(FLOPs)을 극단적으로 아낍니다.

---

## 5. 토큰화와 정렬 (Alignment)
### 🔠 토큰화 기술
- **Subword Tokenization (BPE)**: 희귀 단어를 의미 있는 조각으로 쪼개어 OOV(Out-of-Vocabulary) 문제를 해결하고 어휘 사전 효율성을 높입니다.

### ⚖️ 최신 정렬 알고리즘 (Alignment)
단순 SFT를 넘어 인간의 선호를 반영하는 과정입니다.
- **DPO (Direct Preference Optimization)**: 별도의 보상 모델 없이 수학적으로 선호 데이터를 직접 최적화.
- **GRPO (Group Relative Policy Optimization)**: 답변 묶음 내에서 상대적 우위를 매겨 강화학습을 수행. 추론 모델(DeepSeek R1 등)에서 성능 돌파구를 마련한 핵심 기술입니다.