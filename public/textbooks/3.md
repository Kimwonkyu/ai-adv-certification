# 📘 [대백과] 교재 3. LLM 기본 및 아키텍처 (The LLM Encyclopedia)

이 교재는 트랜스포머의 수리적 원리부터 최신 추론 최적화 기법, 그리고 HuggingFace 라이브러리를 활용한 실무 구현까지 **120개 문항**의 정답 근거를 완벽히 담은 기술 백서입니다.

---

## 1. Transformer: 수리적 이해 (Mathematical Foundation)
### 🧠 Self-Attention 메커니즘
- **수식**: $\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$
    - **Scaling ($\sqrt{d_k}$)**: 차원($d_k$)이 커지면 내적 값이 폭발하여 Softmax 기울기가 0에 수렴하는 문제(Gradient Vanishing)를 방지합니다.
    - **유사도**: $QK^T$는 쿼리와 키 사이의 상관관계를 나타내며, 코사인 유사도와 유사한 개념입니다.
- **복잡도**: 입력 시퀀스 길이 $n$에 대해 $O(n^2)$의 메모리와 연산량이 필요합니다. 이는 긴 문맥 처리를 어렵게 만드는 주된 병목입니다.

### 🎭 아키텍처 유형 비교
- **Encoder-only (BERT)**: 양방향(Bidirectional) 컨텍스트를 봅니다. 분류(Classification), 개체명 인식(NER) 등 이해(Understanding) 태스크에 압도적입니다.
- **Decoder-only (GPT)**: 인과적 마스킹(Causal Masking)을 통해 미래 토큰을 보지 못하게 합니다. 텍스트 생성(Generation)에 특화됩니다.
    - **Masking**: 상삼각 행렬(Upper Triangular Matrix)을 $-\infty$로 채워 Softmax 확률을 0으로 만듭니다. (`torch.triu`)

---

## 2. 추론 최적화 (Inference Optimization)
### ⚡ KV Caching
- **원리**: 디코더는 이전 토큰들의 Key, Value 벡터를 매번 다시 계산할 필요가 없습니다. 이를 캐시(Cache)에 저장해두고 재사용합니다.
- **이점**: 연산량(FLOPs)을 줄이는 것보다, 메모리 대역폭(Memory Bandwidth) 낭비를 막아 토큰 생성 속도(Latency)를 획기적으로 개선합니다.

### 🚀 어텐션 최적화
- **MQA (Multi-Query Attention)**: 모든 헤드가 하나의 KV를 공유합니다. 메모리는 아끼지만 성능 저하 우려가 있습니다.
- **GQA (Grouped-Query Attention)**: 헤드를 몇 개의 그룹으로 묶어 KV를 공유합니다. 성능과 효율의 균형점(Llama 3 등 채택).
- **Flash Attention**: GPU의 HBM(느린 메모리) 접근을 줄이고 SRAM(빠른 캐시)에서 타일링(Tiling) 연산을 수행하여 속도를 2~3배 높입니다.

### 📉 양자화 (Quantization)
- **개념**: FP32(32비트) 가중치를 INT8(8비트)이나 FP4(4비트)로 줄입니다.
- **부작용**: 표현 범위가 줄어들어 정확도(Accuracy)가 필연적으로 하락합니다.
- **BitsAndBytesConfig**: `load_in_4bit=True` 같은 설정을 통해 VRAM 용량을 1/4 수준으로 절약합니다.

---

## 3. 학습과 분산 처리 (Training & Distributed)
### 🛡 DeepSpeed ZeRO (Zero Redundancy Optimizer)
- 데이터 병렬(Data Parallel) 학습 시 각 GPU가 모델 전체를 복사해서 갖는 메모리 낭비를 해결합니다.
    - **Stage 1**: Optimizer States 분할.
    - **Stage 2**: Gradients 분할.
    - **Stage 3**: Parameters(가중치) 분할.

### 📉 학습 데이터 이슈
- **Data Contamination (데이터 오염)**: 평가용 데이터(Test Set)가 학습 데이터에 섞여 들어가면, 모델이 문제를 '이해'하는 게 아니라 '기억'해서 맞추게 됩니다. 성능 평가의 신뢰도를 파괴합니다.
- **Scale Laws**: 모델 크기와 데이터 양이 늘어날수록 성능이 거듭제곱 법칙(Power Law)을 따르며 예측 가능하게 향상됩니다.
- **Emergent Abilities (창발성)**: 특정 임계치(Scale)를 넘으면 갑자기 고등 추론 능력이 생겨나는 현상입니다.

---

## 4. 생성 제어와 샘플링 (Generation Control)
### 🎛 파라미터 튜닝
- **Temperature ($T$)**: `logits / T`. $T < 1$이면 확률 분포가 뾰족해져(Sharpening) 보수적이고 정확한 답변을, $T > 1$이면 평평해져(Flattening) 창의적인 아무말을 합니다.
- **Top-k & Top-p (Nucleus)**:
    - `Top-k`: 확률 상위 $k$개만 후보로 둡니다.
    - `Top-p`: 누적 확률이 $p$(예: 0.9)가 될 때까지의 후보군만 남깁니다. 꼬리(Tail) 부분의 이상한 토큰을 자르는 데 효과적입니다.
- **Beam Search**: 매 스텝마다 가장 확률 높은 $N$개의 경로를 유지하며 탐색합니다. `num_beams`로 경로 수를 조절합니다.

### 🧪 추론 기법
- **CoT (Chain-of-Thought)**: "단계별로 생각"하게 함으로써 추론 성능 향상. 심화 버전으로 **Tree of Thoughts**가 있습니다.
- **Reflection / CoVe (Chain-of-Verification)**: 모델이 만든 답을 스스로 검증하고 수정하는 메타인지 과정입니다. 할루시네이션을 줄입니다.

---

## 5. 실무 코딩 팁 (Coding with HuggingFace)
### 🛠 Transformers API
- **Tokenizer**:
    - `AutoTokenizer.from_pretrained("bert-base")`: 로드.
    - `pad_token_id`: 길이 맞춤용 패딩 토큰 ID 확인.
    - `apply_chat_template()`: 대화형 리스트(`[{"role":...}]`)를 모델 포맷 문자열로 변환.
- **Model**:
    - `.to("cuda")`: GPU로 모델 이동.
    - `generate(..., do_sample=True)`: 샘플링 활성화.
- **PEFT / LoRA**:
    - `LoraConfig(r=8, target_modules=["q_proj"])`: 타겟 모듈과 랭크 설정.

### 📐 수리적 지표 구현
- **Cosine Similarity**: `F.cosine_similarity(v1, v2)`. 벡터 간 각도를 기반으로 유사도 측정.
- **Perplexity (PPL)**: `exp(CrossEntropyLoss)`. 언어 모델의 불확실성을 나타내며 낮을수록 좋습니다.
- **KL Divergence**: `F.kl_div`. 두 확률 분포의 차이를 계산하며 RLHF 손실 함수에 쓰입니다.

이 교재는 LLM 기초 단원의 120개 문항을 완벽히 커버합니다.