# 📘 [학습 노트] 교재 3. LLM 기초 및 아키텍처 (Foundation to Advanced)

이 교재는 Transformer의 기본 구조부터 현대 대규모 언어 모델의 추론 최적화 및 분산 학습 기술까지 모두 포괄합니다.

---

## 1. Transformer: 기초와 수리적 배경
### 🧠 Self-Attention 메커니즘
- **Q, K, V**: Query(질문), Key(대상), Value(값) 행렬을 통해 토큰 간 관계를 계산합니다.
- **Scaling**: 내적 값이 커질 때 그래디언트 소실을 방지하기 위해 $\sqrt{d_k}$로 나눕니다.
- **병렬 연산**: RNN과 달리 모든 토큰을 동시에 처리하여 학습 속도를 비약적으로 향상시켰습니다.

### 🧩 모델 아키텍처 분류
- **Encoder-only (BERT)**: 양방향 문맥 이해. 분류에 적합.
- **Decoder-only (GPT)**: 자기회귀(Auto-regressive) 방식. 생성에 최적화. (Causal Masking 사용)
- **Encoder-Decoder (T5)**: 번역 및 요약 등에 활용.

---

## 2. 추론 최적화 (Inference Engineering)
### ⚡ 메모리 대역폭과 캐싱
- **KV Caching**: 이전 토큰의 K, V 값을 재사용하여 레이턴시 단축.
- **MQA / GQA**: KV 헤드를 공유하여 메모리 사용량을 줄이고 대역폭 병목을 해결합니다. (Llama 3는 GQA 채택)

### 🚀 하드웨어 가속
- **Flash Attention**: SRAM과 HBM 간의 데이터 입출력(I/O)을 최적화하여 긴 컨텍스트 지원을 가능하게 함.
- **Speculative Decoding**: 작은 모델(Draft)이 먼저 쓰고 큰 모델(Target)이 한꺼번에 검증하여 속도 향상.

---

## 3. 대규모 학습과 분산 처리
- **DeepSpeed ZeRO**: 가중치, 그래디언트, 옵티마이저 상태를 GPU 간에 분할 저장하여 중복을 제거함.
- **MoE (Mixture of Experts)**: 추론 시 일부 파라미터(Active Experts)만 활성화하여 연산량 절감.

---

## 4. 언어 모델의 훈련과 발현
### 🔠 토큰화 (Tokenization)
- **BPE (Subword)**: 단어를 의미 있는 조각으로 쪼개어 OOV(사전 외 단어) 문제 해결.

### ⚖️ 정렬 (Alignment) 및 발현
- **Emergent Abilities**: 특정 스케일 이상에서 돌발적으로 나타나는 추론 능력.
- **DPO / GRPO**: 인간의 선호를 학습시키는 최신 알고리즘. (GRPO는 DeepSeek R1의 핵심)
- **Lost in the Middle**: 긴 입력의 중간 정보를 잘 활용하지 못하는 현상.

이상의 내용은 LLM 기초 단원의 720개 문항에 대한 완벽한 정답 근거를 제공합니다.