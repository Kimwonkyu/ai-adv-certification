# 📘 [학습 노트] 교재 3. LLM 기초 및 아키텍처

## 1. Transformer: 모든 LLM의 근간
2017년 "Attention Is All You Need"에서 제안된 Transformer는 병렬 환경에 최적화된 Self-Attention 구조로 자연어 처리의 판도를 바꿨습니다.

### 🧠 Self-Attention의 수리적 원리
모델은 각 단어 간의 상관관계를 **Query(Q), Key(K), Value(V)** 세 행렬로 계산합니다.
1.  **Score**: $Q$와 $K$의 내적(Dot Product)을 통해 단어 간 유사도를 구합니다.
2.  **Scaling**: 연산 결과가 너무 커져 그라디언트 소실이 발생하는 것을 막기 위해 $\sqrt{d_k}$로 나눕니다.
3.  **Softmax**: 합이 1이 되도록 확률값으로 변환하여 어느 글자에 집중(Attention)할지 결정합니다.
4.  **Weighted Sum**: 위 확률값을 $V$에 곱하여 최종적인 맥락 벡터를 생성합니다.

### 🧩 아키텍처의 분류
- **Encoder-only (BERT)**: 문맥을 양방향으로 이해. 분류 및 의미 추출에 적합.
- **Decoder-only (GPT)**: 이전 토큰만 보고 다음 토큰을 생성. 생성형 AI의 표준.
- **Encoder-Decoder (T5)**: 입력과 출력을 분리하여 번역이나 요약에 활용.

---

## 2. 현대 LLM의 최신 기술 트렌드
성능을 높이면서 메모리 사용량(VRAM)을 줄이기 위한 혁신적인 기술들입니다.

### ⚡ 시퀀스 길이와 연산 효율
- **KV Cache**: 추론 시 이미 계산한 Key와 Value 값을 메모리에 저장해두어 중복 계산을 방지합니다. 이때 메모리 병목이 발생하는데, 이를 해결하기 위해 다음 기술들이 쓰입니다.
    - **MQA (Multi-Query Attention)**: 여러 Query가 하나의 K, V를 공유. 메모리는 아끼지만 성능 손실 가능성.
    - **GQA (Grouped Query Attention)**: Query들을 그룹으로 묶어 K, V를 공유. Llama 3 등 최신 모델의 표준입니다.

### 📐 위치 정보 인코딩 (Positional Embedding)
- **RoPE (Rotary Positional Embedding)**: 단어의 위치 정보를 회전 행렬을 통해 주입합니다. 상대적인 거리 정보를 잘 보존하여 긴 문맥 처리에 유리합니다 (Llama, Mistral 필수 적용).

### 🧪 최신 아키텍처 구성 요소
- **RMSNorm**: 기존 LayerNorm에서 평균 계산을 제외하고 제곱평균제곱근만 사용하여 속도를 높입니다.
- **SwiGLU**: ReLU보다 성능이 우수한 최신 활성화 함수입니다.

---

## 3. 모델 스케일링 (MoE: Mixture of Experts)
모델이 커져도 모든 파라미터를 다 쓰지 않는 효율적 구조입니다.
- **Gating Network**: 입력 토큰이 들어오면 수많은 전문 모델(Experts) 중 가장 적합한 **Top-K개**만 선택하여 연산합니다.
- **효과**: 전체 파라미터(예: 47B)는 많아도 실제 연산량(예: 13B)은 적어 추론 속도가 매우 빠릅니다.

---

## 4. 토큰화 (Tokenization)
- **BPE (Byte Pair Encoding)**: 문자 단위에서 시작해 가장 빈번한 쌍을 합쳐나가는 알고리즘입니다.
- **서브워드(Subword)**: '언어'를 의미 있는 부분 단위로 쪼개어, 처음 보는 단어(OOV)도 대응 가능하게 합니다.

---

## 5. 학습과 정렬 (Alignment Strategy)
단순한 다음 단어 예측기에서 지능적인 '비서'로 진화하는 과정입니다.

### 🎯 학습 단계
1.  **Pre-training**: 수조 개의 토큰을 읽어 언어의 '통계적 규칙'을 배웁니다. (Base Model 탄생)
2.  **SFT (Supervised Fine-tuning)**: 지시-답변 쌍을 통해 인간의 명령어에 대답하는 법을 배웁니다.
3.  **Alignment**: AI가 인간의 가치관과 선호에 맞게 답변하도록 조정합니다.

### ⚖️ 최신 정렬 알고리즘
- **RLHF (Reinforcement Learning from Human Feedback)**: 인간의 선호를 학습한 보상 모델(Reward Model)과 PPO 알고리즘을 사용합니다.
- **DPO (Direct Preference Optimization)**: 보상 모델 프로세스를 생략하고 선호 데이터셋을 직접 학습하여 연산 효율과 안정성을 극대화합니다.
- **GRPO (Group Relative Policy Optimization)**: 최신 추론 모델(DeepSeek R1 등)의 핵심. 답변 여러 개를 뽑아 그룹 내 등수(반영)를 매겨 강화학습을 수행합니다. 별도의 보상 모델 없이도 추론 능력이 급상승합니다.

---

## 6. 답변 생성 (Decoding Strategies)
- **Greedy Search**: 확률이 가장 높은 토큰만 선택. 단조로울 수 있음.
- **Beam Search**: 여러 경로를 유지하며 최적의 조합 탐색.
- **Nucleus Sampling (Top-p)**: 누적 확률이 p(예: 0.9)가 될 때까지만 후보를 추려 선택.
- **Temperature**: 높은 값일수록 창의적이고 무작위적인 답변을, 낮은 값일수록 결정론적이고 일관된 답변을 생성합니다.
- **Hallucination**: 학습 데이터에 없는 내용을 사실처럼 말하는 현상. 지식 컷오프나 아첨(Sycophancy) 등이 원인입니다.