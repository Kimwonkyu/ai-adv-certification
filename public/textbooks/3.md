# 📘 [학습 노트] 교재 3. LLM 기본 및 아키텍처 (Foundation to Ultimate Deep Dive)

이 교재는 Transformer의 기본 구조부터 현대 초대형 모델의 추론 최적화와 분산 학습 기술까지 720개 질문의 모든 핵심 개념을 깊이 있게 다룹니다.

---

## 1. Transformer: 기초와 수리적 배경
### 🧠 Self-Attention 메커니즘
- **Q(Query), K(Key), V(Value)**: "무엇을 찾고 싶은가(Q)", "대상들은 어떤 특징을 가졌는가(K)", "실제 전달할 정보는 무엇인가(V)"의 내적 연산입니다.
- **수식**: $\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$
    - $\sqrt{d_k}$ Scaling: 입력 차원이 커질 때 내적값이 비대해져 그래디언트 소실이 발생하는 것을 방지합니다.
- **연산 복잡도**: 시퀀스 길이 $n$에 대해 $O(n^2)$의 복잡도를 가지므로, 문장이 길어질수록 하드웨어 부담이 기하급수적으로 늘어납니다.

### 🧩 아키텍처 유형
- **Encoder-only (BERT)**: 양방향 어텐션을 통해 전체 문맥을 이해합니다.
- **Decoder-only (GPT)**: 이전 토큰만 참조하는 인과적 어텐션(**Causal Masking**)을 사용하여 미래 정보 유출을 차단하고 다음 단어를 생성합니다.

---

## 2. 추론 최적화 (High-Performance Serving)
생성형 모델의 속도는 연산 능력 자체보다 **메모리 대역폭(Memory Bandwidth)**에 의해 결정됩니다.

### ⚡ KV Caching과 메모리 병목
- **KV Cacher**: 자기회귀(Auto-regressive) 생성 시 이미 계산한 K, V 값을 저장하여 중복 연산을 방지합니다.
- **MQA vs GQA**: 
    - **MQA (Multi-Query)**: 모든 Query 헤드가 1개의 K, V 헤드를 공유. 메모리는 획기적으로 아끼지만 정확도가 다소 하락할 수 있습니다.
    - **GQA (Grouped-Query)**: 여러 Query를 그룹으로 묶어 K, V 헤드를 공유. Llama 3의 표준으로 성능과 효율의 균형을 유지합니다.

### 🚀 Flash Attention (HBM I/O 최적화)
- **핵심**: 어텐션 행렬 전체를 VRAM에 쓰지 않고, **Tiling(타일링)** 기법을 사용하여 SRAM(빠른 내부 메모리)에서만 연산한 뒤 최종 결과만 출력합니다. I/O 오버헤드를 줄여 긴 컨텍스트 지원을 가능하게 했습니다.

### 🐕 Speculative Decoding
- 작고 빠른 모델(Draft)이 5~10개의 토큰을 '예측 생성'하고, 큰 모델(Target)이 한꺼번에 '검증'하여 전체 답변 속도를 2~3배 높이는 기술입니다.

---

## 3. 대규모 학습과 정렬 (Alignment)
### 🛡 분산 학습: DeepSpeed ZeRO
- **ZeRO-1**: Optimizer States를 분할 저장.
- **ZeRO-2**: Gradient까지 추가로 분할.
- **ZeRO-3**: 모델 파라미터(Weight)까지 모든 장치에 쪼개어 저장하여 단일 GPU 메모리 한계를 돌파합니다.

### ⚖️ 최신 정렬 알고리즘
- **DPO (Direct Preference Optimization)**: 보상 모델을 훈련시키는 번거로운 단계를 수리적으로 제거하고 선호 데이터를 직접 학습합니다.
- **GRPO (Group Relative Policy Optimization)**: DeepSeek R1의 조상 격인 기술로, 답변 묶음(Group) 내에서 상대적 우열을 가려 추론 능력을 극대화합니다.

---

## 4. 토큰화와 언어 모델의 특성
- **BPE (Subword Tokenization)**: 빈번하게 등장하는 글자 쌍을 합쳐 어휘 사전을 구성하여 OOV(Out-of-Vocabulary) 문제를 해결합니다.
- **Lost in the Middle**: 모델이 긴 입력의 처음과 끝은 잘 기억하나 중간 정보는 누락시키는 현상입니다.
- **MoE (Mixture of Experts)**: 거대 모델 내에 전문가(Experts)를 두고, 입력마다 일부만 활성화하여 연산량(FLOPs)을 절감합니다.

이상의 지식을 토대로 LLM 아키텍처 단원의 모든 문항을 완벽히 해결할 수 있습니다.