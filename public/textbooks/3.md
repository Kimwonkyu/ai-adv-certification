# 📘 [학습 노트] 교재 3. LLM 기초 및 아키텍처

## 1. Transformer: 현대 LLM의 심장
2017년 "Attention Is All You Need" 논문에서 제안된 **Transformer** 구조는 RNN의 순차적 처리 한계를 병렬 연산으로 극복하며 대규모 모델의 시대를 열었습니다.

### 🧠 Self-Attention 메커니즘
- 문장 안의 각 단어가 **"어떤 단어와 가장 밀접하게 연결되어 있는지"**를 수치화합니다.
- **Query (질문)**, **Key (참조)**, **Value (정보)** 세 행렬을 사용하여 연산됩니다.
- 모델은 특정 단어를 처리할 때 문맥상 중요한 다른 단어들에 더 높은 가치를 부여(Attention)하여 풍부한 맥락을 이해합니다.

### 📐 Encoder vs Decoder
- **Encoder (BERT 계열)**: 문장의 앞뒤 맥락을 모두 고려하는 **양방향(Bi-directional)** 구조. 의미 파악, 분류, 개체명 인식에 탁월합니다.
- **Decoder (GPT 계열)**: 앞서 나온 토큰들을 바탕으로 다음 토큰을 맞추는 **인과적(Causal)** 구조. 현재 대부분의 텍스트 생성형 AI의 기반입니다.

---

## 2. 최신 아키텍처 트렌드 (Efficiency)
모델이 커짐에 따라 연산 효율을 높이기 위한 최신 기술들이 비약적으로 발전하고 있습니다.

### 🧩 MoE (Mixture of Experts)
- **개념**: 하나의 큰 모델 대신, 여러 개의 작은 전문 모델(Experts)을 두고 필요한 부분만 활성화하여 계산합니다.
- **장점**: 전체 파라미터 수는 매우 크지만, 추론 시에는 소량의 파라미터만 사용하므로 속도가 빠르고 성능이 높습니다 (예: Mixtral, DeepSeek).

### ⚡ GQA (Grouped Query Attention)
- **개념**: Self-attention 연산 시 Key와 Value 행렬을 그룹화하여 공유합니다.
- **장점**: 성능 손실을 최소화하면서도 KV 캐시(메모리) 사용량을 획기적으로 줄여 긴 문맥(Context Length) 처리를 가능하게 합니다 (예: Llama 3).

---

## 3. 토큰화 (Tokenization)와 BPE
모델은 텍스트를 직접 읽지 않고 '토큰'이라는 수치 단위로 변환하여 처리합니다.

- **BPE (Byte Pair Encoding)**: 가장 자주 등장하는 문자쌍을 반복적으로 합쳐 서브워드(Subword) 단위로 쪼개는 방식입니다.
- **효과**: 신조어로 인한 OOV(Out of Vocabulary) 문제를 방지하고, 토큰 효율을 높여 처리 속도를 최적화합니다.

---

## 4. 모델의 분류와 정렬 (Alignment)
단순한 다음 단어 예측을 넘어, 인간의 지도를 따르도록 만드는 과정입니다.

### 📊 모델의 종류
1.  **Base Model**: 거대한 텍스트 데이터를 읽어 "다음 단어 예측" 능력만 가진 모델.
2.  **Instruct Model (SFT)**: QA 데이터셋으로 학습되어 질문에 대답할 수 있게 된 모델.
3.  **Reasoning Model (DeepSeek R1 등)**: 추론 과정을 스스로 생성하고 보정하도록 강화학습된 고지능 모델.

### ⚖️ 정렬 (Alignment) 기법
- **RLHF**: 인간의 선호를 학습한 보상 모델을 사용하여 모델을 강화학습시킵니다.
- **DPO**: 보상 모델 없이 직접 선호 데이터를 사용하여 학습하는 효율적인 기술입니다.
- **PPO vs GRPO**: 최신 추론 모델(DeepSeek 등)은 그룹 상대 평가 방식인 **GRPO**를 활용하여 고성능을 달성하고 있습니다.