# 📘 [학습 노트] 교재 2. 데이터 분석 (Numpy, Pandas, 텍스트 분석)

## 1. Numpy: 수치 연산과 고성능 배열 처리
Numpy(Numerical Python)는 다차원 배열 객체인 `ndarray`를 중심으로 수치 계산을 효율적으로 수행하는 라이브러리입니다.

### 📐 ndarray의 핵심 속성
- **shape**: 배열의 차원 크기 (예: (3, 4)는 3행 4열)
- **ndim**: 차원 수 (1차원, 2차원 등)
- **dtype**: 데이터 타입 (float64, int32 등). 한 배열 내의 모든 원소는 동일한 타입이어야 합니다.

### 🚀 배열 생성 및 변형
- **생성**: `np.arange()`, `np.linspace()`, `np.zeros()`, `np.ones()`, `np.random.randn()`
- **변형**: 
    - `reshape()`: 차원 변경 (데이터 개수가 맞아야 함)
    - `transpose()` / `.T`: 전치 행렬 생성
    - `flatten()` vs `ravel()`: 다차원을 1차원으로 만듦 (`flatten`은 복사본 생성, `ravel`은 뷰 제공)
- **결합**: `np.concatenate()`, `np.vstack()`, `np.hstack()`

### ⚡ 연산의 마법: 벡터화와 브로드캐스팅
- **벡터화(Vectorization)**: 반복문 없이 배열 전체에 연산을 적용하여 속도가 압도적입니다.
- **브로드캐스팅(Broadcasting)**: 모양이 다른 배열끼리 연산할 때, Numpy가 작은 배열을 큰 배열 모양으로 똑똑하게 확장하여 연산하는 규칙입니다.
    - 뒤에서부터 차원의 크기가 같거나, 둘 중 하나가 1이어야 가능합니다.

### 🔍 선형대수 및 통계
- `np.dot()`: 행렬 내적
- `np.linalg.norm()`: 벡터의 크기(L2 Norm 등) 계산
- `sum()`, `mean()`, `std()`, `var()`: 축(axis)을 지정하여 연산 가능 (axis=0: 행 방향, axis=1: 열 방향)

---

## 2. Pandas: 강력한 정형 데이터 분석
Pandas는 **Series**(1차원)와 **DataFrame**(2차원) 자료구조를 제공합니다.

### 📂 데이터 입출력 및 탐색
- `pd.read_csv()`, `pd.read_json()`, `pd.read_excel()`
- `df.info()`: 결측치 및 자료형 확인
- `df.describe()`: 주요 기술 통계량 (평균, 4분위수 등) 확인
- `df.value_counts()`: 특정 컬럼의 값 분포 확인

```python
import pandas as pd

# Multi-index 생성 예시
data = {
    ('서울', '2023'): [100, 200],
    ('서울', '2024'): [150, 250],
    ('부산', '2023'): [80, 180]
}
df = pd.DataFrame(data)
```

### 📊 계층적 인덱싱 (Multi-Index)
- **`loc`**: 레이블(Label) 기반 선택. `df.loc[행이름, 열이름]`
- **`iloc`**: 정수 위치(Integer) 기반 선택. `df.iloc[행번호, 열번호]`
- **Boolean Indexing**: `df[df['age'] > 20]` 처럼 조건으로 필터링.

```python
import pandas as pd

# Multi-index 생성 및 데이터 선택 예시
data = {
    ('서울', '2023'): [100, 200],
    ('서울', '2024'): [150, 250],
    ('부산', '2023'): [80, 180]
}
df = pd.DataFrame(data)
# 특정 레이블 선택: df.loc[0, ('서울', '2023')]
```

### 🎯 데이터 선택 (Selection)
- **결측치 처리**: `df.isna()`, `df.fillna(값)`, `df.dropna()`
- **변환**:
    - `df.apply()`: 행 또는 열 단위로 함수 적용
    - `df.map()`: Series의 원소별 매핑
    - `df.astype()`: 자료형 명시적 변환

### 🛠 데이터 가공 (Manipulation)
- **정렬**: `df.sort_values(by='col', ascending=False)`

### 📊 그룹화 및 병합
- **Groupby**: `df.groupby('A').agg({'B': 'sum', 'C': 'mean'})`
- **Merge (Join)**: `how` 파라미터가 핵심!
    - `left`: 왼쪽 기준
    - `inner`: 교집합 (기본값)
    - `outer`: 합집합
- **Concat**: 단순 물리적 결합. 축(axis) 방향 설정이 중요합니다.

---

## 3. 텍스트 데이터 전처리 (NLP 파이프라인)
자연어 처리 모델의 성능은 80% 이상 전처리에서 결정됩니다.

### 🧬 전처리 5단계
1.  **정제 (Cleaning)**: Regex(`re` 모듈)를 사용하여 노이즈(HTML 태그, 이특수문자) 제거
2.  **토큰화 (Tokenization)**: 문장을 단어, 형태소, 서브워드 단위로 자름
3.  **한국어 특수 전처리 (KoNLPy)**:
    - **Mecab**: 속도가 가장 빠름
    - **Okt (Twitter)**: SNS 등 비정형 텍스트에 강함
    - **Kkma**: 분석은 정확하나 매우 느림
4.  **불용어 제거**: "은/는/이/가" 등 의미 없는 형태소 제거
5.  **정규화**: "안녕하세용" → "안녕하세요" 등으로 변형어 축소

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

# 재귀적 청킹 설정
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n\n", "\n", " ", ""]
)
docs = splitter.split_text(large_document)
```

### 🔍 검색 고도화 기술 (Search Optimization)
컴퓨터는 텍스트를 숫자로 이해해야 합니다.
- **Bag of Words (BOW)**: 단어의 등장 빈도만 계산. 단어 순서 무시.
- **TF-IDF**: 
    - **TF(Term Frequency)**: 문서 내 자주 나오면 중요
    - **IDF(Inverse Document Frequency)**: 전체 문서에서 흔하면 중요도 낮춤
    - 특정 문서의 고유한 키워드를 찾을 때 탁월합니다.

### 📐 유사도 측정 방법론
- **코사인 유사도 (Cosine Similarity)**: 벡터의 **방향성(각도)**만 고려. 문서 길이에 상관없이 주제의 유사함을 측정할 때 표준입니다 (-1 ~ 1).
- **유클리드 거리 (Euclidean Distance)**: 벡터 간의 **직선 거리** 측정. 
- **L1/L2 정규화**: 연산 성능과 모델 안정성을 위해 벡터의 크기를 1로 맞추는 과정입니다.
- **N-gram**: 연속된 n개의 단어 묶음을 하나의 단위로 처리하여 문맥을 일부 보존합니다.