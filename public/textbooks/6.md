# 📘 [학습 노트] 교재 6. LLM Fine Tuning (파인튜닝)

## 1. 파인튜닝의 현대적 의미
과거의 파인튜닝은 '지식 주입'이 주 목적이었으나, 현대 LLM 체제에서는 **'지시 이행(Instruction Following)'**과 **'추론 능력(Reasoning)'**의 최적화로 중심이 이동하고 있습니다.

### 🧬 학습 단계 (Training Stack)
1.  **CPT (Continuous Pre-training)**: 특정 언어(예: 한국어)나 전문 도메인(의료, 금융) 데이터를 대량으로 기초 학습시킵니다.
2.  **SFT (Supervised Fine-tuning)**: "질문-답변" 쌍을 통해 모델이 대화하는 법을 배웁니다.
3.  **선호 최적화 (Alignment)**: 인간이 좋아하는 답변 방식을 학습합니다 (DPO, RLHF).
4.  **추론 강화 (Reasoning Reinforcement)**: 정답이 명확한 데이터(수학, 코드)에 대해 스스로 사고 과정을 검증하도록 훈련합니다.

---

## 2. 효율적 학습 기술 (PEFT Deep Dive)
거대 모델을 일반인이 학습할 수 있게 만든 핵심 기술들입니다.

### 📉 LoRA와 그 변형들 (The LoRA Family)
- **LoRA (Low-Rank Adaptation)**: 원본 가중치는 얼리고, 아주 작은 두 개의 행렬(A, B)만 학습시켜 메모리를 90% 이상 절약합니다.
- **QLoRA**: 모델을 4비트로 압축(Quantization)한 상태에서 LoRA를 적용합니다. 가정용 GPU에서도 70B급 모델을 돌릴 수 있게 합니다.
- **DoRA (Weight-Decomposed LoRA)**: 가중치를 **'크기(Magnitude)'**와 **'방향(Direction)'**으로 분해하여 학습합니다. LoRA보다 학습 속도가 안정적이고 성능이 우수합니다.
- **GaLore**: 가중치 행렬 전체가 아닌 **'그라디언트(Gradient)'**를 저차원으로 투영하여 학습합니다. 대형 모델의 쌩학습(Full Fine-tuning)과 유사한 효과를 내면서 메모리를 아낍니다.

---

## 3. 정렬(Alignment) 알고리즘의 진화
모델의 말투와 윤리, 지능을 다듬는 과정입니다.

- **DPO (Direct Preference Optimization)**: 보상 모델을 만드는 복잡한 과정 없이, "선택된 답변"과 "거부된 답변"을 직접 비교하여 학습하는 현재의 표준 방식입니다.
- **GRPO (Group Relative Policy Optimization)**: DeepSeek에서 제안한 혁신적인 기법입니다. 여러 답변을 생성하고 그룹 내에서 상대적으로 잘한 놈에게 보상을 줌으로써 연산 비용을 획기적으로 낮췄습니다. (보상 모델 필요 없음)
- **PPO**: 과거의 표준 강화학습 알고리즘이나, 구현이 복잡하고 불안정하여 최근 DPO/GRPO로 대체되는 추세입니다.

---

## 4. 파인튜닝의 리스크와 한계
- **Catastrophic Forgetting (파멸적 망각)**: 새로운 전문 지식을 배우면서 원래 가지고 있던 상식이나 언어 능력이 붕괴되는 현상입니다.
- **Overfitting (과적합)**: 학습 데이터의 말투나 형식을 그대로 베껴서 일반적인 상황에서 유연성이 떨어지게 됩니다.
- **Learns Less, Forgets Less**: PEFT 기법(LoRA 등)은 모델의 지능을 근본적으로 바꾸기보다는 '사용 설명서'를 덧붙이는 것에 가깝기에 망각이 적은 대신 지식 주입 효과도 제한적일 수 있습니다.

---

## 5. 학습 최적화 도구 및 지표
- **Unsloth**: LoRA 학습 속도를 2배 이상 높이고 VRAM 사용량을 절반으로 줄여주는 파이썬 라이브러리입니다.
- **Loss (손실값)**: 학습 진행도를 나타내며, 너무 낮으면 과적합(Overfitting), 너무 높으면 학습 부족을 의미합니다.
- **Perplexity (PPL)**: 모델이 예측하는 단어들의 불확실성을 나타내며, 낮을수록 언어 생성 능력이 전문적임을 뜻합니다.