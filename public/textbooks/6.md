# 📘 [대백과] 교재 6. LLM 파인튜닝 (The Fine-Tuning Encyclopedia)

이 교재는 대규모 모델의 커스텀 학습 파이프라인부터 최신 정렬 알고리즘, 그리고 하드웨어 효율화 기술까지 **120개 문항**의 모든 정답 근거를 제공하는 실무 엔지니어링 가이드입니다.

---

## 1. 파인튜닝의 계층 (The Tuning Stack)
### 🛣 학습 단계
1.  **Continuous Pre-training (CPT)**: 도메인 특화 말뭉치를 대량으로 학습시켜 모델의 '지식'을 확장합니다.
2.  **Supervised Fine-Tuning (SFT / Instruction Tuning)**: 질문(Q)에 대해 원하는 답변(A)을 하도록 '행동'을 교정합니다.
    - **Correction Tuning**: 사용자의 나쁜 질문과 교정된 질문 쌍을 학습시켜 모델이 스스로 교정 능력을 갖추게 합니다.
3.  **Alignment (RLHF / DPO)**: 인간의 선호도에 맞춰 모델의 말투와 안전성을 최종 조율합니다.

### 🚫 핵심 리스크
- **Catastrophic Forgetting (파멸적 망각)**: 새로운 지식을 배우느라 기존 상식을 잊습니다.
    - **Replay Buffer**: 학습 데이터에 기존 일반 상식 데이터를 10~20% 섞어서(Replay) 방지합니다.
- **Knowledge Conflict**: 모델이 아는 사실과 상충되는 데이터를 억지로 학습시키면 환각(Hallucination)이 심해집니다.
- **Over-defensiveness**: RLHF가 과하면 모델이 "죄송합니다, 답변할 수 없습니다"만 반복하는 '회피형'이 됩니다.

---

## 2. 효율적 학습 기법 (PEFT & Quantization)
### 📉 LoRA (Low-Rank Adaptation)
- **원리**: $W = W_0 + AB$. 거대한 원본($W_0$)은 얼리고, 작은 행렬 $A, B$만 학습합니다. 역전파 시 $W_0$에 대한 미분값을 계산할 필요가 없어 메모리를 아낍니다.
- **Scaling Factor**: $\Delta W = \frac{\alpha}{r} AB$.
    - **Rank ($r$)**: 행렬의 크기. 높을수록 표현력이 좋지만 메모리를 씁니다.
    - **Alpha ($\alpha$)**: 학습률과 유사한 스케일링 계수입니다. 보통 $r$과 같거나 2배로 설정합니다.
    - **Target Modules**: `q_proj`, `v_proj` 등 어텐션 모듈을 타겟으로 잡습니다.

### 💎 양자화 (Quantization)
- **QLoRA**: 4비트 양자화된 베이스 모델 위에 LoRA를 얹습니다.
- **Data Types**:
    - **NF4 (NormalFloat 4)**: 가중치 분포에 최적화된 데이터 타입으로, 일반 FP4보다 정보 손실이 적습니다.
- **Double Quantization**: 양자화 스케일 상수(Quantization Constant)조차도 한 번 더 양자화하여 극단적으로 메모리를 아낍니다.

---

## 3. 고급 학습 전략 (Advanced Strategy)
### 🚀 최적화 기법
- **Gradient Accumulation**: GPU 메모리가 부족해 배치 크기(Batch Size)를 작게 잡아야 할 때, 여러 스텝 동안 기울기를 모았다가(Accumulate) 한 번에 업데이트하여 큰 배치를 시뮬레이션합니다.
- **NEFTune**: 임베딩 벡터에 랜덤 노이즈(Noise)를 섞어 학습합니다. 모델이 노이즈에 강해지면서 일반화 성능과 대화 품질이 비약적으로 향상됩니다.
- **Gradient Checkpointing**: 중간 연산 결과(Activation)를 저장하지 않고 필요할 때 다시 계산하여 메모리를 아낍니다. (속도 ↔ 메모리 교환)

### 🧩 모델 병합 (Merging)
학습된 LoRA 어댑터를 합치거나, 다른 모델들을 섞습니다.
- **SLERP (Spherical Linear Interpolation)**: 두 모델의 가중치를 단순 평균(Linear)내지 않고, 구면(Spherical)을 따라 보간합니다. 벡터의 방향성을 보존하여 성능 저하를 막습니다.
- **`merge_and_unload()`**: LoRA 가중치를 베이스 모델에 완전히 흡수시켜 단일 모델 파일로 만듭니다. 추론 속도가 빨라집니다.

---

## 4. 정렬 알고리즘 (Alignment Algorithms)
### ⚖️ RLHF vs DPO
- **RLHF**: 보상 모델(Reward Model) 학습 → PPO 강화학습. 복잡하고 불안정합니다. 학습 데이터는 (Prompt, Chosen, Rejected) 형태입니다.
    - **KL Divergence**: 모델이 원래의 언어 능력에서 너무 멀어지지 않도록(분포 유지) 제약하는 페널티 항입니다.
- **DPO (Direct Preference Optimization)**: 보상 모델 없이 선호 데이터를 직접 최적화합니다. Reference Model(원본)이 필요합니다.

---

## 5. 실무 구현 (Coding with TRL)
### 🛠 라이브러리 활용
- **`SFTTrainer`**: Supervised Fine-Tuning을 위한 통합 클래스.
- **`DataCollatorForCompletionOnlyLM`**: 프롬프트(질문) 부분은 마스킹하여 Loss 계산에서 제외하고, 오직 답변(Completion) 품질만 학습시킵니다.
- **`apply_chat_template(..., add_generation_prompt=True)`**: 대화 리스트 끝에 `<|assistant|>` 토큰을 붙여 모델이 바로 답변을 시작하도록 유도합니다.
- **`.safetensors`**: 기존 `.bin`(PyTorch) 파일의 보안 취약점(pickle 코드 실행)을 해결한 안전한 가중치 저장 포맷입니다.

### 📊 평가 및 모니터링
- **LLM-as-a-Judge**: GPT-4 같은 고성능 모델을 심판으로 사용하여 생성 결과를 채점합니다.
- **WandB (Weights & Biases)**: `report_to="wandb"`로 설정하여 Loss 곡선을 실시간 시각화합니다.
- **Overfitting**: Validation Loss가 줄어들다가 다시 증가하면 과적합입니다. 이때 가장 좋았던 지점을 **Checkpoint**로 저장해둡니다.

이 교재는 파인튜닝 단원의 120개 문항을 완벽히 커버합니다.