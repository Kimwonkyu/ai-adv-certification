# 📘 [학습 노트] 교재 6. LLM Fine Tuning (파인튜닝)

## 1. 파인튜닝의 정의와 로드맵
파인튜닝은 이미 대규모 데이터를 학습한 **Base Model**의 지능과 지식을 특정 목적(도메인 지식, 말투, 업무 형식 등)에 맞게 최적화하는 과정입니다.

### 🛣 학습 파이프라인 (The Tuning Stack)
1.  **CPT (Continuous Pre-training)**: 특정 분야(의료, 법률 등)의 전공 서적을 수만 권 읽히는 과정입니다. 모델의 근본적인 언어 '토양'을 바꿉니다.
2.  **SFT (Supervised Fine-tuning)**: "질문-답변" 쌍을 통해 지시 사항을 따르는 법을 배웁니다. (Instruction Tuning) 
3.  **RLHF / 선호 최적화**: 모델이 생성한 답변 중 인간이 더 좋아하는 것을 선택하게 조정합니다.
4.  **REASONING Reinforcement**: 수학이나 코딩 문제에 대해 생각 과정(`<think>`)을 생성하고 검증하도록 강화학습합니다. (DeepSeek R1 방식)

---

## 2. 효율적 학습 기법 (PEFT: Parameter Efficient Fine-Tuning)
전체 파라미터를 다 학습(Full FT)하려면 엄청난 GPU 장비가 필요합니다. PEFT는 극히 일부(1% 미만)의 파라미터만 학습하여 비용을 아낍니다.

### 📉 LoRA Family (Low-Rank Adaptation)
- **LoRA (기본)**: 기존 가중치($W$)는 얼려두고, 옆에 붙인 작은 두 행렬($A, B$)만 학습합니다. $W = W + AB$ 구조입니다.
    - **Rank ($r$)**: 행렬 $A, B$의 크기를 결정합니다. 높을수록 성능은 좋아지나 메모리가 더 필요합니다.
    - **Alpha**: 학습된 가중치가 원본 모델에 얼마나 강하게 반영될지 조절하는 스케일링 값입니다.
- **QLoRA**: 모델을 4비트(**NF4**)로 꽁꽁 압축하여 VRAM 사용량을 극단적으로 낮춘 상태에서 LoRA를 적용합니다.
- **DoRA (Weight-Decomposed)**: 가중치를 **Magnitude(크기)**와 **Direction(방향)**으로 분해하여 학습합니다. 쌩학습(Full Fine-tuning)과 가장 유사한 학습 궤적을 보여 성능이 매우 우수합니다.
- **GaLore**: 가중치가 아닌 **그라디언트(Gradient)** 자체를 저차원으로 투영하여 학습합니다. 대형 모델의 Full FT 효과를 절반의 메모리로 달성합니다.

```python
from unsloth import FastLanguageModel

# LoRA 설정 적용 (메모리 효율적 학습)
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/llama-3-8b-bnb-4bit",
    max_seq_length = 2048,
    load_in_4bit = True,
)
model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Rank (저차원 행렬 크기)
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_alpha = 16,
    lora_dropout = 0,
)
```

---

## 3. 정렬(Alignment)과 강화학습 알고리즘
모델의 윤리, 안전성, 지능적 답변 수준을 결정하는 최종 단계입니다.

### ⚖️ 최신 알고리즘 비교
- **RLHF (PPO)**: 보상 모델(Reward Model)과 온라인 강화학습을 결합합니다. 구현이 어렵고 자원을 많이 먹습니다.
- **DPO (Direct Preference Optimization)**: 보상 모델 없이도 선호 데이터를 통해 직접 학습하는 방식입니다. 안정적이고 효율적이라 현재 가장 널리 쓰입니다.
- **ORPO (Odds Ratio Preference Optimization)**: SFT와 정렬(Alignment)을 동시에 수행합니다. 별도의 정렬 단계가 필요 없어 매우 경제적입니다.
- **GRPO (Group Relative Policy Optimization)**: 답변을 여러 개 생성한 뒤, 그룹 내에서 상대적으로 잘한 답변에 보상을 줍니다. 보상 모델 없이 '규칙'이나 '채점'만으로 추론 능력을 극대화할 수 있습니다 (DeepSeek R1의 핵심 기법).

---

## 4. 최신 트렌드 기술
- **RAFT (Retrieval-Augmented Fine-Tuning)**: RAG 시스템에서 쓸모없는 문서가 섞여 있어도 무시하고 정답을 잘 찾도록 학습시키는 기법입니다.
- **Unsloth**: LoRA 학습의 수리적 최적화를 통해 학습 속도를 2배 높이고 VRAM을 절반으로 줄여주는 현시점 최고의 학습 라이브러리입니다.
- **MergeKit**: 서로 다르게 학습된 두 모델을 수치적으로 결합(Merging)하여 새로운 모델을 만듭니다 (SLERP, TIES 기법 등).

---

## 5. 학습 성과 및 리스크 관리
- **Catastrophic Forgetting (파멸적 망각)**: 새로운 전문 지식을 배우면서 원래 알던 일반 상식을 잊어버리는 치명적 현상입니다. 
    - **방제법**: LoRA 사용, 다양한 데이터 비율 유지, 낮은 Learning Rate 사용.
- **Perplexity (PPL)**: 모델이 다음 단어를 얼마나 확신 없게 예측하는지 나타내는 지표입니다. 값이 **낮을수록** 언어 생성 능력이 전문적이고 일관성이 높음을 의미합니다.
- **Benchmarking**: MMLU(지식), GSM8K(초등 수학), HumanEval(코딩) 등의 지표로 모델의 능력을 객관적으로 평가합니다.