# π“ [ν•™μµ λ…ΈνΈ] κµμ¬ 6. LLM νμΈνλ‹ (Advanced Tuning & Alignment)

μ΄ κµμ¬λ” λ§¤κ°λ³€μ ν¨μ¨μ  ν•™μµ(PEFT)μ μλ¦¬μ  ν† λ€λ¶€ν„° ν•λ“μ›¨μ–΄ κ°€μ†μ„ μ„ν• μ–‘μν™” μ΄λ΅ , κ·Έλ¦¬κ³  μµμ‹  μΈκ°„ μ„ νΈ μ •λ ¬ μ•κ³ λ¦¬μ¦κΉμ§€ νμΈνλ‹μ λ¨λ“  μ „λ¬Έ μ§€μ‹μ„ λ‹¤λ£Ήλ‹λ‹¤.

---

## 1. PEFT: λ§¤κ°λ³€μ ν¨μ¨μ  νμΈνλ‹
μ „μ²΄ κ°€μ¤‘μΉλ¥Ό μμ •ν•λ” Full Fine-Tuning λ€μ‹ , μ•„μ£Ό μ μ€ μ–‘μ μ¶”κ°€ νλΌλ―Έν„°λ§ ν•™μµν•μ—¬ λ©”λ¨λ¦¬λ¥Ό μ μ•½ν•λ” κΈ°μ μ…λ‹λ‹¤.

### π“‰ LoRA (Low-Rank Adaptation)μ μλ¦¬μ  μ›λ¦¬
- **κ°λ…**: κΈ°μ΅΄ κ°€μ¤‘μΉ ν–‰λ ¬ $W_0 \in \mathbb{R}^{d \times k}$λ” κ³ μ •(Freeze)ν•κ³ , λ§¤μ° μ‘μ€ μ°¨μ›($r$)μ„ κ°€μ§„ λ‘ ν–‰λ ¬ $A \in \mathbb{R}^{d \times r}$μ™€ $B \in \mathbb{R}^{r \times k}$μ κ³±($AB$)μ„ λ”ν•©λ‹λ‹¤. $W = W_0 + \Delta W$ donde $\Delta W = AB$.
- **ν¨κ³Ό**: ν•™μµ νλΌλ―Έν„° μκ°€ μμ² λ¶„μ μΌλ΅ μ¤„μ–΄λ“¤μ§€λ§, λ€ν• λ¨λΈμ λ³Έμ§μ μΈ μ €μ°¨μ› λ‚΄μ¬ κµ¬μ΅°λ¥Ό ν•™μµν•μ—¬ μ„±λ¥ μ†μ‹¤μ„ μµμ†ν™”ν•©λ‹λ‹¤.
- **νλΌλ―Έν„° μ„¤μ •**:
    - **Rank ($r$)**: μ¤‘κ°„ μ°¨μ›μ ν¬κΈ°. μΌλ°μ μΌλ΅ 8, 16, 32 λ“±μ„ μ‚¬μ©ν•λ©° λ†’μ„μλ΅ ν‘ν„λ ¥μ΄ μΆ‹μ•„μ§‘λ‹λ‹¤.
    - **Alpha ($\alpha$)**: ν•™μµλ $\Delta W$ κ°’μ„ μ›λ³Έ λ¨λΈμ— λ”ν•  λ• μ μ©ν•λ” μ¤μΌ€μΌλ§ κ³„μμ…λ‹λ‹¤. λ³΄ν†µ $\alpha = 2r$ μ •λ„λ΅ μ„¤μ •ν•©λ‹λ‹¤.

### π€ μ°¨μ„Έλ€ PEFT κΈ°μ 
- **DoRA (Weight-Decomposed LoRA)**: κ°€μ¤‘μΉλ¥Ό **Magnitude(ν¬κΈ°)**μ™€ **Direction(λ°©ν–¥)**μΌλ΅ λ¶„ν•΄ν•©λ‹λ‹¤. LoRAλ” λ°©ν–¥λ§ ν•™μµν•λ” κ²½ν–¥μ΄ μλ”λ°, DoRAλ” ν¬κΈ°κΉμ§€ μ΅°μ ν•μ—¬ Full FTμ ν•™μµ κ¶¤μ μ„ κ±°μ μ™„λ²½ν•κ² λ¨μ‚¬ν•©λ‹λ‹¤.
- **GaLore (Gradient Low-Rank Projection)**: κ°€μ¤‘μΉκ°€ μ•„λ‹ **κΈ°μΈκΈ°(Gradient)** μμ²΄λ¥Ό μ €μ°¨μ›μΌλ΅ ν¬μ(Projection)ν•μ—¬ λ©”λ¨λ¦¬λ¥Ό μ μ λ¥Ό μ¤„μ…λ‹λ‹¤. μ „μ²΄ λ¨λΈ νλΌλ―Έν„°λ¥Ό μ—…λ°μ΄νΈν•λ©΄μ„λ„ VRAM ν¨μ¨μ„ κ·Ήλ€ν™”ν•©λ‹λ‹¤.

---

## 2. μ–‘μν™”(Quantization)μ™€ λ©”λ¨λ¦¬ κ΄€λ¦¬
λ¨λΈμ μμΉ μ •λ°€λ„λ¥Ό λ‚®μ¶”μ–΄ λ¬Όλ¦¬μ  VRAM μ μ λ¥Ό μµμ†ν™”ν•λ” κΈ°μ μ…λ‹λ‹¤.

### π’ QLoRAμ™€ NF4 λ°μ΄ν„° νƒ€μ…
- **NF4 (NormalFloat 4)**: λ¨λΈ κ°€μ¤‘μΉκ°€ μΌλ°μ μΌλ΅ 0μ„ μ¤‘μ‹¬μΌλ΅ ν•λ” μ •κ· λ¶„ν¬λ¥Ό λ”°λ¥Έλ‹¤λ” μ μ— μ°©μ•ν• μμΉ ν•μ‹μ…λ‹λ‹¤. μΌλ°μ μΈ 4λΉ„νΈ μ •μλ³΄λ‹¤ μ •λ³΄ μ†μ‹¤μ΄ ν›¨μ”¬ μ μµλ‹λ‹¤.
- **Double Quantization**: κ°€μ¤‘μΉλ¥Ό μ••μ¶•ν•  λΏλ§ μ•„λ‹λΌ, μ••μ¶•μ— μ‚¬μ©λ μμΉ(Quantization Constants)λ“¤κΉμ§€ ν• λ² λ” μ–‘μν™”ν•μ—¬ λ©”λ¨λ¦¬λ¥Ό μ¶”κ°€λ΅ μ•„λ‚λ‹λ‹¤.
- **Paged Optimizer**: GPU λ©”λ¨λ¦¬κ°€ μΌμ‹μ μΌλ΅ λ¶€μ΅±ν•  λ• CPU RAMμ„ κ°€μƒ λ©”λ¨λ¦¬(Swap)μ²λΌ ν™μ©ν•μ—¬ OOM(Out of Memory) μ—λ¬λ¥Ό λ°©μ§€ν•©λ‹λ‹¤.

---

## 3. μ •λ ¬(Alignment)κ³Ό κ°•ν™”ν•™μµ
λ‹¨μ μ§€μ‹ μ΄ν–‰(SFT)μ„ λ„μ–΄ μΈκ°„μ μ„ νΈμ™€ κ°€μΉκ΄€μ— λ§κ² λ¨λΈμ„ νλ‹ν•λ” μµμΆ… κ΄€λ¬Έμ…λ‹λ‹¤.

### β–οΈ ν•µμ‹¬ μ•κ³ λ¦¬μ¦ μλ¦¬μ  λ¨λΈ
- **RLHF (PPO)**: λ³΄μƒ λ¨λΈ(Reward Model)μ„ λ”°λ΅ ν›λ ¨μ‹ν‚¨ λ’¤ PPO κ°•ν™”ν•™μµμ„ μν–‰ν•©λ‹λ‹¤. μλ ΄μ΄ μ–΄λ µκ³  μ‹μ¤ν…μ΄ λ³µμ΅ν•©λ‹λ‹¤.
- **DPO (Direct Preference Optimization)**: λ³„λ„μ λ³΄μƒ λ¨λΈ μ—†μ΄, "μΆ‹μ€ λ‹µλ³€μ ν™•λ¥ μ€ λ†’μ΄κ³  λ‚μ λ‹µλ³€μ ν™•λ¥ μ€ λ‚®μ¶λ‹¤"λ” μμ‹μ„ ν†µν•΄ μ§μ ‘ μµμ ν™”ν•©λ‹λ‹¤. μ—°μ‚°μ΄ μ•μ •μ μ΄κ³  ν¨μ¨μ μ…λ‹λ‹¤.
- **GRPO (Group Relative Policy Optimization)**: λ‹µλ³€ λ¬¶μ(Group) μ•μ—μ„ μƒλ€μ μΌλ΅ μ°μν• λ‹µλ³€μ— λ³΄μƒμ„ μ£Όλ” λ°©μ‹μ…λ‹λ‹¤. λ³΄μƒ λ¨λΈμ΄ μ—†λ” ν™κ²½(μ½”λ”©/μν•™ κ·μΉ™ κΈ°λ° μ±„μ )μ—μ„ μ¶”λ΅  μ§€λ¥μ„ ν­λ°μ μΌλ΅ λ†’μ΄λ” λ° μ ν¨ν•©λ‹λ‹¤ (DeepSeek R1μ ν•µμ‹¬).

---

## 4. ν•™μµμ μ„ν— μ”μ†μ™€ κ΄€λ¦¬ μ§€ν‘
### π« νλ©Έμ  λ§κ° (Catastrophic Forgetting)
- νΉμ • μ „λ¬Έ λ¶„μ•Ό(μ: λ²•λ¥ )λ¥Ό μ§‘μ¤‘ ν•™μµν•  λ•, λ¨λΈμ΄ μ›λ κ°€μ§€κ³  μλ μΌλ°μ μΈ λ€ν™” λ¥λ ¥μ΄λ‚ λ‹¤λ¥Έ λ¶„μ•Όμ μ§€μ‹μ„ κΈ‰κ²©ν μμ–΄λ²„λ¦¬λ” ν„μƒμ…λ‹λ‹¤.
- **ν•΄κ²°μ±…**: LoRAμ™€ κ°™μ€ PEFT κΈ°λ²• μ‚¬μ©, ν•™μµ λ°μ΄ν„°μ— μΌλ°μ μΈ μƒμ‹ λ°μ΄ν„°μ…‹(Generalist data)μ„ μ„μ–΄μ£Όλ” **λ°μ΄ν„° λ¦¬ν”λ μ΄(Replay)** κΈ°λ²• μ μ©.

### π“‰ μ„±λ¥ μ§€ν‘
- **Perplexity (PPL)**: λ¨λΈμ μ–Έμ–΄ λ¨λΈλ§ λ¥λ ¥μ„ μΈ΅μ •ν•©λ‹λ‹¤. ν•™μµμ΄ μ§„ν–‰λ¨μ— λ”°λΌ PPLμ΄ λ‚®μ•„μ§„λ‹¤λ” κ²ƒμ€ λ¨λΈμ΄ λ¬Έλ§¥μ„ λ” ν™•μ‹  μκ² μƒμ„±ν•κ³  μμμ„ λ»ν•©λ‹λ‹¤.
- **Mixed Precision**: FP32 λ€μ‹  FP16 λλ” BF16μ„ μ„μ–΄ μ‚¬μ©ν•μ—¬ μ—°μ‚° μ†λ„λ” λ†’μ΄κ³  λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ€ μ λ°μΌλ΅ μ¤„μ…λ‹λ‹¤. (Llama3 λ“± μµμ‹  λ¨λΈμ€ BF16 κ¶μ¥)

μ΄μƒμ λ‚΄μ©μ„ μ™μ§€ν•λ©΄ νμΈνλ‹ λ‹¨μ›μ λ¨λ“  μλ¦¬μ  μ›λ¦¬μ™€ μ‹¤λ¬΄μ  ν•¨μ • λ¬Έν•­μ„ μ™„λ²½ν ν•΄κ²°ν•  μ μμµλ‹λ‹¤.