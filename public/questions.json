[
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python의 동적 타이핑(Dynamic Typing)에 대한 설명으로 가장 적절한 것은?",
    "options": [
      "변수 선언 시 타입을 고정해야 한다.",
      "실행 중에 변수의 타입이 결정된다.",
      "한 번 정해진 타입은 바꿀 수 없다.",
      "컴파일 시점에 타입 에러를 잡는다.",
      "Type Hint가 필수적이다."
    ],
    "answer": "실행 중에 변수의 타입이 결정된다.",
    "why": "값이 할당되는 시점에 인터프리터가 타입을 추론합니다.",
    "hint": "리스트와 튜플, 문자열의 변경 가능 여부를 확인하세요.",
    "difficulty": "easy",
    "id": "1001"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 코드의 실행 결과는? `len('Hello')`",
    "options": [
      "4",
      "5",
      "6",
      "Hello",
      "Error"
    ],
    "answer": "5",
    "why": "Hello는 5글자입니다.",
    "hint": "리스트와 튜플, 문자열의 변경 가능 여부를 확인하세요.",
    "difficulty": "easy",
    "id": "1002"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python의 변수와 자료형에 대한 설명으로 올바른 변형 3는?",
    "options": [
      "변수 선언 시 타입을 명시해야 한다.",
      "int는 소수점이 있는 숫자를 저장한다.",
      "str은 문자열을 저장하며 불변(Immutable)이다.",
      "list는 중괄호({})를 사용한다.",
      "tuple은 요소의 추가/삭제가 자유롭다."
    ],
    "answer": "str은 문자열을 저장하며 불변(Immutable)이다.",
    "why": "Python의 문자열(str)은 변경 불가능한(Immutable) 시퀀스 자료형입니다.",
    "hint": "리스트와 튜플, 문자열의 변경 가능 여부를 확인하세요.",
    "difficulty": "easy",
    "id": "1003"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 중 Python 자료형의 특징으로 옳은 것은?",
    "options": [
      "리스트는 불변이다.",
      "튜플은 가변이다.",
      "딕셔너리는 키의 중복을 허용한다.",
      "집합(Set)은 순서가 없다.",
      "문자열은 수정 가능하다."
    ],
    "answer": "집합(Set)은 순서가 없다.",
    "why": "Set은 순서가 없는 자료형으로 인덱싱이 불가능합니다.",
    "hint": "리스트와 튜플, 문자열의 변경 가능 여부를 확인하세요.",
    "difficulty": "easy",
    "id": "1004"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python의 동적 타이핑(Dynamic Typing)에 대한 설명으로 가장 적절한 것은?",
    "options": [
      "변수 선언 시 타입을 고정해야 한다.",
      "실행 중에 변수의 타입이 결정된다.",
      "한 번 정해진 타입은 바꿀 수 없다.",
      "컴파일 시점에 타입 에러를 잡는다.",
      "Type Hint가 필수적이다."
    ],
    "answer": "실행 중에 변수의 타입이 결정된다.",
    "why": "값이 할당되는 시점에 인터프리터가 타입을 추론합니다.",
    "hint": "리스트와 튜플, 문자열의 변경 가능 여부를 확인하세요.",
    "difficulty": "easy",
    "id": "1005"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 코드의 실행 결과는? `len('Hello')`",
    "options": [
      "4",
      "5",
      "6",
      "Hello",
      "Error"
    ],
    "answer": "5",
    "why": "Hello는 5글자입니다.",
    "hint": "리스트와 튜플, 문자열의 변경 가능 여부를 확인하세요.",
    "difficulty": "easy",
    "id": "1006"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python의 변수와 자료형에 대한 설명으로 올바른 변형 7는?",
    "options": [
      "변수 선언 시 타입을 명시해야 한다.",
      "int는 소수점이 있는 숫자를 저장한다.",
      "str은 문자열을 저장하며 불변(Immutable)이다.",
      "list는 중괄호({})를 사용한다.",
      "tuple은 요소의 추가/삭제가 자유롭다."
    ],
    "answer": "str은 문자열을 저장하며 불변(Immutable)이다.",
    "why": "Python의 문자열(str)은 변경 불가능한(Immutable) 시퀀스 자료형입니다.",
    "hint": "리스트와 튜플, 문자열의 변경 가능 여부를 확인하세요.",
    "difficulty": "easy",
    "id": "1007"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 중 Python 자료형의 특징으로 옳은 것은?",
    "options": [
      "리스트는 불변이다.",
      "튜플은 가변이다.",
      "딕셔너리는 키의 중복을 허용한다.",
      "집합(Set)은 순서가 없다.",
      "문자열은 수정 가능하다."
    ],
    "answer": "집합(Set)은 순서가 없다.",
    "why": "Set은 순서가 없는 자료형으로 인덱싱이 불가능합니다.",
    "hint": "리스트와 튜플, 문자열의 변경 가능 여부를 확인하세요.",
    "difficulty": "easy",
    "id": "1008"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python의 동적 타이핑(Dynamic Typing)에 대한 설명으로 가장 적절한 것은?",
    "options": [
      "변수 선언 시 타입을 고정해야 한다.",
      "실행 중에 변수의 타입이 결정된다.",
      "한 번 정해진 타입은 바꿀 수 없다.",
      "컴파일 시점에 타입 에러를 잡는다.",
      "Type Hint가 필수적이다."
    ],
    "answer": "실행 중에 변수의 타입이 결정된다.",
    "why": "값이 할당되는 시점에 인터프리터가 타입을 추론합니다.",
    "hint": "리스트와 튜플, 문자열의 변경 가능 여부를 확인하세요.",
    "difficulty": "easy",
    "id": "1009"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 코드의 실행 결과는? `len('Hello')`",
    "options": [
      "4",
      "5",
      "6",
      "Hello",
      "Error"
    ],
    "answer": "5",
    "why": "Hello는 5글자입니다.",
    "hint": "리스트와 튜플, 문자열의 변경 가능 여부를 확인하세요.",
    "difficulty": "easy",
    "id": "1010"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python의 변수와 자료형에 대한 설명으로 올바른 변형 11는?",
    "options": [
      "변수 선언 시 타입을 명시해야 한다.",
      "int는 소수점이 있는 숫자를 저장한다.",
      "str은 문자열을 저장하며 불변(Immutable)이다.",
      "list는 중괄호({})를 사용한다.",
      "tuple은 요소의 추가/삭제가 자유롭다."
    ],
    "answer": "str은 문자열을 저장하며 불변(Immutable)이다.",
    "why": "Python의 문자열(str)은 변경 불가능한(Immutable) 시퀀스 자료형입니다.",
    "hint": "리스트와 튜플, 문자열의 변경 가능 여부를 확인하세요.",
    "difficulty": "easy",
    "id": "1011"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 중 Python 자료형의 특징으로 옳은 것은?",
    "options": [
      "리스트는 불변이다.",
      "튜플은 가변이다.",
      "딕셔너리는 키의 중복을 허용한다.",
      "집합(Set)은 순서가 없다.",
      "문자열은 수정 가능하다."
    ],
    "answer": "집합(Set)은 순서가 없다.",
    "why": "Set은 순서가 없는 자료형으로 인덱싱이 불가능합니다.",
    "hint": "리스트와 튜플, 문자열의 변경 가능 여부를 확인하세요.",
    "difficulty": "easy",
    "id": "1012"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python의 동적 타이핑(Dynamic Typing)에 대한 설명으로 가장 적절한 것은?",
    "options": [
      "변수 선언 시 타입을 고정해야 한다.",
      "실행 중에 변수의 타입이 결정된다.",
      "한 번 정해진 타입은 바꿀 수 없다.",
      "컴파일 시점에 타입 에러를 잡는다.",
      "Type Hint가 필수적이다."
    ],
    "answer": "실행 중에 변수의 타입이 결정된다.",
    "why": "값이 할당되는 시점에 인터프리터가 타입을 추론합니다.",
    "hint": "리스트와 튜플, 문자열의 변경 가능 여부를 확인하세요.",
    "difficulty": "easy",
    "id": "1013"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 코드의 실행 결과는? `len('Hello')`",
    "options": [
      "4",
      "5",
      "6",
      "Hello",
      "Error"
    ],
    "answer": "5",
    "why": "Hello는 5글자입니다.",
    "hint": "리스트와 튜플, 문자열의 변경 가능 여부를 확인하세요.",
    "difficulty": "easy",
    "id": "1014"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python의 변수와 자료형에 대한 설명으로 올바른 변형 15는?",
    "options": [
      "변수 선언 시 타입을 명시해야 한다.",
      "int는 소수점이 있는 숫자를 저장한다.",
      "str은 문자열을 저장하며 불변(Immutable)이다.",
      "list는 중괄호({})를 사용한다.",
      "tuple은 요소의 추가/삭제가 자유롭다."
    ],
    "answer": "str은 문자열을 저장하며 불변(Immutable)이다.",
    "why": "Python의 문자열(str)은 변경 불가능한(Immutable) 시퀀스 자료형입니다.",
    "hint": "리스트와 튜플, 문자열의 변경 가능 여부를 확인하세요.",
    "difficulty": "easy",
    "id": "1015"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 중 Python 자료형의 특징으로 옳은 것은?",
    "options": [
      "리스트는 불변이다.",
      "튜플은 가변이다.",
      "딕셔너리는 키의 중복을 허용한다.",
      "집합(Set)은 순서가 없다.",
      "문자열은 수정 가능하다."
    ],
    "answer": "집합(Set)은 순서가 없다.",
    "why": "Set은 순서가 없는 자료형으로 인덱싱이 불가능합니다.",
    "hint": "리스트와 튜플, 문자열의 변경 가능 여부를 확인하세요.",
    "difficulty": "easy",
    "id": "1016"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python의 동적 타이핑(Dynamic Typing)에 대한 설명으로 가장 적절한 것은?",
    "options": [
      "변수 선언 시 타입을 고정해야 한다.",
      "실행 중에 변수의 타입이 결정된다.",
      "한 번 정해진 타입은 바꿀 수 없다.",
      "컴파일 시점에 타입 에러를 잡는다.",
      "Type Hint가 필수적이다."
    ],
    "answer": "실행 중에 변수의 타입이 결정된다.",
    "why": "값이 할당되는 시점에 인터프리터가 타입을 추론합니다.",
    "hint": "리스트와 튜플, 문자열의 변경 가능 여부를 확인하세요.",
    "difficulty": "easy",
    "id": "1017"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 코드의 실행 결과는? `len('Hello')`",
    "options": [
      "4",
      "5",
      "6",
      "Hello",
      "Error"
    ],
    "answer": "5",
    "why": "Hello는 5글자입니다.",
    "hint": "리스트와 튜플, 문자열의 변경 가능 여부를 확인하세요.",
    "difficulty": "easy",
    "id": "1018"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python의 변수와 자료형에 대한 설명으로 올바른 변형 19는?",
    "options": [
      "변수 선언 시 타입을 명시해야 한다.",
      "int는 소수점이 있는 숫자를 저장한다.",
      "str은 문자열을 저장하며 불변(Immutable)이다.",
      "list는 중괄호({})를 사용한다.",
      "tuple은 요소의 추가/삭제가 자유롭다."
    ],
    "answer": "str은 문자열을 저장하며 불변(Immutable)이다.",
    "why": "Python의 문자열(str)은 변경 불가능한(Immutable) 시퀀스 자료형입니다.",
    "hint": "리스트와 튜플, 문자열의 변경 가능 여부를 확인하세요.",
    "difficulty": "easy",
    "id": "1019"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 중 Python 자료형의 특징으로 옳은 것은?",
    "options": [
      "리스트는 불변이다.",
      "튜플은 가변이다.",
      "딕셔너리는 키의 중복을 허용한다.",
      "집합(Set)은 순서가 없다.",
      "문자열은 수정 가능하다."
    ],
    "answer": "집합(Set)은 순서가 없다.",
    "why": "Set은 순서가 없는 자료형으로 인덱싱이 불가능합니다.",
    "hint": "리스트와 튜플, 문자열의 변경 가능 여부를 확인하세요.",
    "difficulty": "easy",
    "id": "1020"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 중 반복문에서 현재 반복을 건너뛰고 다음 반복으로 넘어가는 키워드는?",
    "options": [
      "stop",
      "break",
      "pass",
      "continue",
      "next"
    ],
    "answer": "continue",
    "why": "continue는 남은 코드를 실행하지 않고 다음 반복 조건 검사로 이동합니다.",
    "hint": "break vs continue",
    "difficulty": "easy",
    "id": "1021"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "range(5)가 생성하는 숫자의 범위는?",
    "options": [
      "1, 2, 3, 4, 5",
      "0, 1, 2, 3, 4",
      "0, 1, 2, 3, 4, 5",
      "1, 2, 3, 4",
      "5, 4, 3, 2, 1"
    ],
    "answer": "0, 1, 2, 3, 4",
    "why": "기본 시작값은 0이며, 종료값 5는 포함하지 않습니다.",
    "hint": "break vs continue",
    "difficulty": "easy",
    "id": "1022"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python 제어문에 대한 올바른 설명 23?",
    "options": [
      "if 문 뒤에는 콜론(:)을 붙이지 않는다.",
      "for 문은 조건이 참일 때만 반복한다.",
      "while 문은 정해진 횟수만큼 반복할 때 주로 쓴다.",
      "break는 루프를 완전히 종료한다.",
      "continue는 루프를 종료하고 다음 코드로 넘어간다."
    ],
    "answer": "break는 루프를 완전히 종료한다.",
    "why": "break 키워드는 감싸고 있는 가장 가까운 반복문을 즉시 종료합니다.",
    "hint": "break vs continue",
    "difficulty": "easy",
    "id": "1023"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 중 반복문에서 현재 반복을 건너뛰고 다음 반복으로 넘어가는 키워드는?",
    "options": [
      "stop",
      "break",
      "pass",
      "continue",
      "next"
    ],
    "answer": "continue",
    "why": "continue는 남은 코드를 실행하지 않고 다음 반복 조건 검사로 이동합니다.",
    "hint": "break vs continue",
    "difficulty": "easy",
    "id": "1024"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "range(5)가 생성하는 숫자의 범위는?",
    "options": [
      "1, 2, 3, 4, 5",
      "0, 1, 2, 3, 4",
      "0, 1, 2, 3, 4, 5",
      "1, 2, 3, 4",
      "5, 4, 3, 2, 1"
    ],
    "answer": "0, 1, 2, 3, 4",
    "why": "기본 시작값은 0이며, 종료값 5는 포함하지 않습니다.",
    "hint": "break vs continue",
    "difficulty": "easy",
    "id": "1025"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python 제어문에 대한 올바른 설명 26?",
    "options": [
      "if 문 뒤에는 콜론(:)을 붙이지 않는다.",
      "for 문은 조건이 참일 때만 반복한다.",
      "while 문은 정해진 횟수만큼 반복할 때 주로 쓴다.",
      "break는 루프를 완전히 종료한다.",
      "continue는 루프를 종료하고 다음 코드로 넘어간다."
    ],
    "answer": "break는 루프를 완전히 종료한다.",
    "why": "break 키워드는 감싸고 있는 가장 가까운 반복문을 즉시 종료합니다.",
    "hint": "break vs continue",
    "difficulty": "easy",
    "id": "1026"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 중 반복문에서 현재 반복을 건너뛰고 다음 반복으로 넘어가는 키워드는?",
    "options": [
      "stop",
      "break",
      "pass",
      "continue",
      "next"
    ],
    "answer": "continue",
    "why": "continue는 남은 코드를 실행하지 않고 다음 반복 조건 검사로 이동합니다.",
    "hint": "break vs continue",
    "difficulty": "easy",
    "id": "1027"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "range(5)가 생성하는 숫자의 범위는?",
    "options": [
      "1, 2, 3, 4, 5",
      "0, 1, 2, 3, 4",
      "0, 1, 2, 3, 4, 5",
      "1, 2, 3, 4",
      "5, 4, 3, 2, 1"
    ],
    "answer": "0, 1, 2, 3, 4",
    "why": "기본 시작값은 0이며, 종료값 5는 포함하지 않습니다.",
    "hint": "break vs continue",
    "difficulty": "easy",
    "id": "1028"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python 제어문에 대한 올바른 설명 29?",
    "options": [
      "if 문 뒤에는 콜론(:)을 붙이지 않는다.",
      "for 문은 조건이 참일 때만 반복한다.",
      "while 문은 정해진 횟수만큼 반복할 때 주로 쓴다.",
      "break는 루프를 완전히 종료한다.",
      "continue는 루프를 종료하고 다음 코드로 넘어간다."
    ],
    "answer": "break는 루프를 완전히 종료한다.",
    "why": "break 키워드는 감싸고 있는 가장 가까운 반복문을 즉시 종료합니다.",
    "hint": "break vs continue",
    "difficulty": "easy",
    "id": "1029"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 중 반복문에서 현재 반복을 건너뛰고 다음 반복으로 넘어가는 키워드는?",
    "options": [
      "stop",
      "break",
      "pass",
      "continue",
      "next"
    ],
    "answer": "continue",
    "why": "continue는 남은 코드를 실행하지 않고 다음 반복 조건 검사로 이동합니다.",
    "hint": "break vs continue",
    "difficulty": "easy",
    "id": "1030"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "range(5)가 생성하는 숫자의 범위는?",
    "options": [
      "1, 2, 3, 4, 5",
      "0, 1, 2, 3, 4",
      "0, 1, 2, 3, 4, 5",
      "1, 2, 3, 4",
      "5, 4, 3, 2, 1"
    ],
    "answer": "0, 1, 2, 3, 4",
    "why": "기본 시작값은 0이며, 종료값 5는 포함하지 않습니다.",
    "hint": "break vs continue",
    "difficulty": "easy",
    "id": "1031"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python 제어문에 대한 올바른 설명 32?",
    "options": [
      "if 문 뒤에는 콜론(:)을 붙이지 않는다.",
      "for 문은 조건이 참일 때만 반복한다.",
      "while 문은 정해진 횟수만큼 반복할 때 주로 쓴다.",
      "break는 루프를 완전히 종료한다.",
      "continue는 루프를 종료하고 다음 코드로 넘어간다."
    ],
    "answer": "break는 루프를 완전히 종료한다.",
    "why": "break 키워드는 감싸고 있는 가장 가까운 반복문을 즉시 종료합니다.",
    "hint": "break vs continue",
    "difficulty": "easy",
    "id": "1032"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 중 반복문에서 현재 반복을 건너뛰고 다음 반복으로 넘어가는 키워드는?",
    "options": [
      "stop",
      "break",
      "pass",
      "continue",
      "next"
    ],
    "answer": "continue",
    "why": "continue는 남은 코드를 실행하지 않고 다음 반복 조건 검사로 이동합니다.",
    "hint": "break vs continue",
    "difficulty": "easy",
    "id": "1033"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "range(5)가 생성하는 숫자의 범위는?",
    "options": [
      "1, 2, 3, 4, 5",
      "0, 1, 2, 3, 4",
      "0, 1, 2, 3, 4, 5",
      "1, 2, 3, 4",
      "5, 4, 3, 2, 1"
    ],
    "answer": "0, 1, 2, 3, 4",
    "why": "기본 시작값은 0이며, 종료값 5는 포함하지 않습니다.",
    "hint": "break vs continue",
    "difficulty": "easy",
    "id": "1034"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python 제어문에 대한 올바른 설명 35?",
    "options": [
      "if 문 뒤에는 콜론(:)을 붙이지 않는다.",
      "for 문은 조건이 참일 때만 반복한다.",
      "while 문은 정해진 횟수만큼 반복할 때 주로 쓴다.",
      "break는 루프를 완전히 종료한다.",
      "continue는 루프를 종료하고 다음 코드로 넘어간다."
    ],
    "answer": "break는 루프를 완전히 종료한다.",
    "why": "break 키워드는 감싸고 있는 가장 가까운 반복문을 즉시 종료합니다.",
    "hint": "break vs continue",
    "difficulty": "easy",
    "id": "1035"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 중 반복문에서 현재 반복을 건너뛰고 다음 반복으로 넘어가는 키워드는?",
    "options": [
      "stop",
      "break",
      "pass",
      "continue",
      "next"
    ],
    "answer": "continue",
    "why": "continue는 남은 코드를 실행하지 않고 다음 반복 조건 검사로 이동합니다.",
    "hint": "break vs continue",
    "difficulty": "easy",
    "id": "1036"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "range(5)가 생성하는 숫자의 범위는?",
    "options": [
      "1, 2, 3, 4, 5",
      "0, 1, 2, 3, 4",
      "0, 1, 2, 3, 4, 5",
      "1, 2, 3, 4",
      "5, 4, 3, 2, 1"
    ],
    "answer": "0, 1, 2, 3, 4",
    "why": "기본 시작값은 0이며, 종료값 5는 포함하지 않습니다.",
    "hint": "break vs continue",
    "difficulty": "easy",
    "id": "1037"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python 제어문에 대한 올바른 설명 38?",
    "options": [
      "if 문 뒤에는 콜론(:)을 붙이지 않는다.",
      "for 문은 조건이 참일 때만 반복한다.",
      "while 문은 정해진 횟수만큼 반복할 때 주로 쓴다.",
      "break는 루프를 완전히 종료한다.",
      "continue는 루프를 종료하고 다음 코드로 넘어간다."
    ],
    "answer": "break는 루프를 완전히 종료한다.",
    "why": "break 키워드는 감싸고 있는 가장 가까운 반복문을 즉시 종료합니다.",
    "hint": "break vs continue",
    "difficulty": "easy",
    "id": "1038"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 중 반복문에서 현재 반복을 건너뛰고 다음 반복으로 넘어가는 키워드는?",
    "options": [
      "stop",
      "break",
      "pass",
      "continue",
      "next"
    ],
    "answer": "continue",
    "why": "continue는 남은 코드를 실행하지 않고 다음 반복 조건 검사로 이동합니다.",
    "hint": "break vs continue",
    "difficulty": "easy",
    "id": "1039"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "range(5)가 생성하는 숫자의 범위는?",
    "options": [
      "1, 2, 3, 4, 5",
      "0, 1, 2, 3, 4",
      "0, 1, 2, 3, 4, 5",
      "1, 2, 3, 4",
      "5, 4, 3, 2, 1"
    ],
    "answer": "0, 1, 2, 3, 4",
    "why": "기본 시작값은 0이며, 종료값 5는 포함하지 않습니다.",
    "hint": "break vs continue",
    "difficulty": "easy",
    "id": "1040"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python 함수 정의와 모듈에 대한 설명으로 옳은 것 41?",
    "options": [
      "함수 정의 시 `func` 키워드를 사용한다.",
      "함수의 반환값은 오직 하나여야 한다.",
      "모듈을 불러올 때는 `include`를 사용한다.",
      "`def` 키워드를 사용하여 함수를 정의한다.",
      "전역 변수는 함수 내부에서 수정할 때 선언 없이 가능하다."
    ],
    "answer": "`def` 키워드를 사용하여 함수를 정의한다.",
    "why": "파이썬은 Defintion의 약자인 `def`로 함수를 선언합니다.",
    "hint": "definition",
    "difficulty": "medium",
    "id": "1041"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 중 표준 라이브러리 모듈을 가져오는 올바른 구문은?",
    "options": [
      "load math",
      "import math",
      "using math",
      "#include <math>",
      "require 'math'"
    ],
    "answer": "import math",
    "why": "파이썬의 모듈 임포트 키워드는 import입니다.",
    "hint": "definition",
    "difficulty": "medium",
    "id": "1042"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python 함수 정의와 모듈에 대한 설명으로 옳은 것 43?",
    "options": [
      "함수 정의 시 `func` 키워드를 사용한다.",
      "함수의 반환값은 오직 하나여야 한다.",
      "모듈을 불러올 때는 `include`를 사용한다.",
      "`def` 키워드를 사용하여 함수를 정의한다.",
      "전역 변수는 함수 내부에서 수정할 때 선언 없이 가능하다."
    ],
    "answer": "`def` 키워드를 사용하여 함수를 정의한다.",
    "why": "파이썬은 Defintion의 약자인 `def`로 함수를 선언합니다.",
    "hint": "definition",
    "difficulty": "medium",
    "id": "1043"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 중 표준 라이브러리 모듈을 가져오는 올바른 구문은?",
    "options": [
      "load math",
      "import math",
      "using math",
      "#include <math>",
      "require 'math'"
    ],
    "answer": "import math",
    "why": "파이썬의 모듈 임포트 키워드는 import입니다.",
    "hint": "definition",
    "difficulty": "medium",
    "id": "1044"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python 함수 정의와 모듈에 대한 설명으로 옳은 것 45?",
    "options": [
      "함수 정의 시 `func` 키워드를 사용한다.",
      "함수의 반환값은 오직 하나여야 한다.",
      "모듈을 불러올 때는 `include`를 사용한다.",
      "`def` 키워드를 사용하여 함수를 정의한다.",
      "전역 변수는 함수 내부에서 수정할 때 선언 없이 가능하다."
    ],
    "answer": "`def` 키워드를 사용하여 함수를 정의한다.",
    "why": "파이썬은 Defintion의 약자인 `def`로 함수를 선언합니다.",
    "hint": "definition",
    "difficulty": "medium",
    "id": "1045"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 중 표준 라이브러리 모듈을 가져오는 올바른 구문은?",
    "options": [
      "load math",
      "import math",
      "using math",
      "#include <math>",
      "require 'math'"
    ],
    "answer": "import math",
    "why": "파이썬의 모듈 임포트 키워드는 import입니다.",
    "hint": "definition",
    "difficulty": "medium",
    "id": "1046"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python 함수 정의와 모듈에 대한 설명으로 옳은 것 47?",
    "options": [
      "함수 정의 시 `func` 키워드를 사용한다.",
      "함수의 반환값은 오직 하나여야 한다.",
      "모듈을 불러올 때는 `include`를 사용한다.",
      "`def` 키워드를 사용하여 함수를 정의한다.",
      "전역 변수는 함수 내부에서 수정할 때 선언 없이 가능하다."
    ],
    "answer": "`def` 키워드를 사용하여 함수를 정의한다.",
    "why": "파이썬은 Defintion의 약자인 `def`로 함수를 선언합니다.",
    "hint": "definition",
    "difficulty": "medium",
    "id": "1047"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 중 표준 라이브러리 모듈을 가져오는 올바른 구문은?",
    "options": [
      "load math",
      "import math",
      "using math",
      "#include <math>",
      "require 'math'"
    ],
    "answer": "import math",
    "why": "파이썬의 모듈 임포트 키워드는 import입니다.",
    "hint": "definition",
    "difficulty": "medium",
    "id": "1048"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python 함수 정의와 모듈에 대한 설명으로 옳은 것 49?",
    "options": [
      "함수 정의 시 `func` 키워드를 사용한다.",
      "함수의 반환값은 오직 하나여야 한다.",
      "모듈을 불러올 때는 `include`를 사용한다.",
      "`def` 키워드를 사용하여 함수를 정의한다.",
      "전역 변수는 함수 내부에서 수정할 때 선언 없이 가능하다."
    ],
    "answer": "`def` 키워드를 사용하여 함수를 정의한다.",
    "why": "파이썬은 Defintion의 약자인 `def`로 함수를 선언합니다.",
    "hint": "definition",
    "difficulty": "medium",
    "id": "1049"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 중 표준 라이브러리 모듈을 가져오는 올바른 구문은?",
    "options": [
      "load math",
      "import math",
      "using math",
      "#include <math>",
      "require 'math'"
    ],
    "answer": "import math",
    "why": "파이썬의 모듈 임포트 키워드는 import입니다.",
    "hint": "definition",
    "difficulty": "medium",
    "id": "1050"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python 함수 정의와 모듈에 대한 설명으로 옳은 것 51?",
    "options": [
      "함수 정의 시 `func` 키워드를 사용한다.",
      "함수의 반환값은 오직 하나여야 한다.",
      "모듈을 불러올 때는 `include`를 사용한다.",
      "`def` 키워드를 사용하여 함수를 정의한다.",
      "전역 변수는 함수 내부에서 수정할 때 선언 없이 가능하다."
    ],
    "answer": "`def` 키워드를 사용하여 함수를 정의한다.",
    "why": "파이썬은 Defintion의 약자인 `def`로 함수를 선언합니다.",
    "hint": "definition",
    "difficulty": "medium",
    "id": "1051"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 중 표준 라이브러리 모듈을 가져오는 올바른 구문은?",
    "options": [
      "load math",
      "import math",
      "using math",
      "#include <math>",
      "require 'math'"
    ],
    "answer": "import math",
    "why": "파이썬의 모듈 임포트 키워드는 import입니다.",
    "hint": "definition",
    "difficulty": "medium",
    "id": "1052"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python 함수 정의와 모듈에 대한 설명으로 옳은 것 53?",
    "options": [
      "함수 정의 시 `func` 키워드를 사용한다.",
      "함수의 반환값은 오직 하나여야 한다.",
      "모듈을 불러올 때는 `include`를 사용한다.",
      "`def` 키워드를 사용하여 함수를 정의한다.",
      "전역 변수는 함수 내부에서 수정할 때 선언 없이 가능하다."
    ],
    "answer": "`def` 키워드를 사용하여 함수를 정의한다.",
    "why": "파이썬은 Defintion의 약자인 `def`로 함수를 선언합니다.",
    "hint": "definition",
    "difficulty": "medium",
    "id": "1053"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 중 표준 라이브러리 모듈을 가져오는 올바른 구문은?",
    "options": [
      "load math",
      "import math",
      "using math",
      "#include <math>",
      "require 'math'"
    ],
    "answer": "import math",
    "why": "파이썬의 모듈 임포트 키워드는 import입니다.",
    "hint": "definition",
    "difficulty": "medium",
    "id": "1054"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python 함수 정의와 모듈에 대한 설명으로 옳은 것 55?",
    "options": [
      "함수 정의 시 `func` 키워드를 사용한다.",
      "함수의 반환값은 오직 하나여야 한다.",
      "모듈을 불러올 때는 `include`를 사용한다.",
      "`def` 키워드를 사용하여 함수를 정의한다.",
      "전역 변수는 함수 내부에서 수정할 때 선언 없이 가능하다."
    ],
    "answer": "`def` 키워드를 사용하여 함수를 정의한다.",
    "why": "파이썬은 Defintion의 약자인 `def`로 함수를 선언합니다.",
    "hint": "definition",
    "difficulty": "medium",
    "id": "1055"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 중 표준 라이브러리 모듈을 가져오는 올바른 구문은?",
    "options": [
      "load math",
      "import math",
      "using math",
      "#include <math>",
      "require 'math'"
    ],
    "answer": "import math",
    "why": "파이썬의 모듈 임포트 키워드는 import입니다.",
    "hint": "definition",
    "difficulty": "medium",
    "id": "1056"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python 함수 정의와 모듈에 대한 설명으로 옳은 것 57?",
    "options": [
      "함수 정의 시 `func` 키워드를 사용한다.",
      "함수의 반환값은 오직 하나여야 한다.",
      "모듈을 불러올 때는 `include`를 사용한다.",
      "`def` 키워드를 사용하여 함수를 정의한다.",
      "전역 변수는 함수 내부에서 수정할 때 선언 없이 가능하다."
    ],
    "answer": "`def` 키워드를 사용하여 함수를 정의한다.",
    "why": "파이썬은 Defintion의 약자인 `def`로 함수를 선언합니다.",
    "hint": "definition",
    "difficulty": "medium",
    "id": "1057"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 중 표준 라이브러리 모듈을 가져오는 올바른 구문은?",
    "options": [
      "load math",
      "import math",
      "using math",
      "#include <math>",
      "require 'math'"
    ],
    "answer": "import math",
    "why": "파이썬의 모듈 임포트 키워드는 import입니다.",
    "hint": "definition",
    "difficulty": "medium",
    "id": "1058"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "Python 함수 정의와 모듈에 대한 설명으로 옳은 것 59?",
    "options": [
      "함수 정의 시 `func` 키워드를 사용한다.",
      "함수의 반환값은 오직 하나여야 한다.",
      "모듈을 불러올 때는 `include`를 사용한다.",
      "`def` 키워드를 사용하여 함수를 정의한다.",
      "전역 변수는 함수 내부에서 수정할 때 선언 없이 가능하다."
    ],
    "answer": "`def` 키워드를 사용하여 함수를 정의한다.",
    "why": "파이썬은 Defintion의 약자인 `def`로 함수를 선언합니다.",
    "hint": "definition",
    "difficulty": "medium",
    "id": "1059"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 중 표준 라이브러리 모듈을 가져오는 올바른 구문은?",
    "options": [
      "load math",
      "import math",
      "using math",
      "#include <math>",
      "require 'math'"
    ],
    "answer": "import math",
    "why": "파이썬의 모듈 임포트 키워드는 import입니다.",
    "hint": "definition",
    "difficulty": "medium",
    "id": "1060"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "객체지향 프로그래밍(OOP) 기본 개념 중 올바른 것 61?",
    "options": [
      "클래스는 객체를 생성하기 위한 설계도이다.",
      "인스턴스는 클래스의 설계도이다.",
      "__init__ 메서드는 객체 소멸 시 호출된다.",
      "상속은 부모 클래스의 기능을 자식이 사용할 수 없게 한다.",
      "파이썬은 다중 상속을 지원하지 않는다."
    ],
    "answer": "클래스는 객체를 생성하기 위한 설계도이다.",
    "why": "클래스는 붕어빵 틀과 같은 설계도 역할을 하며, 이를 통해 실체인 인스턴스(객체)를 생성합니다.",
    "hint": "Class vs Instance",
    "difficulty": "medium",
    "id": "1061"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 클래스에서 인스턴스 자신을 가리키는 첫 번째 매개변수 관례 이름은?",
    "options": [
      "this",
      "me",
      "self",
      "my",
      "it"
    ],
    "answer": "self",
    "why": "파이썬은 명시적으로 인스턴스 자신을 `self`라는 이름의 첫 번째 인자로 받습니다.",
    "hint": "Java/C++의 this와 유사하지만 이름이 다릅니다.",
    "difficulty": "medium",
    "id": "1062"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "객체지향 프로그래밍(OOP) 기본 개념 중 올바른 것 63?",
    "options": [
      "클래스는 객체를 생성하기 위한 설계도이다.",
      "인스턴스는 클래스의 설계도이다.",
      "__init__ 메서드는 객체 소멸 시 호출된다.",
      "상속은 부모 클래스의 기능을 자식이 사용할 수 없게 한다.",
      "파이썬은 다중 상속을 지원하지 않는다."
    ],
    "answer": "클래스는 객체를 생성하기 위한 설계도이다.",
    "why": "클래스는 붕어빵 틀과 같은 설계도 역할을 하며, 이를 통해 실체인 인스턴스(객체)를 생성합니다.",
    "hint": "Class vs Instance",
    "difficulty": "medium",
    "id": "1063"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 클래스에서 인스턴스 자신을 가리키는 첫 번째 매개변수 관례 이름은?",
    "options": [
      "this",
      "me",
      "self",
      "my",
      "it"
    ],
    "answer": "self",
    "why": "파이썬은 명시적으로 인스턴스 자신을 `self`라는 이름의 첫 번째 인자로 받습니다.",
    "hint": "Java/C++의 this와 유사하지만 이름이 다릅니다.",
    "difficulty": "medium",
    "id": "1064"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "객체지향 프로그래밍(OOP) 기본 개념 중 올바른 것 65?",
    "options": [
      "클래스는 객체를 생성하기 위한 설계도이다.",
      "인스턴스는 클래스의 설계도이다.",
      "__init__ 메서드는 객체 소멸 시 호출된다.",
      "상속은 부모 클래스의 기능을 자식이 사용할 수 없게 한다.",
      "파이썬은 다중 상속을 지원하지 않는다."
    ],
    "answer": "클래스는 객체를 생성하기 위한 설계도이다.",
    "why": "클래스는 붕어빵 틀과 같은 설계도 역할을 하며, 이를 통해 실체인 인스턴스(객체)를 생성합니다.",
    "hint": "Class vs Instance",
    "difficulty": "medium",
    "id": "1065"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 클래스에서 인스턴스 자신을 가리키는 첫 번째 매개변수 관례 이름은?",
    "options": [
      "this",
      "me",
      "self",
      "my",
      "it"
    ],
    "answer": "self",
    "why": "파이썬은 명시적으로 인스턴스 자신을 `self`라는 이름의 첫 번째 인자로 받습니다.",
    "hint": "Java/C++의 this와 유사하지만 이름이 다릅니다.",
    "difficulty": "medium",
    "id": "1066"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "객체지향 프로그래밍(OOP) 기본 개념 중 올바른 것 67?",
    "options": [
      "클래스는 객체를 생성하기 위한 설계도이다.",
      "인스턴스는 클래스의 설계도이다.",
      "__init__ 메서드는 객체 소멸 시 호출된다.",
      "상속은 부모 클래스의 기능을 자식이 사용할 수 없게 한다.",
      "파이썬은 다중 상속을 지원하지 않는다."
    ],
    "answer": "클래스는 객체를 생성하기 위한 설계도이다.",
    "why": "클래스는 붕어빵 틀과 같은 설계도 역할을 하며, 이를 통해 실체인 인스턴스(객체)를 생성합니다.",
    "hint": "Class vs Instance",
    "difficulty": "medium",
    "id": "1067"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 클래스에서 인스턴스 자신을 가리키는 첫 번째 매개변수 관례 이름은?",
    "options": [
      "this",
      "me",
      "self",
      "my",
      "it"
    ],
    "answer": "self",
    "why": "파이썬은 명시적으로 인스턴스 자신을 `self`라는 이름의 첫 번째 인자로 받습니다.",
    "hint": "Java/C++의 this와 유사하지만 이름이 다릅니다.",
    "difficulty": "medium",
    "id": "1068"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "객체지향 프로그래밍(OOP) 기본 개념 중 올바른 것 69?",
    "options": [
      "클래스는 객체를 생성하기 위한 설계도이다.",
      "인스턴스는 클래스의 설계도이다.",
      "__init__ 메서드는 객체 소멸 시 호출된다.",
      "상속은 부모 클래스의 기능을 자식이 사용할 수 없게 한다.",
      "파이썬은 다중 상속을 지원하지 않는다."
    ],
    "answer": "클래스는 객체를 생성하기 위한 설계도이다.",
    "why": "클래스는 붕어빵 틀과 같은 설계도 역할을 하며, 이를 통해 실체인 인스턴스(객체)를 생성합니다.",
    "hint": "Class vs Instance",
    "difficulty": "medium",
    "id": "1069"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 클래스에서 인스턴스 자신을 가리키는 첫 번째 매개변수 관례 이름은?",
    "options": [
      "this",
      "me",
      "self",
      "my",
      "it"
    ],
    "answer": "self",
    "why": "파이썬은 명시적으로 인스턴스 자신을 `self`라는 이름의 첫 번째 인자로 받습니다.",
    "hint": "Java/C++의 this와 유사하지만 이름이 다릅니다.",
    "difficulty": "medium",
    "id": "1070"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "객체지향 프로그래밍(OOP) 기본 개념 중 올바른 것 71?",
    "options": [
      "클래스는 객체를 생성하기 위한 설계도이다.",
      "인스턴스는 클래스의 설계도이다.",
      "__init__ 메서드는 객체 소멸 시 호출된다.",
      "상속은 부모 클래스의 기능을 자식이 사용할 수 없게 한다.",
      "파이썬은 다중 상속을 지원하지 않는다."
    ],
    "answer": "클래스는 객체를 생성하기 위한 설계도이다.",
    "why": "클래스는 붕어빵 틀과 같은 설계도 역할을 하며, 이를 통해 실체인 인스턴스(객체)를 생성합니다.",
    "hint": "Class vs Instance",
    "difficulty": "medium",
    "id": "1071"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 클래스에서 인스턴스 자신을 가리키는 첫 번째 매개변수 관례 이름은?",
    "options": [
      "this",
      "me",
      "self",
      "my",
      "it"
    ],
    "answer": "self",
    "why": "파이썬은 명시적으로 인스턴스 자신을 `self`라는 이름의 첫 번째 인자로 받습니다.",
    "hint": "Java/C++의 this와 유사하지만 이름이 다릅니다.",
    "difficulty": "medium",
    "id": "1072"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "객체지향 프로그래밍(OOP) 기본 개념 중 올바른 것 73?",
    "options": [
      "클래스는 객체를 생성하기 위한 설계도이다.",
      "인스턴스는 클래스의 설계도이다.",
      "__init__ 메서드는 객체 소멸 시 호출된다.",
      "상속은 부모 클래스의 기능을 자식이 사용할 수 없게 한다.",
      "파이썬은 다중 상속을 지원하지 않는다."
    ],
    "answer": "클래스는 객체를 생성하기 위한 설계도이다.",
    "why": "클래스는 붕어빵 틀과 같은 설계도 역할을 하며, 이를 통해 실체인 인스턴스(객체)를 생성합니다.",
    "hint": "Class vs Instance",
    "difficulty": "medium",
    "id": "1073"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 클래스에서 인스턴스 자신을 가리키는 첫 번째 매개변수 관례 이름은?",
    "options": [
      "this",
      "me",
      "self",
      "my",
      "it"
    ],
    "answer": "self",
    "why": "파이썬은 명시적으로 인스턴스 자신을 `self`라는 이름의 첫 번째 인자로 받습니다.",
    "hint": "Java/C++의 this와 유사하지만 이름이 다릅니다.",
    "difficulty": "medium",
    "id": "1074"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "객체지향 프로그래밍(OOP) 기본 개념 중 올바른 것 75?",
    "options": [
      "클래스는 객체를 생성하기 위한 설계도이다.",
      "인스턴스는 클래스의 설계도이다.",
      "__init__ 메서드는 객체 소멸 시 호출된다.",
      "상속은 부모 클래스의 기능을 자식이 사용할 수 없게 한다.",
      "파이썬은 다중 상속을 지원하지 않는다."
    ],
    "answer": "클래스는 객체를 생성하기 위한 설계도이다.",
    "why": "클래스는 붕어빵 틀과 같은 설계도 역할을 하며, 이를 통해 실체인 인스턴스(객체)를 생성합니다.",
    "hint": "Class vs Instance",
    "difficulty": "medium",
    "id": "1075"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 클래스에서 인스턴스 자신을 가리키는 첫 번째 매개변수 관례 이름은?",
    "options": [
      "this",
      "me",
      "self",
      "my",
      "it"
    ],
    "answer": "self",
    "why": "파이썬은 명시적으로 인스턴스 자신을 `self`라는 이름의 첫 번째 인자로 받습니다.",
    "hint": "Java/C++의 this와 유사하지만 이름이 다릅니다.",
    "difficulty": "medium",
    "id": "1076"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "객체지향 프로그래밍(OOP) 기본 개념 중 올바른 것 77?",
    "options": [
      "클래스는 객체를 생성하기 위한 설계도이다.",
      "인스턴스는 클래스의 설계도이다.",
      "__init__ 메서드는 객체 소멸 시 호출된다.",
      "상속은 부모 클래스의 기능을 자식이 사용할 수 없게 한다.",
      "파이썬은 다중 상속을 지원하지 않는다."
    ],
    "answer": "클래스는 객체를 생성하기 위한 설계도이다.",
    "why": "클래스는 붕어빵 틀과 같은 설계도 역할을 하며, 이를 통해 실체인 인스턴스(객체)를 생성합니다.",
    "hint": "Class vs Instance",
    "difficulty": "medium",
    "id": "1077"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 클래스에서 인스턴스 자신을 가리키는 첫 번째 매개변수 관례 이름은?",
    "options": [
      "this",
      "me",
      "self",
      "my",
      "it"
    ],
    "answer": "self",
    "why": "파이썬은 명시적으로 인스턴스 자신을 `self`라는 이름의 첫 번째 인자로 받습니다.",
    "hint": "Java/C++의 this와 유사하지만 이름이 다릅니다.",
    "difficulty": "medium",
    "id": "1078"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "객체지향 프로그래밍(OOP) 기본 개념 중 올바른 것 79?",
    "options": [
      "클래스는 객체를 생성하기 위한 설계도이다.",
      "인스턴스는 클래스의 설계도이다.",
      "__init__ 메서드는 객체 소멸 시 호출된다.",
      "상속은 부모 클래스의 기능을 자식이 사용할 수 없게 한다.",
      "파이썬은 다중 상속을 지원하지 않는다."
    ],
    "answer": "클래스는 객체를 생성하기 위한 설계도이다.",
    "why": "클래스는 붕어빵 틀과 같은 설계도 역할을 하며, 이를 통해 실체인 인스턴스(객체)를 생성합니다.",
    "hint": "Class vs Instance",
    "difficulty": "medium",
    "id": "1079"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 클래스에서 인스턴스 자신을 가리키는 첫 번째 매개변수 관례 이름은?",
    "options": [
      "this",
      "me",
      "self",
      "my",
      "it"
    ],
    "answer": "self",
    "why": "파이썬은 명시적으로 인스턴스 자신을 `self`라는 이름의 첫 번째 인자로 받습니다.",
    "hint": "Java/C++의 this와 유사하지만 이름이 다릅니다.",
    "difficulty": "medium",
    "id": "1080"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파일 입출력 및 예외 처리에 대한 설명 81",
    "options": [
      "파일을 열고 닫지 않아도 아무 문제가 없다.",
      "try-except 구문은 반복문을 위해 사용한다.",
      "with 문을 사용하면 파일을 자동으로 닫아준다.",
      "open() 함수는 기본적으로 쓰기 모드로 열린다.",
      "ZeroDivisionError는 파일을 찾을 수 없을 때 발생한다."
    ],
    "answer": "with 문을 사용하면 파일을 자동으로 닫아준다.",
    "why": "Context Manager인 with 문을 사용하면 블록 종료 시 `close()`가 자동 호출됩니다.",
    "hint": "자원 관리 편의성",
    "difficulty": "medium",
    "id": "1081"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파일 입출력 및 예외 처리에 대한 설명 82",
    "options": [
      "파일을 열고 닫지 않아도 아무 문제가 없다.",
      "try-except 구문은 반복문을 위해 사용한다.",
      "with 문을 사용하면 파일을 자동으로 닫아준다.",
      "open() 함수는 기본적으로 쓰기 모드로 열린다.",
      "ZeroDivisionError는 파일을 찾을 수 없을 때 발생한다."
    ],
    "answer": "with 문을 사용하면 파일을 자동으로 닫아준다.",
    "why": "Context Manager인 with 문을 사용하면 블록 종료 시 `close()`가 자동 호출됩니다.",
    "hint": "자원 관리 편의성",
    "difficulty": "medium",
    "id": "1082"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파일 입출력 및 예외 처리에 대한 설명 83",
    "options": [
      "파일을 열고 닫지 않아도 아무 문제가 없다.",
      "try-except 구문은 반복문을 위해 사용한다.",
      "with 문을 사용하면 파일을 자동으로 닫아준다.",
      "open() 함수는 기본적으로 쓰기 모드로 열린다.",
      "ZeroDivisionError는 파일을 찾을 수 없을 때 발생한다."
    ],
    "answer": "with 문을 사용하면 파일을 자동으로 닫아준다.",
    "why": "Context Manager인 with 문을 사용하면 블록 종료 시 `close()`가 자동 호출됩니다.",
    "hint": "자원 관리 편의성",
    "difficulty": "medium",
    "id": "1083"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파일 입출력 및 예외 처리에 대한 설명 84",
    "options": [
      "파일을 열고 닫지 않아도 아무 문제가 없다.",
      "try-except 구문은 반복문을 위해 사용한다.",
      "with 문을 사용하면 파일을 자동으로 닫아준다.",
      "open() 함수는 기본적으로 쓰기 모드로 열린다.",
      "ZeroDivisionError는 파일을 찾을 수 없을 때 발생한다."
    ],
    "answer": "with 문을 사용하면 파일을 자동으로 닫아준다.",
    "why": "Context Manager인 with 문을 사용하면 블록 종료 시 `close()`가 자동 호출됩니다.",
    "hint": "자원 관리 편의성",
    "difficulty": "medium",
    "id": "1084"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파일 입출력 및 예외 처리에 대한 설명 85",
    "options": [
      "파일을 열고 닫지 않아도 아무 문제가 없다.",
      "try-except 구문은 반복문을 위해 사용한다.",
      "with 문을 사용하면 파일을 자동으로 닫아준다.",
      "open() 함수는 기본적으로 쓰기 모드로 열린다.",
      "ZeroDivisionError는 파일을 찾을 수 없을 때 발생한다."
    ],
    "answer": "with 문을 사용하면 파일을 자동으로 닫아준다.",
    "why": "Context Manager인 with 문을 사용하면 블록 종료 시 `close()`가 자동 호출됩니다.",
    "hint": "자원 관리 편의성",
    "difficulty": "medium",
    "id": "1085"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파일 입출력 및 예외 처리에 대한 설명 86",
    "options": [
      "파일을 열고 닫지 않아도 아무 문제가 없다.",
      "try-except 구문은 반복문을 위해 사용한다.",
      "with 문을 사용하면 파일을 자동으로 닫아준다.",
      "open() 함수는 기본적으로 쓰기 모드로 열린다.",
      "ZeroDivisionError는 파일을 찾을 수 없을 때 발생한다."
    ],
    "answer": "with 문을 사용하면 파일을 자동으로 닫아준다.",
    "why": "Context Manager인 with 문을 사용하면 블록 종료 시 `close()`가 자동 호출됩니다.",
    "hint": "자원 관리 편의성",
    "difficulty": "medium",
    "id": "1086"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파일 입출력 및 예외 처리에 대한 설명 87",
    "options": [
      "파일을 열고 닫지 않아도 아무 문제가 없다.",
      "try-except 구문은 반복문을 위해 사용한다.",
      "with 문을 사용하면 파일을 자동으로 닫아준다.",
      "open() 함수는 기본적으로 쓰기 모드로 열린다.",
      "ZeroDivisionError는 파일을 찾을 수 없을 때 발생한다."
    ],
    "answer": "with 문을 사용하면 파일을 자동으로 닫아준다.",
    "why": "Context Manager인 with 문을 사용하면 블록 종료 시 `close()`가 자동 호출됩니다.",
    "hint": "자원 관리 편의성",
    "difficulty": "medium",
    "id": "1087"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파일 입출력 및 예외 처리에 대한 설명 88",
    "options": [
      "파일을 열고 닫지 않아도 아무 문제가 없다.",
      "try-except 구문은 반복문을 위해 사용한다.",
      "with 문을 사용하면 파일을 자동으로 닫아준다.",
      "open() 함수는 기본적으로 쓰기 모드로 열린다.",
      "ZeroDivisionError는 파일을 찾을 수 없을 때 발생한다."
    ],
    "answer": "with 문을 사용하면 파일을 자동으로 닫아준다.",
    "why": "Context Manager인 with 문을 사용하면 블록 종료 시 `close()`가 자동 호출됩니다.",
    "hint": "자원 관리 편의성",
    "difficulty": "medium",
    "id": "1088"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파일 입출력 및 예외 처리에 대한 설명 89",
    "options": [
      "파일을 열고 닫지 않아도 아무 문제가 없다.",
      "try-except 구문은 반복문을 위해 사용한다.",
      "with 문을 사용하면 파일을 자동으로 닫아준다.",
      "open() 함수는 기본적으로 쓰기 모드로 열린다.",
      "ZeroDivisionError는 파일을 찾을 수 없을 때 발생한다."
    ],
    "answer": "with 문을 사용하면 파일을 자동으로 닫아준다.",
    "why": "Context Manager인 with 문을 사용하면 블록 종료 시 `close()`가 자동 호출됩니다.",
    "hint": "자원 관리 편의성",
    "difficulty": "medium",
    "id": "1089"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파일 입출력 및 예외 처리에 대한 설명 90",
    "options": [
      "파일을 열고 닫지 않아도 아무 문제가 없다.",
      "try-except 구문은 반복문을 위해 사용한다.",
      "with 문을 사용하면 파일을 자동으로 닫아준다.",
      "open() 함수는 기본적으로 쓰기 모드로 열린다.",
      "ZeroDivisionError는 파일을 찾을 수 없을 때 발생한다."
    ],
    "answer": "with 문을 사용하면 파일을 자동으로 닫아준다.",
    "why": "Context Manager인 with 문을 사용하면 블록 종료 시 `close()`가 자동 호출됩니다.",
    "hint": "자원 관리 편의성",
    "difficulty": "medium",
    "id": "1090"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파일 입출력 및 예외 처리에 대한 설명 91",
    "options": [
      "파일을 열고 닫지 않아도 아무 문제가 없다.",
      "try-except 구문은 반복문을 위해 사용한다.",
      "with 문을 사용하면 파일을 자동으로 닫아준다.",
      "open() 함수는 기본적으로 쓰기 모드로 열린다.",
      "ZeroDivisionError는 파일을 찾을 수 없을 때 발생한다."
    ],
    "answer": "with 문을 사용하면 파일을 자동으로 닫아준다.",
    "why": "Context Manager인 with 문을 사용하면 블록 종료 시 `close()`가 자동 호출됩니다.",
    "hint": "자원 관리 편의성",
    "difficulty": "medium",
    "id": "1091"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파일 입출력 및 예외 처리에 대한 설명 92",
    "options": [
      "파일을 열고 닫지 않아도 아무 문제가 없다.",
      "try-except 구문은 반복문을 위해 사용한다.",
      "with 문을 사용하면 파일을 자동으로 닫아준다.",
      "open() 함수는 기본적으로 쓰기 모드로 열린다.",
      "ZeroDivisionError는 파일을 찾을 수 없을 때 발생한다."
    ],
    "answer": "with 문을 사용하면 파일을 자동으로 닫아준다.",
    "why": "Context Manager인 with 문을 사용하면 블록 종료 시 `close()`가 자동 호출됩니다.",
    "hint": "자원 관리 편의성",
    "difficulty": "medium",
    "id": "1092"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파일 입출력 및 예외 처리에 대한 설명 93",
    "options": [
      "파일을 열고 닫지 않아도 아무 문제가 없다.",
      "try-except 구문은 반복문을 위해 사용한다.",
      "with 문을 사용하면 파일을 자동으로 닫아준다.",
      "open() 함수는 기본적으로 쓰기 모드로 열린다.",
      "ZeroDivisionError는 파일을 찾을 수 없을 때 발생한다."
    ],
    "answer": "with 문을 사용하면 파일을 자동으로 닫아준다.",
    "why": "Context Manager인 with 문을 사용하면 블록 종료 시 `close()`가 자동 호출됩니다.",
    "hint": "자원 관리 편의성",
    "difficulty": "medium",
    "id": "1093"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파일 입출력 및 예외 처리에 대한 설명 94",
    "options": [
      "파일을 열고 닫지 않아도 아무 문제가 없다.",
      "try-except 구문은 반복문을 위해 사용한다.",
      "with 문을 사용하면 파일을 자동으로 닫아준다.",
      "open() 함수는 기본적으로 쓰기 모드로 열린다.",
      "ZeroDivisionError는 파일을 찾을 수 없을 때 발생한다."
    ],
    "answer": "with 문을 사용하면 파일을 자동으로 닫아준다.",
    "why": "Context Manager인 with 문을 사용하면 블록 종료 시 `close()`가 자동 호출됩니다.",
    "hint": "자원 관리 편의성",
    "difficulty": "medium",
    "id": "1094"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파일 입출력 및 예외 처리에 대한 설명 95",
    "options": [
      "파일을 열고 닫지 않아도 아무 문제가 없다.",
      "try-except 구문은 반복문을 위해 사용한다.",
      "with 문을 사용하면 파일을 자동으로 닫아준다.",
      "open() 함수는 기본적으로 쓰기 모드로 열린다.",
      "ZeroDivisionError는 파일을 찾을 수 없을 때 발생한다."
    ],
    "answer": "with 문을 사용하면 파일을 자동으로 닫아준다.",
    "why": "Context Manager인 with 문을 사용하면 블록 종료 시 `close()`가 자동 호출됩니다.",
    "hint": "자원 관리 편의성",
    "difficulty": "medium",
    "id": "1095"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파일 입출력 및 예외 처리에 대한 설명 96",
    "options": [
      "파일을 열고 닫지 않아도 아무 문제가 없다.",
      "try-except 구문은 반복문을 위해 사용한다.",
      "with 문을 사용하면 파일을 자동으로 닫아준다.",
      "open() 함수는 기본적으로 쓰기 모드로 열린다.",
      "ZeroDivisionError는 파일을 찾을 수 없을 때 발생한다."
    ],
    "answer": "with 문을 사용하면 파일을 자동으로 닫아준다.",
    "why": "Context Manager인 with 문을 사용하면 블록 종료 시 `close()`가 자동 호출됩니다.",
    "hint": "자원 관리 편의성",
    "difficulty": "medium",
    "id": "1096"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파일 입출력 및 예외 처리에 대한 설명 97",
    "options": [
      "파일을 열고 닫지 않아도 아무 문제가 없다.",
      "try-except 구문은 반복문을 위해 사용한다.",
      "with 문을 사용하면 파일을 자동으로 닫아준다.",
      "open() 함수는 기본적으로 쓰기 모드로 열린다.",
      "ZeroDivisionError는 파일을 찾을 수 없을 때 발생한다."
    ],
    "answer": "with 문을 사용하면 파일을 자동으로 닫아준다.",
    "why": "Context Manager인 with 문을 사용하면 블록 종료 시 `close()`가 자동 호출됩니다.",
    "hint": "자원 관리 편의성",
    "difficulty": "medium",
    "id": "1097"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파일 입출력 및 예외 처리에 대한 설명 98",
    "options": [
      "파일을 열고 닫지 않아도 아무 문제가 없다.",
      "try-except 구문은 반복문을 위해 사용한다.",
      "with 문을 사용하면 파일을 자동으로 닫아준다.",
      "open() 함수는 기본적으로 쓰기 모드로 열린다.",
      "ZeroDivisionError는 파일을 찾을 수 없을 때 발생한다."
    ],
    "answer": "with 문을 사용하면 파일을 자동으로 닫아준다.",
    "why": "Context Manager인 with 문을 사용하면 블록 종료 시 `close()`가 자동 호출됩니다.",
    "hint": "자원 관리 편의성",
    "difficulty": "medium",
    "id": "1098"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파일 입출력 및 예외 처리에 대한 설명 99",
    "options": [
      "파일을 열고 닫지 않아도 아무 문제가 없다.",
      "try-except 구문은 반복문을 위해 사용한다.",
      "with 문을 사용하면 파일을 자동으로 닫아준다.",
      "open() 함수는 기본적으로 쓰기 모드로 열린다.",
      "ZeroDivisionError는 파일을 찾을 수 없을 때 발생한다."
    ],
    "answer": "with 문을 사용하면 파일을 자동으로 닫아준다.",
    "why": "Context Manager인 with 문을 사용하면 블록 종료 시 `close()`가 자동 호출됩니다.",
    "hint": "자원 관리 편의성",
    "difficulty": "medium",
    "id": "1099"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파일 입출력 및 예외 처리에 대한 설명 100",
    "options": [
      "파일을 열고 닫지 않아도 아무 문제가 없다.",
      "try-except 구문은 반복문을 위해 사용한다.",
      "with 문을 사용하면 파일을 자동으로 닫아준다.",
      "open() 함수는 기본적으로 쓰기 모드로 열린다.",
      "ZeroDivisionError는 파일을 찾을 수 없을 때 발생한다."
    ],
    "answer": "with 문을 사용하면 파일을 자동으로 닫아준다.",
    "why": "Context Manager인 with 문을 사용하면 블록 종료 시 `close()`가 자동 호출됩니다.",
    "hint": "자원 관리 편의성",
    "difficulty": "medium",
    "id": "10100"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "딕셔너리에서 키 목록을 가져오세요.\n```python\nd = {'a': 1, 'b': 2}\nkeys = d.____()\n```",
    "answer": "keys",
    "why": "키 목록 반환 메서드는 keys()입니다.",
    "hint": "출력 함수",
    "difficulty": "easy",
    "id": "1101"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "문자열을 소문자로 변환하세요.\n```python\ns = 'HELLO'\nlower_s = s.____()\n```",
    "answer": "lower",
    "why": "소문자 변환 메서드는 lower()입니다.",
    "hint": "출력 함수",
    "difficulty": "easy",
    "id": "1102"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "리스트에 요소를 추가하세요.\n```python\nli = []\nli.____(1)\n```",
    "answer": "append",
    "why": "리스트 끝 추가는 append입니다.",
    "hint": "출력 함수",
    "difficulty": "easy",
    "id": "1103"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "모듈을 가져오세요.\n```python\n____ math\n```",
    "answer": "import",
    "why": "모듈 가져오기는 import입니다.",
    "hint": "출력 함수",
    "difficulty": "easy",
    "id": "1104"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "리스트 `a`의 길이를 구하세요.\n```python\na = [1, 2, 3]\nlength = ____(a)\n```",
    "answer": "len",
    "why": "길이를 구하는 함수는 len입니다.",
    "hint": "출력 함수",
    "difficulty": "easy",
    "id": "1105"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "딕셔너리에서 키 목록을 가져오세요.\n```python\nd = {'a': 1, 'b': 2}\nkeys = d.____()\n```",
    "answer": "keys",
    "why": "키 목록 반환 메서드는 keys()입니다.",
    "hint": "출력 함수",
    "difficulty": "easy",
    "id": "1106"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "문자열을 소문자로 변환하세요.\n```python\ns = 'HELLO'\nlower_s = s.____()\n```",
    "answer": "lower",
    "why": "소문자 변환 메서드는 lower()입니다.",
    "hint": "출력 함수",
    "difficulty": "easy",
    "id": "1107"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "리스트에 요소를 추가하세요.\n```python\nli = []\nli.____(1)\n```",
    "answer": "append",
    "why": "리스트 끝 추가는 append입니다.",
    "hint": "출력 함수",
    "difficulty": "easy",
    "id": "1108"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "모듈을 가져오세요.\n```python\n____ math\n```",
    "answer": "import",
    "why": "모듈 가져오기는 import입니다.",
    "hint": "출력 함수",
    "difficulty": "easy",
    "id": "1109"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "리스트 `a`의 길이를 구하세요.\n```python\na = [1, 2, 3]\nlength = ____(a)\n```",
    "answer": "len",
    "why": "길이를 구하는 함수는 len입니다.",
    "hint": "출력 함수",
    "difficulty": "easy",
    "id": "1110"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "딕셔너리에서 키 목록을 가져오세요.\n```python\nd = {'a': 1, 'b': 2}\nkeys = d.____()\n```",
    "answer": "keys",
    "why": "키 목록 반환 메서드는 keys()입니다.",
    "hint": "출력 함수",
    "difficulty": "easy",
    "id": "1111"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "문자열을 소문자로 변환하세요.\n```python\ns = 'HELLO'\nlower_s = s.____()\n```",
    "answer": "lower",
    "why": "소문자 변환 메서드는 lower()입니다.",
    "hint": "출력 함수",
    "difficulty": "easy",
    "id": "1112"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "리스트에 요소를 추가하세요.\n```python\nli = []\nli.____(1)\n```",
    "answer": "append",
    "why": "리스트 끝 추가는 append입니다.",
    "hint": "출력 함수",
    "difficulty": "easy",
    "id": "1113"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "모듈을 가져오세요.\n```python\n____ math\n```",
    "answer": "import",
    "why": "모듈 가져오기는 import입니다.",
    "hint": "출력 함수",
    "difficulty": "easy",
    "id": "1114"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "리스트 `a`의 길이를 구하세요.\n```python\na = [1, 2, 3]\nlength = ____(a)\n```",
    "answer": "len",
    "why": "길이를 구하는 함수는 len입니다.",
    "hint": "출력 함수",
    "difficulty": "easy",
    "id": "1115"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "딕셔너리에서 키 목록을 가져오세요.\n```python\nd = {'a': 1, 'b': 2}\nkeys = d.____()\n```",
    "answer": "keys",
    "why": "키 목록 반환 메서드는 keys()입니다.",
    "hint": "출력 함수",
    "difficulty": "easy",
    "id": "1116"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "문자열을 소문자로 변환하세요.\n```python\ns = 'HELLO'\nlower_s = s.____()\n```",
    "answer": "lower",
    "why": "소문자 변환 메서드는 lower()입니다.",
    "hint": "출력 함수",
    "difficulty": "easy",
    "id": "1117"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "리스트에 요소를 추가하세요.\n```python\nli = []\nli.____(1)\n```",
    "answer": "append",
    "why": "리스트 끝 추가는 append입니다.",
    "hint": "출력 함수",
    "difficulty": "easy",
    "id": "1118"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "모듈을 가져오세요.\n```python\n____ math\n```",
    "answer": "import",
    "why": "모듈 가져오기는 import입니다.",
    "hint": "출력 함수",
    "difficulty": "easy",
    "id": "1119"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "리스트 `a`의 길이를 구하세요.\n```python\na = [1, 2, 3]\nlength = ____(a)\n```",
    "answer": "len",
    "why": "길이를 구하는 함수는 len입니다.",
    "hint": "출력 함수",
    "difficulty": "easy",
    "id": "1120"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "NumPy에 대한 설명으로 올바른 변형 1는?",
    "options": [
      "리스트보다 연산 속도가 느리다.",
      "동일한 자료형의 데이터만 담을 수 있다.",
      "GPU 가속을 기본적으로 지원한다.",
      "Python 기본 `math` 라이브러리와 완전히 동일하다.",
      "인덱싱이 1부터 시작한다."
    ],
    "answer": "동일한 자료형의 데이터만 담을 수 있다.",
    "why": "NumPy 배열(ndarray)은 성능 최적화를 위해 모든 요소가 동일한 데이터 타입이어야 합니다.",
    "hint": "Homogeneous array",
    "difficulty": "easy",
    "id": "2001"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "NumPy에 대한 설명으로 올바른 변형 2는?",
    "options": [
      "리스트보다 연산 속도가 느리다.",
      "동일한 자료형의 데이터만 담을 수 있다.",
      "GPU 가속을 기본적으로 지원한다.",
      "Python 기본 `math` 라이브러리와 완전히 동일하다.",
      "인덱싱이 1부터 시작한다."
    ],
    "answer": "동일한 자료형의 데이터만 담을 수 있다.",
    "why": "NumPy 배열(ndarray)은 성능 최적화를 위해 모든 요소가 동일한 데이터 타입이어야 합니다.",
    "hint": "Homogeneous array",
    "difficulty": "easy",
    "id": "2002"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "0으로 초기화된 크기 10의 배열을 생성하는 함수는?",
    "options": [
      "np.array(10)",
      "np.zeros(10)",
      "np.empty(10)",
      "np.nulls(10)",
      "np.init(10)"
    ],
    "answer": "np.zeros(10)",
    "why": "zeros 함수는 0으로 채워진 배열을 생성합니다.",
    "hint": "Homogeneous array",
    "difficulty": "easy",
    "id": "2003"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "NumPy에 대한 설명으로 올바른 변형 4는?",
    "options": [
      "리스트보다 연산 속도가 느리다.",
      "동일한 자료형의 데이터만 담을 수 있다.",
      "GPU 가속을 기본적으로 지원한다.",
      "Python 기본 `math` 라이브러리와 완전히 동일하다.",
      "인덱싱이 1부터 시작한다."
    ],
    "answer": "동일한 자료형의 데이터만 담을 수 있다.",
    "why": "NumPy 배열(ndarray)은 성능 최적화를 위해 모든 요소가 동일한 데이터 타입이어야 합니다.",
    "hint": "Homogeneous array",
    "difficulty": "easy",
    "id": "2004"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "NumPy에 대한 설명으로 올바른 변형 5는?",
    "options": [
      "리스트보다 연산 속도가 느리다.",
      "동일한 자료형의 데이터만 담을 수 있다.",
      "GPU 가속을 기본적으로 지원한다.",
      "Python 기본 `math` 라이브러리와 완전히 동일하다.",
      "인덱싱이 1부터 시작한다."
    ],
    "answer": "동일한 자료형의 데이터만 담을 수 있다.",
    "why": "NumPy 배열(ndarray)은 성능 최적화를 위해 모든 요소가 동일한 데이터 타입이어야 합니다.",
    "hint": "Homogeneous array",
    "difficulty": "easy",
    "id": "2005"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "0으로 초기화된 크기 10의 배열을 생성하는 함수는?",
    "options": [
      "np.array(10)",
      "np.zeros(10)",
      "np.empty(10)",
      "np.nulls(10)",
      "np.init(10)"
    ],
    "answer": "np.zeros(10)",
    "why": "zeros 함수는 0으로 채워진 배열을 생성합니다.",
    "hint": "Homogeneous array",
    "difficulty": "easy",
    "id": "2006"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "NumPy에 대한 설명으로 올바른 변형 7는?",
    "options": [
      "리스트보다 연산 속도가 느리다.",
      "동일한 자료형의 데이터만 담을 수 있다.",
      "GPU 가속을 기본적으로 지원한다.",
      "Python 기본 `math` 라이브러리와 완전히 동일하다.",
      "인덱싱이 1부터 시작한다."
    ],
    "answer": "동일한 자료형의 데이터만 담을 수 있다.",
    "why": "NumPy 배열(ndarray)은 성능 최적화를 위해 모든 요소가 동일한 데이터 타입이어야 합니다.",
    "hint": "Homogeneous array",
    "difficulty": "easy",
    "id": "2007"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "NumPy에 대한 설명으로 올바른 변형 8는?",
    "options": [
      "리스트보다 연산 속도가 느리다.",
      "동일한 자료형의 데이터만 담을 수 있다.",
      "GPU 가속을 기본적으로 지원한다.",
      "Python 기본 `math` 라이브러리와 완전히 동일하다.",
      "인덱싱이 1부터 시작한다."
    ],
    "answer": "동일한 자료형의 데이터만 담을 수 있다.",
    "why": "NumPy 배열(ndarray)은 성능 최적화를 위해 모든 요소가 동일한 데이터 타입이어야 합니다.",
    "hint": "Homogeneous array",
    "difficulty": "easy",
    "id": "2008"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "0으로 초기화된 크기 10의 배열을 생성하는 함수는?",
    "options": [
      "np.array(10)",
      "np.zeros(10)",
      "np.empty(10)",
      "np.nulls(10)",
      "np.init(10)"
    ],
    "answer": "np.zeros(10)",
    "why": "zeros 함수는 0으로 채워진 배열을 생성합니다.",
    "hint": "Homogeneous array",
    "difficulty": "easy",
    "id": "2009"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "NumPy에 대한 설명으로 올바른 변형 10는?",
    "options": [
      "리스트보다 연산 속도가 느리다.",
      "동일한 자료형의 데이터만 담을 수 있다.",
      "GPU 가속을 기본적으로 지원한다.",
      "Python 기본 `math` 라이브러리와 완전히 동일하다.",
      "인덱싱이 1부터 시작한다."
    ],
    "answer": "동일한 자료형의 데이터만 담을 수 있다.",
    "why": "NumPy 배열(ndarray)은 성능 최적화를 위해 모든 요소가 동일한 데이터 타입이어야 합니다.",
    "hint": "Homogeneous array",
    "difficulty": "easy",
    "id": "2010"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "NumPy에 대한 설명으로 올바른 변형 11는?",
    "options": [
      "리스트보다 연산 속도가 느리다.",
      "동일한 자료형의 데이터만 담을 수 있다.",
      "GPU 가속을 기본적으로 지원한다.",
      "Python 기본 `math` 라이브러리와 완전히 동일하다.",
      "인덱싱이 1부터 시작한다."
    ],
    "answer": "동일한 자료형의 데이터만 담을 수 있다.",
    "why": "NumPy 배열(ndarray)은 성능 최적화를 위해 모든 요소가 동일한 데이터 타입이어야 합니다.",
    "hint": "Homogeneous array",
    "difficulty": "easy",
    "id": "2011"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "0으로 초기화된 크기 10의 배열을 생성하는 함수는?",
    "options": [
      "np.array(10)",
      "np.zeros(10)",
      "np.empty(10)",
      "np.nulls(10)",
      "np.init(10)"
    ],
    "answer": "np.zeros(10)",
    "why": "zeros 함수는 0으로 채워진 배열을 생성합니다.",
    "hint": "Homogeneous array",
    "difficulty": "easy",
    "id": "2012"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "NumPy에 대한 설명으로 올바른 변형 13는?",
    "options": [
      "리스트보다 연산 속도가 느리다.",
      "동일한 자료형의 데이터만 담을 수 있다.",
      "GPU 가속을 기본적으로 지원한다.",
      "Python 기본 `math` 라이브러리와 완전히 동일하다.",
      "인덱싱이 1부터 시작한다."
    ],
    "answer": "동일한 자료형의 데이터만 담을 수 있다.",
    "why": "NumPy 배열(ndarray)은 성능 최적화를 위해 모든 요소가 동일한 데이터 타입이어야 합니다.",
    "hint": "Homogeneous array",
    "difficulty": "easy",
    "id": "2013"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "NumPy에 대한 설명으로 올바른 변형 14는?",
    "options": [
      "리스트보다 연산 속도가 느리다.",
      "동일한 자료형의 데이터만 담을 수 있다.",
      "GPU 가속을 기본적으로 지원한다.",
      "Python 기본 `math` 라이브러리와 완전히 동일하다.",
      "인덱싱이 1부터 시작한다."
    ],
    "answer": "동일한 자료형의 데이터만 담을 수 있다.",
    "why": "NumPy 배열(ndarray)은 성능 최적화를 위해 모든 요소가 동일한 데이터 타입이어야 합니다.",
    "hint": "Homogeneous array",
    "difficulty": "easy",
    "id": "2014"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "0으로 초기화된 크기 10의 배열을 생성하는 함수는?",
    "options": [
      "np.array(10)",
      "np.zeros(10)",
      "np.empty(10)",
      "np.nulls(10)",
      "np.init(10)"
    ],
    "answer": "np.zeros(10)",
    "why": "zeros 함수는 0으로 채워진 배열을 생성합니다.",
    "hint": "Homogeneous array",
    "difficulty": "easy",
    "id": "2015"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "NumPy에 대한 설명으로 올바른 변형 16는?",
    "options": [
      "리스트보다 연산 속도가 느리다.",
      "동일한 자료형의 데이터만 담을 수 있다.",
      "GPU 가속을 기본적으로 지원한다.",
      "Python 기본 `math` 라이브러리와 완전히 동일하다.",
      "인덱싱이 1부터 시작한다."
    ],
    "answer": "동일한 자료형의 데이터만 담을 수 있다.",
    "why": "NumPy 배열(ndarray)은 성능 최적화를 위해 모든 요소가 동일한 데이터 타입이어야 합니다.",
    "hint": "Homogeneous array",
    "difficulty": "easy",
    "id": "2016"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "NumPy에 대한 설명으로 올바른 변형 17는?",
    "options": [
      "리스트보다 연산 속도가 느리다.",
      "동일한 자료형의 데이터만 담을 수 있다.",
      "GPU 가속을 기본적으로 지원한다.",
      "Python 기본 `math` 라이브러리와 완전히 동일하다.",
      "인덱싱이 1부터 시작한다."
    ],
    "answer": "동일한 자료형의 데이터만 담을 수 있다.",
    "why": "NumPy 배열(ndarray)은 성능 최적화를 위해 모든 요소가 동일한 데이터 타입이어야 합니다.",
    "hint": "Homogeneous array",
    "difficulty": "easy",
    "id": "2017"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "0으로 초기화된 크기 10의 배열을 생성하는 함수는?",
    "options": [
      "np.array(10)",
      "np.zeros(10)",
      "np.empty(10)",
      "np.nulls(10)",
      "np.init(10)"
    ],
    "answer": "np.zeros(10)",
    "why": "zeros 함수는 0으로 채워진 배열을 생성합니다.",
    "hint": "Homogeneous array",
    "difficulty": "easy",
    "id": "2018"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "NumPy에 대한 설명으로 올바른 변형 19는?",
    "options": [
      "리스트보다 연산 속도가 느리다.",
      "동일한 자료형의 데이터만 담을 수 있다.",
      "GPU 가속을 기본적으로 지원한다.",
      "Python 기본 `math` 라이브러리와 완전히 동일하다.",
      "인덱싱이 1부터 시작한다."
    ],
    "answer": "동일한 자료형의 데이터만 담을 수 있다.",
    "why": "NumPy 배열(ndarray)은 성능 최적화를 위해 모든 요소가 동일한 데이터 타입이어야 합니다.",
    "hint": "Homogeneous array",
    "difficulty": "easy",
    "id": "2019"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "NumPy에 대한 설명으로 올바른 변형 20는?",
    "options": [
      "리스트보다 연산 속도가 느리다.",
      "동일한 자료형의 데이터만 담을 수 있다.",
      "GPU 가속을 기본적으로 지원한다.",
      "Python 기본 `math` 라이브러리와 완전히 동일하다.",
      "인덱싱이 1부터 시작한다."
    ],
    "answer": "동일한 자료형의 데이터만 담을 수 있다.",
    "why": "NumPy 배열(ndarray)은 성능 최적화를 위해 모든 요소가 동일한 데이터 타입이어야 합니다.",
    "hint": "Homogeneous array",
    "difficulty": "easy",
    "id": "2020"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas Series와 DataFrame에 대한 설명 21",
    "options": [
      "Series는 2차원 데이터를 다룬다.",
      "DataFrame은 행과 열이 있는 구조이다.",
      "Pandas는 수치 데이터만 처리할 수 있다.",
      "DataFrame은 엑셀 파일로 저장이 불가능하다.",
      "Index는 중복될 수 없다."
    ],
    "answer": "DataFrame은 행과 열이 있는 구조이다.",
    "why": "DataFrame은 행(Row)과 열(Column)로 구성된 2차원 테이블 형태의 데이터 구조입니다.",
    "hint": "엑셀 시트와 비슷한 구조",
    "difficulty": "easy",
    "id": "2021"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas Series와 DataFrame에 대한 설명 22",
    "options": [
      "Series는 2차원 데이터를 다룬다.",
      "DataFrame은 행과 열이 있는 구조이다.",
      "Pandas는 수치 데이터만 처리할 수 있다.",
      "DataFrame은 엑셀 파일로 저장이 불가능하다.",
      "Index는 중복될 수 없다."
    ],
    "answer": "DataFrame은 행과 열이 있는 구조이다.",
    "why": "DataFrame은 행(Row)과 열(Column)로 구성된 2차원 테이블 형태의 데이터 구조입니다.",
    "hint": "엑셀 시트와 비슷한 구조",
    "difficulty": "easy",
    "id": "2022"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas Series와 DataFrame에 대한 설명 23",
    "options": [
      "Series는 2차원 데이터를 다룬다.",
      "DataFrame은 행과 열이 있는 구조이다.",
      "Pandas는 수치 데이터만 처리할 수 있다.",
      "DataFrame은 엑셀 파일로 저장이 불가능하다.",
      "Index는 중복될 수 없다."
    ],
    "answer": "DataFrame은 행과 열이 있는 구조이다.",
    "why": "DataFrame은 행(Row)과 열(Column)로 구성된 2차원 테이블 형태의 데이터 구조입니다.",
    "hint": "엑셀 시트와 비슷한 구조",
    "difficulty": "easy",
    "id": "2023"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas Series와 DataFrame에 대한 설명 24",
    "options": [
      "Series는 2차원 데이터를 다룬다.",
      "DataFrame은 행과 열이 있는 구조이다.",
      "Pandas는 수치 데이터만 처리할 수 있다.",
      "DataFrame은 엑셀 파일로 저장이 불가능하다.",
      "Index는 중복될 수 없다."
    ],
    "answer": "DataFrame은 행과 열이 있는 구조이다.",
    "why": "DataFrame은 행(Row)과 열(Column)로 구성된 2차원 테이블 형태의 데이터 구조입니다.",
    "hint": "엑셀 시트와 비슷한 구조",
    "difficulty": "easy",
    "id": "2024"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas Series와 DataFrame에 대한 설명 25",
    "options": [
      "Series는 2차원 데이터를 다룬다.",
      "DataFrame은 행과 열이 있는 구조이다.",
      "Pandas는 수치 데이터만 처리할 수 있다.",
      "DataFrame은 엑셀 파일로 저장이 불가능하다.",
      "Index는 중복될 수 없다."
    ],
    "answer": "DataFrame은 행과 열이 있는 구조이다.",
    "why": "DataFrame은 행(Row)과 열(Column)로 구성된 2차원 테이블 형태의 데이터 구조입니다.",
    "hint": "엑셀 시트와 비슷한 구조",
    "difficulty": "easy",
    "id": "2025"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas Series와 DataFrame에 대한 설명 26",
    "options": [
      "Series는 2차원 데이터를 다룬다.",
      "DataFrame은 행과 열이 있는 구조이다.",
      "Pandas는 수치 데이터만 처리할 수 있다.",
      "DataFrame은 엑셀 파일로 저장이 불가능하다.",
      "Index는 중복될 수 없다."
    ],
    "answer": "DataFrame은 행과 열이 있는 구조이다.",
    "why": "DataFrame은 행(Row)과 열(Column)로 구성된 2차원 테이블 형태의 데이터 구조입니다.",
    "hint": "엑셀 시트와 비슷한 구조",
    "difficulty": "easy",
    "id": "2026"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas Series와 DataFrame에 대한 설명 27",
    "options": [
      "Series는 2차원 데이터를 다룬다.",
      "DataFrame은 행과 열이 있는 구조이다.",
      "Pandas는 수치 데이터만 처리할 수 있다.",
      "DataFrame은 엑셀 파일로 저장이 불가능하다.",
      "Index는 중복될 수 없다."
    ],
    "answer": "DataFrame은 행과 열이 있는 구조이다.",
    "why": "DataFrame은 행(Row)과 열(Column)로 구성된 2차원 테이블 형태의 데이터 구조입니다.",
    "hint": "엑셀 시트와 비슷한 구조",
    "difficulty": "easy",
    "id": "2027"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas Series와 DataFrame에 대한 설명 28",
    "options": [
      "Series는 2차원 데이터를 다룬다.",
      "DataFrame은 행과 열이 있는 구조이다.",
      "Pandas는 수치 데이터만 처리할 수 있다.",
      "DataFrame은 엑셀 파일로 저장이 불가능하다.",
      "Index는 중복될 수 없다."
    ],
    "answer": "DataFrame은 행과 열이 있는 구조이다.",
    "why": "DataFrame은 행(Row)과 열(Column)로 구성된 2차원 테이블 형태의 데이터 구조입니다.",
    "hint": "엑셀 시트와 비슷한 구조",
    "difficulty": "easy",
    "id": "2028"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas Series와 DataFrame에 대한 설명 29",
    "options": [
      "Series는 2차원 데이터를 다룬다.",
      "DataFrame은 행과 열이 있는 구조이다.",
      "Pandas는 수치 데이터만 처리할 수 있다.",
      "DataFrame은 엑셀 파일로 저장이 불가능하다.",
      "Index는 중복될 수 없다."
    ],
    "answer": "DataFrame은 행과 열이 있는 구조이다.",
    "why": "DataFrame은 행(Row)과 열(Column)로 구성된 2차원 테이블 형태의 데이터 구조입니다.",
    "hint": "엑셀 시트와 비슷한 구조",
    "difficulty": "easy",
    "id": "2029"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas Series와 DataFrame에 대한 설명 30",
    "options": [
      "Series는 2차원 데이터를 다룬다.",
      "DataFrame은 행과 열이 있는 구조이다.",
      "Pandas는 수치 데이터만 처리할 수 있다.",
      "DataFrame은 엑셀 파일로 저장이 불가능하다.",
      "Index는 중복될 수 없다."
    ],
    "answer": "DataFrame은 행과 열이 있는 구조이다.",
    "why": "DataFrame은 행(Row)과 열(Column)로 구성된 2차원 테이블 형태의 데이터 구조입니다.",
    "hint": "엑셀 시트와 비슷한 구조",
    "difficulty": "easy",
    "id": "2030"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas Series와 DataFrame에 대한 설명 31",
    "options": [
      "Series는 2차원 데이터를 다룬다.",
      "DataFrame은 행과 열이 있는 구조이다.",
      "Pandas는 수치 데이터만 처리할 수 있다.",
      "DataFrame은 엑셀 파일로 저장이 불가능하다.",
      "Index는 중복될 수 없다."
    ],
    "answer": "DataFrame은 행과 열이 있는 구조이다.",
    "why": "DataFrame은 행(Row)과 열(Column)로 구성된 2차원 테이블 형태의 데이터 구조입니다.",
    "hint": "엑셀 시트와 비슷한 구조",
    "difficulty": "easy",
    "id": "2031"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas Series와 DataFrame에 대한 설명 32",
    "options": [
      "Series는 2차원 데이터를 다룬다.",
      "DataFrame은 행과 열이 있는 구조이다.",
      "Pandas는 수치 데이터만 처리할 수 있다.",
      "DataFrame은 엑셀 파일로 저장이 불가능하다.",
      "Index는 중복될 수 없다."
    ],
    "answer": "DataFrame은 행과 열이 있는 구조이다.",
    "why": "DataFrame은 행(Row)과 열(Column)로 구성된 2차원 테이블 형태의 데이터 구조입니다.",
    "hint": "엑셀 시트와 비슷한 구조",
    "difficulty": "easy",
    "id": "2032"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas Series와 DataFrame에 대한 설명 33",
    "options": [
      "Series는 2차원 데이터를 다룬다.",
      "DataFrame은 행과 열이 있는 구조이다.",
      "Pandas는 수치 데이터만 처리할 수 있다.",
      "DataFrame은 엑셀 파일로 저장이 불가능하다.",
      "Index는 중복될 수 없다."
    ],
    "answer": "DataFrame은 행과 열이 있는 구조이다.",
    "why": "DataFrame은 행(Row)과 열(Column)로 구성된 2차원 테이블 형태의 데이터 구조입니다.",
    "hint": "엑셀 시트와 비슷한 구조",
    "difficulty": "easy",
    "id": "2033"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas Series와 DataFrame에 대한 설명 34",
    "options": [
      "Series는 2차원 데이터를 다룬다.",
      "DataFrame은 행과 열이 있는 구조이다.",
      "Pandas는 수치 데이터만 처리할 수 있다.",
      "DataFrame은 엑셀 파일로 저장이 불가능하다.",
      "Index는 중복될 수 없다."
    ],
    "answer": "DataFrame은 행과 열이 있는 구조이다.",
    "why": "DataFrame은 행(Row)과 열(Column)로 구성된 2차원 테이블 형태의 데이터 구조입니다.",
    "hint": "엑셀 시트와 비슷한 구조",
    "difficulty": "easy",
    "id": "2034"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas Series와 DataFrame에 대한 설명 35",
    "options": [
      "Series는 2차원 데이터를 다룬다.",
      "DataFrame은 행과 열이 있는 구조이다.",
      "Pandas는 수치 데이터만 처리할 수 있다.",
      "DataFrame은 엑셀 파일로 저장이 불가능하다.",
      "Index는 중복될 수 없다."
    ],
    "answer": "DataFrame은 행과 열이 있는 구조이다.",
    "why": "DataFrame은 행(Row)과 열(Column)로 구성된 2차원 테이블 형태의 데이터 구조입니다.",
    "hint": "엑셀 시트와 비슷한 구조",
    "difficulty": "easy",
    "id": "2035"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas Series와 DataFrame에 대한 설명 36",
    "options": [
      "Series는 2차원 데이터를 다룬다.",
      "DataFrame은 행과 열이 있는 구조이다.",
      "Pandas는 수치 데이터만 처리할 수 있다.",
      "DataFrame은 엑셀 파일로 저장이 불가능하다.",
      "Index는 중복될 수 없다."
    ],
    "answer": "DataFrame은 행과 열이 있는 구조이다.",
    "why": "DataFrame은 행(Row)과 열(Column)로 구성된 2차원 테이블 형태의 데이터 구조입니다.",
    "hint": "엑셀 시트와 비슷한 구조",
    "difficulty": "easy",
    "id": "2036"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas Series와 DataFrame에 대한 설명 37",
    "options": [
      "Series는 2차원 데이터를 다룬다.",
      "DataFrame은 행과 열이 있는 구조이다.",
      "Pandas는 수치 데이터만 처리할 수 있다.",
      "DataFrame은 엑셀 파일로 저장이 불가능하다.",
      "Index는 중복될 수 없다."
    ],
    "answer": "DataFrame은 행과 열이 있는 구조이다.",
    "why": "DataFrame은 행(Row)과 열(Column)로 구성된 2차원 테이블 형태의 데이터 구조입니다.",
    "hint": "엑셀 시트와 비슷한 구조",
    "difficulty": "easy",
    "id": "2037"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas Series와 DataFrame에 대한 설명 38",
    "options": [
      "Series는 2차원 데이터를 다룬다.",
      "DataFrame은 행과 열이 있는 구조이다.",
      "Pandas는 수치 데이터만 처리할 수 있다.",
      "DataFrame은 엑셀 파일로 저장이 불가능하다.",
      "Index는 중복될 수 없다."
    ],
    "answer": "DataFrame은 행과 열이 있는 구조이다.",
    "why": "DataFrame은 행(Row)과 열(Column)로 구성된 2차원 테이블 형태의 데이터 구조입니다.",
    "hint": "엑셀 시트와 비슷한 구조",
    "difficulty": "easy",
    "id": "2038"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas Series와 DataFrame에 대한 설명 39",
    "options": [
      "Series는 2차원 데이터를 다룬다.",
      "DataFrame은 행과 열이 있는 구조이다.",
      "Pandas는 수치 데이터만 처리할 수 있다.",
      "DataFrame은 엑셀 파일로 저장이 불가능하다.",
      "Index는 중복될 수 없다."
    ],
    "answer": "DataFrame은 행과 열이 있는 구조이다.",
    "why": "DataFrame은 행(Row)과 열(Column)로 구성된 2차원 테이블 형태의 데이터 구조입니다.",
    "hint": "엑셀 시트와 비슷한 구조",
    "difficulty": "easy",
    "id": "2039"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas Series와 DataFrame에 대한 설명 40",
    "options": [
      "Series는 2차원 데이터를 다룬다.",
      "DataFrame은 행과 열이 있는 구조이다.",
      "Pandas는 수치 데이터만 처리할 수 있다.",
      "DataFrame은 엑셀 파일로 저장이 불가능하다.",
      "Index는 중복될 수 없다."
    ],
    "answer": "DataFrame은 행과 열이 있는 구조이다.",
    "why": "DataFrame은 행(Row)과 열(Column)로 구성된 2차원 테이블 형태의 데이터 구조입니다.",
    "hint": "엑셀 시트와 비슷한 구조",
    "difficulty": "easy",
    "id": "2040"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 선택 및 필터링 방법 41",
    "options": [
      "loc은 정수 위치 기반 인덱싱이다.",
      "iloc은 라벨(이름) 기반 인덱싱이다.",
      "df['col']은 열을 선택한다.",
      "특정 조건의 행을 추출할 수 없다.",
      "슬라이싱은 지원하지 않는다."
    ],
    "answer": "df['col']은 열을 선택한다.",
    "why": "대괄호 `[]` 안에 컬럼명을 넣으면 해당 Series(열)를 선택합니다.",
    "hint": "Column Selection",
    "difficulty": "medium",
    "id": "2041"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 인덱스 번호(0, 1, 2...)로 데이터에 접근하는 속성은?",
    "options": [
      "ix",
      "at",
      "loc",
      "iloc",
      "idx"
    ],
    "answer": "iloc",
    "why": "iloc stands for integer location.",
    "hint": "integer location",
    "difficulty": "medium",
    "id": "2042"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 선택 및 필터링 방법 43",
    "options": [
      "loc은 정수 위치 기반 인덱싱이다.",
      "iloc은 라벨(이름) 기반 인덱싱이다.",
      "df['col']은 열을 선택한다.",
      "특정 조건의 행을 추출할 수 없다.",
      "슬라이싱은 지원하지 않는다."
    ],
    "answer": "df['col']은 열을 선택한다.",
    "why": "대괄호 `[]` 안에 컬럼명을 넣으면 해당 Series(열)를 선택합니다.",
    "hint": "Column Selection",
    "difficulty": "medium",
    "id": "2043"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 인덱스 번호(0, 1, 2...)로 데이터에 접근하는 속성은?",
    "options": [
      "ix",
      "at",
      "loc",
      "iloc",
      "idx"
    ],
    "answer": "iloc",
    "why": "iloc stands for integer location.",
    "hint": "integer location",
    "difficulty": "medium",
    "id": "2044"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 선택 및 필터링 방법 45",
    "options": [
      "loc은 정수 위치 기반 인덱싱이다.",
      "iloc은 라벨(이름) 기반 인덱싱이다.",
      "df['col']은 열을 선택한다.",
      "특정 조건의 행을 추출할 수 없다.",
      "슬라이싱은 지원하지 않는다."
    ],
    "answer": "df['col']은 열을 선택한다.",
    "why": "대괄호 `[]` 안에 컬럼명을 넣으면 해당 Series(열)를 선택합니다.",
    "hint": "Column Selection",
    "difficulty": "medium",
    "id": "2045"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 인덱스 번호(0, 1, 2...)로 데이터에 접근하는 속성은?",
    "options": [
      "ix",
      "at",
      "loc",
      "iloc",
      "idx"
    ],
    "answer": "iloc",
    "why": "iloc stands for integer location.",
    "hint": "integer location",
    "difficulty": "medium",
    "id": "2046"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 선택 및 필터링 방법 47",
    "options": [
      "loc은 정수 위치 기반 인덱싱이다.",
      "iloc은 라벨(이름) 기반 인덱싱이다.",
      "df['col']은 열을 선택한다.",
      "특정 조건의 행을 추출할 수 없다.",
      "슬라이싱은 지원하지 않는다."
    ],
    "answer": "df['col']은 열을 선택한다.",
    "why": "대괄호 `[]` 안에 컬럼명을 넣으면 해당 Series(열)를 선택합니다.",
    "hint": "Column Selection",
    "difficulty": "medium",
    "id": "2047"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 인덱스 번호(0, 1, 2...)로 데이터에 접근하는 속성은?",
    "options": [
      "ix",
      "at",
      "loc",
      "iloc",
      "idx"
    ],
    "answer": "iloc",
    "why": "iloc stands for integer location.",
    "hint": "integer location",
    "difficulty": "medium",
    "id": "2048"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 선택 및 필터링 방법 49",
    "options": [
      "loc은 정수 위치 기반 인덱싱이다.",
      "iloc은 라벨(이름) 기반 인덱싱이다.",
      "df['col']은 열을 선택한다.",
      "특정 조건의 행을 추출할 수 없다.",
      "슬라이싱은 지원하지 않는다."
    ],
    "answer": "df['col']은 열을 선택한다.",
    "why": "대괄호 `[]` 안에 컬럼명을 넣으면 해당 Series(열)를 선택합니다.",
    "hint": "Column Selection",
    "difficulty": "medium",
    "id": "2049"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 인덱스 번호(0, 1, 2...)로 데이터에 접근하는 속성은?",
    "options": [
      "ix",
      "at",
      "loc",
      "iloc",
      "idx"
    ],
    "answer": "iloc",
    "why": "iloc stands for integer location.",
    "hint": "integer location",
    "difficulty": "medium",
    "id": "2050"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 선택 및 필터링 방법 51",
    "options": [
      "loc은 정수 위치 기반 인덱싱이다.",
      "iloc은 라벨(이름) 기반 인덱싱이다.",
      "df['col']은 열을 선택한다.",
      "특정 조건의 행을 추출할 수 없다.",
      "슬라이싱은 지원하지 않는다."
    ],
    "answer": "df['col']은 열을 선택한다.",
    "why": "대괄호 `[]` 안에 컬럼명을 넣으면 해당 Series(열)를 선택합니다.",
    "hint": "Column Selection",
    "difficulty": "medium",
    "id": "2051"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 인덱스 번호(0, 1, 2...)로 데이터에 접근하는 속성은?",
    "options": [
      "ix",
      "at",
      "loc",
      "iloc",
      "idx"
    ],
    "answer": "iloc",
    "why": "iloc stands for integer location.",
    "hint": "integer location",
    "difficulty": "medium",
    "id": "2052"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 선택 및 필터링 방법 53",
    "options": [
      "loc은 정수 위치 기반 인덱싱이다.",
      "iloc은 라벨(이름) 기반 인덱싱이다.",
      "df['col']은 열을 선택한다.",
      "특정 조건의 행을 추출할 수 없다.",
      "슬라이싱은 지원하지 않는다."
    ],
    "answer": "df['col']은 열을 선택한다.",
    "why": "대괄호 `[]` 안에 컬럼명을 넣으면 해당 Series(열)를 선택합니다.",
    "hint": "Column Selection",
    "difficulty": "medium",
    "id": "2053"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 인덱스 번호(0, 1, 2...)로 데이터에 접근하는 속성은?",
    "options": [
      "ix",
      "at",
      "loc",
      "iloc",
      "idx"
    ],
    "answer": "iloc",
    "why": "iloc stands for integer location.",
    "hint": "integer location",
    "difficulty": "medium",
    "id": "2054"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 선택 및 필터링 방법 55",
    "options": [
      "loc은 정수 위치 기반 인덱싱이다.",
      "iloc은 라벨(이름) 기반 인덱싱이다.",
      "df['col']은 열을 선택한다.",
      "특정 조건의 행을 추출할 수 없다.",
      "슬라이싱은 지원하지 않는다."
    ],
    "answer": "df['col']은 열을 선택한다.",
    "why": "대괄호 `[]` 안에 컬럼명을 넣으면 해당 Series(열)를 선택합니다.",
    "hint": "Column Selection",
    "difficulty": "medium",
    "id": "2055"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 인덱스 번호(0, 1, 2...)로 데이터에 접근하는 속성은?",
    "options": [
      "ix",
      "at",
      "loc",
      "iloc",
      "idx"
    ],
    "answer": "iloc",
    "why": "iloc stands for integer location.",
    "hint": "integer location",
    "difficulty": "medium",
    "id": "2056"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 선택 및 필터링 방법 57",
    "options": [
      "loc은 정수 위치 기반 인덱싱이다.",
      "iloc은 라벨(이름) 기반 인덱싱이다.",
      "df['col']은 열을 선택한다.",
      "특정 조건의 행을 추출할 수 없다.",
      "슬라이싱은 지원하지 않는다."
    ],
    "answer": "df['col']은 열을 선택한다.",
    "why": "대괄호 `[]` 안에 컬럼명을 넣으면 해당 Series(열)를 선택합니다.",
    "hint": "Column Selection",
    "difficulty": "medium",
    "id": "2057"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 인덱스 번호(0, 1, 2...)로 데이터에 접근하는 속성은?",
    "options": [
      "ix",
      "at",
      "loc",
      "iloc",
      "idx"
    ],
    "answer": "iloc",
    "why": "iloc stands for integer location.",
    "hint": "integer location",
    "difficulty": "medium",
    "id": "2058"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 선택 및 필터링 방법 59",
    "options": [
      "loc은 정수 위치 기반 인덱싱이다.",
      "iloc은 라벨(이름) 기반 인덱싱이다.",
      "df['col']은 열을 선택한다.",
      "특정 조건의 행을 추출할 수 없다.",
      "슬라이싱은 지원하지 않는다."
    ],
    "answer": "df['col']은 열을 선택한다.",
    "why": "대괄호 `[]` 안에 컬럼명을 넣으면 해당 Series(열)를 선택합니다.",
    "hint": "Column Selection",
    "difficulty": "medium",
    "id": "2059"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 인덱스 번호(0, 1, 2...)로 데이터에 접근하는 속성은?",
    "options": [
      "ix",
      "at",
      "loc",
      "iloc",
      "idx"
    ],
    "answer": "iloc",
    "why": "iloc stands for integer location.",
    "hint": "integer location",
    "difficulty": "medium",
    "id": "2060"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 전처리에 관한 내용 61",
    "options": [
      "결측치는 무조건 삭제해야 한다.",
      "fillna()는 결측치를 채우는 함수다.",
      "데이터 타입 변환은 불가능하다.",
      "중복 데이터는 분석에 도움이 되므로 유지한다.",
      "이상치는 항상 평균값으로 대체한다."
    ],
    "answer": "fillna()는 결측치를 채우는 함수다.",
    "why": "fillna() 메서드를 사용하여 NaN 값을 특정 값으로 대체할 수 있습니다.",
    "hint": "채우다(Fill)",
    "difficulty": "medium",
    "id": "2061"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 전처리에 관한 내용 62",
    "options": [
      "결측치는 무조건 삭제해야 한다.",
      "fillna()는 결측치를 채우는 함수다.",
      "데이터 타입 변환은 불가능하다.",
      "중복 데이터는 분석에 도움이 되므로 유지한다.",
      "이상치는 항상 평균값으로 대체한다."
    ],
    "answer": "fillna()는 결측치를 채우는 함수다.",
    "why": "fillna() 메서드를 사용하여 NaN 값을 특정 값으로 대체할 수 있습니다.",
    "hint": "채우다(Fill)",
    "difficulty": "medium",
    "id": "2062"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 전처리에 관한 내용 63",
    "options": [
      "결측치는 무조건 삭제해야 한다.",
      "fillna()는 결측치를 채우는 함수다.",
      "데이터 타입 변환은 불가능하다.",
      "중복 데이터는 분석에 도움이 되므로 유지한다.",
      "이상치는 항상 평균값으로 대체한다."
    ],
    "answer": "fillna()는 결측치를 채우는 함수다.",
    "why": "fillna() 메서드를 사용하여 NaN 값을 특정 값으로 대체할 수 있습니다.",
    "hint": "채우다(Fill)",
    "difficulty": "medium",
    "id": "2063"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 전처리에 관한 내용 64",
    "options": [
      "결측치는 무조건 삭제해야 한다.",
      "fillna()는 결측치를 채우는 함수다.",
      "데이터 타입 변환은 불가능하다.",
      "중복 데이터는 분석에 도움이 되므로 유지한다.",
      "이상치는 항상 평균값으로 대체한다."
    ],
    "answer": "fillna()는 결측치를 채우는 함수다.",
    "why": "fillna() 메서드를 사용하여 NaN 값을 특정 값으로 대체할 수 있습니다.",
    "hint": "채우다(Fill)",
    "difficulty": "medium",
    "id": "2064"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 전처리에 관한 내용 65",
    "options": [
      "결측치는 무조건 삭제해야 한다.",
      "fillna()는 결측치를 채우는 함수다.",
      "데이터 타입 변환은 불가능하다.",
      "중복 데이터는 분석에 도움이 되므로 유지한다.",
      "이상치는 항상 평균값으로 대체한다."
    ],
    "answer": "fillna()는 결측치를 채우는 함수다.",
    "why": "fillna() 메서드를 사용하여 NaN 값을 특정 값으로 대체할 수 있습니다.",
    "hint": "채우다(Fill)",
    "difficulty": "medium",
    "id": "2065"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 전처리에 관한 내용 66",
    "options": [
      "결측치는 무조건 삭제해야 한다.",
      "fillna()는 결측치를 채우는 함수다.",
      "데이터 타입 변환은 불가능하다.",
      "중복 데이터는 분석에 도움이 되므로 유지한다.",
      "이상치는 항상 평균값으로 대체한다."
    ],
    "answer": "fillna()는 결측치를 채우는 함수다.",
    "why": "fillna() 메서드를 사용하여 NaN 값을 특정 값으로 대체할 수 있습니다.",
    "hint": "채우다(Fill)",
    "difficulty": "medium",
    "id": "2066"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 전처리에 관한 내용 67",
    "options": [
      "결측치는 무조건 삭제해야 한다.",
      "fillna()는 결측치를 채우는 함수다.",
      "데이터 타입 변환은 불가능하다.",
      "중복 데이터는 분석에 도움이 되므로 유지한다.",
      "이상치는 항상 평균값으로 대체한다."
    ],
    "answer": "fillna()는 결측치를 채우는 함수다.",
    "why": "fillna() 메서드를 사용하여 NaN 값을 특정 값으로 대체할 수 있습니다.",
    "hint": "채우다(Fill)",
    "difficulty": "medium",
    "id": "2067"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 전처리에 관한 내용 68",
    "options": [
      "결측치는 무조건 삭제해야 한다.",
      "fillna()는 결측치를 채우는 함수다.",
      "데이터 타입 변환은 불가능하다.",
      "중복 데이터는 분석에 도움이 되므로 유지한다.",
      "이상치는 항상 평균값으로 대체한다."
    ],
    "answer": "fillna()는 결측치를 채우는 함수다.",
    "why": "fillna() 메서드를 사용하여 NaN 값을 특정 값으로 대체할 수 있습니다.",
    "hint": "채우다(Fill)",
    "difficulty": "medium",
    "id": "2068"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 전처리에 관한 내용 69",
    "options": [
      "결측치는 무조건 삭제해야 한다.",
      "fillna()는 결측치를 채우는 함수다.",
      "데이터 타입 변환은 불가능하다.",
      "중복 데이터는 분석에 도움이 되므로 유지한다.",
      "이상치는 항상 평균값으로 대체한다."
    ],
    "answer": "fillna()는 결측치를 채우는 함수다.",
    "why": "fillna() 메서드를 사용하여 NaN 값을 특정 값으로 대체할 수 있습니다.",
    "hint": "채우다(Fill)",
    "difficulty": "medium",
    "id": "2069"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 전처리에 관한 내용 70",
    "options": [
      "결측치는 무조건 삭제해야 한다.",
      "fillna()는 결측치를 채우는 함수다.",
      "데이터 타입 변환은 불가능하다.",
      "중복 데이터는 분석에 도움이 되므로 유지한다.",
      "이상치는 항상 평균값으로 대체한다."
    ],
    "answer": "fillna()는 결측치를 채우는 함수다.",
    "why": "fillna() 메서드를 사용하여 NaN 값을 특정 값으로 대체할 수 있습니다.",
    "hint": "채우다(Fill)",
    "difficulty": "medium",
    "id": "2070"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 전처리에 관한 내용 71",
    "options": [
      "결측치는 무조건 삭제해야 한다.",
      "fillna()는 결측치를 채우는 함수다.",
      "데이터 타입 변환은 불가능하다.",
      "중복 데이터는 분석에 도움이 되므로 유지한다.",
      "이상치는 항상 평균값으로 대체한다."
    ],
    "answer": "fillna()는 결측치를 채우는 함수다.",
    "why": "fillna() 메서드를 사용하여 NaN 값을 특정 값으로 대체할 수 있습니다.",
    "hint": "채우다(Fill)",
    "difficulty": "medium",
    "id": "2071"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 전처리에 관한 내용 72",
    "options": [
      "결측치는 무조건 삭제해야 한다.",
      "fillna()는 결측치를 채우는 함수다.",
      "데이터 타입 변환은 불가능하다.",
      "중복 데이터는 분석에 도움이 되므로 유지한다.",
      "이상치는 항상 평균값으로 대체한다."
    ],
    "answer": "fillna()는 결측치를 채우는 함수다.",
    "why": "fillna() 메서드를 사용하여 NaN 값을 특정 값으로 대체할 수 있습니다.",
    "hint": "채우다(Fill)",
    "difficulty": "medium",
    "id": "2072"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 전처리에 관한 내용 73",
    "options": [
      "결측치는 무조건 삭제해야 한다.",
      "fillna()는 결측치를 채우는 함수다.",
      "데이터 타입 변환은 불가능하다.",
      "중복 데이터는 분석에 도움이 되므로 유지한다.",
      "이상치는 항상 평균값으로 대체한다."
    ],
    "answer": "fillna()는 결측치를 채우는 함수다.",
    "why": "fillna() 메서드를 사용하여 NaN 값을 특정 값으로 대체할 수 있습니다.",
    "hint": "채우다(Fill)",
    "difficulty": "medium",
    "id": "2073"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 전처리에 관한 내용 74",
    "options": [
      "결측치는 무조건 삭제해야 한다.",
      "fillna()는 결측치를 채우는 함수다.",
      "데이터 타입 변환은 불가능하다.",
      "중복 데이터는 분석에 도움이 되므로 유지한다.",
      "이상치는 항상 평균값으로 대체한다."
    ],
    "answer": "fillna()는 결측치를 채우는 함수다.",
    "why": "fillna() 메서드를 사용하여 NaN 값을 특정 값으로 대체할 수 있습니다.",
    "hint": "채우다(Fill)",
    "difficulty": "medium",
    "id": "2074"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 전처리에 관한 내용 75",
    "options": [
      "결측치는 무조건 삭제해야 한다.",
      "fillna()는 결측치를 채우는 함수다.",
      "데이터 타입 변환은 불가능하다.",
      "중복 데이터는 분석에 도움이 되므로 유지한다.",
      "이상치는 항상 평균값으로 대체한다."
    ],
    "answer": "fillna()는 결측치를 채우는 함수다.",
    "why": "fillna() 메서드를 사용하여 NaN 값을 특정 값으로 대체할 수 있습니다.",
    "hint": "채우다(Fill)",
    "difficulty": "medium",
    "id": "2075"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 전처리에 관한 내용 76",
    "options": [
      "결측치는 무조건 삭제해야 한다.",
      "fillna()는 결측치를 채우는 함수다.",
      "데이터 타입 변환은 불가능하다.",
      "중복 데이터는 분석에 도움이 되므로 유지한다.",
      "이상치는 항상 평균값으로 대체한다."
    ],
    "answer": "fillna()는 결측치를 채우는 함수다.",
    "why": "fillna() 메서드를 사용하여 NaN 값을 특정 값으로 대체할 수 있습니다.",
    "hint": "채우다(Fill)",
    "difficulty": "medium",
    "id": "2076"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 전처리에 관한 내용 77",
    "options": [
      "결측치는 무조건 삭제해야 한다.",
      "fillna()는 결측치를 채우는 함수다.",
      "데이터 타입 변환은 불가능하다.",
      "중복 데이터는 분석에 도움이 되므로 유지한다.",
      "이상치는 항상 평균값으로 대체한다."
    ],
    "answer": "fillna()는 결측치를 채우는 함수다.",
    "why": "fillna() 메서드를 사용하여 NaN 값을 특정 값으로 대체할 수 있습니다.",
    "hint": "채우다(Fill)",
    "difficulty": "medium",
    "id": "2077"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 전처리에 관한 내용 78",
    "options": [
      "결측치는 무조건 삭제해야 한다.",
      "fillna()는 결측치를 채우는 함수다.",
      "데이터 타입 변환은 불가능하다.",
      "중복 데이터는 분석에 도움이 되므로 유지한다.",
      "이상치는 항상 평균값으로 대체한다."
    ],
    "answer": "fillna()는 결측치를 채우는 함수다.",
    "why": "fillna() 메서드를 사용하여 NaN 값을 특정 값으로 대체할 수 있습니다.",
    "hint": "채우다(Fill)",
    "difficulty": "medium",
    "id": "2078"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 전처리에 관한 내용 79",
    "options": [
      "결측치는 무조건 삭제해야 한다.",
      "fillna()는 결측치를 채우는 함수다.",
      "데이터 타입 변환은 불가능하다.",
      "중복 데이터는 분석에 도움이 되므로 유지한다.",
      "이상치는 항상 평균값으로 대체한다."
    ],
    "answer": "fillna()는 결측치를 채우는 함수다.",
    "why": "fillna() 메서드를 사용하여 NaN 값을 특정 값으로 대체할 수 있습니다.",
    "hint": "채우다(Fill)",
    "difficulty": "medium",
    "id": "2079"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 전처리에 관한 내용 80",
    "options": [
      "결측치는 무조건 삭제해야 한다.",
      "fillna()는 결측치를 채우는 함수다.",
      "데이터 타입 변환은 불가능하다.",
      "중복 데이터는 분석에 도움이 되므로 유지한다.",
      "이상치는 항상 평균값으로 대체한다."
    ],
    "answer": "fillna()는 결측치를 채우는 함수다.",
    "why": "fillna() 메서드를 사용하여 NaN 값을 특정 값으로 대체할 수 있습니다.",
    "hint": "채우다(Fill)",
    "difficulty": "medium",
    "id": "2080"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "고급 데이터 처리 및 시각화 81",
    "options": [
      "groupby는 데이터를 그룹별로 나눈다.",
      "merge는 데이터를 위아래로만 합친다.",
      "concat은 두 테이블을 Key 기준으로 병합한다.",
      "matplotlib은 3D 그래프를 그릴 수 없다.",
      "seaborn은 pandas와 호환되지 않는다."
    ],
    "answer": "groupby는 데이터를 그룹별로 나눈다.",
    "why": "groupby()는 특정 컬럼의 값을 기준으로 데이터를 그룹핑하여 집계 연산을 수행합니다.",
    "hint": "Group By",
    "difficulty": "hard",
    "id": "2081"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "고급 데이터 처리 및 시각화 82",
    "options": [
      "groupby는 데이터를 그룹별로 나눈다.",
      "merge는 데이터를 위아래로만 합친다.",
      "concat은 두 테이블을 Key 기준으로 병합한다.",
      "matplotlib은 3D 그래프를 그릴 수 없다.",
      "seaborn은 pandas와 호환되지 않는다."
    ],
    "answer": "groupby는 데이터를 그룹별로 나눈다.",
    "why": "groupby()는 특정 컬럼의 값을 기준으로 데이터를 그룹핑하여 집계 연산을 수행합니다.",
    "hint": "Group By",
    "difficulty": "hard",
    "id": "2082"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "고급 데이터 처리 및 시각화 83",
    "options": [
      "groupby는 데이터를 그룹별로 나눈다.",
      "merge는 데이터를 위아래로만 합친다.",
      "concat은 두 테이블을 Key 기준으로 병합한다.",
      "matplotlib은 3D 그래프를 그릴 수 없다.",
      "seaborn은 pandas와 호환되지 않는다."
    ],
    "answer": "groupby는 데이터를 그룹별로 나눈다.",
    "why": "groupby()는 특정 컬럼의 값을 기준으로 데이터를 그룹핑하여 집계 연산을 수행합니다.",
    "hint": "Group By",
    "difficulty": "hard",
    "id": "2083"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "고급 데이터 처리 및 시각화 84",
    "options": [
      "groupby는 데이터를 그룹별로 나눈다.",
      "merge는 데이터를 위아래로만 합친다.",
      "concat은 두 테이블을 Key 기준으로 병합한다.",
      "matplotlib은 3D 그래프를 그릴 수 없다.",
      "seaborn은 pandas와 호환되지 않는다."
    ],
    "answer": "groupby는 데이터를 그룹별로 나눈다.",
    "why": "groupby()는 특정 컬럼의 값을 기준으로 데이터를 그룹핑하여 집계 연산을 수행합니다.",
    "hint": "Group By",
    "difficulty": "hard",
    "id": "2084"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "고급 데이터 처리 및 시각화 85",
    "options": [
      "groupby는 데이터를 그룹별로 나눈다.",
      "merge는 데이터를 위아래로만 합친다.",
      "concat은 두 테이블을 Key 기준으로 병합한다.",
      "matplotlib은 3D 그래프를 그릴 수 없다.",
      "seaborn은 pandas와 호환되지 않는다."
    ],
    "answer": "groupby는 데이터를 그룹별로 나눈다.",
    "why": "groupby()는 특정 컬럼의 값을 기준으로 데이터를 그룹핑하여 집계 연산을 수행합니다.",
    "hint": "Group By",
    "difficulty": "hard",
    "id": "2085"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "고급 데이터 처리 및 시각화 86",
    "options": [
      "groupby는 데이터를 그룹별로 나눈다.",
      "merge는 데이터를 위아래로만 합친다.",
      "concat은 두 테이블을 Key 기준으로 병합한다.",
      "matplotlib은 3D 그래프를 그릴 수 없다.",
      "seaborn은 pandas와 호환되지 않는다."
    ],
    "answer": "groupby는 데이터를 그룹별로 나눈다.",
    "why": "groupby()는 특정 컬럼의 값을 기준으로 데이터를 그룹핑하여 집계 연산을 수행합니다.",
    "hint": "Group By",
    "difficulty": "hard",
    "id": "2086"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "고급 데이터 처리 및 시각화 87",
    "options": [
      "groupby는 데이터를 그룹별로 나눈다.",
      "merge는 데이터를 위아래로만 합친다.",
      "concat은 두 테이블을 Key 기준으로 병합한다.",
      "matplotlib은 3D 그래프를 그릴 수 없다.",
      "seaborn은 pandas와 호환되지 않는다."
    ],
    "answer": "groupby는 데이터를 그룹별로 나눈다.",
    "why": "groupby()는 특정 컬럼의 값을 기준으로 데이터를 그룹핑하여 집계 연산을 수행합니다.",
    "hint": "Group By",
    "difficulty": "hard",
    "id": "2087"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "고급 데이터 처리 및 시각화 88",
    "options": [
      "groupby는 데이터를 그룹별로 나눈다.",
      "merge는 데이터를 위아래로만 합친다.",
      "concat은 두 테이블을 Key 기준으로 병합한다.",
      "matplotlib은 3D 그래프를 그릴 수 없다.",
      "seaborn은 pandas와 호환되지 않는다."
    ],
    "answer": "groupby는 데이터를 그룹별로 나눈다.",
    "why": "groupby()는 특정 컬럼의 값을 기준으로 데이터를 그룹핑하여 집계 연산을 수행합니다.",
    "hint": "Group By",
    "difficulty": "hard",
    "id": "2088"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "고급 데이터 처리 및 시각화 89",
    "options": [
      "groupby는 데이터를 그룹별로 나눈다.",
      "merge는 데이터를 위아래로만 합친다.",
      "concat은 두 테이블을 Key 기준으로 병합한다.",
      "matplotlib은 3D 그래프를 그릴 수 없다.",
      "seaborn은 pandas와 호환되지 않는다."
    ],
    "answer": "groupby는 데이터를 그룹별로 나눈다.",
    "why": "groupby()는 특정 컬럼의 값을 기준으로 데이터를 그룹핑하여 집계 연산을 수행합니다.",
    "hint": "Group By",
    "difficulty": "hard",
    "id": "2089"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "고급 데이터 처리 및 시각화 90",
    "options": [
      "groupby는 데이터를 그룹별로 나눈다.",
      "merge는 데이터를 위아래로만 합친다.",
      "concat은 두 테이블을 Key 기준으로 병합한다.",
      "matplotlib은 3D 그래프를 그릴 수 없다.",
      "seaborn은 pandas와 호환되지 않는다."
    ],
    "answer": "groupby는 데이터를 그룹별로 나눈다.",
    "why": "groupby()는 특정 컬럼의 값을 기준으로 데이터를 그룹핑하여 집계 연산을 수행합니다.",
    "hint": "Group By",
    "difficulty": "hard",
    "id": "2090"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "고급 데이터 처리 및 시각화 91",
    "options": [
      "groupby는 데이터를 그룹별로 나눈다.",
      "merge는 데이터를 위아래로만 합친다.",
      "concat은 두 테이블을 Key 기준으로 병합한다.",
      "matplotlib은 3D 그래프를 그릴 수 없다.",
      "seaborn은 pandas와 호환되지 않는다."
    ],
    "answer": "groupby는 데이터를 그룹별로 나눈다.",
    "why": "groupby()는 특정 컬럼의 값을 기준으로 데이터를 그룹핑하여 집계 연산을 수행합니다.",
    "hint": "Group By",
    "difficulty": "hard",
    "id": "2091"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "고급 데이터 처리 및 시각화 92",
    "options": [
      "groupby는 데이터를 그룹별로 나눈다.",
      "merge는 데이터를 위아래로만 합친다.",
      "concat은 두 테이블을 Key 기준으로 병합한다.",
      "matplotlib은 3D 그래프를 그릴 수 없다.",
      "seaborn은 pandas와 호환되지 않는다."
    ],
    "answer": "groupby는 데이터를 그룹별로 나눈다.",
    "why": "groupby()는 특정 컬럼의 값을 기준으로 데이터를 그룹핑하여 집계 연산을 수행합니다.",
    "hint": "Group By",
    "difficulty": "hard",
    "id": "2092"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "고급 데이터 처리 및 시각화 93",
    "options": [
      "groupby는 데이터를 그룹별로 나눈다.",
      "merge는 데이터를 위아래로만 합친다.",
      "concat은 두 테이블을 Key 기준으로 병합한다.",
      "matplotlib은 3D 그래프를 그릴 수 없다.",
      "seaborn은 pandas와 호환되지 않는다."
    ],
    "answer": "groupby는 데이터를 그룹별로 나눈다.",
    "why": "groupby()는 특정 컬럼의 값을 기준으로 데이터를 그룹핑하여 집계 연산을 수행합니다.",
    "hint": "Group By",
    "difficulty": "hard",
    "id": "2093"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "고급 데이터 처리 및 시각화 94",
    "options": [
      "groupby는 데이터를 그룹별로 나눈다.",
      "merge는 데이터를 위아래로만 합친다.",
      "concat은 두 테이블을 Key 기준으로 병합한다.",
      "matplotlib은 3D 그래프를 그릴 수 없다.",
      "seaborn은 pandas와 호환되지 않는다."
    ],
    "answer": "groupby는 데이터를 그룹별로 나눈다.",
    "why": "groupby()는 특정 컬럼의 값을 기준으로 데이터를 그룹핑하여 집계 연산을 수행합니다.",
    "hint": "Group By",
    "difficulty": "hard",
    "id": "2094"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "고급 데이터 처리 및 시각화 95",
    "options": [
      "groupby는 데이터를 그룹별로 나눈다.",
      "merge는 데이터를 위아래로만 합친다.",
      "concat은 두 테이블을 Key 기준으로 병합한다.",
      "matplotlib은 3D 그래프를 그릴 수 없다.",
      "seaborn은 pandas와 호환되지 않는다."
    ],
    "answer": "groupby는 데이터를 그룹별로 나눈다.",
    "why": "groupby()는 특정 컬럼의 값을 기준으로 데이터를 그룹핑하여 집계 연산을 수행합니다.",
    "hint": "Group By",
    "difficulty": "hard",
    "id": "2095"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "고급 데이터 처리 및 시각화 96",
    "options": [
      "groupby는 데이터를 그룹별로 나눈다.",
      "merge는 데이터를 위아래로만 합친다.",
      "concat은 두 테이블을 Key 기준으로 병합한다.",
      "matplotlib은 3D 그래프를 그릴 수 없다.",
      "seaborn은 pandas와 호환되지 않는다."
    ],
    "answer": "groupby는 데이터를 그룹별로 나눈다.",
    "why": "groupby()는 특정 컬럼의 값을 기준으로 데이터를 그룹핑하여 집계 연산을 수행합니다.",
    "hint": "Group By",
    "difficulty": "hard",
    "id": "2096"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "고급 데이터 처리 및 시각화 97",
    "options": [
      "groupby는 데이터를 그룹별로 나눈다.",
      "merge는 데이터를 위아래로만 합친다.",
      "concat은 두 테이블을 Key 기준으로 병합한다.",
      "matplotlib은 3D 그래프를 그릴 수 없다.",
      "seaborn은 pandas와 호환되지 않는다."
    ],
    "answer": "groupby는 데이터를 그룹별로 나눈다.",
    "why": "groupby()는 특정 컬럼의 값을 기준으로 데이터를 그룹핑하여 집계 연산을 수행합니다.",
    "hint": "Group By",
    "difficulty": "hard",
    "id": "2097"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "고급 데이터 처리 및 시각화 98",
    "options": [
      "groupby는 데이터를 그룹별로 나눈다.",
      "merge는 데이터를 위아래로만 합친다.",
      "concat은 두 테이블을 Key 기준으로 병합한다.",
      "matplotlib은 3D 그래프를 그릴 수 없다.",
      "seaborn은 pandas와 호환되지 않는다."
    ],
    "answer": "groupby는 데이터를 그룹별로 나눈다.",
    "why": "groupby()는 특정 컬럼의 값을 기준으로 데이터를 그룹핑하여 집계 연산을 수행합니다.",
    "hint": "Group By",
    "difficulty": "hard",
    "id": "2098"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "고급 데이터 처리 및 시각화 99",
    "options": [
      "groupby는 데이터를 그룹별로 나눈다.",
      "merge는 데이터를 위아래로만 합친다.",
      "concat은 두 테이블을 Key 기준으로 병합한다.",
      "matplotlib은 3D 그래프를 그릴 수 없다.",
      "seaborn은 pandas와 호환되지 않는다."
    ],
    "answer": "groupby는 데이터를 그룹별로 나눈다.",
    "why": "groupby()는 특정 컬럼의 값을 기준으로 데이터를 그룹핑하여 집계 연산을 수행합니다.",
    "hint": "Group By",
    "difficulty": "hard",
    "id": "2099"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "고급 데이터 처리 및 시각화 100",
    "options": [
      "groupby는 데이터를 그룹별로 나눈다.",
      "merge는 데이터를 위아래로만 합친다.",
      "concat은 두 테이블을 Key 기준으로 병합한다.",
      "matplotlib은 3D 그래프를 그릴 수 없다.",
      "seaborn은 pandas와 호환되지 않는다."
    ],
    "answer": "groupby는 데이터를 그룹별로 나눈다.",
    "why": "groupby()는 특정 컬럼의 값을 기준으로 데이터를 그룹핑하여 집계 연산을 수행합니다.",
    "hint": "Group By",
    "difficulty": "hard",
    "id": "20100"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "처음 5개 행을 출력하세요.\n```python\ndf.____()\n```",
    "answer": "head",
    "why": "상위 행 출력은 head()입니다.",
    "hint": "pandas alias",
    "difficulty": "easy",
    "id": "2101"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "결측치를 0으로 채우세요.\n```python\ndf.____(0)\n```",
    "answer": "fillna",
    "why": "결측치 채우기는 fillna입니다.",
    "hint": "pandas alias",
    "difficulty": "easy",
    "id": "2102"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "기초 통계량을 확인하세요.\n```python\ndf.____()\n```",
    "answer": "describe",
    "why": "통계 요약은 describe()입니다.",
    "hint": "pandas alias",
    "difficulty": "easy",
    "id": "2103"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "CSV 파일을 읽어오세요.\n```python\nimport pandas as pd\ndf = pd.____('data.csv')\n```",
    "answer": "read_csv",
    "why": "파일 읽기 함수는 read_csv입니다.",
    "hint": "pandas alias",
    "difficulty": "easy",
    "id": "2104"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "처음 5개 행을 출력하세요.\n```python\ndf.____()\n```",
    "answer": "head",
    "why": "상위 행 출력은 head()입니다.",
    "hint": "pandas alias",
    "difficulty": "easy",
    "id": "2105"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "결측치를 0으로 채우세요.\n```python\ndf.____(0)\n```",
    "answer": "fillna",
    "why": "결측치 채우기는 fillna입니다.",
    "hint": "pandas alias",
    "difficulty": "easy",
    "id": "2106"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "기초 통계량을 확인하세요.\n```python\ndf.____()\n```",
    "answer": "describe",
    "why": "통계 요약은 describe()입니다.",
    "hint": "pandas alias",
    "difficulty": "easy",
    "id": "2107"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "CSV 파일을 읽어오세요.\n```python\nimport pandas as pd\ndf = pd.____('data.csv')\n```",
    "answer": "read_csv",
    "why": "파일 읽기 함수는 read_csv입니다.",
    "hint": "pandas alias",
    "difficulty": "easy",
    "id": "2108"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "처음 5개 행을 출력하세요.\n```python\ndf.____()\n```",
    "answer": "head",
    "why": "상위 행 출력은 head()입니다.",
    "hint": "pandas alias",
    "difficulty": "easy",
    "id": "2109"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "결측치를 0으로 채우세요.\n```python\ndf.____(0)\n```",
    "answer": "fillna",
    "why": "결측치 채우기는 fillna입니다.",
    "hint": "pandas alias",
    "difficulty": "easy",
    "id": "2110"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "기초 통계량을 확인하세요.\n```python\ndf.____()\n```",
    "answer": "describe",
    "why": "통계 요약은 describe()입니다.",
    "hint": "pandas alias",
    "difficulty": "easy",
    "id": "2111"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "CSV 파일을 읽어오세요.\n```python\nimport pandas as pd\ndf = pd.____('data.csv')\n```",
    "answer": "read_csv",
    "why": "파일 읽기 함수는 read_csv입니다.",
    "hint": "pandas alias",
    "difficulty": "easy",
    "id": "2112"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "처음 5개 행을 출력하세요.\n```python\ndf.____()\n```",
    "answer": "head",
    "why": "상위 행 출력은 head()입니다.",
    "hint": "pandas alias",
    "difficulty": "easy",
    "id": "2113"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "결측치를 0으로 채우세요.\n```python\ndf.____(0)\n```",
    "answer": "fillna",
    "why": "결측치 채우기는 fillna입니다.",
    "hint": "pandas alias",
    "difficulty": "easy",
    "id": "2114"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "기초 통계량을 확인하세요.\n```python\ndf.____()\n```",
    "answer": "describe",
    "why": "통계 요약은 describe()입니다.",
    "hint": "pandas alias",
    "difficulty": "easy",
    "id": "2115"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "CSV 파일을 읽어오세요.\n```python\nimport pandas as pd\ndf = pd.____('data.csv')\n```",
    "answer": "read_csv",
    "why": "파일 읽기 함수는 read_csv입니다.",
    "hint": "pandas alias",
    "difficulty": "easy",
    "id": "2116"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "처음 5개 행을 출력하세요.\n```python\ndf.____()\n```",
    "answer": "head",
    "why": "상위 행 출력은 head()입니다.",
    "hint": "pandas alias",
    "difficulty": "easy",
    "id": "2117"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "결측치를 0으로 채우세요.\n```python\ndf.____(0)\n```",
    "answer": "fillna",
    "why": "결측치 채우기는 fillna입니다.",
    "hint": "pandas alias",
    "difficulty": "easy",
    "id": "2118"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "기초 통계량을 확인하세요.\n```python\ndf.____()\n```",
    "answer": "describe",
    "why": "통계 요약은 describe()입니다.",
    "hint": "pandas alias",
    "difficulty": "easy",
    "id": "2119"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "CSV 파일을 읽어오세요.\n```python\nimport pandas as pd\ndf = pd.____('data.csv')\n```",
    "answer": "read_csv",
    "why": "파일 읽기 함수는 read_csv입니다.",
    "hint": "pandas alias",
    "difficulty": "easy",
    "id": "2120"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "Transformer 아키텍처의 특징으로 올바른 설명 1?",
    "options": [
      "순차적으로 데이터를 처리해야 한다(RNN 방식).",
      "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
      "이미지 처리에만 특화된 모델이다.",
      "인코더 없이 디코더만 결합된 구조가 원조다.",
      "장기 의존성(Long-term dependency) 문제를 해결하지 못했다."
    ],
    "answer": "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
    "why": "Transformer는 Self-Attention을 사용하여 문장 전체를 한 번에(병렬로) 처리하며, RNN의 순차적 처리 한계를 극복했습니다.",
    "hint": "Attention Is All You Need",
    "difficulty": "medium",
    "id": "3001"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "Transformer 아키텍처의 특징으로 올바른 설명 2?",
    "options": [
      "순차적으로 데이터를 처리해야 한다(RNN 방식).",
      "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
      "이미지 처리에만 특화된 모델이다.",
      "인코더 없이 디코더만 결합된 구조가 원조다.",
      "장기 의존성(Long-term dependency) 문제를 해결하지 못했다."
    ],
    "answer": "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
    "why": "Transformer는 Self-Attention을 사용하여 문장 전체를 한 번에(병렬로) 처리하며, RNN의 순차적 처리 한계를 극복했습니다.",
    "hint": "Attention Is All You Need",
    "difficulty": "medium",
    "id": "3002"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "Transformer의 핵심 구성 요소인 'Attention'의 역할은?",
    "options": [
      "입력 데이터를 압축하여 손실을 유도한다.",
      "중요한 단어에 가중치를 부여하여 문맥을 파악한다.",
      "무작위 노이즈를 생성하여 창의성을 높인다.",
      "이미지를 텍스트로 변환한다.",
      "데이터를 순서대로 정렬한다."
    ],
    "answer": "중요한 단어에 가중치를 부여하여 문맥을 파악한다.",
    "why": "Attention은 문장 내의 단어들 사이의 연관성을 계산하여 중요한 정보에 집중하게 합니다.",
    "hint": "Attention Is All You Need",
    "difficulty": "medium",
    "id": "3003"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "Transformer 아키텍처의 특징으로 올바른 설명 4?",
    "options": [
      "순차적으로 데이터를 처리해야 한다(RNN 방식).",
      "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
      "이미지 처리에만 특화된 모델이다.",
      "인코더 없이 디코더만 결합된 구조가 원조다.",
      "장기 의존성(Long-term dependency) 문제를 해결하지 못했다."
    ],
    "answer": "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
    "why": "Transformer는 Self-Attention을 사용하여 문장 전체를 한 번에(병렬로) 처리하며, RNN의 순차적 처리 한계를 극복했습니다.",
    "hint": "Attention Is All You Need",
    "difficulty": "medium",
    "id": "3004"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "Transformer 아키텍처의 특징으로 올바른 설명 5?",
    "options": [
      "순차적으로 데이터를 처리해야 한다(RNN 방식).",
      "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
      "이미지 처리에만 특화된 모델이다.",
      "인코더 없이 디코더만 결합된 구조가 원조다.",
      "장기 의존성(Long-term dependency) 문제를 해결하지 못했다."
    ],
    "answer": "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
    "why": "Transformer는 Self-Attention을 사용하여 문장 전체를 한 번에(병렬로) 처리하며, RNN의 순차적 처리 한계를 극복했습니다.",
    "hint": "Attention Is All You Need",
    "difficulty": "medium",
    "id": "3005"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "Transformer의 핵심 구성 요소인 'Attention'의 역할은?",
    "options": [
      "입력 데이터를 압축하여 손실을 유도한다.",
      "중요한 단어에 가중치를 부여하여 문맥을 파악한다.",
      "무작위 노이즈를 생성하여 창의성을 높인다.",
      "이미지를 텍스트로 변환한다.",
      "데이터를 순서대로 정렬한다."
    ],
    "answer": "중요한 단어에 가중치를 부여하여 문맥을 파악한다.",
    "why": "Attention은 문장 내의 단어들 사이의 연관성을 계산하여 중요한 정보에 집중하게 합니다.",
    "hint": "Attention Is All You Need",
    "difficulty": "medium",
    "id": "3006"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "Transformer 아키텍처의 특징으로 올바른 설명 7?",
    "options": [
      "순차적으로 데이터를 처리해야 한다(RNN 방식).",
      "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
      "이미지 처리에만 특화된 모델이다.",
      "인코더 없이 디코더만 결합된 구조가 원조다.",
      "장기 의존성(Long-term dependency) 문제를 해결하지 못했다."
    ],
    "answer": "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
    "why": "Transformer는 Self-Attention을 사용하여 문장 전체를 한 번에(병렬로) 처리하며, RNN의 순차적 처리 한계를 극복했습니다.",
    "hint": "Attention Is All You Need",
    "difficulty": "medium",
    "id": "3007"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "Transformer 아키텍처의 특징으로 올바른 설명 8?",
    "options": [
      "순차적으로 데이터를 처리해야 한다(RNN 방식).",
      "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
      "이미지 처리에만 특화된 모델이다.",
      "인코더 없이 디코더만 결합된 구조가 원조다.",
      "장기 의존성(Long-term dependency) 문제를 해결하지 못했다."
    ],
    "answer": "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
    "why": "Transformer는 Self-Attention을 사용하여 문장 전체를 한 번에(병렬로) 처리하며, RNN의 순차적 처리 한계를 극복했습니다.",
    "hint": "Attention Is All You Need",
    "difficulty": "medium",
    "id": "3008"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "Transformer의 핵심 구성 요소인 'Attention'의 역할은?",
    "options": [
      "입력 데이터를 압축하여 손실을 유도한다.",
      "중요한 단어에 가중치를 부여하여 문맥을 파악한다.",
      "무작위 노이즈를 생성하여 창의성을 높인다.",
      "이미지를 텍스트로 변환한다.",
      "데이터를 순서대로 정렬한다."
    ],
    "answer": "중요한 단어에 가중치를 부여하여 문맥을 파악한다.",
    "why": "Attention은 문장 내의 단어들 사이의 연관성을 계산하여 중요한 정보에 집중하게 합니다.",
    "hint": "Attention Is All You Need",
    "difficulty": "medium",
    "id": "3009"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "Transformer 아키텍처의 특징으로 올바른 설명 10?",
    "options": [
      "순차적으로 데이터를 처리해야 한다(RNN 방식).",
      "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
      "이미지 처리에만 특화된 모델이다.",
      "인코더 없이 디코더만 결합된 구조가 원조다.",
      "장기 의존성(Long-term dependency) 문제를 해결하지 못했다."
    ],
    "answer": "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
    "why": "Transformer는 Self-Attention을 사용하여 문장 전체를 한 번에(병렬로) 처리하며, RNN의 순차적 처리 한계를 극복했습니다.",
    "hint": "Attention Is All You Need",
    "difficulty": "medium",
    "id": "3010"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "Transformer 아키텍처의 특징으로 올바른 설명 11?",
    "options": [
      "순차적으로 데이터를 처리해야 한다(RNN 방식).",
      "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
      "이미지 처리에만 특화된 모델이다.",
      "인코더 없이 디코더만 결합된 구조가 원조다.",
      "장기 의존성(Long-term dependency) 문제를 해결하지 못했다."
    ],
    "answer": "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
    "why": "Transformer는 Self-Attention을 사용하여 문장 전체를 한 번에(병렬로) 처리하며, RNN의 순차적 처리 한계를 극복했습니다.",
    "hint": "Attention Is All You Need",
    "difficulty": "medium",
    "id": "3011"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "Transformer의 핵심 구성 요소인 'Attention'의 역할은?",
    "options": [
      "입력 데이터를 압축하여 손실을 유도한다.",
      "중요한 단어에 가중치를 부여하여 문맥을 파악한다.",
      "무작위 노이즈를 생성하여 창의성을 높인다.",
      "이미지를 텍스트로 변환한다.",
      "데이터를 순서대로 정렬한다."
    ],
    "answer": "중요한 단어에 가중치를 부여하여 문맥을 파악한다.",
    "why": "Attention은 문장 내의 단어들 사이의 연관성을 계산하여 중요한 정보에 집중하게 합니다.",
    "hint": "Attention Is All You Need",
    "difficulty": "medium",
    "id": "3012"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "Transformer 아키텍처의 특징으로 올바른 설명 13?",
    "options": [
      "순차적으로 데이터를 처리해야 한다(RNN 방식).",
      "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
      "이미지 처리에만 특화된 모델이다.",
      "인코더 없이 디코더만 결합된 구조가 원조다.",
      "장기 의존성(Long-term dependency) 문제를 해결하지 못했다."
    ],
    "answer": "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
    "why": "Transformer는 Self-Attention을 사용하여 문장 전체를 한 번에(병렬로) 처리하며, RNN의 순차적 처리 한계를 극복했습니다.",
    "hint": "Attention Is All You Need",
    "difficulty": "medium",
    "id": "3013"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "Transformer 아키텍처의 특징으로 올바른 설명 14?",
    "options": [
      "순차적으로 데이터를 처리해야 한다(RNN 방식).",
      "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
      "이미지 처리에만 특화된 모델이다.",
      "인코더 없이 디코더만 결합된 구조가 원조다.",
      "장기 의존성(Long-term dependency) 문제를 해결하지 못했다."
    ],
    "answer": "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
    "why": "Transformer는 Self-Attention을 사용하여 문장 전체를 한 번에(병렬로) 처리하며, RNN의 순차적 처리 한계를 극복했습니다.",
    "hint": "Attention Is All You Need",
    "difficulty": "medium",
    "id": "3014"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "Transformer의 핵심 구성 요소인 'Attention'의 역할은?",
    "options": [
      "입력 데이터를 압축하여 손실을 유도한다.",
      "중요한 단어에 가중치를 부여하여 문맥을 파악한다.",
      "무작위 노이즈를 생성하여 창의성을 높인다.",
      "이미지를 텍스트로 변환한다.",
      "데이터를 순서대로 정렬한다."
    ],
    "answer": "중요한 단어에 가중치를 부여하여 문맥을 파악한다.",
    "why": "Attention은 문장 내의 단어들 사이의 연관성을 계산하여 중요한 정보에 집중하게 합니다.",
    "hint": "Attention Is All You Need",
    "difficulty": "medium",
    "id": "3015"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "Transformer 아키텍처의 특징으로 올바른 설명 16?",
    "options": [
      "순차적으로 데이터를 처리해야 한다(RNN 방식).",
      "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
      "이미지 처리에만 특화된 모델이다.",
      "인코더 없이 디코더만 결합된 구조가 원조다.",
      "장기 의존성(Long-term dependency) 문제를 해결하지 못했다."
    ],
    "answer": "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
    "why": "Transformer는 Self-Attention을 사용하여 문장 전체를 한 번에(병렬로) 처리하며, RNN의 순차적 처리 한계를 극복했습니다.",
    "hint": "Attention Is All You Need",
    "difficulty": "medium",
    "id": "3016"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "Transformer 아키텍처의 특징으로 올바른 설명 17?",
    "options": [
      "순차적으로 데이터를 처리해야 한다(RNN 방식).",
      "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
      "이미지 처리에만 특화된 모델이다.",
      "인코더 없이 디코더만 결합된 구조가 원조다.",
      "장기 의존성(Long-term dependency) 문제를 해결하지 못했다."
    ],
    "answer": "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
    "why": "Transformer는 Self-Attention을 사용하여 문장 전체를 한 번에(병렬로) 처리하며, RNN의 순차적 처리 한계를 극복했습니다.",
    "hint": "Attention Is All You Need",
    "difficulty": "medium",
    "id": "3017"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "Transformer의 핵심 구성 요소인 'Attention'의 역할은?",
    "options": [
      "입력 데이터를 압축하여 손실을 유도한다.",
      "중요한 단어에 가중치를 부여하여 문맥을 파악한다.",
      "무작위 노이즈를 생성하여 창의성을 높인다.",
      "이미지를 텍스트로 변환한다.",
      "데이터를 순서대로 정렬한다."
    ],
    "answer": "중요한 단어에 가중치를 부여하여 문맥을 파악한다.",
    "why": "Attention은 문장 내의 단어들 사이의 연관성을 계산하여 중요한 정보에 집중하게 합니다.",
    "hint": "Attention Is All You Need",
    "difficulty": "medium",
    "id": "3018"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "Transformer 아키텍처의 특징으로 올바른 설명 19?",
    "options": [
      "순차적으로 데이터를 처리해야 한다(RNN 방식).",
      "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
      "이미지 처리에만 특화된 모델이다.",
      "인코더 없이 디코더만 결합된 구조가 원조다.",
      "장기 의존성(Long-term dependency) 문제를 해결하지 못했다."
    ],
    "answer": "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
    "why": "Transformer는 Self-Attention을 사용하여 문장 전체를 한 번에(병렬로) 처리하며, RNN의 순차적 처리 한계를 극복했습니다.",
    "hint": "Attention Is All You Need",
    "difficulty": "medium",
    "id": "3019"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "Transformer 아키텍처의 특징으로 올바른 설명 20?",
    "options": [
      "순차적으로 데이터를 처리해야 한다(RNN 방식).",
      "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
      "이미지 처리에만 특화된 모델이다.",
      "인코더 없이 디코더만 결합된 구조가 원조다.",
      "장기 의존성(Long-term dependency) 문제를 해결하지 못했다."
    ],
    "answer": "Attention 메커니즘을 통해 병렬 처리가 가능하다.",
    "why": "Transformer는 Self-Attention을 사용하여 문장 전체를 한 번에(병렬로) 처리하며, RNN의 순차적 처리 한계를 극복했습니다.",
    "hint": "Attention Is All You Need",
    "difficulty": "medium",
    "id": "3020"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토큰화(Tokenization)에 대한 설명 21",
    "options": [
      "무조건 글자 단위로 자른다.",
      "무조건 단어 단위로 자른다.",
      "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
      "이미지를 픽셀 단위로 자르는 과정이다.",
      "토큰화 후에는 항상 이미지가 생성된다."
    ],
    "answer": "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
    "why": "현대 LLM은 BPE(Byte Pair Encoding) 등의 알고리즘을 사용하여, 자주 등장하는 문자열 조각을 토큰으로 정의합니다.",
    "hint": "Subword Tokenization",
    "difficulty": "easy",
    "id": "3021"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토큰화(Tokenization)에 대한 설명 22",
    "options": [
      "무조건 글자 단위로 자른다.",
      "무조건 단어 단위로 자른다.",
      "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
      "이미지를 픽셀 단위로 자르는 과정이다.",
      "토큰화 후에는 항상 이미지가 생성된다."
    ],
    "answer": "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
    "why": "현대 LLM은 BPE(Byte Pair Encoding) 등의 알고리즘을 사용하여, 자주 등장하는 문자열 조각을 토큰으로 정의합니다.",
    "hint": "Subword Tokenization",
    "difficulty": "easy",
    "id": "3022"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토큰화(Tokenization)에 대한 설명 23",
    "options": [
      "무조건 글자 단위로 자른다.",
      "무조건 단어 단위로 자른다.",
      "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
      "이미지를 픽셀 단위로 자르는 과정이다.",
      "토큰화 후에는 항상 이미지가 생성된다."
    ],
    "answer": "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
    "why": "현대 LLM은 BPE(Byte Pair Encoding) 등의 알고리즘을 사용하여, 자주 등장하는 문자열 조각을 토큰으로 정의합니다.",
    "hint": "Subword Tokenization",
    "difficulty": "easy",
    "id": "3023"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토큰화(Tokenization)에 대한 설명 24",
    "options": [
      "무조건 글자 단위로 자른다.",
      "무조건 단어 단위로 자른다.",
      "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
      "이미지를 픽셀 단위로 자르는 과정이다.",
      "토큰화 후에는 항상 이미지가 생성된다."
    ],
    "answer": "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
    "why": "현대 LLM은 BPE(Byte Pair Encoding) 등의 알고리즘을 사용하여, 자주 등장하는 문자열 조각을 토큰으로 정의합니다.",
    "hint": "Subword Tokenization",
    "difficulty": "easy",
    "id": "3024"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토큰화(Tokenization)에 대한 설명 25",
    "options": [
      "무조건 글자 단위로 자른다.",
      "무조건 단어 단위로 자른다.",
      "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
      "이미지를 픽셀 단위로 자르는 과정이다.",
      "토큰화 후에는 항상 이미지가 생성된다."
    ],
    "answer": "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
    "why": "현대 LLM은 BPE(Byte Pair Encoding) 등의 알고리즘을 사용하여, 자주 등장하는 문자열 조각을 토큰으로 정의합니다.",
    "hint": "Subword Tokenization",
    "difficulty": "easy",
    "id": "3025"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토큰화(Tokenization)에 대한 설명 26",
    "options": [
      "무조건 글자 단위로 자른다.",
      "무조건 단어 단위로 자른다.",
      "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
      "이미지를 픽셀 단위로 자르는 과정이다.",
      "토큰화 후에는 항상 이미지가 생성된다."
    ],
    "answer": "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
    "why": "현대 LLM은 BPE(Byte Pair Encoding) 등의 알고리즘을 사용하여, 자주 등장하는 문자열 조각을 토큰으로 정의합니다.",
    "hint": "Subword Tokenization",
    "difficulty": "easy",
    "id": "3026"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토큰화(Tokenization)에 대한 설명 27",
    "options": [
      "무조건 글자 단위로 자른다.",
      "무조건 단어 단위로 자른다.",
      "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
      "이미지를 픽셀 단위로 자르는 과정이다.",
      "토큰화 후에는 항상 이미지가 생성된다."
    ],
    "answer": "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
    "why": "현대 LLM은 BPE(Byte Pair Encoding) 등의 알고리즘을 사용하여, 자주 등장하는 문자열 조각을 토큰으로 정의합니다.",
    "hint": "Subword Tokenization",
    "difficulty": "easy",
    "id": "3027"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토큰화(Tokenization)에 대한 설명 28",
    "options": [
      "무조건 글자 단위로 자른다.",
      "무조건 단어 단위로 자른다.",
      "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
      "이미지를 픽셀 단위로 자르는 과정이다.",
      "토큰화 후에는 항상 이미지가 생성된다."
    ],
    "answer": "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
    "why": "현대 LLM은 BPE(Byte Pair Encoding) 등의 알고리즘을 사용하여, 자주 등장하는 문자열 조각을 토큰으로 정의합니다.",
    "hint": "Subword Tokenization",
    "difficulty": "easy",
    "id": "3028"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토큰화(Tokenization)에 대한 설명 29",
    "options": [
      "무조건 글자 단위로 자른다.",
      "무조건 단어 단위로 자른다.",
      "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
      "이미지를 픽셀 단위로 자르는 과정이다.",
      "토큰화 후에는 항상 이미지가 생성된다."
    ],
    "answer": "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
    "why": "현대 LLM은 BPE(Byte Pair Encoding) 등의 알고리즘을 사용하여, 자주 등장하는 문자열 조각을 토큰으로 정의합니다.",
    "hint": "Subword Tokenization",
    "difficulty": "easy",
    "id": "3029"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토큰화(Tokenization)에 대한 설명 30",
    "options": [
      "무조건 글자 단위로 자른다.",
      "무조건 단어 단위로 자른다.",
      "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
      "이미지를 픽셀 단위로 자르는 과정이다.",
      "토큰화 후에는 항상 이미지가 생성된다."
    ],
    "answer": "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
    "why": "현대 LLM은 BPE(Byte Pair Encoding) 등의 알고리즘을 사용하여, 자주 등장하는 문자열 조각을 토큰으로 정의합니다.",
    "hint": "Subword Tokenization",
    "difficulty": "easy",
    "id": "3030"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토큰화(Tokenization)에 대한 설명 31",
    "options": [
      "무조건 글자 단위로 자른다.",
      "무조건 단어 단위로 자른다.",
      "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
      "이미지를 픽셀 단위로 자르는 과정이다.",
      "토큰화 후에는 항상 이미지가 생성된다."
    ],
    "answer": "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
    "why": "현대 LLM은 BPE(Byte Pair Encoding) 등의 알고리즘을 사용하여, 자주 등장하는 문자열 조각을 토큰으로 정의합니다.",
    "hint": "Subword Tokenization",
    "difficulty": "easy",
    "id": "3031"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토큰화(Tokenization)에 대한 설명 32",
    "options": [
      "무조건 글자 단위로 자른다.",
      "무조건 단어 단위로 자른다.",
      "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
      "이미지를 픽셀 단위로 자르는 과정이다.",
      "토큰화 후에는 항상 이미지가 생성된다."
    ],
    "answer": "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
    "why": "현대 LLM은 BPE(Byte Pair Encoding) 등의 알고리즘을 사용하여, 자주 등장하는 문자열 조각을 토큰으로 정의합니다.",
    "hint": "Subword Tokenization",
    "difficulty": "easy",
    "id": "3032"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토큰화(Tokenization)에 대한 설명 33",
    "options": [
      "무조건 글자 단위로 자른다.",
      "무조건 단어 단위로 자른다.",
      "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
      "이미지를 픽셀 단위로 자르는 과정이다.",
      "토큰화 후에는 항상 이미지가 생성된다."
    ],
    "answer": "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
    "why": "현대 LLM은 BPE(Byte Pair Encoding) 등의 알고리즘을 사용하여, 자주 등장하는 문자열 조각을 토큰으로 정의합니다.",
    "hint": "Subword Tokenization",
    "difficulty": "easy",
    "id": "3033"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토큰화(Tokenization)에 대한 설명 34",
    "options": [
      "무조건 글자 단위로 자른다.",
      "무조건 단어 단위로 자른다.",
      "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
      "이미지를 픽셀 단위로 자르는 과정이다.",
      "토큰화 후에는 항상 이미지가 생성된다."
    ],
    "answer": "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
    "why": "현대 LLM은 BPE(Byte Pair Encoding) 등의 알고리즘을 사용하여, 자주 등장하는 문자열 조각을 토큰으로 정의합니다.",
    "hint": "Subword Tokenization",
    "difficulty": "easy",
    "id": "3034"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토큰화(Tokenization)에 대한 설명 35",
    "options": [
      "무조건 글자 단위로 자른다.",
      "무조건 단어 단위로 자른다.",
      "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
      "이미지를 픽셀 단위로 자르는 과정이다.",
      "토큰화 후에는 항상 이미지가 생성된다."
    ],
    "answer": "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
    "why": "현대 LLM은 BPE(Byte Pair Encoding) 등의 알고리즘을 사용하여, 자주 등장하는 문자열 조각을 토큰으로 정의합니다.",
    "hint": "Subword Tokenization",
    "difficulty": "easy",
    "id": "3035"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토큰화(Tokenization)에 대한 설명 36",
    "options": [
      "무조건 글자 단위로 자른다.",
      "무조건 단어 단위로 자른다.",
      "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
      "이미지를 픽셀 단위로 자르는 과정이다.",
      "토큰화 후에는 항상 이미지가 생성된다."
    ],
    "answer": "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
    "why": "현대 LLM은 BPE(Byte Pair Encoding) 등의 알고리즘을 사용하여, 자주 등장하는 문자열 조각을 토큰으로 정의합니다.",
    "hint": "Subword Tokenization",
    "difficulty": "easy",
    "id": "3036"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토큰화(Tokenization)에 대한 설명 37",
    "options": [
      "무조건 글자 단위로 자른다.",
      "무조건 단어 단위로 자른다.",
      "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
      "이미지를 픽셀 단위로 자르는 과정이다.",
      "토큰화 후에는 항상 이미지가 생성된다."
    ],
    "answer": "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
    "why": "현대 LLM은 BPE(Byte Pair Encoding) 등의 알고리즘을 사용하여, 자주 등장하는 문자열 조각을 토큰으로 정의합니다.",
    "hint": "Subword Tokenization",
    "difficulty": "easy",
    "id": "3037"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토큰화(Tokenization)에 대한 설명 38",
    "options": [
      "무조건 글자 단위로 자른다.",
      "무조건 단어 단위로 자른다.",
      "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
      "이미지를 픽셀 단위로 자르는 과정이다.",
      "토큰화 후에는 항상 이미지가 생성된다."
    ],
    "answer": "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
    "why": "현대 LLM은 BPE(Byte Pair Encoding) 등의 알고리즘을 사용하여, 자주 등장하는 문자열 조각을 토큰으로 정의합니다.",
    "hint": "Subword Tokenization",
    "difficulty": "easy",
    "id": "3038"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토큰화(Tokenization)에 대한 설명 39",
    "options": [
      "무조건 글자 단위로 자른다.",
      "무조건 단어 단위로 자른다.",
      "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
      "이미지를 픽셀 단위로 자르는 과정이다.",
      "토큰화 후에는 항상 이미지가 생성된다."
    ],
    "answer": "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
    "why": "현대 LLM은 BPE(Byte Pair Encoding) 등의 알고리즘을 사용하여, 자주 등장하는 문자열 조각을 토큰으로 정의합니다.",
    "hint": "Subword Tokenization",
    "difficulty": "easy",
    "id": "3039"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토큰화(Tokenization)에 대한 설명 40",
    "options": [
      "무조건 글자 단위로 자른다.",
      "무조건 단어 단위로 자른다.",
      "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
      "이미지를 픽셀 단위로 자르는 과정이다.",
      "토큰화 후에는 항상 이미지가 생성된다."
    ],
    "answer": "토큰은 의미를 가진 최소 단위로, 단어의 일부일 수도 있다.",
    "why": "현대 LLM은 BPE(Byte Pair Encoding) 등의 알고리즘을 사용하여, 자주 등장하는 문자열 조각을 토큰으로 정의합니다.",
    "hint": "Subword Tokenization",
    "difficulty": "easy",
    "id": "3040"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "주요 언어 모델의 특징 41",
    "options": [
      "BERT는 디코더(Decoder) 전용 모델이다.",
      "GPT는 인코더(Encoder) 전용 모델이다.",
      "GPT-3는 1750억 개의 파라미터를 가진다.",
      "Transformer는 2023년에 처음 발표되었다.",
      "LLaMA는 구글이 개발한 모델이다."
    ],
    "answer": "GPT-3는 1750억 개의 파라미터를 가진다.",
    "why": "GPT-3는 175B 파라미터의 거대 모델로, In-Context Learning 능력을 보여주었습니다. (BERT=Encoder, GPT=Decoder, Transformer=2017, LLaMA=Meta)",
    "hint": "GPT-3 Parameter",
    "difficulty": "medium",
    "id": "3041"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "주요 언어 모델의 특징 42",
    "options": [
      "BERT는 디코더(Decoder) 전용 모델이다.",
      "GPT는 인코더(Encoder) 전용 모델이다.",
      "GPT-3는 1750억 개의 파라미터를 가진다.",
      "Transformer는 2023년에 처음 발표되었다.",
      "LLaMA는 구글이 개발한 모델이다."
    ],
    "answer": "GPT-3는 1750억 개의 파라미터를 가진다.",
    "why": "GPT-3는 175B 파라미터의 거대 모델로, In-Context Learning 능력을 보여주었습니다. (BERT=Encoder, GPT=Decoder, Transformer=2017, LLaMA=Meta)",
    "hint": "GPT-3 Parameter",
    "difficulty": "medium",
    "id": "3042"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "주요 언어 모델의 특징 43",
    "options": [
      "BERT는 디코더(Decoder) 전용 모델이다.",
      "GPT는 인코더(Encoder) 전용 모델이다.",
      "GPT-3는 1750억 개의 파라미터를 가진다.",
      "Transformer는 2023년에 처음 발표되었다.",
      "LLaMA는 구글이 개발한 모델이다."
    ],
    "answer": "GPT-3는 1750억 개의 파라미터를 가진다.",
    "why": "GPT-3는 175B 파라미터의 거대 모델로, In-Context Learning 능력을 보여주었습니다. (BERT=Encoder, GPT=Decoder, Transformer=2017, LLaMA=Meta)",
    "hint": "GPT-3 Parameter",
    "difficulty": "medium",
    "id": "3043"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "주요 언어 모델의 특징 44",
    "options": [
      "BERT는 디코더(Decoder) 전용 모델이다.",
      "GPT는 인코더(Encoder) 전용 모델이다.",
      "GPT-3는 1750억 개의 파라미터를 가진다.",
      "Transformer는 2023년에 처음 발표되었다.",
      "LLaMA는 구글이 개발한 모델이다."
    ],
    "answer": "GPT-3는 1750억 개의 파라미터를 가진다.",
    "why": "GPT-3는 175B 파라미터의 거대 모델로, In-Context Learning 능력을 보여주었습니다. (BERT=Encoder, GPT=Decoder, Transformer=2017, LLaMA=Meta)",
    "hint": "GPT-3 Parameter",
    "difficulty": "medium",
    "id": "3044"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "주요 언어 모델의 특징 45",
    "options": [
      "BERT는 디코더(Decoder) 전용 모델이다.",
      "GPT는 인코더(Encoder) 전용 모델이다.",
      "GPT-3는 1750억 개의 파라미터를 가진다.",
      "Transformer는 2023년에 처음 발표되었다.",
      "LLaMA는 구글이 개발한 모델이다."
    ],
    "answer": "GPT-3는 1750억 개의 파라미터를 가진다.",
    "why": "GPT-3는 175B 파라미터의 거대 모델로, In-Context Learning 능력을 보여주었습니다. (BERT=Encoder, GPT=Decoder, Transformer=2017, LLaMA=Meta)",
    "hint": "GPT-3 Parameter",
    "difficulty": "medium",
    "id": "3045"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "주요 언어 모델의 특징 46",
    "options": [
      "BERT는 디코더(Decoder) 전용 모델이다.",
      "GPT는 인코더(Encoder) 전용 모델이다.",
      "GPT-3는 1750억 개의 파라미터를 가진다.",
      "Transformer는 2023년에 처음 발표되었다.",
      "LLaMA는 구글이 개발한 모델이다."
    ],
    "answer": "GPT-3는 1750억 개의 파라미터를 가진다.",
    "why": "GPT-3는 175B 파라미터의 거대 모델로, In-Context Learning 능력을 보여주었습니다. (BERT=Encoder, GPT=Decoder, Transformer=2017, LLaMA=Meta)",
    "hint": "GPT-3 Parameter",
    "difficulty": "medium",
    "id": "3046"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "주요 언어 모델의 특징 47",
    "options": [
      "BERT는 디코더(Decoder) 전용 모델이다.",
      "GPT는 인코더(Encoder) 전용 모델이다.",
      "GPT-3는 1750억 개의 파라미터를 가진다.",
      "Transformer는 2023년에 처음 발표되었다.",
      "LLaMA는 구글이 개발한 모델이다."
    ],
    "answer": "GPT-3는 1750억 개의 파라미터를 가진다.",
    "why": "GPT-3는 175B 파라미터의 거대 모델로, In-Context Learning 능력을 보여주었습니다. (BERT=Encoder, GPT=Decoder, Transformer=2017, LLaMA=Meta)",
    "hint": "GPT-3 Parameter",
    "difficulty": "medium",
    "id": "3047"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "주요 언어 모델의 특징 48",
    "options": [
      "BERT는 디코더(Decoder) 전용 모델이다.",
      "GPT는 인코더(Encoder) 전용 모델이다.",
      "GPT-3는 1750억 개의 파라미터를 가진다.",
      "Transformer는 2023년에 처음 발표되었다.",
      "LLaMA는 구글이 개발한 모델이다."
    ],
    "answer": "GPT-3는 1750억 개의 파라미터를 가진다.",
    "why": "GPT-3는 175B 파라미터의 거대 모델로, In-Context Learning 능력을 보여주었습니다. (BERT=Encoder, GPT=Decoder, Transformer=2017, LLaMA=Meta)",
    "hint": "GPT-3 Parameter",
    "difficulty": "medium",
    "id": "3048"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "주요 언어 모델의 특징 49",
    "options": [
      "BERT는 디코더(Decoder) 전용 모델이다.",
      "GPT는 인코더(Encoder) 전용 모델이다.",
      "GPT-3는 1750억 개의 파라미터를 가진다.",
      "Transformer는 2023년에 처음 발표되었다.",
      "LLaMA는 구글이 개발한 모델이다."
    ],
    "answer": "GPT-3는 1750억 개의 파라미터를 가진다.",
    "why": "GPT-3는 175B 파라미터의 거대 모델로, In-Context Learning 능력을 보여주었습니다. (BERT=Encoder, GPT=Decoder, Transformer=2017, LLaMA=Meta)",
    "hint": "GPT-3 Parameter",
    "difficulty": "medium",
    "id": "3049"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "주요 언어 모델의 특징 50",
    "options": [
      "BERT는 디코더(Decoder) 전용 모델이다.",
      "GPT는 인코더(Encoder) 전용 모델이다.",
      "GPT-3는 1750억 개의 파라미터를 가진다.",
      "Transformer는 2023년에 처음 발표되었다.",
      "LLaMA는 구글이 개발한 모델이다."
    ],
    "answer": "GPT-3는 1750억 개의 파라미터를 가진다.",
    "why": "GPT-3는 175B 파라미터의 거대 모델로, In-Context Learning 능력을 보여주었습니다. (BERT=Encoder, GPT=Decoder, Transformer=2017, LLaMA=Meta)",
    "hint": "GPT-3 Parameter",
    "difficulty": "medium",
    "id": "3050"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "주요 언어 모델의 특징 51",
    "options": [
      "BERT는 디코더(Decoder) 전용 모델이다.",
      "GPT는 인코더(Encoder) 전용 모델이다.",
      "GPT-3는 1750억 개의 파라미터를 가진다.",
      "Transformer는 2023년에 처음 발표되었다.",
      "LLaMA는 구글이 개발한 모델이다."
    ],
    "answer": "GPT-3는 1750억 개의 파라미터를 가진다.",
    "why": "GPT-3는 175B 파라미터의 거대 모델로, In-Context Learning 능력을 보여주었습니다. (BERT=Encoder, GPT=Decoder, Transformer=2017, LLaMA=Meta)",
    "hint": "GPT-3 Parameter",
    "difficulty": "medium",
    "id": "3051"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "주요 언어 모델의 특징 52",
    "options": [
      "BERT는 디코더(Decoder) 전용 모델이다.",
      "GPT는 인코더(Encoder) 전용 모델이다.",
      "GPT-3는 1750억 개의 파라미터를 가진다.",
      "Transformer는 2023년에 처음 발표되었다.",
      "LLaMA는 구글이 개발한 모델이다."
    ],
    "answer": "GPT-3는 1750억 개의 파라미터를 가진다.",
    "why": "GPT-3는 175B 파라미터의 거대 모델로, In-Context Learning 능력을 보여주었습니다. (BERT=Encoder, GPT=Decoder, Transformer=2017, LLaMA=Meta)",
    "hint": "GPT-3 Parameter",
    "difficulty": "medium",
    "id": "3052"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "주요 언어 모델의 특징 53",
    "options": [
      "BERT는 디코더(Decoder) 전용 모델이다.",
      "GPT는 인코더(Encoder) 전용 모델이다.",
      "GPT-3는 1750억 개의 파라미터를 가진다.",
      "Transformer는 2023년에 처음 발표되었다.",
      "LLaMA는 구글이 개발한 모델이다."
    ],
    "answer": "GPT-3는 1750억 개의 파라미터를 가진다.",
    "why": "GPT-3는 175B 파라미터의 거대 모델로, In-Context Learning 능력을 보여주었습니다. (BERT=Encoder, GPT=Decoder, Transformer=2017, LLaMA=Meta)",
    "hint": "GPT-3 Parameter",
    "difficulty": "medium",
    "id": "3053"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "주요 언어 모델의 특징 54",
    "options": [
      "BERT는 디코더(Decoder) 전용 모델이다.",
      "GPT는 인코더(Encoder) 전용 모델이다.",
      "GPT-3는 1750억 개의 파라미터를 가진다.",
      "Transformer는 2023년에 처음 발표되었다.",
      "LLaMA는 구글이 개발한 모델이다."
    ],
    "answer": "GPT-3는 1750억 개의 파라미터를 가진다.",
    "why": "GPT-3는 175B 파라미터의 거대 모델로, In-Context Learning 능력을 보여주었습니다. (BERT=Encoder, GPT=Decoder, Transformer=2017, LLaMA=Meta)",
    "hint": "GPT-3 Parameter",
    "difficulty": "medium",
    "id": "3054"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "주요 언어 모델의 특징 55",
    "options": [
      "BERT는 디코더(Decoder) 전용 모델이다.",
      "GPT는 인코더(Encoder) 전용 모델이다.",
      "GPT-3는 1750억 개의 파라미터를 가진다.",
      "Transformer는 2023년에 처음 발표되었다.",
      "LLaMA는 구글이 개발한 모델이다."
    ],
    "answer": "GPT-3는 1750억 개의 파라미터를 가진다.",
    "why": "GPT-3는 175B 파라미터의 거대 모델로, In-Context Learning 능력을 보여주었습니다. (BERT=Encoder, GPT=Decoder, Transformer=2017, LLaMA=Meta)",
    "hint": "GPT-3 Parameter",
    "difficulty": "medium",
    "id": "3055"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "주요 언어 모델의 특징 56",
    "options": [
      "BERT는 디코더(Decoder) 전용 모델이다.",
      "GPT는 인코더(Encoder) 전용 모델이다.",
      "GPT-3는 1750억 개의 파라미터를 가진다.",
      "Transformer는 2023년에 처음 발표되었다.",
      "LLaMA는 구글이 개발한 모델이다."
    ],
    "answer": "GPT-3는 1750억 개의 파라미터를 가진다.",
    "why": "GPT-3는 175B 파라미터의 거대 모델로, In-Context Learning 능력을 보여주었습니다. (BERT=Encoder, GPT=Decoder, Transformer=2017, LLaMA=Meta)",
    "hint": "GPT-3 Parameter",
    "difficulty": "medium",
    "id": "3056"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "주요 언어 모델의 특징 57",
    "options": [
      "BERT는 디코더(Decoder) 전용 모델이다.",
      "GPT는 인코더(Encoder) 전용 모델이다.",
      "GPT-3는 1750억 개의 파라미터를 가진다.",
      "Transformer는 2023년에 처음 발표되었다.",
      "LLaMA는 구글이 개발한 모델이다."
    ],
    "answer": "GPT-3는 1750억 개의 파라미터를 가진다.",
    "why": "GPT-3는 175B 파라미터의 거대 모델로, In-Context Learning 능력을 보여주었습니다. (BERT=Encoder, GPT=Decoder, Transformer=2017, LLaMA=Meta)",
    "hint": "GPT-3 Parameter",
    "difficulty": "medium",
    "id": "3057"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "주요 언어 모델의 특징 58",
    "options": [
      "BERT는 디코더(Decoder) 전용 모델이다.",
      "GPT는 인코더(Encoder) 전용 모델이다.",
      "GPT-3는 1750억 개의 파라미터를 가진다.",
      "Transformer는 2023년에 처음 발표되었다.",
      "LLaMA는 구글이 개발한 모델이다."
    ],
    "answer": "GPT-3는 1750억 개의 파라미터를 가진다.",
    "why": "GPT-3는 175B 파라미터의 거대 모델로, In-Context Learning 능력을 보여주었습니다. (BERT=Encoder, GPT=Decoder, Transformer=2017, LLaMA=Meta)",
    "hint": "GPT-3 Parameter",
    "difficulty": "medium",
    "id": "3058"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "주요 언어 모델의 특징 59",
    "options": [
      "BERT는 디코더(Decoder) 전용 모델이다.",
      "GPT는 인코더(Encoder) 전용 모델이다.",
      "GPT-3는 1750억 개의 파라미터를 가진다.",
      "Transformer는 2023년에 처음 발표되었다.",
      "LLaMA는 구글이 개발한 모델이다."
    ],
    "answer": "GPT-3는 1750억 개의 파라미터를 가진다.",
    "why": "GPT-3는 175B 파라미터의 거대 모델로, In-Context Learning 능력을 보여주었습니다. (BERT=Encoder, GPT=Decoder, Transformer=2017, LLaMA=Meta)",
    "hint": "GPT-3 Parameter",
    "difficulty": "medium",
    "id": "3059"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "주요 언어 모델의 특징 60",
    "options": [
      "BERT는 디코더(Decoder) 전용 모델이다.",
      "GPT는 인코더(Encoder) 전용 모델이다.",
      "GPT-3는 1750억 개의 파라미터를 가진다.",
      "Transformer는 2023년에 처음 발표되었다.",
      "LLaMA는 구글이 개발한 모델이다."
    ],
    "answer": "GPT-3는 1750억 개의 파라미터를 가진다.",
    "why": "GPT-3는 175B 파라미터의 거대 모델로, In-Context Learning 능력을 보여주었습니다. (BERT=Encoder, GPT=Decoder, Transformer=2017, LLaMA=Meta)",
    "hint": "GPT-3 Parameter",
    "difficulty": "medium",
    "id": "3060"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 활용 방식 비교 61",
    "options": [
      "API 방식은 인프라 구축 비용이 매우 높다.",
      "오픈 모델은 데이터 보안 유지에 불리하다.",
      "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
      "오픈 모델은 커스터마이징이 불가능하다.",
      "API 방식은 인터넷 연결 없이 사용 가능하다."
    ],
    "answer": "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
    "why": "OpenAI 등에서 제공하는 API를 사용하면 별도의 GPU 서버 구축 없이 고성능 모델을 호출하여 사용할 수 있습니다.",
    "hint": "API의 편의성",
    "difficulty": "easy",
    "id": "3061"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 활용 방식 비교 62",
    "options": [
      "API 방식은 인프라 구축 비용이 매우 높다.",
      "오픈 모델은 데이터 보안 유지에 불리하다.",
      "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
      "오픈 모델은 커스터마이징이 불가능하다.",
      "API 방식은 인터넷 연결 없이 사용 가능하다."
    ],
    "answer": "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
    "why": "OpenAI 등에서 제공하는 API를 사용하면 별도의 GPU 서버 구축 없이 고성능 모델을 호출하여 사용할 수 있습니다.",
    "hint": "API의 편의성",
    "difficulty": "easy",
    "id": "3062"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 활용 방식 비교 63",
    "options": [
      "API 방식은 인프라 구축 비용이 매우 높다.",
      "오픈 모델은 데이터 보안 유지에 불리하다.",
      "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
      "오픈 모델은 커스터마이징이 불가능하다.",
      "API 방식은 인터넷 연결 없이 사용 가능하다."
    ],
    "answer": "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
    "why": "OpenAI 등에서 제공하는 API를 사용하면 별도의 GPU 서버 구축 없이 고성능 모델을 호출하여 사용할 수 있습니다.",
    "hint": "API의 편의성",
    "difficulty": "easy",
    "id": "3063"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 활용 방식 비교 64",
    "options": [
      "API 방식은 인프라 구축 비용이 매우 높다.",
      "오픈 모델은 데이터 보안 유지에 불리하다.",
      "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
      "오픈 모델은 커스터마이징이 불가능하다.",
      "API 방식은 인터넷 연결 없이 사용 가능하다."
    ],
    "answer": "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
    "why": "OpenAI 등에서 제공하는 API를 사용하면 별도의 GPU 서버 구축 없이 고성능 모델을 호출하여 사용할 수 있습니다.",
    "hint": "API의 편의성",
    "difficulty": "easy",
    "id": "3064"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 활용 방식 비교 65",
    "options": [
      "API 방식은 인프라 구축 비용이 매우 높다.",
      "오픈 모델은 데이터 보안 유지에 불리하다.",
      "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
      "오픈 모델은 커스터마이징이 불가능하다.",
      "API 방식은 인터넷 연결 없이 사용 가능하다."
    ],
    "answer": "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
    "why": "OpenAI 등에서 제공하는 API를 사용하면 별도의 GPU 서버 구축 없이 고성능 모델을 호출하여 사용할 수 있습니다.",
    "hint": "API의 편의성",
    "difficulty": "easy",
    "id": "3065"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 활용 방식 비교 66",
    "options": [
      "API 방식은 인프라 구축 비용이 매우 높다.",
      "오픈 모델은 데이터 보안 유지에 불리하다.",
      "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
      "오픈 모델은 커스터마이징이 불가능하다.",
      "API 방식은 인터넷 연결 없이 사용 가능하다."
    ],
    "answer": "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
    "why": "OpenAI 등에서 제공하는 API를 사용하면 별도의 GPU 서버 구축 없이 고성능 모델을 호출하여 사용할 수 있습니다.",
    "hint": "API의 편의성",
    "difficulty": "easy",
    "id": "3066"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 활용 방식 비교 67",
    "options": [
      "API 방식은 인프라 구축 비용이 매우 높다.",
      "오픈 모델은 데이터 보안 유지에 불리하다.",
      "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
      "오픈 모델은 커스터마이징이 불가능하다.",
      "API 방식은 인터넷 연결 없이 사용 가능하다."
    ],
    "answer": "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
    "why": "OpenAI 등에서 제공하는 API를 사용하면 별도의 GPU 서버 구축 없이 고성능 모델을 호출하여 사용할 수 있습니다.",
    "hint": "API의 편의성",
    "difficulty": "easy",
    "id": "3067"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 활용 방식 비교 68",
    "options": [
      "API 방식은 인프라 구축 비용이 매우 높다.",
      "오픈 모델은 데이터 보안 유지에 불리하다.",
      "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
      "오픈 모델은 커스터마이징이 불가능하다.",
      "API 방식은 인터넷 연결 없이 사용 가능하다."
    ],
    "answer": "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
    "why": "OpenAI 등에서 제공하는 API를 사용하면 별도의 GPU 서버 구축 없이 고성능 모델을 호출하여 사용할 수 있습니다.",
    "hint": "API의 편의성",
    "difficulty": "easy",
    "id": "3068"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 활용 방식 비교 69",
    "options": [
      "API 방식은 인프라 구축 비용이 매우 높다.",
      "오픈 모델은 데이터 보안 유지에 불리하다.",
      "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
      "오픈 모델은 커스터마이징이 불가능하다.",
      "API 방식은 인터넷 연결 없이 사용 가능하다."
    ],
    "answer": "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
    "why": "OpenAI 등에서 제공하는 API를 사용하면 별도의 GPU 서버 구축 없이 고성능 모델을 호출하여 사용할 수 있습니다.",
    "hint": "API의 편의성",
    "difficulty": "easy",
    "id": "3069"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 활용 방식 비교 70",
    "options": [
      "API 방식은 인프라 구축 비용이 매우 높다.",
      "오픈 모델은 데이터 보안 유지에 불리하다.",
      "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
      "오픈 모델은 커스터마이징이 불가능하다.",
      "API 방식은 인터넷 연결 없이 사용 가능하다."
    ],
    "answer": "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
    "why": "OpenAI 등에서 제공하는 API를 사용하면 별도의 GPU 서버 구축 없이 고성능 모델을 호출하여 사용할 수 있습니다.",
    "hint": "API의 편의성",
    "difficulty": "easy",
    "id": "3070"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 활용 방식 비교 71",
    "options": [
      "API 방식은 인프라 구축 비용이 매우 높다.",
      "오픈 모델은 데이터 보안 유지에 불리하다.",
      "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
      "오픈 모델은 커스터마이징이 불가능하다.",
      "API 방식은 인터넷 연결 없이 사용 가능하다."
    ],
    "answer": "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
    "why": "OpenAI 등에서 제공하는 API를 사용하면 별도의 GPU 서버 구축 없이 고성능 모델을 호출하여 사용할 수 있습니다.",
    "hint": "API의 편의성",
    "difficulty": "easy",
    "id": "3071"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 활용 방식 비교 72",
    "options": [
      "API 방식은 인프라 구축 비용이 매우 높다.",
      "오픈 모델은 데이터 보안 유지에 불리하다.",
      "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
      "오픈 모델은 커스터마이징이 불가능하다.",
      "API 방식은 인터넷 연결 없이 사용 가능하다."
    ],
    "answer": "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
    "why": "OpenAI 등에서 제공하는 API를 사용하면 별도의 GPU 서버 구축 없이 고성능 모델을 호출하여 사용할 수 있습니다.",
    "hint": "API의 편의성",
    "difficulty": "easy",
    "id": "3072"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 활용 방식 비교 73",
    "options": [
      "API 방식은 인프라 구축 비용이 매우 높다.",
      "오픈 모델은 데이터 보안 유지에 불리하다.",
      "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
      "오픈 모델은 커스터마이징이 불가능하다.",
      "API 방식은 인터넷 연결 없이 사용 가능하다."
    ],
    "answer": "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
    "why": "OpenAI 등에서 제공하는 API를 사용하면 별도의 GPU 서버 구축 없이 고성능 모델을 호출하여 사용할 수 있습니다.",
    "hint": "API의 편의성",
    "difficulty": "easy",
    "id": "3073"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 활용 방식 비교 74",
    "options": [
      "API 방식은 인프라 구축 비용이 매우 높다.",
      "오픈 모델은 데이터 보안 유지에 불리하다.",
      "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
      "오픈 모델은 커스터마이징이 불가능하다.",
      "API 방식은 인터넷 연결 없이 사용 가능하다."
    ],
    "answer": "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
    "why": "OpenAI 등에서 제공하는 API를 사용하면 별도의 GPU 서버 구축 없이 고성능 모델을 호출하여 사용할 수 있습니다.",
    "hint": "API의 편의성",
    "difficulty": "easy",
    "id": "3074"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 활용 방식 비교 75",
    "options": [
      "API 방식은 인프라 구축 비용이 매우 높다.",
      "오픈 모델은 데이터 보안 유지에 불리하다.",
      "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
      "오픈 모델은 커스터마이징이 불가능하다.",
      "API 방식은 인터넷 연결 없이 사용 가능하다."
    ],
    "answer": "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
    "why": "OpenAI 등에서 제공하는 API를 사용하면 별도의 GPU 서버 구축 없이 고성능 모델을 호출하여 사용할 수 있습니다.",
    "hint": "API의 편의성",
    "difficulty": "easy",
    "id": "3075"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 활용 방식 비교 76",
    "options": [
      "API 방식은 인프라 구축 비용이 매우 높다.",
      "오픈 모델은 데이터 보안 유지에 불리하다.",
      "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
      "오픈 모델은 커스터마이징이 불가능하다.",
      "API 방식은 인터넷 연결 없이 사용 가능하다."
    ],
    "answer": "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
    "why": "OpenAI 등에서 제공하는 API를 사용하면 별도의 GPU 서버 구축 없이 고성능 모델을 호출하여 사용할 수 있습니다.",
    "hint": "API의 편의성",
    "difficulty": "easy",
    "id": "3076"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 활용 방식 비교 77",
    "options": [
      "API 방식은 인프라 구축 비용이 매우 높다.",
      "오픈 모델은 데이터 보안 유지에 불리하다.",
      "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
      "오픈 모델은 커스터마이징이 불가능하다.",
      "API 방식은 인터넷 연결 없이 사용 가능하다."
    ],
    "answer": "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
    "why": "OpenAI 등에서 제공하는 API를 사용하면 별도의 GPU 서버 구축 없이 고성능 모델을 호출하여 사용할 수 있습니다.",
    "hint": "API의 편의성",
    "difficulty": "easy",
    "id": "3077"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 활용 방식 비교 78",
    "options": [
      "API 방식은 인프라 구축 비용이 매우 높다.",
      "오픈 모델은 데이터 보안 유지에 불리하다.",
      "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
      "오픈 모델은 커스터마이징이 불가능하다.",
      "API 방식은 인터넷 연결 없이 사용 가능하다."
    ],
    "answer": "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
    "why": "OpenAI 등에서 제공하는 API를 사용하면 별도의 GPU 서버 구축 없이 고성능 모델을 호출하여 사용할 수 있습니다.",
    "hint": "API의 편의성",
    "difficulty": "easy",
    "id": "3078"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 활용 방식 비교 79",
    "options": [
      "API 방식은 인프라 구축 비용이 매우 높다.",
      "오픈 모델은 데이터 보안 유지에 불리하다.",
      "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
      "오픈 모델은 커스터마이징이 불가능하다.",
      "API 방식은 인터넷 연결 없이 사용 가능하다."
    ],
    "answer": "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
    "why": "OpenAI 등에서 제공하는 API를 사용하면 별도의 GPU 서버 구축 없이 고성능 모델을 호출하여 사용할 수 있습니다.",
    "hint": "API의 편의성",
    "difficulty": "easy",
    "id": "3079"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 활용 방식 비교 80",
    "options": [
      "API 방식은 인프라 구축 비용이 매우 높다.",
      "오픈 모델은 데이터 보안 유지에 불리하다.",
      "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
      "오픈 모델은 커스터마이징이 불가능하다.",
      "API 방식은 인터넷 연결 없이 사용 가능하다."
    ],
    "answer": "API 방식은 최신 고성능 모델을 쉽게 사용할 수 있다.",
    "why": "OpenAI 등에서 제공하는 API를 사용하면 별도의 GPU 서버 구축 없이 고성능 모델을 호출하여 사용할 수 있습니다.",
    "hint": "API의 편의성",
    "difficulty": "easy",
    "id": "3080"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 한계점과 환각(Hallucination) 81",
    "options": [
      "LLM은 모든 사실관계를 완벽하게 검증한다.",
      "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
      "LLM은 최신 뉴스를 실시간으로 스스로 학습한다.",
      "LLM은 수학 계산에 실수가 없다.",
      "환각은 데이터가 많으면 100% 사라진다."
    ],
    "answer": "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
    "why": "LLM은 확률적으로 다음 단어를 예측하므로, 사실이 아닌 내용을 그럴듯하게 생성하는 환각 현상이 발생할 수 있습니다.",
    "hint": "거짓말을 그럴듯하게 하는 것",
    "difficulty": "medium",
    "id": "3081"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 한계점과 환각(Hallucination) 82",
    "options": [
      "LLM은 모든 사실관계를 완벽하게 검증한다.",
      "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
      "LLM은 최신 뉴스를 실시간으로 스스로 학습한다.",
      "LLM은 수학 계산에 실수가 없다.",
      "환각은 데이터가 많으면 100% 사라진다."
    ],
    "answer": "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
    "why": "LLM은 확률적으로 다음 단어를 예측하므로, 사실이 아닌 내용을 그럴듯하게 생성하는 환각 현상이 발생할 수 있습니다.",
    "hint": "거짓말을 그럴듯하게 하는 것",
    "difficulty": "medium",
    "id": "3082"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 한계점과 환각(Hallucination) 83",
    "options": [
      "LLM은 모든 사실관계를 완벽하게 검증한다.",
      "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
      "LLM은 최신 뉴스를 실시간으로 스스로 학습한다.",
      "LLM은 수학 계산에 실수가 없다.",
      "환각은 데이터가 많으면 100% 사라진다."
    ],
    "answer": "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
    "why": "LLM은 확률적으로 다음 단어를 예측하므로, 사실이 아닌 내용을 그럴듯하게 생성하는 환각 현상이 발생할 수 있습니다.",
    "hint": "거짓말을 그럴듯하게 하는 것",
    "difficulty": "medium",
    "id": "3083"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 한계점과 환각(Hallucination) 84",
    "options": [
      "LLM은 모든 사실관계를 완벽하게 검증한다.",
      "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
      "LLM은 최신 뉴스를 실시간으로 스스로 학습한다.",
      "LLM은 수학 계산에 실수가 없다.",
      "환각은 데이터가 많으면 100% 사라진다."
    ],
    "answer": "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
    "why": "LLM은 확률적으로 다음 단어를 예측하므로, 사실이 아닌 내용을 그럴듯하게 생성하는 환각 현상이 발생할 수 있습니다.",
    "hint": "거짓말을 그럴듯하게 하는 것",
    "difficulty": "medium",
    "id": "3084"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 한계점과 환각(Hallucination) 85",
    "options": [
      "LLM은 모든 사실관계를 완벽하게 검증한다.",
      "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
      "LLM은 최신 뉴스를 실시간으로 스스로 학습한다.",
      "LLM은 수학 계산에 실수가 없다.",
      "환각은 데이터가 많으면 100% 사라진다."
    ],
    "answer": "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
    "why": "LLM은 확률적으로 다음 단어를 예측하므로, 사실이 아닌 내용을 그럴듯하게 생성하는 환각 현상이 발생할 수 있습니다.",
    "hint": "거짓말을 그럴듯하게 하는 것",
    "difficulty": "medium",
    "id": "3085"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 한계점과 환각(Hallucination) 86",
    "options": [
      "LLM은 모든 사실관계를 완벽하게 검증한다.",
      "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
      "LLM은 최신 뉴스를 실시간으로 스스로 학습한다.",
      "LLM은 수학 계산에 실수가 없다.",
      "환각은 데이터가 많으면 100% 사라진다."
    ],
    "answer": "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
    "why": "LLM은 확률적으로 다음 단어를 예측하므로, 사실이 아닌 내용을 그럴듯하게 생성하는 환각 현상이 발생할 수 있습니다.",
    "hint": "거짓말을 그럴듯하게 하는 것",
    "difficulty": "medium",
    "id": "3086"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 한계점과 환각(Hallucination) 87",
    "options": [
      "LLM은 모든 사실관계를 완벽하게 검증한다.",
      "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
      "LLM은 최신 뉴스를 실시간으로 스스로 학습한다.",
      "LLM은 수학 계산에 실수가 없다.",
      "환각은 데이터가 많으면 100% 사라진다."
    ],
    "answer": "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
    "why": "LLM은 확률적으로 다음 단어를 예측하므로, 사실이 아닌 내용을 그럴듯하게 생성하는 환각 현상이 발생할 수 있습니다.",
    "hint": "거짓말을 그럴듯하게 하는 것",
    "difficulty": "medium",
    "id": "3087"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 한계점과 환각(Hallucination) 88",
    "options": [
      "LLM은 모든 사실관계를 완벽하게 검증한다.",
      "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
      "LLM은 최신 뉴스를 실시간으로 스스로 학습한다.",
      "LLM은 수학 계산에 실수가 없다.",
      "환각은 데이터가 많으면 100% 사라진다."
    ],
    "answer": "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
    "why": "LLM은 확률적으로 다음 단어를 예측하므로, 사실이 아닌 내용을 그럴듯하게 생성하는 환각 현상이 발생할 수 있습니다.",
    "hint": "거짓말을 그럴듯하게 하는 것",
    "difficulty": "medium",
    "id": "3088"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 한계점과 환각(Hallucination) 89",
    "options": [
      "LLM은 모든 사실관계를 완벽하게 검증한다.",
      "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
      "LLM은 최신 뉴스를 실시간으로 스스로 학습한다.",
      "LLM은 수학 계산에 실수가 없다.",
      "환각은 데이터가 많으면 100% 사라진다."
    ],
    "answer": "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
    "why": "LLM은 확률적으로 다음 단어를 예측하므로, 사실이 아닌 내용을 그럴듯하게 생성하는 환각 현상이 발생할 수 있습니다.",
    "hint": "거짓말을 그럴듯하게 하는 것",
    "difficulty": "medium",
    "id": "3089"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 한계점과 환각(Hallucination) 90",
    "options": [
      "LLM은 모든 사실관계를 완벽하게 검증한다.",
      "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
      "LLM은 최신 뉴스를 실시간으로 스스로 학습한다.",
      "LLM은 수학 계산에 실수가 없다.",
      "환각은 데이터가 많으면 100% 사라진다."
    ],
    "answer": "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
    "why": "LLM은 확률적으로 다음 단어를 예측하므로, 사실이 아닌 내용을 그럴듯하게 생성하는 환각 현상이 발생할 수 있습니다.",
    "hint": "거짓말을 그럴듯하게 하는 것",
    "difficulty": "medium",
    "id": "3090"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 한계점과 환각(Hallucination) 91",
    "options": [
      "LLM은 모든 사실관계를 완벽하게 검증한다.",
      "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
      "LLM은 최신 뉴스를 실시간으로 스스로 학습한다.",
      "LLM은 수학 계산에 실수가 없다.",
      "환각은 데이터가 많으면 100% 사라진다."
    ],
    "answer": "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
    "why": "LLM은 확률적으로 다음 단어를 예측하므로, 사실이 아닌 내용을 그럴듯하게 생성하는 환각 현상이 발생할 수 있습니다.",
    "hint": "거짓말을 그럴듯하게 하는 것",
    "difficulty": "medium",
    "id": "3091"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 한계점과 환각(Hallucination) 92",
    "options": [
      "LLM은 모든 사실관계를 완벽하게 검증한다.",
      "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
      "LLM은 최신 뉴스를 실시간으로 스스로 학습한다.",
      "LLM은 수학 계산에 실수가 없다.",
      "환각은 데이터가 많으면 100% 사라진다."
    ],
    "answer": "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
    "why": "LLM은 확률적으로 다음 단어를 예측하므로, 사실이 아닌 내용을 그럴듯하게 생성하는 환각 현상이 발생할 수 있습니다.",
    "hint": "거짓말을 그럴듯하게 하는 것",
    "difficulty": "medium",
    "id": "3092"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 한계점과 환각(Hallucination) 93",
    "options": [
      "LLM은 모든 사실관계를 완벽하게 검증한다.",
      "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
      "LLM은 최신 뉴스를 실시간으로 스스로 학습한다.",
      "LLM은 수학 계산에 실수가 없다.",
      "환각은 데이터가 많으면 100% 사라진다."
    ],
    "answer": "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
    "why": "LLM은 확률적으로 다음 단어를 예측하므로, 사실이 아닌 내용을 그럴듯하게 생성하는 환각 현상이 발생할 수 있습니다.",
    "hint": "거짓말을 그럴듯하게 하는 것",
    "difficulty": "medium",
    "id": "3093"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 한계점과 환각(Hallucination) 94",
    "options": [
      "LLM은 모든 사실관계를 완벽하게 검증한다.",
      "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
      "LLM은 최신 뉴스를 실시간으로 스스로 학습한다.",
      "LLM은 수학 계산에 실수가 없다.",
      "환각은 데이터가 많으면 100% 사라진다."
    ],
    "answer": "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
    "why": "LLM은 확률적으로 다음 단어를 예측하므로, 사실이 아닌 내용을 그럴듯하게 생성하는 환각 현상이 발생할 수 있습니다.",
    "hint": "거짓말을 그럴듯하게 하는 것",
    "difficulty": "medium",
    "id": "3094"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 한계점과 환각(Hallucination) 95",
    "options": [
      "LLM은 모든 사실관계를 완벽하게 검증한다.",
      "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
      "LLM은 최신 뉴스를 실시간으로 스스로 학습한다.",
      "LLM은 수학 계산에 실수가 없다.",
      "환각은 데이터가 많으면 100% 사라진다."
    ],
    "answer": "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
    "why": "LLM은 확률적으로 다음 단어를 예측하므로, 사실이 아닌 내용을 그럴듯하게 생성하는 환각 현상이 발생할 수 있습니다.",
    "hint": "거짓말을 그럴듯하게 하는 것",
    "difficulty": "medium",
    "id": "3095"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 한계점과 환각(Hallucination) 96",
    "options": [
      "LLM은 모든 사실관계를 완벽하게 검증한다.",
      "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
      "LLM은 최신 뉴스를 실시간으로 스스로 학습한다.",
      "LLM은 수학 계산에 실수가 없다.",
      "환각은 데이터가 많으면 100% 사라진다."
    ],
    "answer": "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
    "why": "LLM은 확률적으로 다음 단어를 예측하므로, 사실이 아닌 내용을 그럴듯하게 생성하는 환각 현상이 발생할 수 있습니다.",
    "hint": "거짓말을 그럴듯하게 하는 것",
    "difficulty": "medium",
    "id": "3096"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 한계점과 환각(Hallucination) 97",
    "options": [
      "LLM은 모든 사실관계를 완벽하게 검증한다.",
      "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
      "LLM은 최신 뉴스를 실시간으로 스스로 학습한다.",
      "LLM은 수학 계산에 실수가 없다.",
      "환각은 데이터가 많으면 100% 사라진다."
    ],
    "answer": "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
    "why": "LLM은 확률적으로 다음 단어를 예측하므로, 사실이 아닌 내용을 그럴듯하게 생성하는 환각 현상이 발생할 수 있습니다.",
    "hint": "거짓말을 그럴듯하게 하는 것",
    "difficulty": "medium",
    "id": "3097"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 한계점과 환각(Hallucination) 98",
    "options": [
      "LLM은 모든 사실관계를 완벽하게 검증한다.",
      "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
      "LLM은 최신 뉴스를 실시간으로 스스로 학습한다.",
      "LLM은 수학 계산에 실수가 없다.",
      "환각은 데이터가 많으면 100% 사라진다."
    ],
    "answer": "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
    "why": "LLM은 확률적으로 다음 단어를 예측하므로, 사실이 아닌 내용을 그럴듯하게 생성하는 환각 현상이 발생할 수 있습니다.",
    "hint": "거짓말을 그럴듯하게 하는 것",
    "difficulty": "medium",
    "id": "3098"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 한계점과 환각(Hallucination) 99",
    "options": [
      "LLM은 모든 사실관계를 완벽하게 검증한다.",
      "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
      "LLM은 최신 뉴스를 실시간으로 스스로 학습한다.",
      "LLM은 수학 계산에 실수가 없다.",
      "환각은 데이터가 많으면 100% 사라진다."
    ],
    "answer": "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
    "why": "LLM은 확률적으로 다음 단어를 예측하므로, 사실이 아닌 내용을 그럴듯하게 생성하는 환각 현상이 발생할 수 있습니다.",
    "hint": "거짓말을 그럴듯하게 하는 것",
    "difficulty": "medium",
    "id": "3099"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 한계점과 환각(Hallucination) 100",
    "options": [
      "LLM은 모든 사실관계를 완벽하게 검증한다.",
      "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
      "LLM은 최신 뉴스를 실시간으로 스스로 학습한다.",
      "LLM은 수학 계산에 실수가 없다.",
      "환각은 데이터가 많으면 100% 사라진다."
    ],
    "answer": "환각은 모델이 사실인 것처럼 거짓 정보를 생성하는 현상이다.",
    "why": "LLM은 확률적으로 다음 단어를 예측하므로, 사실이 아닌 내용을 그럴듯하게 생성하는 환각 현상이 발생할 수 있습니다.",
    "hint": "거짓말을 그럴듯하게 하는 것",
    "difficulty": "medium",
    "id": "30100"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "텍스트를 토큰으로 변환하세요.\n```python\ntext = 'Hello'\ntokens = tokenizer.____(text)\n```",
    "answer": "encode",
    "why": "텍스트 -> 토큰 ID 변환은 encode()",
    "hint": "HuggingFace Library",
    "difficulty": "easy",
    "id": "3101"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "LLM 관련 코드를 완성하세요. (문제 2)",
    "answer": "transformers",
    "why": "HuggingFace의 핵심 라이브러리 이름은 transformers입니다.",
    "hint": "HuggingFace Library",
    "difficulty": "easy",
    "id": "3102"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "LLM 관련 코드를 완성하세요. (문제 3)",
    "answer": "transformers",
    "why": "HuggingFace의 핵심 라이브러리 이름은 transformers입니다.",
    "hint": "HuggingFace Library",
    "difficulty": "easy",
    "id": "3103"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "토크나이저를 로드하세요.\n```python\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.____('gpt2')\n```",
    "answer": "from_pretrained",
    "why": "사전 학습된 모델 로드 메서드",
    "hint": "HuggingFace Library",
    "difficulty": "easy",
    "id": "3104"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "텍스트를 토큰으로 변환하세요.\n```python\ntext = 'Hello'\ntokens = tokenizer.____(text)\n```",
    "answer": "encode",
    "why": "텍스트 -> 토큰 ID 변환은 encode()",
    "hint": "HuggingFace Library",
    "difficulty": "easy",
    "id": "3105"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "LLM 관련 코드를 완성하세요. (문제 6)",
    "answer": "transformers",
    "why": "HuggingFace의 핵심 라이브러리 이름은 transformers입니다.",
    "hint": "HuggingFace Library",
    "difficulty": "easy",
    "id": "3106"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "LLM 관련 코드를 완성하세요. (문제 7)",
    "answer": "transformers",
    "why": "HuggingFace의 핵심 라이브러리 이름은 transformers입니다.",
    "hint": "HuggingFace Library",
    "difficulty": "easy",
    "id": "3107"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "토크나이저를 로드하세요.\n```python\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.____('gpt2')\n```",
    "answer": "from_pretrained",
    "why": "사전 학습된 모델 로드 메서드",
    "hint": "HuggingFace Library",
    "difficulty": "easy",
    "id": "3108"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "텍스트를 토큰으로 변환하세요.\n```python\ntext = 'Hello'\ntokens = tokenizer.____(text)\n```",
    "answer": "encode",
    "why": "텍스트 -> 토큰 ID 변환은 encode()",
    "hint": "HuggingFace Library",
    "difficulty": "easy",
    "id": "3109"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "LLM 관련 코드를 완성하세요. (문제 10)",
    "answer": "transformers",
    "why": "HuggingFace의 핵심 라이브러리 이름은 transformers입니다.",
    "hint": "HuggingFace Library",
    "difficulty": "easy",
    "id": "3110"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "LLM 관련 코드를 완성하세요. (문제 11)",
    "answer": "transformers",
    "why": "HuggingFace의 핵심 라이브러리 이름은 transformers입니다.",
    "hint": "HuggingFace Library",
    "difficulty": "easy",
    "id": "3111"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "토크나이저를 로드하세요.\n```python\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.____('gpt2')\n```",
    "answer": "from_pretrained",
    "why": "사전 학습된 모델 로드 메서드",
    "hint": "HuggingFace Library",
    "difficulty": "easy",
    "id": "3112"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "텍스트를 토큰으로 변환하세요.\n```python\ntext = 'Hello'\ntokens = tokenizer.____(text)\n```",
    "answer": "encode",
    "why": "텍스트 -> 토큰 ID 변환은 encode()",
    "hint": "HuggingFace Library",
    "difficulty": "easy",
    "id": "3113"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "LLM 관련 코드를 완성하세요. (문제 14)",
    "answer": "transformers",
    "why": "HuggingFace의 핵심 라이브러리 이름은 transformers입니다.",
    "hint": "HuggingFace Library",
    "difficulty": "easy",
    "id": "3114"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "LLM 관련 코드를 완성하세요. (문제 15)",
    "answer": "transformers",
    "why": "HuggingFace의 핵심 라이브러리 이름은 transformers입니다.",
    "hint": "HuggingFace Library",
    "difficulty": "easy",
    "id": "3115"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "토크나이저를 로드하세요.\n```python\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.____('gpt2')\n```",
    "answer": "from_pretrained",
    "why": "사전 학습된 모델 로드 메서드",
    "hint": "HuggingFace Library",
    "difficulty": "easy",
    "id": "3116"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "텍스트를 토큰으로 변환하세요.\n```python\ntext = 'Hello'\ntokens = tokenizer.____(text)\n```",
    "answer": "encode",
    "why": "텍스트 -> 토큰 ID 변환은 encode()",
    "hint": "HuggingFace Library",
    "difficulty": "easy",
    "id": "3117"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "LLM 관련 코드를 완성하세요. (문제 18)",
    "answer": "transformers",
    "why": "HuggingFace의 핵심 라이브러리 이름은 transformers입니다.",
    "hint": "HuggingFace Library",
    "difficulty": "easy",
    "id": "3118"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "LLM 관련 코드를 완성하세요. (문제 19)",
    "answer": "transformers",
    "why": "HuggingFace의 핵심 라이브러리 이름은 transformers입니다.",
    "hint": "HuggingFace Library",
    "difficulty": "easy",
    "id": "3119"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "토크나이저를 로드하세요.\n```python\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.____('gpt2')\n```",
    "answer": "from_pretrained",
    "why": "사전 학습된 모델 로드 메서드",
    "hint": "HuggingFace Library",
    "difficulty": "easy",
    "id": "3120"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링의 기초 개념 1",
    "options": [
      "프롬프트는 모델의 가중치를 영구적으로 변경한다.",
      "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
      "프롬프트는 항상 영어로만 작성해야 한다.",
      "프롬프트 길이는 짧을수록 성능이 좋다.",
      "Context는 프롬프트에 포함될 수 없다."
    ],
    "answer": "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
    "why": "프롬프트 엔지니어링은 모델 파라미터를 수정하지 않고, 입력(질문)을 최적화하여 원하는 결과를 얻는 기술입니다.",
    "hint": "입력값의 중요성",
    "difficulty": "easy",
    "id": "4001"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링의 기초 개념 2",
    "options": [
      "프롬프트는 모델의 가중치를 영구적으로 변경한다.",
      "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
      "프롬프트는 항상 영어로만 작성해야 한다.",
      "프롬프트 길이는 짧을수록 성능이 좋다.",
      "Context는 프롬프트에 포함될 수 없다."
    ],
    "answer": "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
    "why": "프롬프트 엔지니어링은 모델 파라미터를 수정하지 않고, 입력(질문)을 최적화하여 원하는 결과를 얻는 기술입니다.",
    "hint": "입력값의 중요성",
    "difficulty": "easy",
    "id": "4002"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링의 기초 개념 3",
    "options": [
      "프롬프트는 모델의 가중치를 영구적으로 변경한다.",
      "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
      "프롬프트는 항상 영어로만 작성해야 한다.",
      "프롬프트 길이는 짧을수록 성능이 좋다.",
      "Context는 프롬프트에 포함될 수 없다."
    ],
    "answer": "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
    "why": "프롬프트 엔지니어링은 모델 파라미터를 수정하지 않고, 입력(질문)을 최적화하여 원하는 결과를 얻는 기술입니다.",
    "hint": "입력값의 중요성",
    "difficulty": "easy",
    "id": "4003"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링의 기초 개념 4",
    "options": [
      "프롬프트는 모델의 가중치를 영구적으로 변경한다.",
      "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
      "프롬프트는 항상 영어로만 작성해야 한다.",
      "프롬프트 길이는 짧을수록 성능이 좋다.",
      "Context는 프롬프트에 포함될 수 없다."
    ],
    "answer": "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
    "why": "프롬프트 엔지니어링은 모델 파라미터를 수정하지 않고, 입력(질문)을 최적화하여 원하는 결과를 얻는 기술입니다.",
    "hint": "입력값의 중요성",
    "difficulty": "easy",
    "id": "4004"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링의 기초 개념 5",
    "options": [
      "프롬프트는 모델의 가중치를 영구적으로 변경한다.",
      "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
      "프롬프트는 항상 영어로만 작성해야 한다.",
      "프롬프트 길이는 짧을수록 성능이 좋다.",
      "Context는 프롬프트에 포함될 수 없다."
    ],
    "answer": "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
    "why": "프롬프트 엔지니어링은 모델 파라미터를 수정하지 않고, 입력(질문)을 최적화하여 원하는 결과를 얻는 기술입니다.",
    "hint": "입력값의 중요성",
    "difficulty": "easy",
    "id": "4005"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링의 기초 개념 6",
    "options": [
      "프롬프트는 모델의 가중치를 영구적으로 변경한다.",
      "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
      "프롬프트는 항상 영어로만 작성해야 한다.",
      "프롬프트 길이는 짧을수록 성능이 좋다.",
      "Context는 프롬프트에 포함될 수 없다."
    ],
    "answer": "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
    "why": "프롬프트 엔지니어링은 모델 파라미터를 수정하지 않고, 입력(질문)을 최적화하여 원하는 결과를 얻는 기술입니다.",
    "hint": "입력값의 중요성",
    "difficulty": "easy",
    "id": "4006"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링의 기초 개념 7",
    "options": [
      "프롬프트는 모델의 가중치를 영구적으로 변경한다.",
      "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
      "프롬프트는 항상 영어로만 작성해야 한다.",
      "프롬프트 길이는 짧을수록 성능이 좋다.",
      "Context는 프롬프트에 포함될 수 없다."
    ],
    "answer": "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
    "why": "프롬프트 엔지니어링은 모델 파라미터를 수정하지 않고, 입력(질문)을 최적화하여 원하는 결과를 얻는 기술입니다.",
    "hint": "입력값의 중요성",
    "difficulty": "easy",
    "id": "4007"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링의 기초 개념 8",
    "options": [
      "프롬프트는 모델의 가중치를 영구적으로 변경한다.",
      "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
      "프롬프트는 항상 영어로만 작성해야 한다.",
      "프롬프트 길이는 짧을수록 성능이 좋다.",
      "Context는 프롬프트에 포함될 수 없다."
    ],
    "answer": "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
    "why": "프롬프트 엔지니어링은 모델 파라미터를 수정하지 않고, 입력(질문)을 최적화하여 원하는 결과를 얻는 기술입니다.",
    "hint": "입력값의 중요성",
    "difficulty": "easy",
    "id": "4008"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링의 기초 개념 9",
    "options": [
      "프롬프트는 모델의 가중치를 영구적으로 변경한다.",
      "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
      "프롬프트는 항상 영어로만 작성해야 한다.",
      "프롬프트 길이는 짧을수록 성능이 좋다.",
      "Context는 프롬프트에 포함될 수 없다."
    ],
    "answer": "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
    "why": "프롬프트 엔지니어링은 모델 파라미터를 수정하지 않고, 입력(질문)을 최적화하여 원하는 결과를 얻는 기술입니다.",
    "hint": "입력값의 중요성",
    "difficulty": "easy",
    "id": "4009"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링의 기초 개념 10",
    "options": [
      "프롬프트는 모델의 가중치를 영구적으로 변경한다.",
      "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
      "프롬프트는 항상 영어로만 작성해야 한다.",
      "프롬프트 길이는 짧을수록 성능이 좋다.",
      "Context는 프롬프트에 포함될 수 없다."
    ],
    "answer": "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
    "why": "프롬프트 엔지니어링은 모델 파라미터를 수정하지 않고, 입력(질문)을 최적화하여 원하는 결과를 얻는 기술입니다.",
    "hint": "입력값의 중요성",
    "difficulty": "easy",
    "id": "4010"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링의 기초 개념 11",
    "options": [
      "프롬프트는 모델의 가중치를 영구적으로 변경한다.",
      "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
      "프롬프트는 항상 영어로만 작성해야 한다.",
      "프롬프트 길이는 짧을수록 성능이 좋다.",
      "Context는 프롬프트에 포함될 수 없다."
    ],
    "answer": "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
    "why": "프롬프트 엔지니어링은 모델 파라미터를 수정하지 않고, 입력(질문)을 최적화하여 원하는 결과를 얻는 기술입니다.",
    "hint": "입력값의 중요성",
    "difficulty": "easy",
    "id": "4011"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링의 기초 개념 12",
    "options": [
      "프롬프트는 모델의 가중치를 영구적으로 변경한다.",
      "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
      "프롬프트는 항상 영어로만 작성해야 한다.",
      "프롬프트 길이는 짧을수록 성능이 좋다.",
      "Context는 프롬프트에 포함될 수 없다."
    ],
    "answer": "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
    "why": "프롬프트 엔지니어링은 모델 파라미터를 수정하지 않고, 입력(질문)을 최적화하여 원하는 결과를 얻는 기술입니다.",
    "hint": "입력값의 중요성",
    "difficulty": "easy",
    "id": "4012"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링의 기초 개념 13",
    "options": [
      "프롬프트는 모델의 가중치를 영구적으로 변경한다.",
      "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
      "프롬프트는 항상 영어로만 작성해야 한다.",
      "프롬프트 길이는 짧을수록 성능이 좋다.",
      "Context는 프롬프트에 포함될 수 없다."
    ],
    "answer": "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
    "why": "프롬프트 엔지니어링은 모델 파라미터를 수정하지 않고, 입력(질문)을 최적화하여 원하는 결과를 얻는 기술입니다.",
    "hint": "입력값의 중요성",
    "difficulty": "easy",
    "id": "4013"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링의 기초 개념 14",
    "options": [
      "프롬프트는 모델의 가중치를 영구적으로 변경한다.",
      "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
      "프롬프트는 항상 영어로만 작성해야 한다.",
      "프롬프트 길이는 짧을수록 성능이 좋다.",
      "Context는 프롬프트에 포함될 수 없다."
    ],
    "answer": "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
    "why": "프롬프트 엔지니어링은 모델 파라미터를 수정하지 않고, 입력(질문)을 최적화하여 원하는 결과를 얻는 기술입니다.",
    "hint": "입력값의 중요성",
    "difficulty": "easy",
    "id": "4014"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링의 기초 개념 15",
    "options": [
      "프롬프트는 모델의 가중치를 영구적으로 변경한다.",
      "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
      "프롬프트는 항상 영어로만 작성해야 한다.",
      "프롬프트 길이는 짧을수록 성능이 좋다.",
      "Context는 프롬프트에 포함될 수 없다."
    ],
    "answer": "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
    "why": "프롬프트 엔지니어링은 모델 파라미터를 수정하지 않고, 입력(질문)을 최적화하여 원하는 결과를 얻는 기술입니다.",
    "hint": "입력값의 중요성",
    "difficulty": "easy",
    "id": "4015"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링의 기초 개념 16",
    "options": [
      "프롬프트는 모델의 가중치를 영구적으로 변경한다.",
      "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
      "프롬프트는 항상 영어로만 작성해야 한다.",
      "프롬프트 길이는 짧을수록 성능이 좋다.",
      "Context는 프롬프트에 포함될 수 없다."
    ],
    "answer": "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
    "why": "프롬프트 엔지니어링은 모델 파라미터를 수정하지 않고, 입력(질문)을 최적화하여 원하는 결과를 얻는 기술입니다.",
    "hint": "입력값의 중요성",
    "difficulty": "easy",
    "id": "4016"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링의 기초 개념 17",
    "options": [
      "프롬프트는 모델의 가중치를 영구적으로 변경한다.",
      "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
      "프롬프트는 항상 영어로만 작성해야 한다.",
      "프롬프트 길이는 짧을수록 성능이 좋다.",
      "Context는 프롬프트에 포함될 수 없다."
    ],
    "answer": "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
    "why": "프롬프트 엔지니어링은 모델 파라미터를 수정하지 않고, 입력(질문)을 최적화하여 원하는 결과를 얻는 기술입니다.",
    "hint": "입력값의 중요성",
    "difficulty": "easy",
    "id": "4017"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링의 기초 개념 18",
    "options": [
      "프롬프트는 모델의 가중치를 영구적으로 변경한다.",
      "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
      "프롬프트는 항상 영어로만 작성해야 한다.",
      "프롬프트 길이는 짧을수록 성능이 좋다.",
      "Context는 프롬프트에 포함될 수 없다."
    ],
    "answer": "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
    "why": "프롬프트 엔지니어링은 모델 파라미터를 수정하지 않고, 입력(질문)을 최적화하여 원하는 결과를 얻는 기술입니다.",
    "hint": "입력값의 중요성",
    "difficulty": "easy",
    "id": "4018"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링의 기초 개념 19",
    "options": [
      "프롬프트는 모델의 가중치를 영구적으로 변경한다.",
      "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
      "프롬프트는 항상 영어로만 작성해야 한다.",
      "프롬프트 길이는 짧을수록 성능이 좋다.",
      "Context는 프롬프트에 포함될 수 없다."
    ],
    "answer": "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
    "why": "프롬프트 엔지니어링은 모델 파라미터를 수정하지 않고, 입력(질문)을 최적화하여 원하는 결과를 얻는 기술입니다.",
    "hint": "입력값의 중요성",
    "difficulty": "easy",
    "id": "4019"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링의 기초 개념 20",
    "options": [
      "프롬프트는 모델의 가중치를 영구적으로 변경한다.",
      "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
      "프롬프트는 항상 영어로만 작성해야 한다.",
      "프롬프트 길이는 짧을수록 성능이 좋다.",
      "Context는 프롬프트에 포함될 수 없다."
    ],
    "answer": "좋은 질문이 좋은 답변을 만든다는 'Garbage In, Garbage Out' 원칙이 적용된다.",
    "why": "프롬프트 엔지니어링은 모델 파라미터를 수정하지 않고, 입력(질문)을 최적화하여 원하는 결과를 얻는 기술입니다.",
    "hint": "입력값의 중요성",
    "difficulty": "easy",
    "id": "4020"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "복잡한 추론 문제를 해결하기 위해 '단계별로 생각해보자'라고 유도하는 기법은?",
    "options": [
      "Zero-shot",
      "One-shot",
      "Chain-of-Thought (CoT)",
      "Persona",
      "ReAct"
    ],
    "answer": "Chain-of-Thought (CoT)",
    "why": "CoT는 사고 과정을 단계별로 풀어서 추론 성능을 높입니다.",
    "hint": "Few(소량) + Shot(예시)",
    "difficulty": "medium",
    "id": "4021"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 기법에 대한 설명 22",
    "options": [
      "Zero-shot은 예시를 100개 이상 주는 것이다.",
      "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
      "CoT는 '생각하지 말고 답만 말해'라고 지시하는 기법이다.",
      "Persona Prompting은 모델에게 역할을 부여하지 않는다.",
      "Self-Consistency는 한 번만 생성하고 끝내는 방식이다."
    ],
    "answer": "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
    "why": "Few-shot Prompting은 소량의 예시(Shot)를 제공하여 모델이 작업의 패턴을 파악하게 돕습니다.",
    "hint": "Few(소량) + Shot(예시)",
    "difficulty": "medium",
    "id": "4022"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 기법에 대한 설명 23",
    "options": [
      "Zero-shot은 예시를 100개 이상 주는 것이다.",
      "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
      "CoT는 '생각하지 말고 답만 말해'라고 지시하는 기법이다.",
      "Persona Prompting은 모델에게 역할을 부여하지 않는다.",
      "Self-Consistency는 한 번만 생성하고 끝내는 방식이다."
    ],
    "answer": "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
    "why": "Few-shot Prompting은 소량의 예시(Shot)를 제공하여 모델이 작업의 패턴을 파악하게 돕습니다.",
    "hint": "Few(소량) + Shot(예시)",
    "difficulty": "medium",
    "id": "4023"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "복잡한 추론 문제를 해결하기 위해 '단계별로 생각해보자'라고 유도하는 기법은?",
    "options": [
      "Zero-shot",
      "One-shot",
      "Chain-of-Thought (CoT)",
      "Persona",
      "ReAct"
    ],
    "answer": "Chain-of-Thought (CoT)",
    "why": "CoT는 사고 과정을 단계별로 풀어서 추론 성능을 높입니다.",
    "hint": "Few(소량) + Shot(예시)",
    "difficulty": "medium",
    "id": "4024"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 기법에 대한 설명 25",
    "options": [
      "Zero-shot은 예시를 100개 이상 주는 것이다.",
      "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
      "CoT는 '생각하지 말고 답만 말해'라고 지시하는 기법이다.",
      "Persona Prompting은 모델에게 역할을 부여하지 않는다.",
      "Self-Consistency는 한 번만 생성하고 끝내는 방식이다."
    ],
    "answer": "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
    "why": "Few-shot Prompting은 소량의 예시(Shot)를 제공하여 모델이 작업의 패턴을 파악하게 돕습니다.",
    "hint": "Few(소량) + Shot(예시)",
    "difficulty": "medium",
    "id": "4025"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 기법에 대한 설명 26",
    "options": [
      "Zero-shot은 예시를 100개 이상 주는 것이다.",
      "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
      "CoT는 '생각하지 말고 답만 말해'라고 지시하는 기법이다.",
      "Persona Prompting은 모델에게 역할을 부여하지 않는다.",
      "Self-Consistency는 한 번만 생성하고 끝내는 방식이다."
    ],
    "answer": "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
    "why": "Few-shot Prompting은 소량의 예시(Shot)를 제공하여 모델이 작업의 패턴을 파악하게 돕습니다.",
    "hint": "Few(소량) + Shot(예시)",
    "difficulty": "medium",
    "id": "4026"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "복잡한 추론 문제를 해결하기 위해 '단계별로 생각해보자'라고 유도하는 기법은?",
    "options": [
      "Zero-shot",
      "One-shot",
      "Chain-of-Thought (CoT)",
      "Persona",
      "ReAct"
    ],
    "answer": "Chain-of-Thought (CoT)",
    "why": "CoT는 사고 과정을 단계별로 풀어서 추론 성능을 높입니다.",
    "hint": "Few(소량) + Shot(예시)",
    "difficulty": "medium",
    "id": "4027"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 기법에 대한 설명 28",
    "options": [
      "Zero-shot은 예시를 100개 이상 주는 것이다.",
      "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
      "CoT는 '생각하지 말고 답만 말해'라고 지시하는 기법이다.",
      "Persona Prompting은 모델에게 역할을 부여하지 않는다.",
      "Self-Consistency는 한 번만 생성하고 끝내는 방식이다."
    ],
    "answer": "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
    "why": "Few-shot Prompting은 소량의 예시(Shot)를 제공하여 모델이 작업의 패턴을 파악하게 돕습니다.",
    "hint": "Few(소량) + Shot(예시)",
    "difficulty": "medium",
    "id": "4028"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 기법에 대한 설명 29",
    "options": [
      "Zero-shot은 예시를 100개 이상 주는 것이다.",
      "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
      "CoT는 '생각하지 말고 답만 말해'라고 지시하는 기법이다.",
      "Persona Prompting은 모델에게 역할을 부여하지 않는다.",
      "Self-Consistency는 한 번만 생성하고 끝내는 방식이다."
    ],
    "answer": "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
    "why": "Few-shot Prompting은 소량의 예시(Shot)를 제공하여 모델이 작업의 패턴을 파악하게 돕습니다.",
    "hint": "Few(소량) + Shot(예시)",
    "difficulty": "medium",
    "id": "4029"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "복잡한 추론 문제를 해결하기 위해 '단계별로 생각해보자'라고 유도하는 기법은?",
    "options": [
      "Zero-shot",
      "One-shot",
      "Chain-of-Thought (CoT)",
      "Persona",
      "ReAct"
    ],
    "answer": "Chain-of-Thought (CoT)",
    "why": "CoT는 사고 과정을 단계별로 풀어서 추론 성능을 높입니다.",
    "hint": "Few(소량) + Shot(예시)",
    "difficulty": "medium",
    "id": "4030"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 기법에 대한 설명 31",
    "options": [
      "Zero-shot은 예시를 100개 이상 주는 것이다.",
      "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
      "CoT는 '생각하지 말고 답만 말해'라고 지시하는 기법이다.",
      "Persona Prompting은 모델에게 역할을 부여하지 않는다.",
      "Self-Consistency는 한 번만 생성하고 끝내는 방식이다."
    ],
    "answer": "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
    "why": "Few-shot Prompting은 소량의 예시(Shot)를 제공하여 모델이 작업의 패턴을 파악하게 돕습니다.",
    "hint": "Few(소량) + Shot(예시)",
    "difficulty": "medium",
    "id": "4031"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 기법에 대한 설명 32",
    "options": [
      "Zero-shot은 예시를 100개 이상 주는 것이다.",
      "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
      "CoT는 '생각하지 말고 답만 말해'라고 지시하는 기법이다.",
      "Persona Prompting은 모델에게 역할을 부여하지 않는다.",
      "Self-Consistency는 한 번만 생성하고 끝내는 방식이다."
    ],
    "answer": "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
    "why": "Few-shot Prompting은 소량의 예시(Shot)를 제공하여 모델이 작업의 패턴을 파악하게 돕습니다.",
    "hint": "Few(소량) + Shot(예시)",
    "difficulty": "medium",
    "id": "4032"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "복잡한 추론 문제를 해결하기 위해 '단계별로 생각해보자'라고 유도하는 기법은?",
    "options": [
      "Zero-shot",
      "One-shot",
      "Chain-of-Thought (CoT)",
      "Persona",
      "ReAct"
    ],
    "answer": "Chain-of-Thought (CoT)",
    "why": "CoT는 사고 과정을 단계별로 풀어서 추론 성능을 높입니다.",
    "hint": "Few(소량) + Shot(예시)",
    "difficulty": "medium",
    "id": "4033"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 기법에 대한 설명 34",
    "options": [
      "Zero-shot은 예시를 100개 이상 주는 것이다.",
      "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
      "CoT는 '생각하지 말고 답만 말해'라고 지시하는 기법이다.",
      "Persona Prompting은 모델에게 역할을 부여하지 않는다.",
      "Self-Consistency는 한 번만 생성하고 끝내는 방식이다."
    ],
    "answer": "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
    "why": "Few-shot Prompting은 소량의 예시(Shot)를 제공하여 모델이 작업의 패턴을 파악하게 돕습니다.",
    "hint": "Few(소량) + Shot(예시)",
    "difficulty": "medium",
    "id": "4034"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 기법에 대한 설명 35",
    "options": [
      "Zero-shot은 예시를 100개 이상 주는 것이다.",
      "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
      "CoT는 '생각하지 말고 답만 말해'라고 지시하는 기법이다.",
      "Persona Prompting은 모델에게 역할을 부여하지 않는다.",
      "Self-Consistency는 한 번만 생성하고 끝내는 방식이다."
    ],
    "answer": "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
    "why": "Few-shot Prompting은 소량의 예시(Shot)를 제공하여 모델이 작업의 패턴을 파악하게 돕습니다.",
    "hint": "Few(소량) + Shot(예시)",
    "difficulty": "medium",
    "id": "4035"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "복잡한 추론 문제를 해결하기 위해 '단계별로 생각해보자'라고 유도하는 기법은?",
    "options": [
      "Zero-shot",
      "One-shot",
      "Chain-of-Thought (CoT)",
      "Persona",
      "ReAct"
    ],
    "answer": "Chain-of-Thought (CoT)",
    "why": "CoT는 사고 과정을 단계별로 풀어서 추론 성능을 높입니다.",
    "hint": "Few(소량) + Shot(예시)",
    "difficulty": "medium",
    "id": "4036"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 기법에 대한 설명 37",
    "options": [
      "Zero-shot은 예시를 100개 이상 주는 것이다.",
      "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
      "CoT는 '생각하지 말고 답만 말해'라고 지시하는 기법이다.",
      "Persona Prompting은 모델에게 역할을 부여하지 않는다.",
      "Self-Consistency는 한 번만 생성하고 끝내는 방식이다."
    ],
    "answer": "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
    "why": "Few-shot Prompting은 소량의 예시(Shot)를 제공하여 모델이 작업의 패턴을 파악하게 돕습니다.",
    "hint": "Few(소량) + Shot(예시)",
    "difficulty": "medium",
    "id": "4037"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 기법에 대한 설명 38",
    "options": [
      "Zero-shot은 예시를 100개 이상 주는 것이다.",
      "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
      "CoT는 '생각하지 말고 답만 말해'라고 지시하는 기법이다.",
      "Persona Prompting은 모델에게 역할을 부여하지 않는다.",
      "Self-Consistency는 한 번만 생성하고 끝내는 방식이다."
    ],
    "answer": "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
    "why": "Few-shot Prompting은 소량의 예시(Shot)를 제공하여 모델이 작업의 패턴을 파악하게 돕습니다.",
    "hint": "Few(소량) + Shot(예시)",
    "difficulty": "medium",
    "id": "4038"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "복잡한 추론 문제를 해결하기 위해 '단계별로 생각해보자'라고 유도하는 기법은?",
    "options": [
      "Zero-shot",
      "One-shot",
      "Chain-of-Thought (CoT)",
      "Persona",
      "ReAct"
    ],
    "answer": "Chain-of-Thought (CoT)",
    "why": "CoT는 사고 과정을 단계별로 풀어서 추론 성능을 높입니다.",
    "hint": "Few(소량) + Shot(예시)",
    "difficulty": "medium",
    "id": "4039"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 기법에 대한 설명 40",
    "options": [
      "Zero-shot은 예시를 100개 이상 주는 것이다.",
      "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
      "CoT는 '생각하지 말고 답만 말해'라고 지시하는 기법이다.",
      "Persona Prompting은 모델에게 역할을 부여하지 않는다.",
      "Self-Consistency는 한 번만 생성하고 끝내는 방식이다."
    ],
    "answer": "Few-shot은 예시를 제공하여 패턴을 학습시킨다.",
    "why": "Few-shot Prompting은 소량의 예시(Shot)를 제공하여 모델이 작업의 패턴을 파악하게 돕습니다.",
    "hint": "Few(소량) + Shot(예시)",
    "difficulty": "medium",
    "id": "4040"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "Persona 및 출력 형식 설정 41",
    "options": [
      "모델에게 역할을 부여해도 답변 품질은 변하지 않는다.",
      "출력 형식을 JSON으로 지정하는 것은 불가능하다.",
      "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
      "부정적인 지시('~하지 마')가 긍정적인 지시보다 항상 효과적이다.",
      "구분자(Delimiter) 사용은 모델을 혼란스럽게 한다."
    ],
    "answer": "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
    "why": "Persona Prompting은 모델에게 특정 페르소나(역할)를 부여하여 전문적이고 일관된 답변을 유도합니다.",
    "hint": "Role Playing",
    "difficulty": "easy",
    "id": "4041"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "Persona 및 출력 형식 설정 42",
    "options": [
      "모델에게 역할을 부여해도 답변 품질은 변하지 않는다.",
      "출력 형식을 JSON으로 지정하는 것은 불가능하다.",
      "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
      "부정적인 지시('~하지 마')가 긍정적인 지시보다 항상 효과적이다.",
      "구분자(Delimiter) 사용은 모델을 혼란스럽게 한다."
    ],
    "answer": "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
    "why": "Persona Prompting은 모델에게 특정 페르소나(역할)를 부여하여 전문적이고 일관된 답변을 유도합니다.",
    "hint": "Role Playing",
    "difficulty": "easy",
    "id": "4042"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "Persona 및 출력 형식 설정 43",
    "options": [
      "모델에게 역할을 부여해도 답변 품질은 변하지 않는다.",
      "출력 형식을 JSON으로 지정하는 것은 불가능하다.",
      "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
      "부정적인 지시('~하지 마')가 긍정적인 지시보다 항상 효과적이다.",
      "구분자(Delimiter) 사용은 모델을 혼란스럽게 한다."
    ],
    "answer": "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
    "why": "Persona Prompting은 모델에게 특정 페르소나(역할)를 부여하여 전문적이고 일관된 답변을 유도합니다.",
    "hint": "Role Playing",
    "difficulty": "easy",
    "id": "4043"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "Persona 및 출력 형식 설정 44",
    "options": [
      "모델에게 역할을 부여해도 답변 품질은 변하지 않는다.",
      "출력 형식을 JSON으로 지정하는 것은 불가능하다.",
      "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
      "부정적인 지시('~하지 마')가 긍정적인 지시보다 항상 효과적이다.",
      "구분자(Delimiter) 사용은 모델을 혼란스럽게 한다."
    ],
    "answer": "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
    "why": "Persona Prompting은 모델에게 특정 페르소나(역할)를 부여하여 전문적이고 일관된 답변을 유도합니다.",
    "hint": "Role Playing",
    "difficulty": "easy",
    "id": "4044"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "Persona 및 출력 형식 설정 45",
    "options": [
      "모델에게 역할을 부여해도 답변 품질은 변하지 않는다.",
      "출력 형식을 JSON으로 지정하는 것은 불가능하다.",
      "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
      "부정적인 지시('~하지 마')가 긍정적인 지시보다 항상 효과적이다.",
      "구분자(Delimiter) 사용은 모델을 혼란스럽게 한다."
    ],
    "answer": "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
    "why": "Persona Prompting은 모델에게 특정 페르소나(역할)를 부여하여 전문적이고 일관된 답변을 유도합니다.",
    "hint": "Role Playing",
    "difficulty": "easy",
    "id": "4045"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "Persona 및 출력 형식 설정 46",
    "options": [
      "모델에게 역할을 부여해도 답변 품질은 변하지 않는다.",
      "출력 형식을 JSON으로 지정하는 것은 불가능하다.",
      "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
      "부정적인 지시('~하지 마')가 긍정적인 지시보다 항상 효과적이다.",
      "구분자(Delimiter) 사용은 모델을 혼란스럽게 한다."
    ],
    "answer": "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
    "why": "Persona Prompting은 모델에게 특정 페르소나(역할)를 부여하여 전문적이고 일관된 답변을 유도합니다.",
    "hint": "Role Playing",
    "difficulty": "easy",
    "id": "4046"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "Persona 및 출력 형식 설정 47",
    "options": [
      "모델에게 역할을 부여해도 답변 품질은 변하지 않는다.",
      "출력 형식을 JSON으로 지정하는 것은 불가능하다.",
      "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
      "부정적인 지시('~하지 마')가 긍정적인 지시보다 항상 효과적이다.",
      "구분자(Delimiter) 사용은 모델을 혼란스럽게 한다."
    ],
    "answer": "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
    "why": "Persona Prompting은 모델에게 특정 페르소나(역할)를 부여하여 전문적이고 일관된 답변을 유도합니다.",
    "hint": "Role Playing",
    "difficulty": "easy",
    "id": "4047"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "Persona 및 출력 형식 설정 48",
    "options": [
      "모델에게 역할을 부여해도 답변 품질은 변하지 않는다.",
      "출력 형식을 JSON으로 지정하는 것은 불가능하다.",
      "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
      "부정적인 지시('~하지 마')가 긍정적인 지시보다 항상 효과적이다.",
      "구분자(Delimiter) 사용은 모델을 혼란스럽게 한다."
    ],
    "answer": "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
    "why": "Persona Prompting은 모델에게 특정 페르소나(역할)를 부여하여 전문적이고 일관된 답변을 유도합니다.",
    "hint": "Role Playing",
    "difficulty": "easy",
    "id": "4048"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "Persona 및 출력 형식 설정 49",
    "options": [
      "모델에게 역할을 부여해도 답변 품질은 변하지 않는다.",
      "출력 형식을 JSON으로 지정하는 것은 불가능하다.",
      "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
      "부정적인 지시('~하지 마')가 긍정적인 지시보다 항상 효과적이다.",
      "구분자(Delimiter) 사용은 모델을 혼란스럽게 한다."
    ],
    "answer": "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
    "why": "Persona Prompting은 모델에게 특정 페르소나(역할)를 부여하여 전문적이고 일관된 답변을 유도합니다.",
    "hint": "Role Playing",
    "difficulty": "easy",
    "id": "4049"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "Persona 및 출력 형식 설정 50",
    "options": [
      "모델에게 역할을 부여해도 답변 품질은 변하지 않는다.",
      "출력 형식을 JSON으로 지정하는 것은 불가능하다.",
      "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
      "부정적인 지시('~하지 마')가 긍정적인 지시보다 항상 효과적이다.",
      "구분자(Delimiter) 사용은 모델을 혼란스럽게 한다."
    ],
    "answer": "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
    "why": "Persona Prompting은 모델에게 특정 페르소나(역할)를 부여하여 전문적이고 일관된 답변을 유도합니다.",
    "hint": "Role Playing",
    "difficulty": "easy",
    "id": "4050"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "Persona 및 출력 형식 설정 51",
    "options": [
      "모델에게 역할을 부여해도 답변 품질은 변하지 않는다.",
      "출력 형식을 JSON으로 지정하는 것은 불가능하다.",
      "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
      "부정적인 지시('~하지 마')가 긍정적인 지시보다 항상 효과적이다.",
      "구분자(Delimiter) 사용은 모델을 혼란스럽게 한다."
    ],
    "answer": "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
    "why": "Persona Prompting은 모델에게 특정 페르소나(역할)를 부여하여 전문적이고 일관된 답변을 유도합니다.",
    "hint": "Role Playing",
    "difficulty": "easy",
    "id": "4051"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "Persona 및 출력 형식 설정 52",
    "options": [
      "모델에게 역할을 부여해도 답변 품질은 변하지 않는다.",
      "출력 형식을 JSON으로 지정하는 것은 불가능하다.",
      "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
      "부정적인 지시('~하지 마')가 긍정적인 지시보다 항상 효과적이다.",
      "구분자(Delimiter) 사용은 모델을 혼란스럽게 한다."
    ],
    "answer": "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
    "why": "Persona Prompting은 모델에게 특정 페르소나(역할)를 부여하여 전문적이고 일관된 답변을 유도합니다.",
    "hint": "Role Playing",
    "difficulty": "easy",
    "id": "4052"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "Persona 및 출력 형식 설정 53",
    "options": [
      "모델에게 역할을 부여해도 답변 품질은 변하지 않는다.",
      "출력 형식을 JSON으로 지정하는 것은 불가능하다.",
      "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
      "부정적인 지시('~하지 마')가 긍정적인 지시보다 항상 효과적이다.",
      "구분자(Delimiter) 사용은 모델을 혼란스럽게 한다."
    ],
    "answer": "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
    "why": "Persona Prompting은 모델에게 특정 페르소나(역할)를 부여하여 전문적이고 일관된 답변을 유도합니다.",
    "hint": "Role Playing",
    "difficulty": "easy",
    "id": "4053"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "Persona 및 출력 형식 설정 54",
    "options": [
      "모델에게 역할을 부여해도 답변 품질은 변하지 않는다.",
      "출력 형식을 JSON으로 지정하는 것은 불가능하다.",
      "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
      "부정적인 지시('~하지 마')가 긍정적인 지시보다 항상 효과적이다.",
      "구분자(Delimiter) 사용은 모델을 혼란스럽게 한다."
    ],
    "answer": "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
    "why": "Persona Prompting은 모델에게 특정 페르소나(역할)를 부여하여 전문적이고 일관된 답변을 유도합니다.",
    "hint": "Role Playing",
    "difficulty": "easy",
    "id": "4054"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "Persona 및 출력 형식 설정 55",
    "options": [
      "모델에게 역할을 부여해도 답변 품질은 변하지 않는다.",
      "출력 형식을 JSON으로 지정하는 것은 불가능하다.",
      "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
      "부정적인 지시('~하지 마')가 긍정적인 지시보다 항상 효과적이다.",
      "구분자(Delimiter) 사용은 모델을 혼란스럽게 한다."
    ],
    "answer": "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
    "why": "Persona Prompting은 모델에게 특정 페르소나(역할)를 부여하여 전문적이고 일관된 답변을 유도합니다.",
    "hint": "Role Playing",
    "difficulty": "easy",
    "id": "4055"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "Persona 및 출력 형식 설정 56",
    "options": [
      "모델에게 역할을 부여해도 답변 품질은 변하지 않는다.",
      "출력 형식을 JSON으로 지정하는 것은 불가능하다.",
      "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
      "부정적인 지시('~하지 마')가 긍정적인 지시보다 항상 효과적이다.",
      "구분자(Delimiter) 사용은 모델을 혼란스럽게 한다."
    ],
    "answer": "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
    "why": "Persona Prompting은 모델에게 특정 페르소나(역할)를 부여하여 전문적이고 일관된 답변을 유도합니다.",
    "hint": "Role Playing",
    "difficulty": "easy",
    "id": "4056"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "Persona 및 출력 형식 설정 57",
    "options": [
      "모델에게 역할을 부여해도 답변 품질은 변하지 않는다.",
      "출력 형식을 JSON으로 지정하는 것은 불가능하다.",
      "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
      "부정적인 지시('~하지 마')가 긍정적인 지시보다 항상 효과적이다.",
      "구분자(Delimiter) 사용은 모델을 혼란스럽게 한다."
    ],
    "answer": "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
    "why": "Persona Prompting은 모델에게 특정 페르소나(역할)를 부여하여 전문적이고 일관된 답변을 유도합니다.",
    "hint": "Role Playing",
    "difficulty": "easy",
    "id": "4057"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "Persona 및 출력 형식 설정 58",
    "options": [
      "모델에게 역할을 부여해도 답변 품질은 변하지 않는다.",
      "출력 형식을 JSON으로 지정하는 것은 불가능하다.",
      "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
      "부정적인 지시('~하지 마')가 긍정적인 지시보다 항상 효과적이다.",
      "구분자(Delimiter) 사용은 모델을 혼란스럽게 한다."
    ],
    "answer": "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
    "why": "Persona Prompting은 모델에게 특정 페르소나(역할)를 부여하여 전문적이고 일관된 답변을 유도합니다.",
    "hint": "Role Playing",
    "difficulty": "easy",
    "id": "4058"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "Persona 및 출력 형식 설정 59",
    "options": [
      "모델에게 역할을 부여해도 답변 품질은 변하지 않는다.",
      "출력 형식을 JSON으로 지정하는 것은 불가능하다.",
      "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
      "부정적인 지시('~하지 마')가 긍정적인 지시보다 항상 효과적이다.",
      "구분자(Delimiter) 사용은 모델을 혼란스럽게 한다."
    ],
    "answer": "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
    "why": "Persona Prompting은 모델에게 특정 페르소나(역할)를 부여하여 전문적이고 일관된 답변을 유도합니다.",
    "hint": "Role Playing",
    "difficulty": "easy",
    "id": "4059"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "Persona 및 출력 형식 설정 60",
    "options": [
      "모델에게 역할을 부여해도 답변 품질은 변하지 않는다.",
      "출력 형식을 JSON으로 지정하는 것은 불가능하다.",
      "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
      "부정적인 지시('~하지 마')가 긍정적인 지시보다 항상 효과적이다.",
      "구분자(Delimiter) 사용은 모델을 혼란스럽게 한다."
    ],
    "answer": "'너는 수학 선생님이야'와 같은 지시는 Persona Prompting에 해당한다.",
    "why": "Persona Prompting은 모델에게 특정 페르소나(역할)를 부여하여 전문적이고 일관된 답변을 유도합니다.",
    "hint": "Role Playing",
    "difficulty": "easy",
    "id": "4060"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 보안 및 고급 주제 61",
    "options": [
      "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
      "샌드위치 기법은 프롬프트 인이 불필요하다는 이론이다.",
      "LLM은 편향(Bias)이 전혀 없는 완벽한 답변만 생성한다.",
      "환각(Hallucination)은 프롬프트 엔지니어링으로 100% 제거할 수 있다.",
      "프롬프트 길이는 무제한이다."
    ],
    "answer": "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
    "why": "Prompt Injection은 악의적인 사용자가 프롬프트를 조작하여 모델이 의도치 않은 동작을 하도록 만드는 보안 위협입니다.",
    "hint": "Injection(주입) 공격",
    "difficulty": "hard",
    "id": "4061"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 보안 및 고급 주제 62",
    "options": [
      "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
      "샌드위치 기법은 프롬프트 인이 불필요하다는 이론이다.",
      "LLM은 편향(Bias)이 전혀 없는 완벽한 답변만 생성한다.",
      "환각(Hallucination)은 프롬프트 엔지니어링으로 100% 제거할 수 있다.",
      "프롬프트 길이는 무제한이다."
    ],
    "answer": "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
    "why": "Prompt Injection은 악의적인 사용자가 프롬프트를 조작하여 모델이 의도치 않은 동작을 하도록 만드는 보안 위협입니다.",
    "hint": "Injection(주입) 공격",
    "difficulty": "hard",
    "id": "4062"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 보안 및 고급 주제 63",
    "options": [
      "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
      "샌드위치 기법은 프롬프트 인이 불필요하다는 이론이다.",
      "LLM은 편향(Bias)이 전혀 없는 완벽한 답변만 생성한다.",
      "환각(Hallucination)은 프롬프트 엔지니어링으로 100% 제거할 수 있다.",
      "프롬프트 길이는 무제한이다."
    ],
    "answer": "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
    "why": "Prompt Injection은 악의적인 사용자가 프롬프트를 조작하여 모델이 의도치 않은 동작을 하도록 만드는 보안 위협입니다.",
    "hint": "Injection(주입) 공격",
    "difficulty": "hard",
    "id": "4063"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 보안 및 고급 주제 64",
    "options": [
      "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
      "샌드위치 기법은 프롬프트 인이 불필요하다는 이론이다.",
      "LLM은 편향(Bias)이 전혀 없는 완벽한 답변만 생성한다.",
      "환각(Hallucination)은 프롬프트 엔지니어링으로 100% 제거할 수 있다.",
      "프롬프트 길이는 무제한이다."
    ],
    "answer": "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
    "why": "Prompt Injection은 악의적인 사용자가 프롬프트를 조작하여 모델이 의도치 않은 동작을 하도록 만드는 보안 위협입니다.",
    "hint": "Injection(주입) 공격",
    "difficulty": "hard",
    "id": "4064"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 보안 및 고급 주제 65",
    "options": [
      "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
      "샌드위치 기법은 프롬프트 인이 불필요하다는 이론이다.",
      "LLM은 편향(Bias)이 전혀 없는 완벽한 답변만 생성한다.",
      "환각(Hallucination)은 프롬프트 엔지니어링으로 100% 제거할 수 있다.",
      "프롬프트 길이는 무제한이다."
    ],
    "answer": "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
    "why": "Prompt Injection은 악의적인 사용자가 프롬프트를 조작하여 모델이 의도치 않은 동작을 하도록 만드는 보안 위협입니다.",
    "hint": "Injection(주입) 공격",
    "difficulty": "hard",
    "id": "4065"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 보안 및 고급 주제 66",
    "options": [
      "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
      "샌드위치 기법은 프롬프트 인이 불필요하다는 이론이다.",
      "LLM은 편향(Bias)이 전혀 없는 완벽한 답변만 생성한다.",
      "환각(Hallucination)은 프롬프트 엔지니어링으로 100% 제거할 수 있다.",
      "프롬프트 길이는 무제한이다."
    ],
    "answer": "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
    "why": "Prompt Injection은 악의적인 사용자가 프롬프트를 조작하여 모델이 의도치 않은 동작을 하도록 만드는 보안 위협입니다.",
    "hint": "Injection(주입) 공격",
    "difficulty": "hard",
    "id": "4066"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 보안 및 고급 주제 67",
    "options": [
      "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
      "샌드위치 기법은 프롬프트 인이 불필요하다는 이론이다.",
      "LLM은 편향(Bias)이 전혀 없는 완벽한 답변만 생성한다.",
      "환각(Hallucination)은 프롬프트 엔지니어링으로 100% 제거할 수 있다.",
      "프롬프트 길이는 무제한이다."
    ],
    "answer": "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
    "why": "Prompt Injection은 악의적인 사용자가 프롬프트를 조작하여 모델이 의도치 않은 동작을 하도록 만드는 보안 위협입니다.",
    "hint": "Injection(주입) 공격",
    "difficulty": "hard",
    "id": "4067"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 보안 및 고급 주제 68",
    "options": [
      "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
      "샌드위치 기법은 프롬프트 인이 불필요하다는 이론이다.",
      "LLM은 편향(Bias)이 전혀 없는 완벽한 답변만 생성한다.",
      "환각(Hallucination)은 프롬프트 엔지니어링으로 100% 제거할 수 있다.",
      "프롬프트 길이는 무제한이다."
    ],
    "answer": "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
    "why": "Prompt Injection은 악의적인 사용자가 프롬프트를 조작하여 모델이 의도치 않은 동작을 하도록 만드는 보안 위협입니다.",
    "hint": "Injection(주입) 공격",
    "difficulty": "hard",
    "id": "4068"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 보안 및 고급 주제 69",
    "options": [
      "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
      "샌드위치 기법은 프롬프트 인이 불필요하다는 이론이다.",
      "LLM은 편향(Bias)이 전혀 없는 완벽한 답변만 생성한다.",
      "환각(Hallucination)은 프롬프트 엔지니어링으로 100% 제거할 수 있다.",
      "프롬프트 길이는 무제한이다."
    ],
    "answer": "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
    "why": "Prompt Injection은 악의적인 사용자가 프롬프트를 조작하여 모델이 의도치 않은 동작을 하도록 만드는 보안 위협입니다.",
    "hint": "Injection(주입) 공격",
    "difficulty": "hard",
    "id": "4069"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 보안 및 고급 주제 70",
    "options": [
      "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
      "샌드위치 기법은 프롬프트 인이 불필요하다는 이론이다.",
      "LLM은 편향(Bias)이 전혀 없는 완벽한 답변만 생성한다.",
      "환각(Hallucination)은 프롬프트 엔지니어링으로 100% 제거할 수 있다.",
      "프롬프트 길이는 무제한이다."
    ],
    "answer": "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
    "why": "Prompt Injection은 악의적인 사용자가 프롬프트를 조작하여 모델이 의도치 않은 동작을 하도록 만드는 보안 위협입니다.",
    "hint": "Injection(주입) 공격",
    "difficulty": "hard",
    "id": "4070"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 보안 및 고급 주제 71",
    "options": [
      "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
      "샌드위치 기법은 프롬프트 인이 불필요하다는 이론이다.",
      "LLM은 편향(Bias)이 전혀 없는 완벽한 답변만 생성한다.",
      "환각(Hallucination)은 프롬프트 엔지니어링으로 100% 제거할 수 있다.",
      "프롬프트 길이는 무제한이다."
    ],
    "answer": "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
    "why": "Prompt Injection은 악의적인 사용자가 프롬프트를 조작하여 모델이 의도치 않은 동작을 하도록 만드는 보안 위협입니다.",
    "hint": "Injection(주입) 공격",
    "difficulty": "hard",
    "id": "4071"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 보안 및 고급 주제 72",
    "options": [
      "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
      "샌드위치 기법은 프롬프트 인이 불필요하다는 이론이다.",
      "LLM은 편향(Bias)이 전혀 없는 완벽한 답변만 생성한다.",
      "환각(Hallucination)은 프롬프트 엔지니어링으로 100% 제거할 수 있다.",
      "프롬프트 길이는 무제한이다."
    ],
    "answer": "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
    "why": "Prompt Injection은 악의적인 사용자가 프롬프트를 조작하여 모델이 의도치 않은 동작을 하도록 만드는 보안 위협입니다.",
    "hint": "Injection(주입) 공격",
    "difficulty": "hard",
    "id": "4072"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 보안 및 고급 주제 73",
    "options": [
      "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
      "샌드위치 기법은 프롬프트 인이 불필요하다는 이론이다.",
      "LLM은 편향(Bias)이 전혀 없는 완벽한 답변만 생성한다.",
      "환각(Hallucination)은 프롬프트 엔지니어링으로 100% 제거할 수 있다.",
      "프롬프트 길이는 무제한이다."
    ],
    "answer": "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
    "why": "Prompt Injection은 악의적인 사용자가 프롬프트를 조작하여 모델이 의도치 않은 동작을 하도록 만드는 보안 위협입니다.",
    "hint": "Injection(주입) 공격",
    "difficulty": "hard",
    "id": "4073"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 보안 및 고급 주제 74",
    "options": [
      "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
      "샌드위치 기법은 프롬프트 인이 불필요하다는 이론이다.",
      "LLM은 편향(Bias)이 전혀 없는 완벽한 답변만 생성한다.",
      "환각(Hallucination)은 프롬프트 엔지니어링으로 100% 제거할 수 있다.",
      "프롬프트 길이는 무제한이다."
    ],
    "answer": "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
    "why": "Prompt Injection은 악의적인 사용자가 프롬프트를 조작하여 모델이 의도치 않은 동작을 하도록 만드는 보안 위협입니다.",
    "hint": "Injection(주입) 공격",
    "difficulty": "hard",
    "id": "4074"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 보안 및 고급 주제 75",
    "options": [
      "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
      "샌드위치 기법은 프롬프트 인이 불필요하다는 이론이다.",
      "LLM은 편향(Bias)이 전혀 없는 완벽한 답변만 생성한다.",
      "환각(Hallucination)은 프롬프트 엔지니어링으로 100% 제거할 수 있다.",
      "프롬프트 길이는 무제한이다."
    ],
    "answer": "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
    "why": "Prompt Injection은 악의적인 사용자가 프롬프트를 조작하여 모델이 의도치 않은 동작을 하도록 만드는 보안 위협입니다.",
    "hint": "Injection(주입) 공격",
    "difficulty": "hard",
    "id": "4075"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 보안 및 고급 주제 76",
    "options": [
      "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
      "샌드위치 기법은 프롬프트 인이 불필요하다는 이론이다.",
      "LLM은 편향(Bias)이 전혀 없는 완벽한 답변만 생성한다.",
      "환각(Hallucination)은 프롬프트 엔지니어링으로 100% 제거할 수 있다.",
      "프롬프트 길이는 무제한이다."
    ],
    "answer": "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
    "why": "Prompt Injection은 악의적인 사용자가 프롬프트를 조작하여 모델이 의도치 않은 동작을 하도록 만드는 보안 위협입니다.",
    "hint": "Injection(주입) 공격",
    "difficulty": "hard",
    "id": "4076"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 보안 및 고급 주제 77",
    "options": [
      "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
      "샌드위치 기법은 프롬프트 인이 불필요하다는 이론이다.",
      "LLM은 편향(Bias)이 전혀 없는 완벽한 답변만 생성한다.",
      "환각(Hallucination)은 프롬프트 엔지니어링으로 100% 제거할 수 있다.",
      "프롬프트 길이는 무제한이다."
    ],
    "answer": "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
    "why": "Prompt Injection은 악의적인 사용자가 프롬프트를 조작하여 모델이 의도치 않은 동작을 하도록 만드는 보안 위협입니다.",
    "hint": "Injection(주입) 공격",
    "difficulty": "hard",
    "id": "4077"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 보안 및 고급 주제 78",
    "options": [
      "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
      "샌드위치 기법은 프롬프트 인이 불필요하다는 이론이다.",
      "LLM은 편향(Bias)이 전혀 없는 완벽한 답변만 생성한다.",
      "환각(Hallucination)은 프롬프트 엔지니어링으로 100% 제거할 수 있다.",
      "프롬프트 길이는 무제한이다."
    ],
    "answer": "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
    "why": "Prompt Injection은 악의적인 사용자가 프롬프트를 조작하여 모델이 의도치 않은 동작을 하도록 만드는 보안 위협입니다.",
    "hint": "Injection(주입) 공격",
    "difficulty": "hard",
    "id": "4078"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 보안 및 고급 주제 79",
    "options": [
      "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
      "샌드위치 기법은 프롬프트 인이 불필요하다는 이론이다.",
      "LLM은 편향(Bias)이 전혀 없는 완벽한 답변만 생성한다.",
      "환각(Hallucination)은 프롬프트 엔지니어링으로 100% 제거할 수 있다.",
      "프롬프트 길이는 무제한이다."
    ],
    "answer": "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
    "why": "Prompt Injection은 악의적인 사용자가 프롬프트를 조작하여 모델이 의도치 않은 동작을 하도록 만드는 보안 위협입니다.",
    "hint": "Injection(주입) 공격",
    "difficulty": "hard",
    "id": "4079"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 보안 및 고급 주제 80",
    "options": [
      "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
      "샌드위치 기법은 프롬프트 인이 불필요하다는 이론이다.",
      "LLM은 편향(Bias)이 전혀 없는 완벽한 답변만 생성한다.",
      "환각(Hallucination)은 프롬프트 엔지니어링으로 100% 제거할 수 있다.",
      "프롬프트 길이는 무제한이다."
    ],
    "answer": "프롬프트 인젝션은 모델을 해킹하여 원래 지시를 무시하게 만드는 공격이다.",
    "why": "Prompt Injection은 악의적인 사용자가 프롬프트를 조작하여 모델이 의도치 않은 동작을 하도록 만드는 보안 위협입니다.",
    "hint": "Injection(주입) 공격",
    "difficulty": "hard",
    "id": "4080"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "실무 프롬프트 작성 팁 81",
    "options": [
      "지시사항을 최대한 모호하게 쓴다.",
      "예시는 많이 줄수록 항상 좋다(Context Window 무시).",
      "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
      "복잡한 작업은 한 번의 프롬프트로 모두 해결해야 한다.",
      "영어보다 한국어 프롬프트가 토큰 효율이 더 좋다."
    ],
    "answer": "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
    "why": "구분자를 사용하면 모델이 지시문(Instruction)과 처리할 데이터(Context)를 명확히 구분하여 성능이 향상됩니다.",
    "hint": "Delimiter",
    "difficulty": "medium",
    "id": "4081"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "실무 프롬프트 작성 팁 82",
    "options": [
      "지시사항을 최대한 모호하게 쓴다.",
      "예시는 많이 줄수록 항상 좋다(Context Window 무시).",
      "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
      "복잡한 작업은 한 번의 프롬프트로 모두 해결해야 한다.",
      "영어보다 한국어 프롬프트가 토큰 효율이 더 좋다."
    ],
    "answer": "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
    "why": "구분자를 사용하면 모델이 지시문(Instruction)과 처리할 데이터(Context)를 명확히 구분하여 성능이 향상됩니다.",
    "hint": "Delimiter",
    "difficulty": "medium",
    "id": "4082"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "실무 프롬프트 작성 팁 83",
    "options": [
      "지시사항을 최대한 모호하게 쓴다.",
      "예시는 많이 줄수록 항상 좋다(Context Window 무시).",
      "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
      "복잡한 작업은 한 번의 프롬프트로 모두 해결해야 한다.",
      "영어보다 한국어 프롬프트가 토큰 효율이 더 좋다."
    ],
    "answer": "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
    "why": "구분자를 사용하면 모델이 지시문(Instruction)과 처리할 데이터(Context)를 명확히 구분하여 성능이 향상됩니다.",
    "hint": "Delimiter",
    "difficulty": "medium",
    "id": "4083"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "실무 프롬프트 작성 팁 84",
    "options": [
      "지시사항을 최대한 모호하게 쓴다.",
      "예시는 많이 줄수록 항상 좋다(Context Window 무시).",
      "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
      "복잡한 작업은 한 번의 프롬프트로 모두 해결해야 한다.",
      "영어보다 한국어 프롬프트가 토큰 효율이 더 좋다."
    ],
    "answer": "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
    "why": "구분자를 사용하면 모델이 지시문(Instruction)과 처리할 데이터(Context)를 명확히 구분하여 성능이 향상됩니다.",
    "hint": "Delimiter",
    "difficulty": "medium",
    "id": "4084"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "실무 프롬프트 작성 팁 85",
    "options": [
      "지시사항을 최대한 모호하게 쓴다.",
      "예시는 많이 줄수록 항상 좋다(Context Window 무시).",
      "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
      "복잡한 작업은 한 번의 프롬프트로 모두 해결해야 한다.",
      "영어보다 한국어 프롬프트가 토큰 효율이 더 좋다."
    ],
    "answer": "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
    "why": "구분자를 사용하면 모델이 지시문(Instruction)과 처리할 데이터(Context)를 명확히 구분하여 성능이 향상됩니다.",
    "hint": "Delimiter",
    "difficulty": "medium",
    "id": "4085"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "실무 프롬프트 작성 팁 86",
    "options": [
      "지시사항을 최대한 모호하게 쓴다.",
      "예시는 많이 줄수록 항상 좋다(Context Window 무시).",
      "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
      "복잡한 작업은 한 번의 프롬프트로 모두 해결해야 한다.",
      "영어보다 한국어 프롬프트가 토큰 효율이 더 좋다."
    ],
    "answer": "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
    "why": "구분자를 사용하면 모델이 지시문(Instruction)과 처리할 데이터(Context)를 명확히 구분하여 성능이 향상됩니다.",
    "hint": "Delimiter",
    "difficulty": "medium",
    "id": "4086"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "실무 프롬프트 작성 팁 87",
    "options": [
      "지시사항을 최대한 모호하게 쓴다.",
      "예시는 많이 줄수록 항상 좋다(Context Window 무시).",
      "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
      "복잡한 작업은 한 번의 프롬프트로 모두 해결해야 한다.",
      "영어보다 한국어 프롬프트가 토큰 효율이 더 좋다."
    ],
    "answer": "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
    "why": "구분자를 사용하면 모델이 지시문(Instruction)과 처리할 데이터(Context)를 명확히 구분하여 성능이 향상됩니다.",
    "hint": "Delimiter",
    "difficulty": "medium",
    "id": "4087"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "실무 프롬프트 작성 팁 88",
    "options": [
      "지시사항을 최대한 모호하게 쓴다.",
      "예시는 많이 줄수록 항상 좋다(Context Window 무시).",
      "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
      "복잡한 작업은 한 번의 프롬프트로 모두 해결해야 한다.",
      "영어보다 한국어 프롬프트가 토큰 효율이 더 좋다."
    ],
    "answer": "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
    "why": "구분자를 사용하면 모델이 지시문(Instruction)과 처리할 데이터(Context)를 명확히 구분하여 성능이 향상됩니다.",
    "hint": "Delimiter",
    "difficulty": "medium",
    "id": "4088"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "실무 프롬프트 작성 팁 89",
    "options": [
      "지시사항을 최대한 모호하게 쓴다.",
      "예시는 많이 줄수록 항상 좋다(Context Window 무시).",
      "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
      "복잡한 작업은 한 번의 프롬프트로 모두 해결해야 한다.",
      "영어보다 한국어 프롬프트가 토큰 효율이 더 좋다."
    ],
    "answer": "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
    "why": "구분자를 사용하면 모델이 지시문(Instruction)과 처리할 데이터(Context)를 명확히 구분하여 성능이 향상됩니다.",
    "hint": "Delimiter",
    "difficulty": "medium",
    "id": "4089"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "실무 프롬프트 작성 팁 90",
    "options": [
      "지시사항을 최대한 모호하게 쓴다.",
      "예시는 많이 줄수록 항상 좋다(Context Window 무시).",
      "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
      "복잡한 작업은 한 번의 프롬프트로 모두 해결해야 한다.",
      "영어보다 한국어 프롬프트가 토큰 효율이 더 좋다."
    ],
    "answer": "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
    "why": "구분자를 사용하면 모델이 지시문(Instruction)과 처리할 데이터(Context)를 명확히 구분하여 성능이 향상됩니다.",
    "hint": "Delimiter",
    "difficulty": "medium",
    "id": "4090"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "실무 프롬프트 작성 팁 91",
    "options": [
      "지시사항을 최대한 모호하게 쓴다.",
      "예시는 많이 줄수록 항상 좋다(Context Window 무시).",
      "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
      "복잡한 작업은 한 번의 프롬프트로 모두 해결해야 한다.",
      "영어보다 한국어 프롬프트가 토큰 효율이 더 좋다."
    ],
    "answer": "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
    "why": "구분자를 사용하면 모델이 지시문(Instruction)과 처리할 데이터(Context)를 명확히 구분하여 성능이 향상됩니다.",
    "hint": "Delimiter",
    "difficulty": "medium",
    "id": "4091"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "실무 프롬프트 작성 팁 92",
    "options": [
      "지시사항을 최대한 모호하게 쓴다.",
      "예시는 많이 줄수록 항상 좋다(Context Window 무시).",
      "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
      "복잡한 작업은 한 번의 프롬프트로 모두 해결해야 한다.",
      "영어보다 한국어 프롬프트가 토큰 효율이 더 좋다."
    ],
    "answer": "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
    "why": "구분자를 사용하면 모델이 지시문(Instruction)과 처리할 데이터(Context)를 명확히 구분하여 성능이 향상됩니다.",
    "hint": "Delimiter",
    "difficulty": "medium",
    "id": "4092"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "실무 프롬프트 작성 팁 93",
    "options": [
      "지시사항을 최대한 모호하게 쓴다.",
      "예시는 많이 줄수록 항상 좋다(Context Window 무시).",
      "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
      "복잡한 작업은 한 번의 프롬프트로 모두 해결해야 한다.",
      "영어보다 한국어 프롬프트가 토큰 효율이 더 좋다."
    ],
    "answer": "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
    "why": "구분자를 사용하면 모델이 지시문(Instruction)과 처리할 데이터(Context)를 명확히 구분하여 성능이 향상됩니다.",
    "hint": "Delimiter",
    "difficulty": "medium",
    "id": "4093"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "실무 프롬프트 작성 팁 94",
    "options": [
      "지시사항을 최대한 모호하게 쓴다.",
      "예시는 많이 줄수록 항상 좋다(Context Window 무시).",
      "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
      "복잡한 작업은 한 번의 프롬프트로 모두 해결해야 한다.",
      "영어보다 한국어 프롬프트가 토큰 효율이 더 좋다."
    ],
    "answer": "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
    "why": "구분자를 사용하면 모델이 지시문(Instruction)과 처리할 데이터(Context)를 명확히 구분하여 성능이 향상됩니다.",
    "hint": "Delimiter",
    "difficulty": "medium",
    "id": "4094"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "실무 프롬프트 작성 팁 95",
    "options": [
      "지시사항을 최대한 모호하게 쓴다.",
      "예시는 많이 줄수록 항상 좋다(Context Window 무시).",
      "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
      "복잡한 작업은 한 번의 프롬프트로 모두 해결해야 한다.",
      "영어보다 한국어 프롬프트가 토큰 효율이 더 좋다."
    ],
    "answer": "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
    "why": "구분자를 사용하면 모델이 지시문(Instruction)과 처리할 데이터(Context)를 명확히 구분하여 성능이 향상됩니다.",
    "hint": "Delimiter",
    "difficulty": "medium",
    "id": "4095"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "실무 프롬프트 작성 팁 96",
    "options": [
      "지시사항을 최대한 모호하게 쓴다.",
      "예시는 많이 줄수록 항상 좋다(Context Window 무시).",
      "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
      "복잡한 작업은 한 번의 프롬프트로 모두 해결해야 한다.",
      "영어보다 한국어 프롬프트가 토큰 효율이 더 좋다."
    ],
    "answer": "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
    "why": "구분자를 사용하면 모델이 지시문(Instruction)과 처리할 데이터(Context)를 명확히 구분하여 성능이 향상됩니다.",
    "hint": "Delimiter",
    "difficulty": "medium",
    "id": "4096"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "실무 프롬프트 작성 팁 97",
    "options": [
      "지시사항을 최대한 모호하게 쓴다.",
      "예시는 많이 줄수록 항상 좋다(Context Window 무시).",
      "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
      "복잡한 작업은 한 번의 프롬프트로 모두 해결해야 한다.",
      "영어보다 한국어 프롬프트가 토큰 효율이 더 좋다."
    ],
    "answer": "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
    "why": "구분자를 사용하면 모델이 지시문(Instruction)과 처리할 데이터(Context)를 명확히 구분하여 성능이 향상됩니다.",
    "hint": "Delimiter",
    "difficulty": "medium",
    "id": "4097"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "실무 프롬프트 작성 팁 98",
    "options": [
      "지시사항을 최대한 모호하게 쓴다.",
      "예시는 많이 줄수록 항상 좋다(Context Window 무시).",
      "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
      "복잡한 작업은 한 번의 프롬프트로 모두 해결해야 한다.",
      "영어보다 한국어 프롬프트가 토큰 효율이 더 좋다."
    ],
    "answer": "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
    "why": "구분자를 사용하면 모델이 지시문(Instruction)과 처리할 데이터(Context)를 명확히 구분하여 성능이 향상됩니다.",
    "hint": "Delimiter",
    "difficulty": "medium",
    "id": "4098"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "실무 프롬프트 작성 팁 99",
    "options": [
      "지시사항을 최대한 모호하게 쓴다.",
      "예시는 많이 줄수록 항상 좋다(Context Window 무시).",
      "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
      "복잡한 작업은 한 번의 프롬프트로 모두 해결해야 한다.",
      "영어보다 한국어 프롬프트가 토큰 효율이 더 좋다."
    ],
    "answer": "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
    "why": "구분자를 사용하면 모델이 지시문(Instruction)과 처리할 데이터(Context)를 명확히 구분하여 성능이 향상됩니다.",
    "hint": "Delimiter",
    "difficulty": "medium",
    "id": "4099"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "실무 프롬프트 작성 팁 100",
    "options": [
      "지시사항을 최대한 모호하게 쓴다.",
      "예시는 많이 줄수록 항상 좋다(Context Window 무시).",
      "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
      "복잡한 작업은 한 번의 프롬프트로 모두 해결해야 한다.",
      "영어보다 한국어 프롬프트가 토큰 효율이 더 좋다."
    ],
    "answer": "구분자(###, \"\"\")를 사용하여 지시와 데이터를 분리한다.",
    "why": "구분자를 사용하면 모델이 지시문(Instruction)과 처리할 데이터(Context)를 명확히 구분하여 성능이 향상됩니다.",
    "hint": "Delimiter",
    "difficulty": "medium",
    "id": "40100"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "템플릿에 변수를 채워 넣으세요.\n```python\nprompt = template.____(name='World')\n```",
    "answer": "format",
    "why": "변수 바인딩 메서드는 format입니다.",
    "hint": "Template Class",
    "difficulty": "medium",
    "id": "4101"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "LangChain 프롬프트 템플릿 코드 완성 (문제 2)",
    "answer": "PromptTemplate",
    "why": "LangChain의 기본 프롬프트 템플릿 클래스입니다.",
    "hint": "Template Class",
    "difficulty": "medium",
    "id": "4102"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "LangChain 프롬프트 템플릿 코드 완성 (문제 3)",
    "answer": "PromptTemplate",
    "why": "LangChain의 기본 프롬프트 템플릿 클래스입니다.",
    "hint": "Template Class",
    "difficulty": "medium",
    "id": "4103"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "문자열 템플릿을 생성하세요.\n```python\nfrom langchain.prompts import ____\nt = ____.from_template('Hello {name}')\n```",
    "answer": "PromptTemplate",
    "why": "LangChain의 기본 프롬프트 템플릿 클래스입니다.",
    "hint": "Template Class",
    "difficulty": "medium",
    "id": "4104"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "템플릿에 변수를 채워 넣으세요.\n```python\nprompt = template.____(name='World')\n```",
    "answer": "format",
    "why": "변수 바인딩 메서드는 format입니다.",
    "hint": "Template Class",
    "difficulty": "medium",
    "id": "4105"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "LangChain 프롬프트 템플릿 코드 완성 (문제 6)",
    "answer": "PromptTemplate",
    "why": "LangChain의 기본 프롬프트 템플릿 클래스입니다.",
    "hint": "Template Class",
    "difficulty": "medium",
    "id": "4106"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "LangChain 프롬프트 템플릿 코드 완성 (문제 7)",
    "answer": "PromptTemplate",
    "why": "LangChain의 기본 프롬프트 템플릿 클래스입니다.",
    "hint": "Template Class",
    "difficulty": "medium",
    "id": "4107"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "문자열 템플릿을 생성하세요.\n```python\nfrom langchain.prompts import ____\nt = ____.from_template('Hello {name}')\n```",
    "answer": "PromptTemplate",
    "why": "LangChain의 기본 프롬프트 템플릿 클래스입니다.",
    "hint": "Template Class",
    "difficulty": "medium",
    "id": "4108"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "템플릿에 변수를 채워 넣으세요.\n```python\nprompt = template.____(name='World')\n```",
    "answer": "format",
    "why": "변수 바인딩 메서드는 format입니다.",
    "hint": "Template Class",
    "difficulty": "medium",
    "id": "4109"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "LangChain 프롬프트 템플릿 코드 완성 (문제 10)",
    "answer": "PromptTemplate",
    "why": "LangChain의 기본 프롬프트 템플릿 클래스입니다.",
    "hint": "Template Class",
    "difficulty": "medium",
    "id": "4110"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "LangChain 프롬프트 템플릿 코드 완성 (문제 11)",
    "answer": "PromptTemplate",
    "why": "LangChain의 기본 프롬프트 템플릿 클래스입니다.",
    "hint": "Template Class",
    "difficulty": "medium",
    "id": "4111"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "문자열 템플릿을 생성하세요.\n```python\nfrom langchain.prompts import ____\nt = ____.from_template('Hello {name}')\n```",
    "answer": "PromptTemplate",
    "why": "LangChain의 기본 프롬프트 템플릿 클래스입니다.",
    "hint": "Template Class",
    "difficulty": "medium",
    "id": "4112"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "템플릿에 변수를 채워 넣으세요.\n```python\nprompt = template.____(name='World')\n```",
    "answer": "format",
    "why": "변수 바인딩 메서드는 format입니다.",
    "hint": "Template Class",
    "difficulty": "medium",
    "id": "4113"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "LangChain 프롬프트 템플릿 코드 완성 (문제 14)",
    "answer": "PromptTemplate",
    "why": "LangChain의 기본 프롬프트 템플릿 클래스입니다.",
    "hint": "Template Class",
    "difficulty": "medium",
    "id": "4114"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "LangChain 프롬프트 템플릿 코드 완성 (문제 15)",
    "answer": "PromptTemplate",
    "why": "LangChain의 기본 프롬프트 템플릿 클래스입니다.",
    "hint": "Template Class",
    "difficulty": "medium",
    "id": "4115"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "문자열 템플릿을 생성하세요.\n```python\nfrom langchain.prompts import ____\nt = ____.from_template('Hello {name}')\n```",
    "answer": "PromptTemplate",
    "why": "LangChain의 기본 프롬프트 템플릿 클래스입니다.",
    "hint": "Template Class",
    "difficulty": "medium",
    "id": "4116"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "템플릿에 변수를 채워 넣으세요.\n```python\nprompt = template.____(name='World')\n```",
    "answer": "format",
    "why": "변수 바인딩 메서드는 format입니다.",
    "hint": "Template Class",
    "difficulty": "medium",
    "id": "4117"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "LangChain 프롬프트 템플릿 코드 완성 (문제 18)",
    "answer": "PromptTemplate",
    "why": "LangChain의 기본 프롬프트 템플릿 클래스입니다.",
    "hint": "Template Class",
    "difficulty": "medium",
    "id": "4118"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "LangChain 프롬프트 템플릿 코드 완성 (문제 19)",
    "answer": "PromptTemplate",
    "why": "LangChain의 기본 프롬프트 템플릿 클래스입니다.",
    "hint": "Template Class",
    "difficulty": "medium",
    "id": "4119"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "문자열 템플릿을 생성하세요.\n```python\nfrom langchain.prompts import ____\nt = ____.from_template('Hello {name}')\n```",
    "answer": "PromptTemplate",
    "why": "LangChain의 기본 프롬프트 템플릿 클래스입니다.",
    "hint": "Template Class",
    "difficulty": "medium",
    "id": "4120"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG(Retrieval-Augmented Generation)의 개념과 필요성 1",
    "options": [
      "RAG는 LLM을 처음부터 다시 학습시키는 방법이다.",
      "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
      "RAG는 외부 데이터 없이 모델 내부 지식만 사용한다.",
      "RAG는 항상 파인튜닝보다 비용이 많이 든다.",
      "RAG는 이미지 생성 전용 기술이다."
    ],
    "answer": "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
    "why": "RAG는 신뢰할 수 있는 외부 지식을 검색하여 제공하므로, 모델이 사실이 아닌 내용을 지어내는 환각 현상을 완화합니다.",
    "hint": "근거 있는 답변",
    "difficulty": "easy",
    "id": "5001"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG(Retrieval-Augmented Generation)의 개념과 필요성 2",
    "options": [
      "RAG는 LLM을 처음부터 다시 학습시키는 방법이다.",
      "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
      "RAG는 외부 데이터 없이 모델 내부 지식만 사용한다.",
      "RAG는 항상 파인튜닝보다 비용이 많이 든다.",
      "RAG는 이미지 생성 전용 기술이다."
    ],
    "answer": "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
    "why": "RAG는 신뢰할 수 있는 외부 지식을 검색하여 제공하므로, 모델이 사실이 아닌 내용을 지어내는 환각 현상을 완화합니다.",
    "hint": "근거 있는 답변",
    "difficulty": "easy",
    "id": "5002"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG(Retrieval-Augmented Generation)의 개념과 필요성 3",
    "options": [
      "RAG는 LLM을 처음부터 다시 학습시키는 방법이다.",
      "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
      "RAG는 외부 데이터 없이 모델 내부 지식만 사용한다.",
      "RAG는 항상 파인튜닝보다 비용이 많이 든다.",
      "RAG는 이미지 생성 전용 기술이다."
    ],
    "answer": "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
    "why": "RAG는 신뢰할 수 있는 외부 지식을 검색하여 제공하므로, 모델이 사실이 아닌 내용을 지어내는 환각 현상을 완화합니다.",
    "hint": "근거 있는 답변",
    "difficulty": "easy",
    "id": "5003"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG(Retrieval-Augmented Generation)의 개념과 필요성 4",
    "options": [
      "RAG는 LLM을 처음부터 다시 학습시키는 방법이다.",
      "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
      "RAG는 외부 데이터 없이 모델 내부 지식만 사용한다.",
      "RAG는 항상 파인튜닝보다 비용이 많이 든다.",
      "RAG는 이미지 생성 전용 기술이다."
    ],
    "answer": "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
    "why": "RAG는 신뢰할 수 있는 외부 지식을 검색하여 제공하므로, 모델이 사실이 아닌 내용을 지어내는 환각 현상을 완화합니다.",
    "hint": "근거 있는 답변",
    "difficulty": "easy",
    "id": "5004"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG(Retrieval-Augmented Generation)의 개념과 필요성 5",
    "options": [
      "RAG는 LLM을 처음부터 다시 학습시키는 방법이다.",
      "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
      "RAG는 외부 데이터 없이 모델 내부 지식만 사용한다.",
      "RAG는 항상 파인튜닝보다 비용이 많이 든다.",
      "RAG는 이미지 생성 전용 기술이다."
    ],
    "answer": "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
    "why": "RAG는 신뢰할 수 있는 외부 지식을 검색하여 제공하므로, 모델이 사실이 아닌 내용을 지어내는 환각 현상을 완화합니다.",
    "hint": "근거 있는 답변",
    "difficulty": "easy",
    "id": "5005"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG(Retrieval-Augmented Generation)의 개념과 필요성 6",
    "options": [
      "RAG는 LLM을 처음부터 다시 학습시키는 방법이다.",
      "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
      "RAG는 외부 데이터 없이 모델 내부 지식만 사용한다.",
      "RAG는 항상 파인튜닝보다 비용이 많이 든다.",
      "RAG는 이미지 생성 전용 기술이다."
    ],
    "answer": "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
    "why": "RAG는 신뢰할 수 있는 외부 지식을 검색하여 제공하므로, 모델이 사실이 아닌 내용을 지어내는 환각 현상을 완화합니다.",
    "hint": "근거 있는 답변",
    "difficulty": "easy",
    "id": "5006"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG(Retrieval-Augmented Generation)의 개념과 필요성 7",
    "options": [
      "RAG는 LLM을 처음부터 다시 학습시키는 방법이다.",
      "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
      "RAG는 외부 데이터 없이 모델 내부 지식만 사용한다.",
      "RAG는 항상 파인튜닝보다 비용이 많이 든다.",
      "RAG는 이미지 생성 전용 기술이다."
    ],
    "answer": "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
    "why": "RAG는 신뢰할 수 있는 외부 지식을 검색하여 제공하므로, 모델이 사실이 아닌 내용을 지어내는 환각 현상을 완화합니다.",
    "hint": "근거 있는 답변",
    "difficulty": "easy",
    "id": "5007"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG(Retrieval-Augmented Generation)의 개념과 필요성 8",
    "options": [
      "RAG는 LLM을 처음부터 다시 학습시키는 방법이다.",
      "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
      "RAG는 외부 데이터 없이 모델 내부 지식만 사용한다.",
      "RAG는 항상 파인튜닝보다 비용이 많이 든다.",
      "RAG는 이미지 생성 전용 기술이다."
    ],
    "answer": "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
    "why": "RAG는 신뢰할 수 있는 외부 지식을 검색하여 제공하므로, 모델이 사실이 아닌 내용을 지어내는 환각 현상을 완화합니다.",
    "hint": "근거 있는 답변",
    "difficulty": "easy",
    "id": "5008"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG(Retrieval-Augmented Generation)의 개념과 필요성 9",
    "options": [
      "RAG는 LLM을 처음부터 다시 학습시키는 방법이다.",
      "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
      "RAG는 외부 데이터 없이 모델 내부 지식만 사용한다.",
      "RAG는 항상 파인튜닝보다 비용이 많이 든다.",
      "RAG는 이미지 생성 전용 기술이다."
    ],
    "answer": "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
    "why": "RAG는 신뢰할 수 있는 외부 지식을 검색하여 제공하므로, 모델이 사실이 아닌 내용을 지어내는 환각 현상을 완화합니다.",
    "hint": "근거 있는 답변",
    "difficulty": "easy",
    "id": "5009"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG(Retrieval-Augmented Generation)의 개념과 필요성 10",
    "options": [
      "RAG는 LLM을 처음부터 다시 학습시키는 방법이다.",
      "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
      "RAG는 외부 데이터 없이 모델 내부 지식만 사용한다.",
      "RAG는 항상 파인튜닝보다 비용이 많이 든다.",
      "RAG는 이미지 생성 전용 기술이다."
    ],
    "answer": "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
    "why": "RAG는 신뢰할 수 있는 외부 지식을 검색하여 제공하므로, 모델이 사실이 아닌 내용을 지어내는 환각 현상을 완화합니다.",
    "hint": "근거 있는 답변",
    "difficulty": "easy",
    "id": "5010"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG(Retrieval-Augmented Generation)의 개념과 필요성 11",
    "options": [
      "RAG는 LLM을 처음부터 다시 학습시키는 방법이다.",
      "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
      "RAG는 외부 데이터 없이 모델 내부 지식만 사용한다.",
      "RAG는 항상 파인튜닝보다 비용이 많이 든다.",
      "RAG는 이미지 생성 전용 기술이다."
    ],
    "answer": "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
    "why": "RAG는 신뢰할 수 있는 외부 지식을 검색하여 제공하므로, 모델이 사실이 아닌 내용을 지어내는 환각 현상을 완화합니다.",
    "hint": "근거 있는 답변",
    "difficulty": "easy",
    "id": "5011"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG(Retrieval-Augmented Generation)의 개념과 필요성 12",
    "options": [
      "RAG는 LLM을 처음부터 다시 학습시키는 방법이다.",
      "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
      "RAG는 외부 데이터 없이 모델 내부 지식만 사용한다.",
      "RAG는 항상 파인튜닝보다 비용이 많이 든다.",
      "RAG는 이미지 생성 전용 기술이다."
    ],
    "answer": "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
    "why": "RAG는 신뢰할 수 있는 외부 지식을 검색하여 제공하므로, 모델이 사실이 아닌 내용을 지어내는 환각 현상을 완화합니다.",
    "hint": "근거 있는 답변",
    "difficulty": "easy",
    "id": "5012"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG(Retrieval-Augmented Generation)의 개념과 필요성 13",
    "options": [
      "RAG는 LLM을 처음부터 다시 학습시키는 방법이다.",
      "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
      "RAG는 외부 데이터 없이 모델 내부 지식만 사용한다.",
      "RAG는 항상 파인튜닝보다 비용이 많이 든다.",
      "RAG는 이미지 생성 전용 기술이다."
    ],
    "answer": "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
    "why": "RAG는 신뢰할 수 있는 외부 지식을 검색하여 제공하므로, 모델이 사실이 아닌 내용을 지어내는 환각 현상을 완화합니다.",
    "hint": "근거 있는 답변",
    "difficulty": "easy",
    "id": "5013"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG(Retrieval-Augmented Generation)의 개념과 필요성 14",
    "options": [
      "RAG는 LLM을 처음부터 다시 학습시키는 방법이다.",
      "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
      "RAG는 외부 데이터 없이 모델 내부 지식만 사용한다.",
      "RAG는 항상 파인튜닝보다 비용이 많이 든다.",
      "RAG는 이미지 생성 전용 기술이다."
    ],
    "answer": "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
    "why": "RAG는 신뢰할 수 있는 외부 지식을 검색하여 제공하므로, 모델이 사실이 아닌 내용을 지어내는 환각 현상을 완화합니다.",
    "hint": "근거 있는 답변",
    "difficulty": "easy",
    "id": "5014"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG(Retrieval-Augmented Generation)의 개념과 필요성 15",
    "options": [
      "RAG는 LLM을 처음부터 다시 학습시키는 방법이다.",
      "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
      "RAG는 외부 데이터 없이 모델 내부 지식만 사용한다.",
      "RAG는 항상 파인튜닝보다 비용이 많이 든다.",
      "RAG는 이미지 생성 전용 기술이다."
    ],
    "answer": "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
    "why": "RAG는 신뢰할 수 있는 외부 지식을 검색하여 제공하므로, 모델이 사실이 아닌 내용을 지어내는 환각 현상을 완화합니다.",
    "hint": "근거 있는 답변",
    "difficulty": "easy",
    "id": "5015"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG(Retrieval-Augmented Generation)의 개념과 필요성 16",
    "options": [
      "RAG는 LLM을 처음부터 다시 학습시키는 방법이다.",
      "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
      "RAG는 외부 데이터 없이 모델 내부 지식만 사용한다.",
      "RAG는 항상 파인튜닝보다 비용이 많이 든다.",
      "RAG는 이미지 생성 전용 기술이다."
    ],
    "answer": "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
    "why": "RAG는 신뢰할 수 있는 외부 지식을 검색하여 제공하므로, 모델이 사실이 아닌 내용을 지어내는 환각 현상을 완화합니다.",
    "hint": "근거 있는 답변",
    "difficulty": "easy",
    "id": "5016"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG(Retrieval-Augmented Generation)의 개념과 필요성 17",
    "options": [
      "RAG는 LLM을 처음부터 다시 학습시키는 방법이다.",
      "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
      "RAG는 외부 데이터 없이 모델 내부 지식만 사용한다.",
      "RAG는 항상 파인튜닝보다 비용이 많이 든다.",
      "RAG는 이미지 생성 전용 기술이다."
    ],
    "answer": "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
    "why": "RAG는 신뢰할 수 있는 외부 지식을 검색하여 제공하므로, 모델이 사실이 아닌 내용을 지어내는 환각 현상을 완화합니다.",
    "hint": "근거 있는 답변",
    "difficulty": "easy",
    "id": "5017"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG(Retrieval-Augmented Generation)의 개념과 필요성 18",
    "options": [
      "RAG는 LLM을 처음부터 다시 학습시키는 방법이다.",
      "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
      "RAG는 외부 데이터 없이 모델 내부 지식만 사용한다.",
      "RAG는 항상 파인튜닝보다 비용이 많이 든다.",
      "RAG는 이미지 생성 전용 기술이다."
    ],
    "answer": "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
    "why": "RAG는 신뢰할 수 있는 외부 지식을 검색하여 제공하므로, 모델이 사실이 아닌 내용을 지어내는 환각 현상을 완화합니다.",
    "hint": "근거 있는 답변",
    "difficulty": "easy",
    "id": "5018"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG(Retrieval-Augmented Generation)의 개념과 필요성 19",
    "options": [
      "RAG는 LLM을 처음부터 다시 학습시키는 방법이다.",
      "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
      "RAG는 외부 데이터 없이 모델 내부 지식만 사용한다.",
      "RAG는 항상 파인튜닝보다 비용이 많이 든다.",
      "RAG는 이미지 생성 전용 기술이다."
    ],
    "answer": "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
    "why": "RAG는 신뢰할 수 있는 외부 지식을 검색하여 제공하므로, 모델이 사실이 아닌 내용을 지어내는 환각 현상을 완화합니다.",
    "hint": "근거 있는 답변",
    "difficulty": "easy",
    "id": "5019"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG(Retrieval-Augmented Generation)의 개념과 필요성 20",
    "options": [
      "RAG는 LLM을 처음부터 다시 학습시키는 방법이다.",
      "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
      "RAG는 외부 데이터 없이 모델 내부 지식만 사용한다.",
      "RAG는 항상 파인튜닝보다 비용이 많이 든다.",
      "RAG는 이미지 생성 전용 기술이다."
    ],
    "answer": "RAG를 사용하면 Hallucination(환각)을 줄일 수 있다.",
    "why": "RAG는 신뢰할 수 있는 외부 지식을 검색하여 제공하므로, 모델이 사실이 아닌 내용을 지어내는 환각 현상을 완화합니다.",
    "hint": "근거 있는 답변",
    "difficulty": "easy",
    "id": "5020"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인 및 벡터 DB 구조 21",
    "options": [
      "Vector DB는 텍스트를 이미지로 저장한다.",
      "Chunking은 문서를 통째로 저장하는 것이다.",
      "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
      "Retrieval 단계에서는 가장 관련 없는 문서를 찾는다.",
      "Ingestion은 답변을 생성하는 단계다."
    ],
    "answer": "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
    "why": "임베딩 모델을 통해 텍스트를 고차원의 벡터 공간에 매핑하여 의미적 유사도를 계산할 수 있게 합니다.",
    "hint": "Text to Vector",
    "difficulty": "medium",
    "id": "5021"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인 및 벡터 DB 구조 22",
    "options": [
      "Vector DB는 텍스트를 이미지로 저장한다.",
      "Chunking은 문서를 통째로 저장하는 것이다.",
      "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
      "Retrieval 단계에서는 가장 관련 없는 문서를 찾는다.",
      "Ingestion은 답변을 생성하는 단계다."
    ],
    "answer": "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
    "why": "임베딩 모델을 통해 텍스트를 고차원의 벡터 공간에 매핑하여 의미적 유사도를 계산할 수 있게 합니다.",
    "hint": "Text to Vector",
    "difficulty": "medium",
    "id": "5022"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인 및 벡터 DB 구조 23",
    "options": [
      "Vector DB는 텍스트를 이미지로 저장한다.",
      "Chunking은 문서를 통째로 저장하는 것이다.",
      "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
      "Retrieval 단계에서는 가장 관련 없는 문서를 찾는다.",
      "Ingestion은 답변을 생성하는 단계다."
    ],
    "answer": "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
    "why": "임베딩 모델을 통해 텍스트를 고차원의 벡터 공간에 매핑하여 의미적 유사도를 계산할 수 있게 합니다.",
    "hint": "Text to Vector",
    "difficulty": "medium",
    "id": "5023"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인 및 벡터 DB 구조 24",
    "options": [
      "Vector DB는 텍스트를 이미지로 저장한다.",
      "Chunking은 문서를 통째로 저장하는 것이다.",
      "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
      "Retrieval 단계에서는 가장 관련 없는 문서를 찾는다.",
      "Ingestion은 답변을 생성하는 단계다."
    ],
    "answer": "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
    "why": "임베딩 모델을 통해 텍스트를 고차원의 벡터 공간에 매핑하여 의미적 유사도를 계산할 수 있게 합니다.",
    "hint": "Text to Vector",
    "difficulty": "medium",
    "id": "5024"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인 및 벡터 DB 구조 25",
    "options": [
      "Vector DB는 텍스트를 이미지로 저장한다.",
      "Chunking은 문서를 통째로 저장하는 것이다.",
      "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
      "Retrieval 단계에서는 가장 관련 없는 문서를 찾는다.",
      "Ingestion은 답변을 생성하는 단계다."
    ],
    "answer": "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
    "why": "임베딩 모델을 통해 텍스트를 고차원의 벡터 공간에 매핑하여 의미적 유사도를 계산할 수 있게 합니다.",
    "hint": "Text to Vector",
    "difficulty": "medium",
    "id": "5025"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인 및 벡터 DB 구조 26",
    "options": [
      "Vector DB는 텍스트를 이미지로 저장한다.",
      "Chunking은 문서를 통째로 저장하는 것이다.",
      "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
      "Retrieval 단계에서는 가장 관련 없는 문서를 찾는다.",
      "Ingestion은 답변을 생성하는 단계다."
    ],
    "answer": "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
    "why": "임베딩 모델을 통해 텍스트를 고차원의 벡터 공간에 매핑하여 의미적 유사도를 계산할 수 있게 합니다.",
    "hint": "Text to Vector",
    "difficulty": "medium",
    "id": "5026"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인 및 벡터 DB 구조 27",
    "options": [
      "Vector DB는 텍스트를 이미지로 저장한다.",
      "Chunking은 문서를 통째로 저장하는 것이다.",
      "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
      "Retrieval 단계에서는 가장 관련 없는 문서를 찾는다.",
      "Ingestion은 답변을 생성하는 단계다."
    ],
    "answer": "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
    "why": "임베딩 모델을 통해 텍스트를 고차원의 벡터 공간에 매핑하여 의미적 유사도를 계산할 수 있게 합니다.",
    "hint": "Text to Vector",
    "difficulty": "medium",
    "id": "5027"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인 및 벡터 DB 구조 28",
    "options": [
      "Vector DB는 텍스트를 이미지로 저장한다.",
      "Chunking은 문서를 통째로 저장하는 것이다.",
      "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
      "Retrieval 단계에서는 가장 관련 없는 문서를 찾는다.",
      "Ingestion은 답변을 생성하는 단계다."
    ],
    "answer": "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
    "why": "임베딩 모델을 통해 텍스트를 고차원의 벡터 공간에 매핑하여 의미적 유사도를 계산할 수 있게 합니다.",
    "hint": "Text to Vector",
    "difficulty": "medium",
    "id": "5028"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인 및 벡터 DB 구조 29",
    "options": [
      "Vector DB는 텍스트를 이미지로 저장한다.",
      "Chunking은 문서를 통째로 저장하는 것이다.",
      "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
      "Retrieval 단계에서는 가장 관련 없는 문서를 찾는다.",
      "Ingestion은 답변을 생성하는 단계다."
    ],
    "answer": "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
    "why": "임베딩 모델을 통해 텍스트를 고차원의 벡터 공간에 매핑하여 의미적 유사도를 계산할 수 있게 합니다.",
    "hint": "Text to Vector",
    "difficulty": "medium",
    "id": "5029"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인 및 벡터 DB 구조 30",
    "options": [
      "Vector DB는 텍스트를 이미지로 저장한다.",
      "Chunking은 문서를 통째로 저장하는 것이다.",
      "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
      "Retrieval 단계에서는 가장 관련 없는 문서를 찾는다.",
      "Ingestion은 답변을 생성하는 단계다."
    ],
    "answer": "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
    "why": "임베딩 모델을 통해 텍스트를 고차원의 벡터 공간에 매핑하여 의미적 유사도를 계산할 수 있게 합니다.",
    "hint": "Text to Vector",
    "difficulty": "medium",
    "id": "5030"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인 및 벡터 DB 구조 31",
    "options": [
      "Vector DB는 텍스트를 이미지로 저장한다.",
      "Chunking은 문서를 통째로 저장하는 것이다.",
      "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
      "Retrieval 단계에서는 가장 관련 없는 문서를 찾는다.",
      "Ingestion은 답변을 생성하는 단계다."
    ],
    "answer": "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
    "why": "임베딩 모델을 통해 텍스트를 고차원의 벡터 공간에 매핑하여 의미적 유사도를 계산할 수 있게 합니다.",
    "hint": "Text to Vector",
    "difficulty": "medium",
    "id": "5031"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인 및 벡터 DB 구조 32",
    "options": [
      "Vector DB는 텍스트를 이미지로 저장한다.",
      "Chunking은 문서를 통째로 저장하는 것이다.",
      "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
      "Retrieval 단계에서는 가장 관련 없는 문서를 찾는다.",
      "Ingestion은 답변을 생성하는 단계다."
    ],
    "answer": "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
    "why": "임베딩 모델을 통해 텍스트를 고차원의 벡터 공간에 매핑하여 의미적 유사도를 계산할 수 있게 합니다.",
    "hint": "Text to Vector",
    "difficulty": "medium",
    "id": "5032"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인 및 벡터 DB 구조 33",
    "options": [
      "Vector DB는 텍스트를 이미지로 저장한다.",
      "Chunking은 문서를 통째로 저장하는 것이다.",
      "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
      "Retrieval 단계에서는 가장 관련 없는 문서를 찾는다.",
      "Ingestion은 답변을 생성하는 단계다."
    ],
    "answer": "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
    "why": "임베딩 모델을 통해 텍스트를 고차원의 벡터 공간에 매핑하여 의미적 유사도를 계산할 수 있게 합니다.",
    "hint": "Text to Vector",
    "difficulty": "medium",
    "id": "5033"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인 및 벡터 DB 구조 34",
    "options": [
      "Vector DB는 텍스트를 이미지로 저장한다.",
      "Chunking은 문서를 통째로 저장하는 것이다.",
      "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
      "Retrieval 단계에서는 가장 관련 없는 문서를 찾는다.",
      "Ingestion은 답변을 생성하는 단계다."
    ],
    "answer": "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
    "why": "임베딩 모델을 통해 텍스트를 고차원의 벡터 공간에 매핑하여 의미적 유사도를 계산할 수 있게 합니다.",
    "hint": "Text to Vector",
    "difficulty": "medium",
    "id": "5034"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인 및 벡터 DB 구조 35",
    "options": [
      "Vector DB는 텍스트를 이미지로 저장한다.",
      "Chunking은 문서를 통째로 저장하는 것이다.",
      "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
      "Retrieval 단계에서는 가장 관련 없는 문서를 찾는다.",
      "Ingestion은 답변을 생성하는 단계다."
    ],
    "answer": "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
    "why": "임베딩 모델을 통해 텍스트를 고차원의 벡터 공간에 매핑하여 의미적 유사도를 계산할 수 있게 합니다.",
    "hint": "Text to Vector",
    "difficulty": "medium",
    "id": "5035"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인 및 벡터 DB 구조 36",
    "options": [
      "Vector DB는 텍스트를 이미지로 저장한다.",
      "Chunking은 문서를 통째로 저장하는 것이다.",
      "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
      "Retrieval 단계에서는 가장 관련 없는 문서를 찾는다.",
      "Ingestion은 답변을 생성하는 단계다."
    ],
    "answer": "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
    "why": "임베딩 모델을 통해 텍스트를 고차원의 벡터 공간에 매핑하여 의미적 유사도를 계산할 수 있게 합니다.",
    "hint": "Text to Vector",
    "difficulty": "medium",
    "id": "5036"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인 및 벡터 DB 구조 37",
    "options": [
      "Vector DB는 텍스트를 이미지로 저장한다.",
      "Chunking은 문서를 통째로 저장하는 것이다.",
      "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
      "Retrieval 단계에서는 가장 관련 없는 문서를 찾는다.",
      "Ingestion은 답변을 생성하는 단계다."
    ],
    "answer": "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
    "why": "임베딩 모델을 통해 텍스트를 고차원의 벡터 공간에 매핑하여 의미적 유사도를 계산할 수 있게 합니다.",
    "hint": "Text to Vector",
    "difficulty": "medium",
    "id": "5037"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인 및 벡터 DB 구조 38",
    "options": [
      "Vector DB는 텍스트를 이미지로 저장한다.",
      "Chunking은 문서를 통째로 저장하는 것이다.",
      "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
      "Retrieval 단계에서는 가장 관련 없는 문서를 찾는다.",
      "Ingestion은 답변을 생성하는 단계다."
    ],
    "answer": "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
    "why": "임베딩 모델을 통해 텍스트를 고차원의 벡터 공간에 매핑하여 의미적 유사도를 계산할 수 있게 합니다.",
    "hint": "Text to Vector",
    "difficulty": "medium",
    "id": "5038"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인 및 벡터 DB 구조 39",
    "options": [
      "Vector DB는 텍스트를 이미지로 저장한다.",
      "Chunking은 문서를 통째로 저장하는 것이다.",
      "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
      "Retrieval 단계에서는 가장 관련 없는 문서를 찾는다.",
      "Ingestion은 답변을 생성하는 단계다."
    ],
    "answer": "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
    "why": "임베딩 모델을 통해 텍스트를 고차원의 벡터 공간에 매핑하여 의미적 유사도를 계산할 수 있게 합니다.",
    "hint": "Text to Vector",
    "difficulty": "medium",
    "id": "5039"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인 및 벡터 DB 구조 40",
    "options": [
      "Vector DB는 텍스트를 이미지로 저장한다.",
      "Chunking은 문서를 통째로 저장하는 것이다.",
      "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
      "Retrieval 단계에서는 가장 관련 없는 문서를 찾는다.",
      "Ingestion은 답변을 생성하는 단계다."
    ],
    "answer": "Embedding은 텍스트를 벡터(수치)로 변환하는 과정이다.",
    "why": "임베딩 모델을 통해 텍스트를 고차원의 벡터 공간에 매핑하여 의미적 유사도를 계산할 수 있게 합니다.",
    "hint": "Text to Vector",
    "difficulty": "medium",
    "id": "5040"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "LangChain 프레임워크 기초 41",
    "options": [
      "LangChain은 오직 Python만 지원한다.",
      "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
      "LCEL은 복잡한 클래스 상속을 강제한다.",
      "LangChain은 LLM을 직접 학습시키는 도구다.",
      "PromptTemplate은 사용할 수 없다."
    ],
    "answer": "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
    "why": "LangChain은 프롬프트, 모델, 파서 등을 체인(Chain)으로 엮어 복잡한 애플리케이션을 구축하게 해줍니다.",
    "hint": "사슬(Chain)",
    "difficulty": "medium",
    "id": "5041"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "LangChain 프레임워크 기초 42",
    "options": [
      "LangChain은 오직 Python만 지원한다.",
      "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
      "LCEL은 복잡한 클래스 상속을 강제한다.",
      "LangChain은 LLM을 직접 학습시키는 도구다.",
      "PromptTemplate은 사용할 수 없다."
    ],
    "answer": "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
    "why": "LangChain은 프롬프트, 모델, 파서 등을 체인(Chain)으로 엮어 복잡한 애플리케이션을 구축하게 해줍니다.",
    "hint": "사슬(Chain)",
    "difficulty": "medium",
    "id": "5042"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "LangChain 프레임워크 기초 43",
    "options": [
      "LangChain은 오직 Python만 지원한다.",
      "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
      "LCEL은 복잡한 클래스 상속을 강제한다.",
      "LangChain은 LLM을 직접 학습시키는 도구다.",
      "PromptTemplate은 사용할 수 없다."
    ],
    "answer": "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
    "why": "LangChain은 프롬프트, 모델, 파서 등을 체인(Chain)으로 엮어 복잡한 애플리케이션을 구축하게 해줍니다.",
    "hint": "사슬(Chain)",
    "difficulty": "medium",
    "id": "5043"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "LangChain 프레임워크 기초 44",
    "options": [
      "LangChain은 오직 Python만 지원한다.",
      "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
      "LCEL은 복잡한 클래스 상속을 강제한다.",
      "LangChain은 LLM을 직접 학습시키는 도구다.",
      "PromptTemplate은 사용할 수 없다."
    ],
    "answer": "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
    "why": "LangChain은 프롬프트, 모델, 파서 등을 체인(Chain)으로 엮어 복잡한 애플리케이션을 구축하게 해줍니다.",
    "hint": "사슬(Chain)",
    "difficulty": "medium",
    "id": "5044"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "LangChain 프레임워크 기초 45",
    "options": [
      "LangChain은 오직 Python만 지원한다.",
      "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
      "LCEL은 복잡한 클래스 상속을 강제한다.",
      "LangChain은 LLM을 직접 학습시키는 도구다.",
      "PromptTemplate은 사용할 수 없다."
    ],
    "answer": "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
    "why": "LangChain은 프롬프트, 모델, 파서 등을 체인(Chain)으로 엮어 복잡한 애플리케이션을 구축하게 해줍니다.",
    "hint": "사슬(Chain)",
    "difficulty": "medium",
    "id": "5045"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "LangChain 프레임워크 기초 46",
    "options": [
      "LangChain은 오직 Python만 지원한다.",
      "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
      "LCEL은 복잡한 클래스 상속을 강제한다.",
      "LangChain은 LLM을 직접 학습시키는 도구다.",
      "PromptTemplate은 사용할 수 없다."
    ],
    "answer": "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
    "why": "LangChain은 프롬프트, 모델, 파서 등을 체인(Chain)으로 엮어 복잡한 애플리케이션을 구축하게 해줍니다.",
    "hint": "사슬(Chain)",
    "difficulty": "medium",
    "id": "5046"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "LangChain 프레임워크 기초 47",
    "options": [
      "LangChain은 오직 Python만 지원한다.",
      "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
      "LCEL은 복잡한 클래스 상속을 강제한다.",
      "LangChain은 LLM을 직접 학습시키는 도구다.",
      "PromptTemplate은 사용할 수 없다."
    ],
    "answer": "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
    "why": "LangChain은 프롬프트, 모델, 파서 등을 체인(Chain)으로 엮어 복잡한 애플리케이션을 구축하게 해줍니다.",
    "hint": "사슬(Chain)",
    "difficulty": "medium",
    "id": "5047"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "LangChain 프레임워크 기초 48",
    "options": [
      "LangChain은 오직 Python만 지원한다.",
      "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
      "LCEL은 복잡한 클래스 상속을 강제한다.",
      "LangChain은 LLM을 직접 학습시키는 도구다.",
      "PromptTemplate은 사용할 수 없다."
    ],
    "answer": "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
    "why": "LangChain은 프롬프트, 모델, 파서 등을 체인(Chain)으로 엮어 복잡한 애플리케이션을 구축하게 해줍니다.",
    "hint": "사슬(Chain)",
    "difficulty": "medium",
    "id": "5048"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "LangChain 프레임워크 기초 49",
    "options": [
      "LangChain은 오직 Python만 지원한다.",
      "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
      "LCEL은 복잡한 클래스 상속을 강제한다.",
      "LangChain은 LLM을 직접 학습시키는 도구다.",
      "PromptTemplate은 사용할 수 없다."
    ],
    "answer": "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
    "why": "LangChain은 프롬프트, 모델, 파서 등을 체인(Chain)으로 엮어 복잡한 애플리케이션을 구축하게 해줍니다.",
    "hint": "사슬(Chain)",
    "difficulty": "medium",
    "id": "5049"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "LangChain 프레임워크 기초 50",
    "options": [
      "LangChain은 오직 Python만 지원한다.",
      "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
      "LCEL은 복잡한 클래스 상속을 강제한다.",
      "LangChain은 LLM을 직접 학습시키는 도구다.",
      "PromptTemplate은 사용할 수 없다."
    ],
    "answer": "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
    "why": "LangChain은 프롬프트, 모델, 파서 등을 체인(Chain)으로 엮어 복잡한 애플리케이션을 구축하게 해줍니다.",
    "hint": "사슬(Chain)",
    "difficulty": "medium",
    "id": "5050"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "LangChain 프레임워크 기초 51",
    "options": [
      "LangChain은 오직 Python만 지원한다.",
      "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
      "LCEL은 복잡한 클래스 상속을 강제한다.",
      "LangChain은 LLM을 직접 학습시키는 도구다.",
      "PromptTemplate은 사용할 수 없다."
    ],
    "answer": "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
    "why": "LangChain은 프롬프트, 모델, 파서 등을 체인(Chain)으로 엮어 복잡한 애플리케이션을 구축하게 해줍니다.",
    "hint": "사슬(Chain)",
    "difficulty": "medium",
    "id": "5051"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "LangChain 프레임워크 기초 52",
    "options": [
      "LangChain은 오직 Python만 지원한다.",
      "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
      "LCEL은 복잡한 클래스 상속을 강제한다.",
      "LangChain은 LLM을 직접 학습시키는 도구다.",
      "PromptTemplate은 사용할 수 없다."
    ],
    "answer": "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
    "why": "LangChain은 프롬프트, 모델, 파서 등을 체인(Chain)으로 엮어 복잡한 애플리케이션을 구축하게 해줍니다.",
    "hint": "사슬(Chain)",
    "difficulty": "medium",
    "id": "5052"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "LangChain 프레임워크 기초 53",
    "options": [
      "LangChain은 오직 Python만 지원한다.",
      "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
      "LCEL은 복잡한 클래스 상속을 강제한다.",
      "LangChain은 LLM을 직접 학습시키는 도구다.",
      "PromptTemplate은 사용할 수 없다."
    ],
    "answer": "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
    "why": "LangChain은 프롬프트, 모델, 파서 등을 체인(Chain)으로 엮어 복잡한 애플리케이션을 구축하게 해줍니다.",
    "hint": "사슬(Chain)",
    "difficulty": "medium",
    "id": "5053"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "LangChain 프레임워크 기초 54",
    "options": [
      "LangChain은 오직 Python만 지원한다.",
      "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
      "LCEL은 복잡한 클래스 상속을 강제한다.",
      "LangChain은 LLM을 직접 학습시키는 도구다.",
      "PromptTemplate은 사용할 수 없다."
    ],
    "answer": "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
    "why": "LangChain은 프롬프트, 모델, 파서 등을 체인(Chain)으로 엮어 복잡한 애플리케이션을 구축하게 해줍니다.",
    "hint": "사슬(Chain)",
    "difficulty": "medium",
    "id": "5054"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "LangChain 프레임워크 기초 55",
    "options": [
      "LangChain은 오직 Python만 지원한다.",
      "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
      "LCEL은 복잡한 클래스 상속을 강제한다.",
      "LangChain은 LLM을 직접 학습시키는 도구다.",
      "PromptTemplate은 사용할 수 없다."
    ],
    "answer": "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
    "why": "LangChain은 프롬프트, 모델, 파서 등을 체인(Chain)으로 엮어 복잡한 애플리케이션을 구축하게 해줍니다.",
    "hint": "사슬(Chain)",
    "difficulty": "medium",
    "id": "5055"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "LangChain 프레임워크 기초 56",
    "options": [
      "LangChain은 오직 Python만 지원한다.",
      "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
      "LCEL은 복잡한 클래스 상속을 강제한다.",
      "LangChain은 LLM을 직접 학습시키는 도구다.",
      "PromptTemplate은 사용할 수 없다."
    ],
    "answer": "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
    "why": "LangChain은 프롬프트, 모델, 파서 등을 체인(Chain)으로 엮어 복잡한 애플리케이션을 구축하게 해줍니다.",
    "hint": "사슬(Chain)",
    "difficulty": "medium",
    "id": "5056"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "LangChain 프레임워크 기초 57",
    "options": [
      "LangChain은 오직 Python만 지원한다.",
      "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
      "LCEL은 복잡한 클래스 상속을 강제한다.",
      "LangChain은 LLM을 직접 학습시키는 도구다.",
      "PromptTemplate은 사용할 수 없다."
    ],
    "answer": "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
    "why": "LangChain은 프롬프트, 모델, 파서 등을 체인(Chain)으로 엮어 복잡한 애플리케이션을 구축하게 해줍니다.",
    "hint": "사슬(Chain)",
    "difficulty": "medium",
    "id": "5057"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "LangChain 프레임워크 기초 58",
    "options": [
      "LangChain은 오직 Python만 지원한다.",
      "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
      "LCEL은 복잡한 클래스 상속을 강제한다.",
      "LangChain은 LLM을 직접 학습시키는 도구다.",
      "PromptTemplate은 사용할 수 없다."
    ],
    "answer": "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
    "why": "LangChain은 프롬프트, 모델, 파서 등을 체인(Chain)으로 엮어 복잡한 애플리케이션을 구축하게 해줍니다.",
    "hint": "사슬(Chain)",
    "difficulty": "medium",
    "id": "5058"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "LangChain 프레임워크 기초 59",
    "options": [
      "LangChain은 오직 Python만 지원한다.",
      "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
      "LCEL은 복잡한 클래스 상속을 강제한다.",
      "LangChain은 LLM을 직접 학습시키는 도구다.",
      "PromptTemplate은 사용할 수 없다."
    ],
    "answer": "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
    "why": "LangChain은 프롬프트, 모델, 파서 등을 체인(Chain)으로 엮어 복잡한 애플리케이션을 구축하게 해줍니다.",
    "hint": "사슬(Chain)",
    "difficulty": "medium",
    "id": "5059"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "LangChain 프레임워크 기초 60",
    "options": [
      "LangChain은 오직 Python만 지원한다.",
      "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
      "LCEL은 복잡한 클래스 상속을 강제한다.",
      "LangChain은 LLM을 직접 학습시키는 도구다.",
      "PromptTemplate은 사용할 수 없다."
    ],
    "answer": "Chain은 여러 컴포넌트를 연결하는 핵심 개념이다.",
    "why": "LangChain은 프롬프트, 모델, 파서 등을 체인(Chain)으로 엮어 복잡한 애플리케이션을 구축하게 해줍니다.",
    "hint": "사슬(Chain)",
    "difficulty": "medium",
    "id": "5060"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트와 도구 활용 61",
    "options": [
      "에이전트는 정해진 각본대로만 움직인다.",
      "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
      "Tool은 에이전트가 사용할 수 없는 외부 기능이다.",
      "에이전트는 스스로 계획(Planning)하지 못한다.",
      "웹 검색은 Tool로 사용할 수 없다."
    ],
    "answer": "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
    "why": "ReAct 프롬프팅을 통해 에이전트는 상황을 판단하고 필요한 도구를 선택하여 실행한 뒤 결과를 관찰합니다.",
    "hint": "Reason + Act",
    "difficulty": "hard",
    "id": "5061"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트와 도구 활용 62",
    "options": [
      "에이전트는 정해진 각본대로만 움직인다.",
      "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
      "Tool은 에이전트가 사용할 수 없는 외부 기능이다.",
      "에이전트는 스스로 계획(Planning)하지 못한다.",
      "웹 검색은 Tool로 사용할 수 없다."
    ],
    "answer": "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
    "why": "ReAct 프롬프팅을 통해 에이전트는 상황을 판단하고 필요한 도구를 선택하여 실행한 뒤 결과를 관찰합니다.",
    "hint": "Reason + Act",
    "difficulty": "hard",
    "id": "5062"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트와 도구 활용 63",
    "options": [
      "에이전트는 정해진 각본대로만 움직인다.",
      "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
      "Tool은 에이전트가 사용할 수 없는 외부 기능이다.",
      "에이전트는 스스로 계획(Planning)하지 못한다.",
      "웹 검색은 Tool로 사용할 수 없다."
    ],
    "answer": "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
    "why": "ReAct 프롬프팅을 통해 에이전트는 상황을 판단하고 필요한 도구를 선택하여 실행한 뒤 결과를 관찰합니다.",
    "hint": "Reason + Act",
    "difficulty": "hard",
    "id": "5063"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트와 도구 활용 64",
    "options": [
      "에이전트는 정해진 각본대로만 움직인다.",
      "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
      "Tool은 에이전트가 사용할 수 없는 외부 기능이다.",
      "에이전트는 스스로 계획(Planning)하지 못한다.",
      "웹 검색은 Tool로 사용할 수 없다."
    ],
    "answer": "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
    "why": "ReAct 프롬프팅을 통해 에이전트는 상황을 판단하고 필요한 도구를 선택하여 실행한 뒤 결과를 관찰합니다.",
    "hint": "Reason + Act",
    "difficulty": "hard",
    "id": "5064"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트와 도구 활용 65",
    "options": [
      "에이전트는 정해진 각본대로만 움직인다.",
      "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
      "Tool은 에이전트가 사용할 수 없는 외부 기능이다.",
      "에이전트는 스스로 계획(Planning)하지 못한다.",
      "웹 검색은 Tool로 사용할 수 없다."
    ],
    "answer": "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
    "why": "ReAct 프롬프팅을 통해 에이전트는 상황을 판단하고 필요한 도구를 선택하여 실행한 뒤 결과를 관찰합니다.",
    "hint": "Reason + Act",
    "difficulty": "hard",
    "id": "5065"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트와 도구 활용 66",
    "options": [
      "에이전트는 정해진 각본대로만 움직인다.",
      "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
      "Tool은 에이전트가 사용할 수 없는 외부 기능이다.",
      "에이전트는 스스로 계획(Planning)하지 못한다.",
      "웹 검색은 Tool로 사용할 수 없다."
    ],
    "answer": "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
    "why": "ReAct 프롬프팅을 통해 에이전트는 상황을 판단하고 필요한 도구를 선택하여 실행한 뒤 결과를 관찰합니다.",
    "hint": "Reason + Act",
    "difficulty": "hard",
    "id": "5066"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트와 도구 활용 67",
    "options": [
      "에이전트는 정해진 각본대로만 움직인다.",
      "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
      "Tool은 에이전트가 사용할 수 없는 외부 기능이다.",
      "에이전트는 스스로 계획(Planning)하지 못한다.",
      "웹 검색은 Tool로 사용할 수 없다."
    ],
    "answer": "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
    "why": "ReAct 프롬프팅을 통해 에이전트는 상황을 판단하고 필요한 도구를 선택하여 실행한 뒤 결과를 관찰합니다.",
    "hint": "Reason + Act",
    "difficulty": "hard",
    "id": "5067"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트와 도구 활용 68",
    "options": [
      "에이전트는 정해진 각본대로만 움직인다.",
      "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
      "Tool은 에이전트가 사용할 수 없는 외부 기능이다.",
      "에이전트는 스스로 계획(Planning)하지 못한다.",
      "웹 검색은 Tool로 사용할 수 없다."
    ],
    "answer": "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
    "why": "ReAct 프롬프팅을 통해 에이전트는 상황을 판단하고 필요한 도구를 선택하여 실행한 뒤 결과를 관찰합니다.",
    "hint": "Reason + Act",
    "difficulty": "hard",
    "id": "5068"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트와 도구 활용 69",
    "options": [
      "에이전트는 정해진 각본대로만 움직인다.",
      "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
      "Tool은 에이전트가 사용할 수 없는 외부 기능이다.",
      "에이전트는 스스로 계획(Planning)하지 못한다.",
      "웹 검색은 Tool로 사용할 수 없다."
    ],
    "answer": "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
    "why": "ReAct 프롬프팅을 통해 에이전트는 상황을 판단하고 필요한 도구를 선택하여 실행한 뒤 결과를 관찰합니다.",
    "hint": "Reason + Act",
    "difficulty": "hard",
    "id": "5069"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트와 도구 활용 70",
    "options": [
      "에이전트는 정해진 각본대로만 움직인다.",
      "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
      "Tool은 에이전트가 사용할 수 없는 외부 기능이다.",
      "에이전트는 스스로 계획(Planning)하지 못한다.",
      "웹 검색은 Tool로 사용할 수 없다."
    ],
    "answer": "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
    "why": "ReAct 프롬프팅을 통해 에이전트는 상황을 판단하고 필요한 도구를 선택하여 실행한 뒤 결과를 관찰합니다.",
    "hint": "Reason + Act",
    "difficulty": "hard",
    "id": "5070"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트와 도구 활용 71",
    "options": [
      "에이전트는 정해진 각본대로만 움직인다.",
      "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
      "Tool은 에이전트가 사용할 수 없는 외부 기능이다.",
      "에이전트는 스스로 계획(Planning)하지 못한다.",
      "웹 검색은 Tool로 사용할 수 없다."
    ],
    "answer": "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
    "why": "ReAct 프롬프팅을 통해 에이전트는 상황을 판단하고 필요한 도구를 선택하여 실행한 뒤 결과를 관찰합니다.",
    "hint": "Reason + Act",
    "difficulty": "hard",
    "id": "5071"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트와 도구 활용 72",
    "options": [
      "에이전트는 정해진 각본대로만 움직인다.",
      "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
      "Tool은 에이전트가 사용할 수 없는 외부 기능이다.",
      "에이전트는 스스로 계획(Planning)하지 못한다.",
      "웹 검색은 Tool로 사용할 수 없다."
    ],
    "answer": "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
    "why": "ReAct 프롬프팅을 통해 에이전트는 상황을 판단하고 필요한 도구를 선택하여 실행한 뒤 결과를 관찰합니다.",
    "hint": "Reason + Act",
    "difficulty": "hard",
    "id": "5072"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트와 도구 활용 73",
    "options": [
      "에이전트는 정해진 각본대로만 움직인다.",
      "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
      "Tool은 에이전트가 사용할 수 없는 외부 기능이다.",
      "에이전트는 스스로 계획(Planning)하지 못한다.",
      "웹 검색은 Tool로 사용할 수 없다."
    ],
    "answer": "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
    "why": "ReAct 프롬프팅을 통해 에이전트는 상황을 판단하고 필요한 도구를 선택하여 실행한 뒤 결과를 관찰합니다.",
    "hint": "Reason + Act",
    "difficulty": "hard",
    "id": "5073"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트와 도구 활용 74",
    "options": [
      "에이전트는 정해진 각본대로만 움직인다.",
      "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
      "Tool은 에이전트가 사용할 수 없는 외부 기능이다.",
      "에이전트는 스스로 계획(Planning)하지 못한다.",
      "웹 검색은 Tool로 사용할 수 없다."
    ],
    "answer": "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
    "why": "ReAct 프롬프팅을 통해 에이전트는 상황을 판단하고 필요한 도구를 선택하여 실행한 뒤 결과를 관찰합니다.",
    "hint": "Reason + Act",
    "difficulty": "hard",
    "id": "5074"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트와 도구 활용 75",
    "options": [
      "에이전트는 정해진 각본대로만 움직인다.",
      "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
      "Tool은 에이전트가 사용할 수 없는 외부 기능이다.",
      "에이전트는 스스로 계획(Planning)하지 못한다.",
      "웹 검색은 Tool로 사용할 수 없다."
    ],
    "answer": "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
    "why": "ReAct 프롬프팅을 통해 에이전트는 상황을 판단하고 필요한 도구를 선택하여 실행한 뒤 결과를 관찰합니다.",
    "hint": "Reason + Act",
    "difficulty": "hard",
    "id": "5075"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트와 도구 활용 76",
    "options": [
      "에이전트는 정해진 각본대로만 움직인다.",
      "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
      "Tool은 에이전트가 사용할 수 없는 외부 기능이다.",
      "에이전트는 스스로 계획(Planning)하지 못한다.",
      "웹 검색은 Tool로 사용할 수 없다."
    ],
    "answer": "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
    "why": "ReAct 프롬프팅을 통해 에이전트는 상황을 판단하고 필요한 도구를 선택하여 실행한 뒤 결과를 관찰합니다.",
    "hint": "Reason + Act",
    "difficulty": "hard",
    "id": "5076"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트와 도구 활용 77",
    "options": [
      "에이전트는 정해진 각본대로만 움직인다.",
      "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
      "Tool은 에이전트가 사용할 수 없는 외부 기능이다.",
      "에이전트는 스스로 계획(Planning)하지 못한다.",
      "웹 검색은 Tool로 사용할 수 없다."
    ],
    "answer": "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
    "why": "ReAct 프롬프팅을 통해 에이전트는 상황을 판단하고 필요한 도구를 선택하여 실행한 뒤 결과를 관찰합니다.",
    "hint": "Reason + Act",
    "difficulty": "hard",
    "id": "5077"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트와 도구 활용 78",
    "options": [
      "에이전트는 정해진 각본대로만 움직인다.",
      "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
      "Tool은 에이전트가 사용할 수 없는 외부 기능이다.",
      "에이전트는 스스로 계획(Planning)하지 못한다.",
      "웹 검색은 Tool로 사용할 수 없다."
    ],
    "answer": "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
    "why": "ReAct 프롬프팅을 통해 에이전트는 상황을 판단하고 필요한 도구를 선택하여 실행한 뒤 결과를 관찰합니다.",
    "hint": "Reason + Act",
    "difficulty": "hard",
    "id": "5078"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트와 도구 활용 79",
    "options": [
      "에이전트는 정해진 각본대로만 움직인다.",
      "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
      "Tool은 에이전트가 사용할 수 없는 외부 기능이다.",
      "에이전트는 스스로 계획(Planning)하지 못한다.",
      "웹 검색은 Tool로 사용할 수 없다."
    ],
    "answer": "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
    "why": "ReAct 프롬프팅을 통해 에이전트는 상황을 판단하고 필요한 도구를 선택하여 실행한 뒤 결과를 관찰합니다.",
    "hint": "Reason + Act",
    "difficulty": "hard",
    "id": "5079"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트와 도구 활용 80",
    "options": [
      "에이전트는 정해진 각본대로만 움직인다.",
      "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
      "Tool은 에이전트가 사용할 수 없는 외부 기능이다.",
      "에이전트는 스스로 계획(Planning)하지 못한다.",
      "웹 검색은 Tool로 사용할 수 없다."
    ],
    "answer": "ReAct는 추론(Reasoning)과 행동(Acting)을 결합한 패턴이다.",
    "why": "ReAct 프롬프팅을 통해 에이전트는 상황을 판단하고 필요한 도구를 선택하여 실행한 뒤 결과를 관찰합니다.",
    "hint": "Reason + Act",
    "difficulty": "hard",
    "id": "5080"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 심화 및 최적화 81",
    "options": [
      "Self-Querying은 질문을 스스로 수정하지 않는다.",
      "Multi-Vector Retriever는 문서 전체만 저장한다.",
      "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
      "RAG는 문맥 길이(Context Window) 제한이 없다.",
      "Parent Document Retriever는 자식 문서만 검색한다."
    ],
    "answer": "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
    "why": "BM25(키워드)와 Vector(의미) 검색을 결합하여 검색 정확도(Recall/Precision)를 높이는 방식입니다.",
    "hint": "섞어서 쓴다(Hybrid)",
    "difficulty": "hard",
    "id": "5081"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 심화 및 최적화 82",
    "options": [
      "Self-Querying은 질문을 스스로 수정하지 않는다.",
      "Multi-Vector Retriever는 문서 전체만 저장한다.",
      "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
      "RAG는 문맥 길이(Context Window) 제한이 없다.",
      "Parent Document Retriever는 자식 문서만 검색한다."
    ],
    "answer": "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
    "why": "BM25(키워드)와 Vector(의미) 검색을 결합하여 검색 정확도(Recall/Precision)를 높이는 방식입니다.",
    "hint": "섞어서 쓴다(Hybrid)",
    "difficulty": "hard",
    "id": "5082"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 심화 및 최적화 83",
    "options": [
      "Self-Querying은 질문을 스스로 수정하지 않는다.",
      "Multi-Vector Retriever는 문서 전체만 저장한다.",
      "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
      "RAG는 문맥 길이(Context Window) 제한이 없다.",
      "Parent Document Retriever는 자식 문서만 검색한다."
    ],
    "answer": "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
    "why": "BM25(키워드)와 Vector(의미) 검색을 결합하여 검색 정확도(Recall/Precision)를 높이는 방식입니다.",
    "hint": "섞어서 쓴다(Hybrid)",
    "difficulty": "hard",
    "id": "5083"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 심화 및 최적화 84",
    "options": [
      "Self-Querying은 질문을 스스로 수정하지 않는다.",
      "Multi-Vector Retriever는 문서 전체만 저장한다.",
      "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
      "RAG는 문맥 길이(Context Window) 제한이 없다.",
      "Parent Document Retriever는 자식 문서만 검색한다."
    ],
    "answer": "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
    "why": "BM25(키워드)와 Vector(의미) 검색을 결합하여 검색 정확도(Recall/Precision)를 높이는 방식입니다.",
    "hint": "섞어서 쓴다(Hybrid)",
    "difficulty": "hard",
    "id": "5084"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 심화 및 최적화 85",
    "options": [
      "Self-Querying은 질문을 스스로 수정하지 않는다.",
      "Multi-Vector Retriever는 문서 전체만 저장한다.",
      "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
      "RAG는 문맥 길이(Context Window) 제한이 없다.",
      "Parent Document Retriever는 자식 문서만 검색한다."
    ],
    "answer": "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
    "why": "BM25(키워드)와 Vector(의미) 검색을 결합하여 검색 정확도(Recall/Precision)를 높이는 방식입니다.",
    "hint": "섞어서 쓴다(Hybrid)",
    "difficulty": "hard",
    "id": "5085"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 심화 및 최적화 86",
    "options": [
      "Self-Querying은 질문을 스스로 수정하지 않는다.",
      "Multi-Vector Retriever는 문서 전체만 저장한다.",
      "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
      "RAG는 문맥 길이(Context Window) 제한이 없다.",
      "Parent Document Retriever는 자식 문서만 검색한다."
    ],
    "answer": "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
    "why": "BM25(키워드)와 Vector(의미) 검색을 결합하여 검색 정확도(Recall/Precision)를 높이는 방식입니다.",
    "hint": "섞어서 쓴다(Hybrid)",
    "difficulty": "hard",
    "id": "5086"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 심화 및 최적화 87",
    "options": [
      "Self-Querying은 질문을 스스로 수정하지 않는다.",
      "Multi-Vector Retriever는 문서 전체만 저장한다.",
      "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
      "RAG는 문맥 길이(Context Window) 제한이 없다.",
      "Parent Document Retriever는 자식 문서만 검색한다."
    ],
    "answer": "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
    "why": "BM25(키워드)와 Vector(의미) 검색을 결합하여 검색 정확도(Recall/Precision)를 높이는 방식입니다.",
    "hint": "섞어서 쓴다(Hybrid)",
    "difficulty": "hard",
    "id": "5087"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 심화 및 최적화 88",
    "options": [
      "Self-Querying은 질문을 스스로 수정하지 않는다.",
      "Multi-Vector Retriever는 문서 전체만 저장한다.",
      "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
      "RAG는 문맥 길이(Context Window) 제한이 없다.",
      "Parent Document Retriever는 자식 문서만 검색한다."
    ],
    "answer": "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
    "why": "BM25(키워드)와 Vector(의미) 검색을 결합하여 검색 정확도(Recall/Precision)를 높이는 방식입니다.",
    "hint": "섞어서 쓴다(Hybrid)",
    "difficulty": "hard",
    "id": "5088"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 심화 및 최적화 89",
    "options": [
      "Self-Querying은 질문을 스스로 수정하지 않는다.",
      "Multi-Vector Retriever는 문서 전체만 저장한다.",
      "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
      "RAG는 문맥 길이(Context Window) 제한이 없다.",
      "Parent Document Retriever는 자식 문서만 검색한다."
    ],
    "answer": "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
    "why": "BM25(키워드)와 Vector(의미) 검색을 결합하여 검색 정확도(Recall/Precision)를 높이는 방식입니다.",
    "hint": "섞어서 쓴다(Hybrid)",
    "difficulty": "hard",
    "id": "5089"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 심화 및 최적화 90",
    "options": [
      "Self-Querying은 질문을 스스로 수정하지 않는다.",
      "Multi-Vector Retriever는 문서 전체만 저장한다.",
      "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
      "RAG는 문맥 길이(Context Window) 제한이 없다.",
      "Parent Document Retriever는 자식 문서만 검색한다."
    ],
    "answer": "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
    "why": "BM25(키워드)와 Vector(의미) 검색을 결합하여 검색 정확도(Recall/Precision)를 높이는 방식입니다.",
    "hint": "섞어서 쓴다(Hybrid)",
    "difficulty": "hard",
    "id": "5090"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 심화 및 최적화 91",
    "options": [
      "Self-Querying은 질문을 스스로 수정하지 않는다.",
      "Multi-Vector Retriever는 문서 전체만 저장한다.",
      "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
      "RAG는 문맥 길이(Context Window) 제한이 없다.",
      "Parent Document Retriever는 자식 문서만 검색한다."
    ],
    "answer": "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
    "why": "BM25(키워드)와 Vector(의미) 검색을 결합하여 검색 정확도(Recall/Precision)를 높이는 방식입니다.",
    "hint": "섞어서 쓴다(Hybrid)",
    "difficulty": "hard",
    "id": "5091"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 심화 및 최적화 92",
    "options": [
      "Self-Querying은 질문을 스스로 수정하지 않는다.",
      "Multi-Vector Retriever는 문서 전체만 저장한다.",
      "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
      "RAG는 문맥 길이(Context Window) 제한이 없다.",
      "Parent Document Retriever는 자식 문서만 검색한다."
    ],
    "answer": "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
    "why": "BM25(키워드)와 Vector(의미) 검색을 결합하여 검색 정확도(Recall/Precision)를 높이는 방식입니다.",
    "hint": "섞어서 쓴다(Hybrid)",
    "difficulty": "hard",
    "id": "5092"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 심화 및 최적화 93",
    "options": [
      "Self-Querying은 질문을 스스로 수정하지 않는다.",
      "Multi-Vector Retriever는 문서 전체만 저장한다.",
      "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
      "RAG는 문맥 길이(Context Window) 제한이 없다.",
      "Parent Document Retriever는 자식 문서만 검색한다."
    ],
    "answer": "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
    "why": "BM25(키워드)와 Vector(의미) 검색을 결합하여 검색 정확도(Recall/Precision)를 높이는 방식입니다.",
    "hint": "섞어서 쓴다(Hybrid)",
    "difficulty": "hard",
    "id": "5093"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 심화 및 최적화 94",
    "options": [
      "Self-Querying은 질문을 스스로 수정하지 않는다.",
      "Multi-Vector Retriever는 문서 전체만 저장한다.",
      "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
      "RAG는 문맥 길이(Context Window) 제한이 없다.",
      "Parent Document Retriever는 자식 문서만 검색한다."
    ],
    "answer": "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
    "why": "BM25(키워드)와 Vector(의미) 검색을 결합하여 검색 정확도(Recall/Precision)를 높이는 방식입니다.",
    "hint": "섞어서 쓴다(Hybrid)",
    "difficulty": "hard",
    "id": "5094"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 심화 및 최적화 95",
    "options": [
      "Self-Querying은 질문을 스스로 수정하지 않는다.",
      "Multi-Vector Retriever는 문서 전체만 저장한다.",
      "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
      "RAG는 문맥 길이(Context Window) 제한이 없다.",
      "Parent Document Retriever는 자식 문서만 검색한다."
    ],
    "answer": "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
    "why": "BM25(키워드)와 Vector(의미) 검색을 결합하여 검색 정확도(Recall/Precision)를 높이는 방식입니다.",
    "hint": "섞어서 쓴다(Hybrid)",
    "difficulty": "hard",
    "id": "5095"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 심화 및 최적화 96",
    "options": [
      "Self-Querying은 질문을 스스로 수정하지 않는다.",
      "Multi-Vector Retriever는 문서 전체만 저장한다.",
      "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
      "RAG는 문맥 길이(Context Window) 제한이 없다.",
      "Parent Document Retriever는 자식 문서만 검색한다."
    ],
    "answer": "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
    "why": "BM25(키워드)와 Vector(의미) 검색을 결합하여 검색 정확도(Recall/Precision)를 높이는 방식입니다.",
    "hint": "섞어서 쓴다(Hybrid)",
    "difficulty": "hard",
    "id": "5096"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 심화 및 최적화 97",
    "options": [
      "Self-Querying은 질문을 스스로 수정하지 않는다.",
      "Multi-Vector Retriever는 문서 전체만 저장한다.",
      "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
      "RAG는 문맥 길이(Context Window) 제한이 없다.",
      "Parent Document Retriever는 자식 문서만 검색한다."
    ],
    "answer": "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
    "why": "BM25(키워드)와 Vector(의미) 검색을 결합하여 검색 정확도(Recall/Precision)를 높이는 방식입니다.",
    "hint": "섞어서 쓴다(Hybrid)",
    "difficulty": "hard",
    "id": "5097"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 심화 및 최적화 98",
    "options": [
      "Self-Querying은 질문을 스스로 수정하지 않는다.",
      "Multi-Vector Retriever는 문서 전체만 저장한다.",
      "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
      "RAG는 문맥 길이(Context Window) 제한이 없다.",
      "Parent Document Retriever는 자식 문서만 검색한다."
    ],
    "answer": "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
    "why": "BM25(키워드)와 Vector(의미) 검색을 결합하여 검색 정확도(Recall/Precision)를 높이는 방식입니다.",
    "hint": "섞어서 쓴다(Hybrid)",
    "difficulty": "hard",
    "id": "5098"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 심화 및 최적화 99",
    "options": [
      "Self-Querying은 질문을 스스로 수정하지 않는다.",
      "Multi-Vector Retriever는 문서 전체만 저장한다.",
      "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
      "RAG는 문맥 길이(Context Window) 제한이 없다.",
      "Parent Document Retriever는 자식 문서만 검색한다."
    ],
    "answer": "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
    "why": "BM25(키워드)와 Vector(의미) 검색을 결합하여 검색 정확도(Recall/Precision)를 높이는 방식입니다.",
    "hint": "섞어서 쓴다(Hybrid)",
    "difficulty": "hard",
    "id": "5099"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 심화 및 최적화 100",
    "options": [
      "Self-Querying은 질문을 스스로 수정하지 않는다.",
      "Multi-Vector Retriever는 문서 전체만 저장한다.",
      "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
      "RAG는 문맥 길이(Context Window) 제한이 없다.",
      "Parent Document Retriever는 자식 문서만 검색한다."
    ],
    "answer": "Hybrid Search는 키워드 검색과 시맨틱 검색을 결합한다.",
    "why": "BM25(키워드)와 Vector(의미) 검색을 결합하여 검색 정확도(Recall/Precision)를 높이는 방식입니다.",
    "hint": "섞어서 쓴다(Hybrid)",
    "difficulty": "hard",
    "id": "50100"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "텍스트를 분할하세요.\n```python\ntext_splitter = CharacterTextSplitter(chunk_size=1000)\ntexts = text_splitter.____(docs)\n```",
    "answer": "split_documents",
    "why": "문서 분할 메서드입니다.",
    "hint": "LangChain Class",
    "difficulty": "medium",
    "id": "5101"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "RAG/LangChain 코드를 완성하세요 (문제 2)",
    "answer": "Chain",
    "why": "LangChain의 기본 구성 단위",
    "hint": "LangChain Class",
    "difficulty": "medium",
    "id": "5102"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "RAG/LangChain 코드를 완성하세요 (문제 3)",
    "answer": "Chain",
    "why": "LangChain의 기본 구성 단위",
    "hint": "LangChain Class",
    "difficulty": "medium",
    "id": "5103"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "문서를 로드하세요.\n```python\nfrom langchain_community.document_loaders import TextLoader\nloader = TextLoader('data.txt')\ndocs = loader.____()\n```",
    "answer": "load",
    "why": "문서 로드 메서드는 load()입니다.",
    "hint": "LangChain Class",
    "difficulty": "medium",
    "id": "5104"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "텍스트를 분할하세요.\n```python\ntext_splitter = CharacterTextSplitter(chunk_size=1000)\ntexts = text_splitter.____(docs)\n```",
    "answer": "split_documents",
    "why": "문서 분할 메서드입니다.",
    "hint": "LangChain Class",
    "difficulty": "medium",
    "id": "5105"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "RAG/LangChain 코드를 완성하세요 (문제 6)",
    "answer": "Chain",
    "why": "LangChain의 기본 구성 단위",
    "hint": "LangChain Class",
    "difficulty": "medium",
    "id": "5106"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "RAG/LangChain 코드를 완성하세요 (문제 7)",
    "answer": "Chain",
    "why": "LangChain의 기본 구성 단위",
    "hint": "LangChain Class",
    "difficulty": "medium",
    "id": "5107"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "문서를 로드하세요.\n```python\nfrom langchain_community.document_loaders import TextLoader\nloader = TextLoader('data.txt')\ndocs = loader.____()\n```",
    "answer": "load",
    "why": "문서 로드 메서드는 load()입니다.",
    "hint": "LangChain Class",
    "difficulty": "medium",
    "id": "5108"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "텍스트를 분할하세요.\n```python\ntext_splitter = CharacterTextSplitter(chunk_size=1000)\ntexts = text_splitter.____(docs)\n```",
    "answer": "split_documents",
    "why": "문서 분할 메서드입니다.",
    "hint": "LangChain Class",
    "difficulty": "medium",
    "id": "5109"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "RAG/LangChain 코드를 완성하세요 (문제 10)",
    "answer": "Chain",
    "why": "LangChain의 기본 구성 단위",
    "hint": "LangChain Class",
    "difficulty": "medium",
    "id": "5110"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "RAG/LangChain 코드를 완성하세요 (문제 11)",
    "answer": "Chain",
    "why": "LangChain의 기본 구성 단위",
    "hint": "LangChain Class",
    "difficulty": "medium",
    "id": "5111"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "문서를 로드하세요.\n```python\nfrom langchain_community.document_loaders import TextLoader\nloader = TextLoader('data.txt')\ndocs = loader.____()\n```",
    "answer": "load",
    "why": "문서 로드 메서드는 load()입니다.",
    "hint": "LangChain Class",
    "difficulty": "medium",
    "id": "5112"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "텍스트를 분할하세요.\n```python\ntext_splitter = CharacterTextSplitter(chunk_size=1000)\ntexts = text_splitter.____(docs)\n```",
    "answer": "split_documents",
    "why": "문서 분할 메서드입니다.",
    "hint": "LangChain Class",
    "difficulty": "medium",
    "id": "5113"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "RAG/LangChain 코드를 완성하세요 (문제 14)",
    "answer": "Chain",
    "why": "LangChain의 기본 구성 단위",
    "hint": "LangChain Class",
    "difficulty": "medium",
    "id": "5114"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "RAG/LangChain 코드를 완성하세요 (문제 15)",
    "answer": "Chain",
    "why": "LangChain의 기본 구성 단위",
    "hint": "LangChain Class",
    "difficulty": "medium",
    "id": "5115"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "문서를 로드하세요.\n```python\nfrom langchain_community.document_loaders import TextLoader\nloader = TextLoader('data.txt')\ndocs = loader.____()\n```",
    "answer": "load",
    "why": "문서 로드 메서드는 load()입니다.",
    "hint": "LangChain Class",
    "difficulty": "medium",
    "id": "5116"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "텍스트를 분할하세요.\n```python\ntext_splitter = CharacterTextSplitter(chunk_size=1000)\ntexts = text_splitter.____(docs)\n```",
    "answer": "split_documents",
    "why": "문서 분할 메서드입니다.",
    "hint": "LangChain Class",
    "difficulty": "medium",
    "id": "5117"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "RAG/LangChain 코드를 완성하세요 (문제 18)",
    "answer": "Chain",
    "why": "LangChain의 기본 구성 단위",
    "hint": "LangChain Class",
    "difficulty": "medium",
    "id": "5118"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "RAG/LangChain 코드를 완성하세요 (문제 19)",
    "answer": "Chain",
    "why": "LangChain의 기본 구성 단위",
    "hint": "LangChain Class",
    "difficulty": "medium",
    "id": "5119"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "문서를 로드하세요.\n```python\nfrom langchain_community.document_loaders import TextLoader\nloader = TextLoader('data.txt')\ndocs = loader.____()\n```",
    "answer": "load",
    "why": "문서 로드 메서드는 load()입니다.",
    "hint": "LangChain Class",
    "difficulty": "medium",
    "id": "5120"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "Fine Tuning 기초 및 필요성 1",
    "options": [
      "Fine Tuning은 모델의 파라미터를 전혀 수정하지 않는다.",
      "Pre-training과 Fine-Tuning은 완전히 같은 과정이다.",
      "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
      "Fine Tuning을 하면 항상 모델이 똑똑해진다(Catastrophic Forgetting 없음).",
      "Fine Tuning은 데이터가 없어도 수행할 수 있다."
    ],
    "answer": "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
    "why": "사전 학습된 일반적인 지식 위에 특정 도메인의 전문 지식이나 스타일을 입히는 최적화 과정입니다.",
    "hint": "Domain Adaptation",
    "difficulty": "easy",
    "id": "6001"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "Fine Tuning 기초 및 필요성 2",
    "options": [
      "Fine Tuning은 모델의 파라미터를 전혀 수정하지 않는다.",
      "Pre-training과 Fine-Tuning은 완전히 같은 과정이다.",
      "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
      "Fine Tuning을 하면 항상 모델이 똑똑해진다(Catastrophic Forgetting 없음).",
      "Fine Tuning은 데이터가 없어도 수행할 수 있다."
    ],
    "answer": "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
    "why": "사전 학습된 일반적인 지식 위에 특정 도메인의 전문 지식이나 스타일을 입히는 최적화 과정입니다.",
    "hint": "Domain Adaptation",
    "difficulty": "easy",
    "id": "6002"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "Fine Tuning 기초 및 필요성 3",
    "options": [
      "Fine Tuning은 모델의 파라미터를 전혀 수정하지 않는다.",
      "Pre-training과 Fine-Tuning은 완전히 같은 과정이다.",
      "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
      "Fine Tuning을 하면 항상 모델이 똑똑해진다(Catastrophic Forgetting 없음).",
      "Fine Tuning은 데이터가 없어도 수행할 수 있다."
    ],
    "answer": "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
    "why": "사전 학습된 일반적인 지식 위에 특정 도메인의 전문 지식이나 스타일을 입히는 최적화 과정입니다.",
    "hint": "Domain Adaptation",
    "difficulty": "easy",
    "id": "6003"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "Fine Tuning 기초 및 필요성 4",
    "options": [
      "Fine Tuning은 모델의 파라미터를 전혀 수정하지 않는다.",
      "Pre-training과 Fine-Tuning은 완전히 같은 과정이다.",
      "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
      "Fine Tuning을 하면 항상 모델이 똑똑해진다(Catastrophic Forgetting 없음).",
      "Fine Tuning은 데이터가 없어도 수행할 수 있다."
    ],
    "answer": "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
    "why": "사전 학습된 일반적인 지식 위에 특정 도메인의 전문 지식이나 스타일을 입히는 최적화 과정입니다.",
    "hint": "Domain Adaptation",
    "difficulty": "easy",
    "id": "6004"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "Fine Tuning 기초 및 필요성 5",
    "options": [
      "Fine Tuning은 모델의 파라미터를 전혀 수정하지 않는다.",
      "Pre-training과 Fine-Tuning은 완전히 같은 과정이다.",
      "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
      "Fine Tuning을 하면 항상 모델이 똑똑해진다(Catastrophic Forgetting 없음).",
      "Fine Tuning은 데이터가 없어도 수행할 수 있다."
    ],
    "answer": "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
    "why": "사전 학습된 일반적인 지식 위에 특정 도메인의 전문 지식이나 스타일을 입히는 최적화 과정입니다.",
    "hint": "Domain Adaptation",
    "difficulty": "easy",
    "id": "6005"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "Fine Tuning 기초 및 필요성 6",
    "options": [
      "Fine Tuning은 모델의 파라미터를 전혀 수정하지 않는다.",
      "Pre-training과 Fine-Tuning은 완전히 같은 과정이다.",
      "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
      "Fine Tuning을 하면 항상 모델이 똑똑해진다(Catastrophic Forgetting 없음).",
      "Fine Tuning은 데이터가 없어도 수행할 수 있다."
    ],
    "answer": "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
    "why": "사전 학습된 일반적인 지식 위에 특정 도메인의 전문 지식이나 스타일을 입히는 최적화 과정입니다.",
    "hint": "Domain Adaptation",
    "difficulty": "easy",
    "id": "6006"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "Fine Tuning 기초 및 필요성 7",
    "options": [
      "Fine Tuning은 모델의 파라미터를 전혀 수정하지 않는다.",
      "Pre-training과 Fine-Tuning은 완전히 같은 과정이다.",
      "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
      "Fine Tuning을 하면 항상 모델이 똑똑해진다(Catastrophic Forgetting 없음).",
      "Fine Tuning은 데이터가 없어도 수행할 수 있다."
    ],
    "answer": "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
    "why": "사전 학습된 일반적인 지식 위에 특정 도메인의 전문 지식이나 스타일을 입히는 최적화 과정입니다.",
    "hint": "Domain Adaptation",
    "difficulty": "easy",
    "id": "6007"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "Fine Tuning 기초 및 필요성 8",
    "options": [
      "Fine Tuning은 모델의 파라미터를 전혀 수정하지 않는다.",
      "Pre-training과 Fine-Tuning은 완전히 같은 과정이다.",
      "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
      "Fine Tuning을 하면 항상 모델이 똑똑해진다(Catastrophic Forgetting 없음).",
      "Fine Tuning은 데이터가 없어도 수행할 수 있다."
    ],
    "answer": "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
    "why": "사전 학습된 일반적인 지식 위에 특정 도메인의 전문 지식이나 스타일을 입히는 최적화 과정입니다.",
    "hint": "Domain Adaptation",
    "difficulty": "easy",
    "id": "6008"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "Fine Tuning 기초 및 필요성 9",
    "options": [
      "Fine Tuning은 모델의 파라미터를 전혀 수정하지 않는다.",
      "Pre-training과 Fine-Tuning은 완전히 같은 과정이다.",
      "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
      "Fine Tuning을 하면 항상 모델이 똑똑해진다(Catastrophic Forgetting 없음).",
      "Fine Tuning은 데이터가 없어도 수행할 수 있다."
    ],
    "answer": "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
    "why": "사전 학습된 일반적인 지식 위에 특정 도메인의 전문 지식이나 스타일을 입히는 최적화 과정입니다.",
    "hint": "Domain Adaptation",
    "difficulty": "easy",
    "id": "6009"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "Fine Tuning 기초 및 필요성 10",
    "options": [
      "Fine Tuning은 모델의 파라미터를 전혀 수정하지 않는다.",
      "Pre-training과 Fine-Tuning은 완전히 같은 과정이다.",
      "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
      "Fine Tuning을 하면 항상 모델이 똑똑해진다(Catastrophic Forgetting 없음).",
      "Fine Tuning은 데이터가 없어도 수행할 수 있다."
    ],
    "answer": "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
    "why": "사전 학습된 일반적인 지식 위에 특정 도메인의 전문 지식이나 스타일을 입히는 최적화 과정입니다.",
    "hint": "Domain Adaptation",
    "difficulty": "easy",
    "id": "6010"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "Fine Tuning 기초 및 필요성 11",
    "options": [
      "Fine Tuning은 모델의 파라미터를 전혀 수정하지 않는다.",
      "Pre-training과 Fine-Tuning은 완전히 같은 과정이다.",
      "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
      "Fine Tuning을 하면 항상 모델이 똑똑해진다(Catastrophic Forgetting 없음).",
      "Fine Tuning은 데이터가 없어도 수행할 수 있다."
    ],
    "answer": "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
    "why": "사전 학습된 일반적인 지식 위에 특정 도메인의 전문 지식이나 스타일을 입히는 최적화 과정입니다.",
    "hint": "Domain Adaptation",
    "difficulty": "easy",
    "id": "6011"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "Fine Tuning 기초 및 필요성 12",
    "options": [
      "Fine Tuning은 모델의 파라미터를 전혀 수정하지 않는다.",
      "Pre-training과 Fine-Tuning은 완전히 같은 과정이다.",
      "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
      "Fine Tuning을 하면 항상 모델이 똑똑해진다(Catastrophic Forgetting 없음).",
      "Fine Tuning은 데이터가 없어도 수행할 수 있다."
    ],
    "answer": "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
    "why": "사전 학습된 일반적인 지식 위에 특정 도메인의 전문 지식이나 스타일을 입히는 최적화 과정입니다.",
    "hint": "Domain Adaptation",
    "difficulty": "easy",
    "id": "6012"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "Fine Tuning 기초 및 필요성 13",
    "options": [
      "Fine Tuning은 모델의 파라미터를 전혀 수정하지 않는다.",
      "Pre-training과 Fine-Tuning은 완전히 같은 과정이다.",
      "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
      "Fine Tuning을 하면 항상 모델이 똑똑해진다(Catastrophic Forgetting 없음).",
      "Fine Tuning은 데이터가 없어도 수행할 수 있다."
    ],
    "answer": "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
    "why": "사전 학습된 일반적인 지식 위에 특정 도메인의 전문 지식이나 스타일을 입히는 최적화 과정입니다.",
    "hint": "Domain Adaptation",
    "difficulty": "easy",
    "id": "6013"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "Fine Tuning 기초 및 필요성 14",
    "options": [
      "Fine Tuning은 모델의 파라미터를 전혀 수정하지 않는다.",
      "Pre-training과 Fine-Tuning은 완전히 같은 과정이다.",
      "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
      "Fine Tuning을 하면 항상 모델이 똑똑해진다(Catastrophic Forgetting 없음).",
      "Fine Tuning은 데이터가 없어도 수행할 수 있다."
    ],
    "answer": "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
    "why": "사전 학습된 일반적인 지식 위에 특정 도메인의 전문 지식이나 스타일을 입히는 최적화 과정입니다.",
    "hint": "Domain Adaptation",
    "difficulty": "easy",
    "id": "6014"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "Fine Tuning 기초 및 필요성 15",
    "options": [
      "Fine Tuning은 모델의 파라미터를 전혀 수정하지 않는다.",
      "Pre-training과 Fine-Tuning은 완전히 같은 과정이다.",
      "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
      "Fine Tuning을 하면 항상 모델이 똑똑해진다(Catastrophic Forgetting 없음).",
      "Fine Tuning은 데이터가 없어도 수행할 수 있다."
    ],
    "answer": "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
    "why": "사전 학습된 일반적인 지식 위에 특정 도메인의 전문 지식이나 스타일을 입히는 최적화 과정입니다.",
    "hint": "Domain Adaptation",
    "difficulty": "easy",
    "id": "6015"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "Fine Tuning 기초 및 필요성 16",
    "options": [
      "Fine Tuning은 모델의 파라미터를 전혀 수정하지 않는다.",
      "Pre-training과 Fine-Tuning은 완전히 같은 과정이다.",
      "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
      "Fine Tuning을 하면 항상 모델이 똑똑해진다(Catastrophic Forgetting 없음).",
      "Fine Tuning은 데이터가 없어도 수행할 수 있다."
    ],
    "answer": "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
    "why": "사전 학습된 일반적인 지식 위에 특정 도메인의 전문 지식이나 스타일을 입히는 최적화 과정입니다.",
    "hint": "Domain Adaptation",
    "difficulty": "easy",
    "id": "6016"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "Fine Tuning 기초 및 필요성 17",
    "options": [
      "Fine Tuning은 모델의 파라미터를 전혀 수정하지 않는다.",
      "Pre-training과 Fine-Tuning은 완전히 같은 과정이다.",
      "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
      "Fine Tuning을 하면 항상 모델이 똑똑해진다(Catastrophic Forgetting 없음).",
      "Fine Tuning은 데이터가 없어도 수행할 수 있다."
    ],
    "answer": "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
    "why": "사전 학습된 일반적인 지식 위에 특정 도메인의 전문 지식이나 스타일을 입히는 최적화 과정입니다.",
    "hint": "Domain Adaptation",
    "difficulty": "easy",
    "id": "6017"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "Fine Tuning 기초 및 필요성 18",
    "options": [
      "Fine Tuning은 모델의 파라미터를 전혀 수정하지 않는다.",
      "Pre-training과 Fine-Tuning은 완전히 같은 과정이다.",
      "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
      "Fine Tuning을 하면 항상 모델이 똑똑해진다(Catastrophic Forgetting 없음).",
      "Fine Tuning은 데이터가 없어도 수행할 수 있다."
    ],
    "answer": "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
    "why": "사전 학습된 일반적인 지식 위에 특정 도메인의 전문 지식이나 스타일을 입히는 최적화 과정입니다.",
    "hint": "Domain Adaptation",
    "difficulty": "easy",
    "id": "6018"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "Fine Tuning 기초 및 필요성 19",
    "options": [
      "Fine Tuning은 모델의 파라미터를 전혀 수정하지 않는다.",
      "Pre-training과 Fine-Tuning은 완전히 같은 과정이다.",
      "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
      "Fine Tuning을 하면 항상 모델이 똑똑해진다(Catastrophic Forgetting 없음).",
      "Fine Tuning은 데이터가 없어도 수행할 수 있다."
    ],
    "answer": "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
    "why": "사전 학습된 일반적인 지식 위에 특정 도메인의 전문 지식이나 스타일을 입히는 최적화 과정입니다.",
    "hint": "Domain Adaptation",
    "difficulty": "easy",
    "id": "6019"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "Fine Tuning 기초 및 필요성 20",
    "options": [
      "Fine Tuning은 모델의 파라미터를 전혀 수정하지 않는다.",
      "Pre-training과 Fine-Tuning은 완전히 같은 과정이다.",
      "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
      "Fine Tuning을 하면 항상 모델이 똑똑해진다(Catastrophic Forgetting 없음).",
      "Fine Tuning은 데이터가 없어도 수행할 수 있다."
    ],
    "answer": "Fine Tuning은 특정 도메인이나 작업에 모델을 적응시키는 과정이다.",
    "why": "사전 학습된 일반적인 지식 위에 특정 도메인의 전문 지식이나 스타일을 입히는 최적화 과정입니다.",
    "hint": "Domain Adaptation",
    "difficulty": "easy",
    "id": "6020"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "SFT(지도 미세 조정)의 특징 21",
    "options": [
      "SFT는 정답이 없는 데이터로 학습한다.",
      "Instruction Tuning은 SFT의 일종이다.",
      "SFT는 강화학습을 필수적으로 포함한다.",
      "SFT 데이터셋은 입력(Prompt)만 있고 출력(Response)은 없다.",
      "SFT는 모델 크기를 줄이는 기술이다."
    ],
    "answer": "Instruction Tuning은 SFT의 일종이다.",
    "why": "사용자의 지시(Instruction)에 따르는 능력을 향상시키기 위해 질문-답변 쌍으로 학습하는 것이 Instruction Tuning(SFT)입니다.",
    "hint": "Instruction Following",
    "difficulty": "medium",
    "id": "6021"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "SFT(지도 미세 조정)의 특징 22",
    "options": [
      "SFT는 정답이 없는 데이터로 학습한다.",
      "Instruction Tuning은 SFT의 일종이다.",
      "SFT는 강화학습을 필수적으로 포함한다.",
      "SFT 데이터셋은 입력(Prompt)만 있고 출력(Response)은 없다.",
      "SFT는 모델 크기를 줄이는 기술이다."
    ],
    "answer": "Instruction Tuning은 SFT의 일종이다.",
    "why": "사용자의 지시(Instruction)에 따르는 능력을 향상시키기 위해 질문-답변 쌍으로 학습하는 것이 Instruction Tuning(SFT)입니다.",
    "hint": "Instruction Following",
    "difficulty": "medium",
    "id": "6022"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "SFT(지도 미세 조정)의 특징 23",
    "options": [
      "SFT는 정답이 없는 데이터로 학습한다.",
      "Instruction Tuning은 SFT의 일종이다.",
      "SFT는 강화학습을 필수적으로 포함한다.",
      "SFT 데이터셋은 입력(Prompt)만 있고 출력(Response)은 없다.",
      "SFT는 모델 크기를 줄이는 기술이다."
    ],
    "answer": "Instruction Tuning은 SFT의 일종이다.",
    "why": "사용자의 지시(Instruction)에 따르는 능력을 향상시키기 위해 질문-답변 쌍으로 학습하는 것이 Instruction Tuning(SFT)입니다.",
    "hint": "Instruction Following",
    "difficulty": "medium",
    "id": "6023"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "SFT(지도 미세 조정)의 특징 24",
    "options": [
      "SFT는 정답이 없는 데이터로 학습한다.",
      "Instruction Tuning은 SFT의 일종이다.",
      "SFT는 강화학습을 필수적으로 포함한다.",
      "SFT 데이터셋은 입력(Prompt)만 있고 출력(Response)은 없다.",
      "SFT는 모델 크기를 줄이는 기술이다."
    ],
    "answer": "Instruction Tuning은 SFT의 일종이다.",
    "why": "사용자의 지시(Instruction)에 따르는 능력을 향상시키기 위해 질문-답변 쌍으로 학습하는 것이 Instruction Tuning(SFT)입니다.",
    "hint": "Instruction Following",
    "difficulty": "medium",
    "id": "6024"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "SFT(지도 미세 조정)의 특징 25",
    "options": [
      "SFT는 정답이 없는 데이터로 학습한다.",
      "Instruction Tuning은 SFT의 일종이다.",
      "SFT는 강화학습을 필수적으로 포함한다.",
      "SFT 데이터셋은 입력(Prompt)만 있고 출력(Response)은 없다.",
      "SFT는 모델 크기를 줄이는 기술이다."
    ],
    "answer": "Instruction Tuning은 SFT의 일종이다.",
    "why": "사용자의 지시(Instruction)에 따르는 능력을 향상시키기 위해 질문-답변 쌍으로 학습하는 것이 Instruction Tuning(SFT)입니다.",
    "hint": "Instruction Following",
    "difficulty": "medium",
    "id": "6025"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "SFT(지도 미세 조정)의 특징 26",
    "options": [
      "SFT는 정답이 없는 데이터로 학습한다.",
      "Instruction Tuning은 SFT의 일종이다.",
      "SFT는 강화학습을 필수적으로 포함한다.",
      "SFT 데이터셋은 입력(Prompt)만 있고 출력(Response)은 없다.",
      "SFT는 모델 크기를 줄이는 기술이다."
    ],
    "answer": "Instruction Tuning은 SFT의 일종이다.",
    "why": "사용자의 지시(Instruction)에 따르는 능력을 향상시키기 위해 질문-답변 쌍으로 학습하는 것이 Instruction Tuning(SFT)입니다.",
    "hint": "Instruction Following",
    "difficulty": "medium",
    "id": "6026"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "SFT(지도 미세 조정)의 특징 27",
    "options": [
      "SFT는 정답이 없는 데이터로 학습한다.",
      "Instruction Tuning은 SFT의 일종이다.",
      "SFT는 강화학습을 필수적으로 포함한다.",
      "SFT 데이터셋은 입력(Prompt)만 있고 출력(Response)은 없다.",
      "SFT는 모델 크기를 줄이는 기술이다."
    ],
    "answer": "Instruction Tuning은 SFT의 일종이다.",
    "why": "사용자의 지시(Instruction)에 따르는 능력을 향상시키기 위해 질문-답변 쌍으로 학습하는 것이 Instruction Tuning(SFT)입니다.",
    "hint": "Instruction Following",
    "difficulty": "medium",
    "id": "6027"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "SFT(지도 미세 조정)의 특징 28",
    "options": [
      "SFT는 정답이 없는 데이터로 학습한다.",
      "Instruction Tuning은 SFT의 일종이다.",
      "SFT는 강화학습을 필수적으로 포함한다.",
      "SFT 데이터셋은 입력(Prompt)만 있고 출력(Response)은 없다.",
      "SFT는 모델 크기를 줄이는 기술이다."
    ],
    "answer": "Instruction Tuning은 SFT의 일종이다.",
    "why": "사용자의 지시(Instruction)에 따르는 능력을 향상시키기 위해 질문-답변 쌍으로 학습하는 것이 Instruction Tuning(SFT)입니다.",
    "hint": "Instruction Following",
    "difficulty": "medium",
    "id": "6028"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "SFT(지도 미세 조정)의 특징 29",
    "options": [
      "SFT는 정답이 없는 데이터로 학습한다.",
      "Instruction Tuning은 SFT의 일종이다.",
      "SFT는 강화학습을 필수적으로 포함한다.",
      "SFT 데이터셋은 입력(Prompt)만 있고 출력(Response)은 없다.",
      "SFT는 모델 크기를 줄이는 기술이다."
    ],
    "answer": "Instruction Tuning은 SFT의 일종이다.",
    "why": "사용자의 지시(Instruction)에 따르는 능력을 향상시키기 위해 질문-답변 쌍으로 학습하는 것이 Instruction Tuning(SFT)입니다.",
    "hint": "Instruction Following",
    "difficulty": "medium",
    "id": "6029"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "SFT(지도 미세 조정)의 특징 30",
    "options": [
      "SFT는 정답이 없는 데이터로 학습한다.",
      "Instruction Tuning은 SFT의 일종이다.",
      "SFT는 강화학습을 필수적으로 포함한다.",
      "SFT 데이터셋은 입력(Prompt)만 있고 출력(Response)은 없다.",
      "SFT는 모델 크기를 줄이는 기술이다."
    ],
    "answer": "Instruction Tuning은 SFT의 일종이다.",
    "why": "사용자의 지시(Instruction)에 따르는 능력을 향상시키기 위해 질문-답변 쌍으로 학습하는 것이 Instruction Tuning(SFT)입니다.",
    "hint": "Instruction Following",
    "difficulty": "medium",
    "id": "6030"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "SFT(지도 미세 조정)의 특징 31",
    "options": [
      "SFT는 정답이 없는 데이터로 학습한다.",
      "Instruction Tuning은 SFT의 일종이다.",
      "SFT는 강화학습을 필수적으로 포함한다.",
      "SFT 데이터셋은 입력(Prompt)만 있고 출력(Response)은 없다.",
      "SFT는 모델 크기를 줄이는 기술이다."
    ],
    "answer": "Instruction Tuning은 SFT의 일종이다.",
    "why": "사용자의 지시(Instruction)에 따르는 능력을 향상시키기 위해 질문-답변 쌍으로 학습하는 것이 Instruction Tuning(SFT)입니다.",
    "hint": "Instruction Following",
    "difficulty": "medium",
    "id": "6031"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "SFT(지도 미세 조정)의 특징 32",
    "options": [
      "SFT는 정답이 없는 데이터로 학습한다.",
      "Instruction Tuning은 SFT의 일종이다.",
      "SFT는 강화학습을 필수적으로 포함한다.",
      "SFT 데이터셋은 입력(Prompt)만 있고 출력(Response)은 없다.",
      "SFT는 모델 크기를 줄이는 기술이다."
    ],
    "answer": "Instruction Tuning은 SFT의 일종이다.",
    "why": "사용자의 지시(Instruction)에 따르는 능력을 향상시키기 위해 질문-답변 쌍으로 학습하는 것이 Instruction Tuning(SFT)입니다.",
    "hint": "Instruction Following",
    "difficulty": "medium",
    "id": "6032"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "SFT(지도 미세 조정)의 특징 33",
    "options": [
      "SFT는 정답이 없는 데이터로 학습한다.",
      "Instruction Tuning은 SFT의 일종이다.",
      "SFT는 강화학습을 필수적으로 포함한다.",
      "SFT 데이터셋은 입력(Prompt)만 있고 출력(Response)은 없다.",
      "SFT는 모델 크기를 줄이는 기술이다."
    ],
    "answer": "Instruction Tuning은 SFT의 일종이다.",
    "why": "사용자의 지시(Instruction)에 따르는 능력을 향상시키기 위해 질문-답변 쌍으로 학습하는 것이 Instruction Tuning(SFT)입니다.",
    "hint": "Instruction Following",
    "difficulty": "medium",
    "id": "6033"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "SFT(지도 미세 조정)의 특징 34",
    "options": [
      "SFT는 정답이 없는 데이터로 학습한다.",
      "Instruction Tuning은 SFT의 일종이다.",
      "SFT는 강화학습을 필수적으로 포함한다.",
      "SFT 데이터셋은 입력(Prompt)만 있고 출력(Response)은 없다.",
      "SFT는 모델 크기를 줄이는 기술이다."
    ],
    "answer": "Instruction Tuning은 SFT의 일종이다.",
    "why": "사용자의 지시(Instruction)에 따르는 능력을 향상시키기 위해 질문-답변 쌍으로 학습하는 것이 Instruction Tuning(SFT)입니다.",
    "hint": "Instruction Following",
    "difficulty": "medium",
    "id": "6034"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "SFT(지도 미세 조정)의 특징 35",
    "options": [
      "SFT는 정답이 없는 데이터로 학습한다.",
      "Instruction Tuning은 SFT의 일종이다.",
      "SFT는 강화학습을 필수적으로 포함한다.",
      "SFT 데이터셋은 입력(Prompt)만 있고 출력(Response)은 없다.",
      "SFT는 모델 크기를 줄이는 기술이다."
    ],
    "answer": "Instruction Tuning은 SFT의 일종이다.",
    "why": "사용자의 지시(Instruction)에 따르는 능력을 향상시키기 위해 질문-답변 쌍으로 학습하는 것이 Instruction Tuning(SFT)입니다.",
    "hint": "Instruction Following",
    "difficulty": "medium",
    "id": "6035"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "SFT(지도 미세 조정)의 특징 36",
    "options": [
      "SFT는 정답이 없는 데이터로 학습한다.",
      "Instruction Tuning은 SFT의 일종이다.",
      "SFT는 강화학습을 필수적으로 포함한다.",
      "SFT 데이터셋은 입력(Prompt)만 있고 출력(Response)은 없다.",
      "SFT는 모델 크기를 줄이는 기술이다."
    ],
    "answer": "Instruction Tuning은 SFT의 일종이다.",
    "why": "사용자의 지시(Instruction)에 따르는 능력을 향상시키기 위해 질문-답변 쌍으로 학습하는 것이 Instruction Tuning(SFT)입니다.",
    "hint": "Instruction Following",
    "difficulty": "medium",
    "id": "6036"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "SFT(지도 미세 조정)의 특징 37",
    "options": [
      "SFT는 정답이 없는 데이터로 학습한다.",
      "Instruction Tuning은 SFT의 일종이다.",
      "SFT는 강화학습을 필수적으로 포함한다.",
      "SFT 데이터셋은 입력(Prompt)만 있고 출력(Response)은 없다.",
      "SFT는 모델 크기를 줄이는 기술이다."
    ],
    "answer": "Instruction Tuning은 SFT의 일종이다.",
    "why": "사용자의 지시(Instruction)에 따르는 능력을 향상시키기 위해 질문-답변 쌍으로 학습하는 것이 Instruction Tuning(SFT)입니다.",
    "hint": "Instruction Following",
    "difficulty": "medium",
    "id": "6037"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "SFT(지도 미세 조정)의 특징 38",
    "options": [
      "SFT는 정답이 없는 데이터로 학습한다.",
      "Instruction Tuning은 SFT의 일종이다.",
      "SFT는 강화학습을 필수적으로 포함한다.",
      "SFT 데이터셋은 입력(Prompt)만 있고 출력(Response)은 없다.",
      "SFT는 모델 크기를 줄이는 기술이다."
    ],
    "answer": "Instruction Tuning은 SFT의 일종이다.",
    "why": "사용자의 지시(Instruction)에 따르는 능력을 향상시키기 위해 질문-답변 쌍으로 학습하는 것이 Instruction Tuning(SFT)입니다.",
    "hint": "Instruction Following",
    "difficulty": "medium",
    "id": "6038"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "SFT(지도 미세 조정)의 특징 39",
    "options": [
      "SFT는 정답이 없는 데이터로 학습한다.",
      "Instruction Tuning은 SFT의 일종이다.",
      "SFT는 강화학습을 필수적으로 포함한다.",
      "SFT 데이터셋은 입력(Prompt)만 있고 출력(Response)은 없다.",
      "SFT는 모델 크기를 줄이는 기술이다."
    ],
    "answer": "Instruction Tuning은 SFT의 일종이다.",
    "why": "사용자의 지시(Instruction)에 따르는 능력을 향상시키기 위해 질문-답변 쌍으로 학습하는 것이 Instruction Tuning(SFT)입니다.",
    "hint": "Instruction Following",
    "difficulty": "medium",
    "id": "6039"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "SFT(지도 미세 조정)의 특징 40",
    "options": [
      "SFT는 정답이 없는 데이터로 학습한다.",
      "Instruction Tuning은 SFT의 일종이다.",
      "SFT는 강화학습을 필수적으로 포함한다.",
      "SFT 데이터셋은 입력(Prompt)만 있고 출력(Response)은 없다.",
      "SFT는 모델 크기를 줄이는 기술이다."
    ],
    "answer": "Instruction Tuning은 SFT의 일종이다.",
    "why": "사용자의 지시(Instruction)에 따르는 능력을 향상시키기 위해 질문-답변 쌍으로 학습하는 것이 Instruction Tuning(SFT)입니다.",
    "hint": "Instruction Following",
    "difficulty": "medium",
    "id": "6040"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "효율적 파인튜닝(PEFT)과 LoRA 41",
    "options": [
      "PEFT는 전체 파라미터를 모두 업데이트한다.",
      "LoRA는 모델의 크기를 물리적으로 줄이는 압축 기술이다.",
      "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
      "LoRA를 사용하면 추론 속도가 느려진다.",
      "PEFT는 GPU 메모리를 더 많이 사용한다."
    ],
    "answer": "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
    "why": "LoRA는 전체 가중치를 고정하고, 변화량(Delta)을 나타내는 작은 행렬 두 개만 학습하여 메모리 사용량을 획기적으로 줄입니다.",
    "hint": "Low-Rank Adaptation",
    "difficulty": "medium",
    "id": "6041"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "효율적 파인튜닝(PEFT)과 LoRA 42",
    "options": [
      "PEFT는 전체 파라미터를 모두 업데이트한다.",
      "LoRA는 모델의 크기를 물리적으로 줄이는 압축 기술이다.",
      "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
      "LoRA를 사용하면 추론 속도가 느려진다.",
      "PEFT는 GPU 메모리를 더 많이 사용한다."
    ],
    "answer": "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
    "why": "LoRA는 전체 가중치를 고정하고, 변화량(Delta)을 나타내는 작은 행렬 두 개만 학습하여 메모리 사용량을 획기적으로 줄입니다.",
    "hint": "Low-Rank Adaptation",
    "difficulty": "medium",
    "id": "6042"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "효율적 파인튜닝(PEFT)과 LoRA 43",
    "options": [
      "PEFT는 전체 파라미터를 모두 업데이트한다.",
      "LoRA는 모델의 크기를 물리적으로 줄이는 압축 기술이다.",
      "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
      "LoRA를 사용하면 추론 속도가 느려진다.",
      "PEFT는 GPU 메모리를 더 많이 사용한다."
    ],
    "answer": "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
    "why": "LoRA는 전체 가중치를 고정하고, 변화량(Delta)을 나타내는 작은 행렬 두 개만 학습하여 메모리 사용량을 획기적으로 줄입니다.",
    "hint": "Low-Rank Adaptation",
    "difficulty": "medium",
    "id": "6043"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "효율적 파인튜닝(PEFT)과 LoRA 44",
    "options": [
      "PEFT는 전체 파라미터를 모두 업데이트한다.",
      "LoRA는 모델의 크기를 물리적으로 줄이는 압축 기술이다.",
      "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
      "LoRA를 사용하면 추론 속도가 느려진다.",
      "PEFT는 GPU 메모리를 더 많이 사용한다."
    ],
    "answer": "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
    "why": "LoRA는 전체 가중치를 고정하고, 변화량(Delta)을 나타내는 작은 행렬 두 개만 학습하여 메모리 사용량을 획기적으로 줄입니다.",
    "hint": "Low-Rank Adaptation",
    "difficulty": "medium",
    "id": "6044"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "효율적 파인튜닝(PEFT)과 LoRA 45",
    "options": [
      "PEFT는 전체 파라미터를 모두 업데이트한다.",
      "LoRA는 모델의 크기를 물리적으로 줄이는 압축 기술이다.",
      "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
      "LoRA를 사용하면 추론 속도가 느려진다.",
      "PEFT는 GPU 메모리를 더 많이 사용한다."
    ],
    "answer": "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
    "why": "LoRA는 전체 가중치를 고정하고, 변화량(Delta)을 나타내는 작은 행렬 두 개만 학습하여 메모리 사용량을 획기적으로 줄입니다.",
    "hint": "Low-Rank Adaptation",
    "difficulty": "medium",
    "id": "6045"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "효율적 파인튜닝(PEFT)과 LoRA 46",
    "options": [
      "PEFT는 전체 파라미터를 모두 업데이트한다.",
      "LoRA는 모델의 크기를 물리적으로 줄이는 압축 기술이다.",
      "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
      "LoRA를 사용하면 추론 속도가 느려진다.",
      "PEFT는 GPU 메모리를 더 많이 사용한다."
    ],
    "answer": "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
    "why": "LoRA는 전체 가중치를 고정하고, 변화량(Delta)을 나타내는 작은 행렬 두 개만 학습하여 메모리 사용량을 획기적으로 줄입니다.",
    "hint": "Low-Rank Adaptation",
    "difficulty": "medium",
    "id": "6046"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "효율적 파인튜닝(PEFT)과 LoRA 47",
    "options": [
      "PEFT는 전체 파라미터를 모두 업데이트한다.",
      "LoRA는 모델의 크기를 물리적으로 줄이는 압축 기술이다.",
      "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
      "LoRA를 사용하면 추론 속도가 느려진다.",
      "PEFT는 GPU 메모리를 더 많이 사용한다."
    ],
    "answer": "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
    "why": "LoRA는 전체 가중치를 고정하고, 변화량(Delta)을 나타내는 작은 행렬 두 개만 학습하여 메모리 사용량을 획기적으로 줄입니다.",
    "hint": "Low-Rank Adaptation",
    "difficulty": "medium",
    "id": "6047"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "효율적 파인튜닝(PEFT)과 LoRA 48",
    "options": [
      "PEFT는 전체 파라미터를 모두 업데이트한다.",
      "LoRA는 모델의 크기를 물리적으로 줄이는 압축 기술이다.",
      "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
      "LoRA를 사용하면 추론 속도가 느려진다.",
      "PEFT는 GPU 메모리를 더 많이 사용한다."
    ],
    "answer": "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
    "why": "LoRA는 전체 가중치를 고정하고, 변화량(Delta)을 나타내는 작은 행렬 두 개만 학습하여 메모리 사용량을 획기적으로 줄입니다.",
    "hint": "Low-Rank Adaptation",
    "difficulty": "medium",
    "id": "6048"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "효율적 파인튜닝(PEFT)과 LoRA 49",
    "options": [
      "PEFT는 전체 파라미터를 모두 업데이트한다.",
      "LoRA는 모델의 크기를 물리적으로 줄이는 압축 기술이다.",
      "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
      "LoRA를 사용하면 추론 속도가 느려진다.",
      "PEFT는 GPU 메모리를 더 많이 사용한다."
    ],
    "answer": "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
    "why": "LoRA는 전체 가중치를 고정하고, 변화량(Delta)을 나타내는 작은 행렬 두 개만 학습하여 메모리 사용량을 획기적으로 줄입니다.",
    "hint": "Low-Rank Adaptation",
    "difficulty": "medium",
    "id": "6049"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "효율적 파인튜닝(PEFT)과 LoRA 50",
    "options": [
      "PEFT는 전체 파라미터를 모두 업데이트한다.",
      "LoRA는 모델의 크기를 물리적으로 줄이는 압축 기술이다.",
      "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
      "LoRA를 사용하면 추론 속도가 느려진다.",
      "PEFT는 GPU 메모리를 더 많이 사용한다."
    ],
    "answer": "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
    "why": "LoRA는 전체 가중치를 고정하고, 변화량(Delta)을 나타내는 작은 행렬 두 개만 학습하여 메모리 사용량을 획기적으로 줄입니다.",
    "hint": "Low-Rank Adaptation",
    "difficulty": "medium",
    "id": "6050"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "효율적 파인튜닝(PEFT)과 LoRA 51",
    "options": [
      "PEFT는 전체 파라미터를 모두 업데이트한다.",
      "LoRA는 모델의 크기를 물리적으로 줄이는 압축 기술이다.",
      "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
      "LoRA를 사용하면 추론 속도가 느려진다.",
      "PEFT는 GPU 메모리를 더 많이 사용한다."
    ],
    "answer": "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
    "why": "LoRA는 전체 가중치를 고정하고, 변화량(Delta)을 나타내는 작은 행렬 두 개만 학습하여 메모리 사용량을 획기적으로 줄입니다.",
    "hint": "Low-Rank Adaptation",
    "difficulty": "medium",
    "id": "6051"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "효율적 파인튜닝(PEFT)과 LoRA 52",
    "options": [
      "PEFT는 전체 파라미터를 모두 업데이트한다.",
      "LoRA는 모델의 크기를 물리적으로 줄이는 압축 기술이다.",
      "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
      "LoRA를 사용하면 추론 속도가 느려진다.",
      "PEFT는 GPU 메모리를 더 많이 사용한다."
    ],
    "answer": "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
    "why": "LoRA는 전체 가중치를 고정하고, 변화량(Delta)을 나타내는 작은 행렬 두 개만 학습하여 메모리 사용량을 획기적으로 줄입니다.",
    "hint": "Low-Rank Adaptation",
    "difficulty": "medium",
    "id": "6052"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "효율적 파인튜닝(PEFT)과 LoRA 53",
    "options": [
      "PEFT는 전체 파라미터를 모두 업데이트한다.",
      "LoRA는 모델의 크기를 물리적으로 줄이는 압축 기술이다.",
      "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
      "LoRA를 사용하면 추론 속도가 느려진다.",
      "PEFT는 GPU 메모리를 더 많이 사용한다."
    ],
    "answer": "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
    "why": "LoRA는 전체 가중치를 고정하고, 변화량(Delta)을 나타내는 작은 행렬 두 개만 학습하여 메모리 사용량을 획기적으로 줄입니다.",
    "hint": "Low-Rank Adaptation",
    "difficulty": "medium",
    "id": "6053"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "효율적 파인튜닝(PEFT)과 LoRA 54",
    "options": [
      "PEFT는 전체 파라미터를 모두 업데이트한다.",
      "LoRA는 모델의 크기를 물리적으로 줄이는 압축 기술이다.",
      "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
      "LoRA를 사용하면 추론 속도가 느려진다.",
      "PEFT는 GPU 메모리를 더 많이 사용한다."
    ],
    "answer": "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
    "why": "LoRA는 전체 가중치를 고정하고, 변화량(Delta)을 나타내는 작은 행렬 두 개만 학습하여 메모리 사용량을 획기적으로 줄입니다.",
    "hint": "Low-Rank Adaptation",
    "difficulty": "medium",
    "id": "6054"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "효율적 파인튜닝(PEFT)과 LoRA 55",
    "options": [
      "PEFT는 전체 파라미터를 모두 업데이트한다.",
      "LoRA는 모델의 크기를 물리적으로 줄이는 압축 기술이다.",
      "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
      "LoRA를 사용하면 추론 속도가 느려진다.",
      "PEFT는 GPU 메모리를 더 많이 사용한다."
    ],
    "answer": "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
    "why": "LoRA는 전체 가중치를 고정하고, 변화량(Delta)을 나타내는 작은 행렬 두 개만 학습하여 메모리 사용량을 획기적으로 줄입니다.",
    "hint": "Low-Rank Adaptation",
    "difficulty": "medium",
    "id": "6055"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "효율적 파인튜닝(PEFT)과 LoRA 56",
    "options": [
      "PEFT는 전체 파라미터를 모두 업데이트한다.",
      "LoRA는 모델의 크기를 물리적으로 줄이는 압축 기술이다.",
      "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
      "LoRA를 사용하면 추론 속도가 느려진다.",
      "PEFT는 GPU 메모리를 더 많이 사용한다."
    ],
    "answer": "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
    "why": "LoRA는 전체 가중치를 고정하고, 변화량(Delta)을 나타내는 작은 행렬 두 개만 학습하여 메모리 사용량을 획기적으로 줄입니다.",
    "hint": "Low-Rank Adaptation",
    "difficulty": "medium",
    "id": "6056"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "효율적 파인튜닝(PEFT)과 LoRA 57",
    "options": [
      "PEFT는 전체 파라미터를 모두 업데이트한다.",
      "LoRA는 모델의 크기를 물리적으로 줄이는 압축 기술이다.",
      "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
      "LoRA를 사용하면 추론 속도가 느려진다.",
      "PEFT는 GPU 메모리를 더 많이 사용한다."
    ],
    "answer": "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
    "why": "LoRA는 전체 가중치를 고정하고, 변화량(Delta)을 나타내는 작은 행렬 두 개만 학습하여 메모리 사용량을 획기적으로 줄입니다.",
    "hint": "Low-Rank Adaptation",
    "difficulty": "medium",
    "id": "6057"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "효율적 파인튜닝(PEFT)과 LoRA 58",
    "options": [
      "PEFT는 전체 파라미터를 모두 업데이트한다.",
      "LoRA는 모델의 크기를 물리적으로 줄이는 압축 기술이다.",
      "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
      "LoRA를 사용하면 추론 속도가 느려진다.",
      "PEFT는 GPU 메모리를 더 많이 사용한다."
    ],
    "answer": "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
    "why": "LoRA는 전체 가중치를 고정하고, 변화량(Delta)을 나타내는 작은 행렬 두 개만 학습하여 메모리 사용량을 획기적으로 줄입니다.",
    "hint": "Low-Rank Adaptation",
    "difficulty": "medium",
    "id": "6058"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "효율적 파인튜닝(PEFT)과 LoRA 59",
    "options": [
      "PEFT는 전체 파라미터를 모두 업데이트한다.",
      "LoRA는 모델의 크기를 물리적으로 줄이는 압축 기술이다.",
      "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
      "LoRA를 사용하면 추론 속도가 느려진다.",
      "PEFT는 GPU 메모리를 더 많이 사용한다."
    ],
    "answer": "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
    "why": "LoRA는 전체 가중치를 고정하고, 변화량(Delta)을 나타내는 작은 행렬 두 개만 학습하여 메모리 사용량을 획기적으로 줄입니다.",
    "hint": "Low-Rank Adaptation",
    "difficulty": "medium",
    "id": "6059"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "효율적 파인튜닝(PEFT)과 LoRA 60",
    "options": [
      "PEFT는 전체 파라미터를 모두 업데이트한다.",
      "LoRA는 모델의 크기를 물리적으로 줄이는 압축 기술이다.",
      "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
      "LoRA를 사용하면 추론 속도가 느려진다.",
      "PEFT는 GPU 메모리를 더 많이 사용한다."
    ],
    "answer": "LoRA는 추가적인 저랭크 행렬(Low-Rank Matrix)만 학습한다.",
    "why": "LoRA는 전체 가중치를 고정하고, 변화량(Delta)을 나타내는 작은 행렬 두 개만 학습하여 메모리 사용량을 획기적으로 줄입니다.",
    "hint": "Low-Rank Adaptation",
    "difficulty": "medium",
    "id": "6060"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "RLHF(인간 피드백 강화학습) 및 정렬 61",
    "options": [
      "RLHF는 인간이 직접 모든 답변을 작성해야 한다.",
      "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
      "PPO 알고리즘은 사용되지 않는다.",
      "RLHF는 모델의 사실성을 높이는 데 주력한다.",
      "RLHF는 사전 학습 단계에서 수행된다."
    ],
    "answer": "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
    "why": "사람이 더 선호하는 답변에 높은 점수를 주도록 보상 모델을 학습시키고, 이를 통해 LLM을 강화학습합니다.",
    "hint": "Reward Model",
    "difficulty": "hard",
    "id": "6061"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "RLHF(인간 피드백 강화학습) 및 정렬 62",
    "options": [
      "RLHF는 인간이 직접 모든 답변을 작성해야 한다.",
      "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
      "PPO 알고리즘은 사용되지 않는다.",
      "RLHF는 모델의 사실성을 높이는 데 주력한다.",
      "RLHF는 사전 학습 단계에서 수행된다."
    ],
    "answer": "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
    "why": "사람이 더 선호하는 답변에 높은 점수를 주도록 보상 모델을 학습시키고, 이를 통해 LLM을 강화학습합니다.",
    "hint": "Reward Model",
    "difficulty": "hard",
    "id": "6062"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "RLHF(인간 피드백 강화학습) 및 정렬 63",
    "options": [
      "RLHF는 인간이 직접 모든 답변을 작성해야 한다.",
      "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
      "PPO 알고리즘은 사용되지 않는다.",
      "RLHF는 모델의 사실성을 높이는 데 주력한다.",
      "RLHF는 사전 학습 단계에서 수행된다."
    ],
    "answer": "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
    "why": "사람이 더 선호하는 답변에 높은 점수를 주도록 보상 모델을 학습시키고, 이를 통해 LLM을 강화학습합니다.",
    "hint": "Reward Model",
    "difficulty": "hard",
    "id": "6063"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "RLHF(인간 피드백 강화학습) 및 정렬 64",
    "options": [
      "RLHF는 인간이 직접 모든 답변을 작성해야 한다.",
      "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
      "PPO 알고리즘은 사용되지 않는다.",
      "RLHF는 모델의 사실성을 높이는 데 주력한다.",
      "RLHF는 사전 학습 단계에서 수행된다."
    ],
    "answer": "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
    "why": "사람이 더 선호하는 답변에 높은 점수를 주도록 보상 모델을 학습시키고, 이를 통해 LLM을 강화학습합니다.",
    "hint": "Reward Model",
    "difficulty": "hard",
    "id": "6064"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "RLHF(인간 피드백 강화학습) 및 정렬 65",
    "options": [
      "RLHF는 인간이 직접 모든 답변을 작성해야 한다.",
      "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
      "PPO 알고리즘은 사용되지 않는다.",
      "RLHF는 모델의 사실성을 높이는 데 주력한다.",
      "RLHF는 사전 학습 단계에서 수행된다."
    ],
    "answer": "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
    "why": "사람이 더 선호하는 답변에 높은 점수를 주도록 보상 모델을 학습시키고, 이를 통해 LLM을 강화학습합니다.",
    "hint": "Reward Model",
    "difficulty": "hard",
    "id": "6065"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "RLHF(인간 피드백 강화학습) 및 정렬 66",
    "options": [
      "RLHF는 인간이 직접 모든 답변을 작성해야 한다.",
      "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
      "PPO 알고리즘은 사용되지 않는다.",
      "RLHF는 모델의 사실성을 높이는 데 주력한다.",
      "RLHF는 사전 학습 단계에서 수행된다."
    ],
    "answer": "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
    "why": "사람이 더 선호하는 답변에 높은 점수를 주도록 보상 모델을 학습시키고, 이를 통해 LLM을 강화학습합니다.",
    "hint": "Reward Model",
    "difficulty": "hard",
    "id": "6066"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "RLHF(인간 피드백 강화학습) 및 정렬 67",
    "options": [
      "RLHF는 인간이 직접 모든 답변을 작성해야 한다.",
      "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
      "PPO 알고리즘은 사용되지 않는다.",
      "RLHF는 모델의 사실성을 높이는 데 주력한다.",
      "RLHF는 사전 학습 단계에서 수행된다."
    ],
    "answer": "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
    "why": "사람이 더 선호하는 답변에 높은 점수를 주도록 보상 모델을 학습시키고, 이를 통해 LLM을 강화학습합니다.",
    "hint": "Reward Model",
    "difficulty": "hard",
    "id": "6067"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "RLHF(인간 피드백 강화학습) 및 정렬 68",
    "options": [
      "RLHF는 인간이 직접 모든 답변을 작성해야 한다.",
      "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
      "PPO 알고리즘은 사용되지 않는다.",
      "RLHF는 모델의 사실성을 높이는 데 주력한다.",
      "RLHF는 사전 학습 단계에서 수행된다."
    ],
    "answer": "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
    "why": "사람이 더 선호하는 답변에 높은 점수를 주도록 보상 모델을 학습시키고, 이를 통해 LLM을 강화학습합니다.",
    "hint": "Reward Model",
    "difficulty": "hard",
    "id": "6068"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "RLHF(인간 피드백 강화학습) 및 정렬 69",
    "options": [
      "RLHF는 인간이 직접 모든 답변을 작성해야 한다.",
      "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
      "PPO 알고리즘은 사용되지 않는다.",
      "RLHF는 모델의 사실성을 높이는 데 주력한다.",
      "RLHF는 사전 학습 단계에서 수행된다."
    ],
    "answer": "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
    "why": "사람이 더 선호하는 답변에 높은 점수를 주도록 보상 모델을 학습시키고, 이를 통해 LLM을 강화학습합니다.",
    "hint": "Reward Model",
    "difficulty": "hard",
    "id": "6069"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "RLHF(인간 피드백 강화학습) 및 정렬 70",
    "options": [
      "RLHF는 인간이 직접 모든 답변을 작성해야 한다.",
      "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
      "PPO 알고리즘은 사용되지 않는다.",
      "RLHF는 모델의 사실성을 높이는 데 주력한다.",
      "RLHF는 사전 학습 단계에서 수행된다."
    ],
    "answer": "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
    "why": "사람이 더 선호하는 답변에 높은 점수를 주도록 보상 모델을 학습시키고, 이를 통해 LLM을 강화학습합니다.",
    "hint": "Reward Model",
    "difficulty": "hard",
    "id": "6070"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "RLHF(인간 피드백 강화학습) 및 정렬 71",
    "options": [
      "RLHF는 인간이 직접 모든 답변을 작성해야 한다.",
      "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
      "PPO 알고리즘은 사용되지 않는다.",
      "RLHF는 모델의 사실성을 높이는 데 주력한다.",
      "RLHF는 사전 학습 단계에서 수행된다."
    ],
    "answer": "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
    "why": "사람이 더 선호하는 답변에 높은 점수를 주도록 보상 모델을 학습시키고, 이를 통해 LLM을 강화학습합니다.",
    "hint": "Reward Model",
    "difficulty": "hard",
    "id": "6071"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "RLHF(인간 피드백 강화학습) 및 정렬 72",
    "options": [
      "RLHF는 인간이 직접 모든 답변을 작성해야 한다.",
      "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
      "PPO 알고리즘은 사용되지 않는다.",
      "RLHF는 모델의 사실성을 높이는 데 주력한다.",
      "RLHF는 사전 학습 단계에서 수행된다."
    ],
    "answer": "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
    "why": "사람이 더 선호하는 답변에 높은 점수를 주도록 보상 모델을 학습시키고, 이를 통해 LLM을 강화학습합니다.",
    "hint": "Reward Model",
    "difficulty": "hard",
    "id": "6072"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "RLHF(인간 피드백 강화학습) 및 정렬 73",
    "options": [
      "RLHF는 인간이 직접 모든 답변을 작성해야 한다.",
      "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
      "PPO 알고리즘은 사용되지 않는다.",
      "RLHF는 모델의 사실성을 높이는 데 주력한다.",
      "RLHF는 사전 학습 단계에서 수행된다."
    ],
    "answer": "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
    "why": "사람이 더 선호하는 답변에 높은 점수를 주도록 보상 모델을 학습시키고, 이를 통해 LLM을 강화학습합니다.",
    "hint": "Reward Model",
    "difficulty": "hard",
    "id": "6073"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "RLHF(인간 피드백 강화학습) 및 정렬 74",
    "options": [
      "RLHF는 인간이 직접 모든 답변을 작성해야 한다.",
      "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
      "PPO 알고리즘은 사용되지 않는다.",
      "RLHF는 모델의 사실성을 높이는 데 주력한다.",
      "RLHF는 사전 학습 단계에서 수행된다."
    ],
    "answer": "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
    "why": "사람이 더 선호하는 답변에 높은 점수를 주도록 보상 모델을 학습시키고, 이를 통해 LLM을 강화학습합니다.",
    "hint": "Reward Model",
    "difficulty": "hard",
    "id": "6074"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "RLHF(인간 피드백 강화학습) 및 정렬 75",
    "options": [
      "RLHF는 인간이 직접 모든 답변을 작성해야 한다.",
      "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
      "PPO 알고리즘은 사용되지 않는다.",
      "RLHF는 모델의 사실성을 높이는 데 주력한다.",
      "RLHF는 사전 학습 단계에서 수행된다."
    ],
    "answer": "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
    "why": "사람이 더 선호하는 답변에 높은 점수를 주도록 보상 모델을 학습시키고, 이를 통해 LLM을 강화학습합니다.",
    "hint": "Reward Model",
    "difficulty": "hard",
    "id": "6075"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "RLHF(인간 피드백 강화학습) 및 정렬 76",
    "options": [
      "RLHF는 인간이 직접 모든 답변을 작성해야 한다.",
      "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
      "PPO 알고리즘은 사용되지 않는다.",
      "RLHF는 모델의 사실성을 높이는 데 주력한다.",
      "RLHF는 사전 학습 단계에서 수행된다."
    ],
    "answer": "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
    "why": "사람이 더 선호하는 답변에 높은 점수를 주도록 보상 모델을 학습시키고, 이를 통해 LLM을 강화학습합니다.",
    "hint": "Reward Model",
    "difficulty": "hard",
    "id": "6076"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "RLHF(인간 피드백 강화학습) 및 정렬 77",
    "options": [
      "RLHF는 인간이 직접 모든 답변을 작성해야 한다.",
      "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
      "PPO 알고리즘은 사용되지 않는다.",
      "RLHF는 모델의 사실성을 높이는 데 주력한다.",
      "RLHF는 사전 학습 단계에서 수행된다."
    ],
    "answer": "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
    "why": "사람이 더 선호하는 답변에 높은 점수를 주도록 보상 모델을 학습시키고, 이를 통해 LLM을 강화학습합니다.",
    "hint": "Reward Model",
    "difficulty": "hard",
    "id": "6077"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "RLHF(인간 피드백 강화학습) 및 정렬 78",
    "options": [
      "RLHF는 인간이 직접 모든 답변을 작성해야 한다.",
      "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
      "PPO 알고리즘은 사용되지 않는다.",
      "RLHF는 모델의 사실성을 높이는 데 주력한다.",
      "RLHF는 사전 학습 단계에서 수행된다."
    ],
    "answer": "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
    "why": "사람이 더 선호하는 답변에 높은 점수를 주도록 보상 모델을 학습시키고, 이를 통해 LLM을 강화학습합니다.",
    "hint": "Reward Model",
    "difficulty": "hard",
    "id": "6078"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "RLHF(인간 피드백 강화학습) 및 정렬 79",
    "options": [
      "RLHF는 인간이 직접 모든 답변을 작성해야 한다.",
      "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
      "PPO 알고리즘은 사용되지 않는다.",
      "RLHF는 모델의 사실성을 높이는 데 주력한다.",
      "RLHF는 사전 학습 단계에서 수행된다."
    ],
    "answer": "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
    "why": "사람이 더 선호하는 답변에 높은 점수를 주도록 보상 모델을 학습시키고, 이를 통해 LLM을 강화학습합니다.",
    "hint": "Reward Model",
    "difficulty": "hard",
    "id": "6079"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "RLHF(인간 피드백 강화학습) 및 정렬 80",
    "options": [
      "RLHF는 인간이 직접 모든 답변을 작성해야 한다.",
      "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
      "PPO 알고리즘은 사용되지 않는다.",
      "RLHF는 모델의 사실성을 높이는 데 주력한다.",
      "RLHF는 사전 학습 단계에서 수행된다."
    ],
    "answer": "보상 모델(Reward Model)은 사람이 평가한 데이터를 학습한다.",
    "why": "사람이 더 선호하는 답변에 높은 점수를 주도록 보상 모델을 학습시키고, 이를 통해 LLM을 강화학습합니다.",
    "hint": "Reward Model",
    "difficulty": "hard",
    "id": "6080"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "심화 튜닝 전략 81",
    "options": [
      "Catastrophic Forgetting을 방지하려면 모든 데이터를 버려야 한다.",
      "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
      "QLoRA는 32비트 정밀도로 학습한다.",
      "Full Fine-Tuning이 항상 LoRA보다 성능이 뛰어나다.",
      "데이터 품질은 중요하지 않다."
    ],
    "answer": "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
    "why": "임베딩에 노이즈를 섞어 학습함으로써 모델이 과적합되지 않고 더 강건하게 동작하도록 돕는 기법입니다.",
    "hint": "Noise Embedding",
    "difficulty": "hard",
    "id": "6081"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "심화 튜닝 전략 82",
    "options": [
      "Catastrophic Forgetting을 방지하려면 모든 데이터를 버려야 한다.",
      "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
      "QLoRA는 32비트 정밀도로 학습한다.",
      "Full Fine-Tuning이 항상 LoRA보다 성능이 뛰어나다.",
      "데이터 품질은 중요하지 않다."
    ],
    "answer": "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
    "why": "임베딩에 노이즈를 섞어 학습함으로써 모델이 과적합되지 않고 더 강건하게 동작하도록 돕는 기법입니다.",
    "hint": "Noise Embedding",
    "difficulty": "hard",
    "id": "6082"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "심화 튜닝 전략 83",
    "options": [
      "Catastrophic Forgetting을 방지하려면 모든 데이터를 버려야 한다.",
      "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
      "QLoRA는 32비트 정밀도로 학습한다.",
      "Full Fine-Tuning이 항상 LoRA보다 성능이 뛰어나다.",
      "데이터 품질은 중요하지 않다."
    ],
    "answer": "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
    "why": "임베딩에 노이즈를 섞어 학습함으로써 모델이 과적합되지 않고 더 강건하게 동작하도록 돕는 기법입니다.",
    "hint": "Noise Embedding",
    "difficulty": "hard",
    "id": "6083"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "심화 튜닝 전략 84",
    "options": [
      "Catastrophic Forgetting을 방지하려면 모든 데이터를 버려야 한다.",
      "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
      "QLoRA는 32비트 정밀도로 학습한다.",
      "Full Fine-Tuning이 항상 LoRA보다 성능이 뛰어나다.",
      "데이터 품질은 중요하지 않다."
    ],
    "answer": "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
    "why": "임베딩에 노이즈를 섞어 학습함으로써 모델이 과적합되지 않고 더 강건하게 동작하도록 돕는 기법입니다.",
    "hint": "Noise Embedding",
    "difficulty": "hard",
    "id": "6084"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "심화 튜닝 전략 85",
    "options": [
      "Catastrophic Forgetting을 방지하려면 모든 데이터를 버려야 한다.",
      "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
      "QLoRA는 32비트 정밀도로 학습한다.",
      "Full Fine-Tuning이 항상 LoRA보다 성능이 뛰어나다.",
      "데이터 품질은 중요하지 않다."
    ],
    "answer": "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
    "why": "임베딩에 노이즈를 섞어 학습함으로써 모델이 과적합되지 않고 더 강건하게 동작하도록 돕는 기법입니다.",
    "hint": "Noise Embedding",
    "difficulty": "hard",
    "id": "6085"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "심화 튜닝 전략 86",
    "options": [
      "Catastrophic Forgetting을 방지하려면 모든 데이터를 버려야 한다.",
      "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
      "QLoRA는 32비트 정밀도로 학습한다.",
      "Full Fine-Tuning이 항상 LoRA보다 성능이 뛰어나다.",
      "데이터 품질은 중요하지 않다."
    ],
    "answer": "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
    "why": "임베딩에 노이즈를 섞어 학습함으로써 모델이 과적합되지 않고 더 강건하게 동작하도록 돕는 기법입니다.",
    "hint": "Noise Embedding",
    "difficulty": "hard",
    "id": "6086"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "심화 튜닝 전략 87",
    "options": [
      "Catastrophic Forgetting을 방지하려면 모든 데이터를 버려야 한다.",
      "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
      "QLoRA는 32비트 정밀도로 학습한다.",
      "Full Fine-Tuning이 항상 LoRA보다 성능이 뛰어나다.",
      "데이터 품질은 중요하지 않다."
    ],
    "answer": "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
    "why": "임베딩에 노이즈를 섞어 학습함으로써 모델이 과적합되지 않고 더 강건하게 동작하도록 돕는 기법입니다.",
    "hint": "Noise Embedding",
    "difficulty": "hard",
    "id": "6087"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "심화 튜닝 전략 88",
    "options": [
      "Catastrophic Forgetting을 방지하려면 모든 데이터를 버려야 한다.",
      "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
      "QLoRA는 32비트 정밀도로 학습한다.",
      "Full Fine-Tuning이 항상 LoRA보다 성능이 뛰어나다.",
      "데이터 품질은 중요하지 않다."
    ],
    "answer": "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
    "why": "임베딩에 노이즈를 섞어 학습함으로써 모델이 과적합되지 않고 더 강건하게 동작하도록 돕는 기법입니다.",
    "hint": "Noise Embedding",
    "difficulty": "hard",
    "id": "6088"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "심화 튜닝 전략 89",
    "options": [
      "Catastrophic Forgetting을 방지하려면 모든 데이터를 버려야 한다.",
      "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
      "QLoRA는 32비트 정밀도로 학습한다.",
      "Full Fine-Tuning이 항상 LoRA보다 성능이 뛰어나다.",
      "데이터 품질은 중요하지 않다."
    ],
    "answer": "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
    "why": "임베딩에 노이즈를 섞어 학습함으로써 모델이 과적합되지 않고 더 강건하게 동작하도록 돕는 기법입니다.",
    "hint": "Noise Embedding",
    "difficulty": "hard",
    "id": "6089"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "심화 튜닝 전략 90",
    "options": [
      "Catastrophic Forgetting을 방지하려면 모든 데이터를 버려야 한다.",
      "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
      "QLoRA는 32비트 정밀도로 학습한다.",
      "Full Fine-Tuning이 항상 LoRA보다 성능이 뛰어나다.",
      "데이터 품질은 중요하지 않다."
    ],
    "answer": "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
    "why": "임베딩에 노이즈를 섞어 학습함으로써 모델이 과적합되지 않고 더 강건하게 동작하도록 돕는 기법입니다.",
    "hint": "Noise Embedding",
    "difficulty": "hard",
    "id": "6090"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "심화 튜닝 전략 91",
    "options": [
      "Catastrophic Forgetting을 방지하려면 모든 데이터를 버려야 한다.",
      "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
      "QLoRA는 32비트 정밀도로 학습한다.",
      "Full Fine-Tuning이 항상 LoRA보다 성능이 뛰어나다.",
      "데이터 품질은 중요하지 않다."
    ],
    "answer": "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
    "why": "임베딩에 노이즈를 섞어 학습함으로써 모델이 과적합되지 않고 더 강건하게 동작하도록 돕는 기법입니다.",
    "hint": "Noise Embedding",
    "difficulty": "hard",
    "id": "6091"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "심화 튜닝 전략 92",
    "options": [
      "Catastrophic Forgetting을 방지하려면 모든 데이터를 버려야 한다.",
      "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
      "QLoRA는 32비트 정밀도로 학습한다.",
      "Full Fine-Tuning이 항상 LoRA보다 성능이 뛰어나다.",
      "데이터 품질은 중요하지 않다."
    ],
    "answer": "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
    "why": "임베딩에 노이즈를 섞어 학습함으로써 모델이 과적합되지 않고 더 강건하게 동작하도록 돕는 기법입니다.",
    "hint": "Noise Embedding",
    "difficulty": "hard",
    "id": "6092"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "심화 튜닝 전략 93",
    "options": [
      "Catastrophic Forgetting을 방지하려면 모든 데이터를 버려야 한다.",
      "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
      "QLoRA는 32비트 정밀도로 학습한다.",
      "Full Fine-Tuning이 항상 LoRA보다 성능이 뛰어나다.",
      "데이터 품질은 중요하지 않다."
    ],
    "answer": "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
    "why": "임베딩에 노이즈를 섞어 학습함으로써 모델이 과적합되지 않고 더 강건하게 동작하도록 돕는 기법입니다.",
    "hint": "Noise Embedding",
    "difficulty": "hard",
    "id": "6093"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "심화 튜닝 전략 94",
    "options": [
      "Catastrophic Forgetting을 방지하려면 모든 데이터를 버려야 한다.",
      "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
      "QLoRA는 32비트 정밀도로 학습한다.",
      "Full Fine-Tuning이 항상 LoRA보다 성능이 뛰어나다.",
      "데이터 품질은 중요하지 않다."
    ],
    "answer": "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
    "why": "임베딩에 노이즈를 섞어 학습함으로써 모델이 과적합되지 않고 더 강건하게 동작하도록 돕는 기법입니다.",
    "hint": "Noise Embedding",
    "difficulty": "hard",
    "id": "6094"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "심화 튜닝 전략 95",
    "options": [
      "Catastrophic Forgetting을 방지하려면 모든 데이터를 버려야 한다.",
      "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
      "QLoRA는 32비트 정밀도로 학습한다.",
      "Full Fine-Tuning이 항상 LoRA보다 성능이 뛰어나다.",
      "데이터 품질은 중요하지 않다."
    ],
    "answer": "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
    "why": "임베딩에 노이즈를 섞어 학습함으로써 모델이 과적합되지 않고 더 강건하게 동작하도록 돕는 기법입니다.",
    "hint": "Noise Embedding",
    "difficulty": "hard",
    "id": "6095"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "심화 튜닝 전략 96",
    "options": [
      "Catastrophic Forgetting을 방지하려면 모든 데이터를 버려야 한다.",
      "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
      "QLoRA는 32비트 정밀도로 학습한다.",
      "Full Fine-Tuning이 항상 LoRA보다 성능이 뛰어나다.",
      "데이터 품질은 중요하지 않다."
    ],
    "answer": "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
    "why": "임베딩에 노이즈를 섞어 학습함으로써 모델이 과적합되지 않고 더 강건하게 동작하도록 돕는 기법입니다.",
    "hint": "Noise Embedding",
    "difficulty": "hard",
    "id": "6096"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "심화 튜닝 전략 97",
    "options": [
      "Catastrophic Forgetting을 방지하려면 모든 데이터를 버려야 한다.",
      "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
      "QLoRA는 32비트 정밀도로 학습한다.",
      "Full Fine-Tuning이 항상 LoRA보다 성능이 뛰어나다.",
      "데이터 품질은 중요하지 않다."
    ],
    "answer": "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
    "why": "임베딩에 노이즈를 섞어 학습함으로써 모델이 과적합되지 않고 더 강건하게 동작하도록 돕는 기법입니다.",
    "hint": "Noise Embedding",
    "difficulty": "hard",
    "id": "6097"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "심화 튜닝 전략 98",
    "options": [
      "Catastrophic Forgetting을 방지하려면 모든 데이터를 버려야 한다.",
      "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
      "QLoRA는 32비트 정밀도로 학습한다.",
      "Full Fine-Tuning이 항상 LoRA보다 성능이 뛰어나다.",
      "데이터 품질은 중요하지 않다."
    ],
    "answer": "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
    "why": "임베딩에 노이즈를 섞어 학습함으로써 모델이 과적합되지 않고 더 강건하게 동작하도록 돕는 기법입니다.",
    "hint": "Noise Embedding",
    "difficulty": "hard",
    "id": "6098"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "심화 튜닝 전략 99",
    "options": [
      "Catastrophic Forgetting을 방지하려면 모든 데이터를 버려야 한다.",
      "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
      "QLoRA는 32비트 정밀도로 학습한다.",
      "Full Fine-Tuning이 항상 LoRA보다 성능이 뛰어나다.",
      "데이터 품질은 중요하지 않다."
    ],
    "answer": "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
    "why": "임베딩에 노이즈를 섞어 학습함으로써 모델이 과적합되지 않고 더 강건하게 동작하도록 돕는 기법입니다.",
    "hint": "Noise Embedding",
    "difficulty": "hard",
    "id": "6099"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "심화 튜닝 전략 100",
    "options": [
      "Catastrophic Forgetting을 방지하려면 모든 데이터를 버려야 한다.",
      "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
      "QLoRA는 32비트 정밀도로 학습한다.",
      "Full Fine-Tuning이 항상 LoRA보다 성능이 뛰어나다.",
      "데이터 품질은 중요하지 않다."
    ],
    "answer": "NEFTune은 노이즈를 추가하여 일반화 성능을 높인다.",
    "why": "임베딩에 노이즈를 섞어 학습함으로써 모델이 과적합되지 않고 더 강건하게 동작하도록 돕는 기법입니다.",
    "hint": "Noise Embedding",
    "difficulty": "hard",
    "id": "60100"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "LoRA 설정을 정의하세요.\n```python\npeft_config = ____(r=8, lora_alpha=32, ...)\n```",
    "answer": "LoraConfig",
    "why": "LoRA 설정 클래스입니다.",
    "hint": "HuggingFace Trainer Config",
    "difficulty": "medium",
    "id": "6101"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "Fine Tuning 관련 코드를 완성하세요 (문제 2)",
    "answer": "TrainingArguments",
    "why": "학습 설정을 정의하는 클래스",
    "hint": "HuggingFace Trainer Config",
    "difficulty": "medium",
    "id": "6102"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "Fine Tuning 관련 코드를 완성하세요 (문제 3)",
    "answer": "TrainingArguments",
    "why": "학습 설정을 정의하는 클래스",
    "hint": "HuggingFace Trainer Config",
    "difficulty": "medium",
    "id": "6103"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "모델 학습을 시작하세요.\n```python\ntrainer = Trainer(model=model, ...)\ntrainer.____()\n```",
    "answer": "train",
    "why": "학습 시작 메서드는 train()입니다.",
    "hint": "HuggingFace Trainer Config",
    "difficulty": "medium",
    "id": "6104"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "LoRA 설정을 정의하세요.\n```python\npeft_config = ____(r=8, lora_alpha=32, ...)\n```",
    "answer": "LoraConfig",
    "why": "LoRA 설정 클래스입니다.",
    "hint": "HuggingFace Trainer Config",
    "difficulty": "medium",
    "id": "6105"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "Fine Tuning 관련 코드를 완성하세요 (문제 6)",
    "answer": "TrainingArguments",
    "why": "학습 설정을 정의하는 클래스",
    "hint": "HuggingFace Trainer Config",
    "difficulty": "medium",
    "id": "6106"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "Fine Tuning 관련 코드를 완성하세요 (문제 7)",
    "answer": "TrainingArguments",
    "why": "학습 설정을 정의하는 클래스",
    "hint": "HuggingFace Trainer Config",
    "difficulty": "medium",
    "id": "6107"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "모델 학습을 시작하세요.\n```python\ntrainer = Trainer(model=model, ...)\ntrainer.____()\n```",
    "answer": "train",
    "why": "학습 시작 메서드는 train()입니다.",
    "hint": "HuggingFace Trainer Config",
    "difficulty": "medium",
    "id": "6108"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "LoRA 설정을 정의하세요.\n```python\npeft_config = ____(r=8, lora_alpha=32, ...)\n```",
    "answer": "LoraConfig",
    "why": "LoRA 설정 클래스입니다.",
    "hint": "HuggingFace Trainer Config",
    "difficulty": "medium",
    "id": "6109"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "Fine Tuning 관련 코드를 완성하세요 (문제 10)",
    "answer": "TrainingArguments",
    "why": "학습 설정을 정의하는 클래스",
    "hint": "HuggingFace Trainer Config",
    "difficulty": "medium",
    "id": "6110"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "Fine Tuning 관련 코드를 완성하세요 (문제 11)",
    "answer": "TrainingArguments",
    "why": "학습 설정을 정의하는 클래스",
    "hint": "HuggingFace Trainer Config",
    "difficulty": "medium",
    "id": "6111"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "모델 학습을 시작하세요.\n```python\ntrainer = Trainer(model=model, ...)\ntrainer.____()\n```",
    "answer": "train",
    "why": "학습 시작 메서드는 train()입니다.",
    "hint": "HuggingFace Trainer Config",
    "difficulty": "medium",
    "id": "6112"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "LoRA 설정을 정의하세요.\n```python\npeft_config = ____(r=8, lora_alpha=32, ...)\n```",
    "answer": "LoraConfig",
    "why": "LoRA 설정 클래스입니다.",
    "hint": "HuggingFace Trainer Config",
    "difficulty": "medium",
    "id": "6113"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "Fine Tuning 관련 코드를 완성하세요 (문제 14)",
    "answer": "TrainingArguments",
    "why": "학습 설정을 정의하는 클래스",
    "hint": "HuggingFace Trainer Config",
    "difficulty": "medium",
    "id": "6114"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "Fine Tuning 관련 코드를 완성하세요 (문제 15)",
    "answer": "TrainingArguments",
    "why": "학습 설정을 정의하는 클래스",
    "hint": "HuggingFace Trainer Config",
    "difficulty": "medium",
    "id": "6115"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "모델 학습을 시작하세요.\n```python\ntrainer = Trainer(model=model, ...)\ntrainer.____()\n```",
    "answer": "train",
    "why": "학습 시작 메서드는 train()입니다.",
    "hint": "HuggingFace Trainer Config",
    "difficulty": "medium",
    "id": "6116"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "LoRA 설정을 정의하세요.\n```python\npeft_config = ____(r=8, lora_alpha=32, ...)\n```",
    "answer": "LoraConfig",
    "why": "LoRA 설정 클래스입니다.",
    "hint": "HuggingFace Trainer Config",
    "difficulty": "medium",
    "id": "6117"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "Fine Tuning 관련 코드를 완성하세요 (문제 18)",
    "answer": "TrainingArguments",
    "why": "학습 설정을 정의하는 클래스",
    "hint": "HuggingFace Trainer Config",
    "difficulty": "medium",
    "id": "6118"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "Fine Tuning 관련 코드를 완성하세요 (문제 19)",
    "answer": "TrainingArguments",
    "why": "학습 설정을 정의하는 클래스",
    "hint": "HuggingFace Trainer Config",
    "difficulty": "medium",
    "id": "6119"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "모델 학습을 시작하세요.\n```python\ntrainer = Trainer(model=model, ...)\ntrainer.____()\n```",
    "answer": "train",
    "why": "학습 시작 메서드는 train()입니다.",
    "hint": "HuggingFace Trainer Config",
    "difficulty": "medium",
    "id": "6120"
  }
]