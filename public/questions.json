[
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 클래스 생성을 제어하는 '클래스의 클래스'를 무엇이라 하나요?",
    "options": [
      "Subclass",
      "Superclass",
      "Metaclass (메타클래스)",
      "Abstract class",
      "Base class"
    ],
    "answer": "Metaclass (메타클래스)",
    "why": "메타클래스는 클래스가 어떻게 만들어지는지 정의하며, 보통 type을 상속받아 구현합니다.",
    "hint": "클래스 위의 계층입니다. 메타(Meta)라는 접두사가 붙습니다.",
    "trap_points": [
      "일반적인 상속과는 다른 차원의 개념임"
    ],
    "difficulty": "hard",
    "id": "0001"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 객체의 속성 접근(get, set, delete)을 커스터마이징하는 클래스는?",
    "options": [
      "Decorator",
      "Descriptor (디스크립터)",
      "Generator",
      "Iterator",
      "Selector"
    ],
    "answer": "Descriptor (디스크립터)",
    "why": "__get__, __set__ 등을 구현한 클래스는 다른 클래스의 속성으로 쓰여 접근 로직을 제어할 수 있습니다.",
    "hint": "설명자, 묘사자라는 뜻입니다.",
    "trap_points": [
      "@property 데코레이터가 내부적으로 이 방식을 사용함"
    ],
    "difficulty": "hard",
    "id": "0002"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 대규모 데이터를 병렬 처리할 때 'GIL' 문제를 피하기 위해 사용하는 표준 라이브러리는?",
    "options": [
      "threading",
      "multiprocessing",
      "asyncio",
      "itertools",
      "functools"
    ],
    "answer": "multiprocessing",
    "why": "멀티프로세싱은 별도의 파이썬 인터프리터 프로세스를 띄우므로 GIL의 영향을 받지 않고 멀티 코어를 활용합니다.",
    "hint": "여러 개의 프로세스(Process)입니다.",
    "trap_points": [
      "threading은 GIL 때문에 CPU 집약적 작업에는 부적합함"
    ],
    "difficulty": "medium",
    "id": "0003"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬의 `match-case` 문에서 와일드카드 역할을 하며 모든 값에 매칭되는 기호는?",
    "options": [
      "*",
      "?",
      "_ (언더바)",
      "default",
      "..."
    ],
    "answer": "_ (언더바)",
    "why": "매칭되지 않은 나머지 모든 경우를 처리하는 패턴으로 사용됩니다.",
    "hint": "변수 이름을 짓기 귀찮을 때 쓰는 기호를 생각하세요.",
    "trap_points": [
      "다른 언어의 default 키워드와 같은 역할임"
    ],
    "difficulty": "medium",
    "id": "0004"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 `functools.lru_cache` 데코레이터의 주된 역할은?",
    "options": [
      "함수 실행 속도를 제한한다.",
      "함수의 반환값을 메모리에 캐싱하여 동일 인자 호출 시 연산을 생략한다.",
      "함수를 삭제한다.",
      "함수 이름을 바꾼다.",
      "함수를 파일로 저장한다."
    ],
    "answer": "함수의 반환값을 메모리에 캐싱하여 동일 인자 호출 시 연산을 생략한다.",
    "why": "최근에 사용된(Least Recently Used) 결과물을 보관하여 재귀 함수 등의 성능을 비약적으로 높입니다.",
    "hint": "기억(Cache) 해둔다는 뜻입니다.",
    "trap_points": [
      "인자가 해시 가능(Hashable)해야 사용할 수 있음"
    ],
    "difficulty": "medium",
    "id": "0005"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 문자열 내의 정규표현식 등을 작성할 때 백슬래시 등을 그대로 처리하기 위해 문자열 앞에 붙이는 기호는?",
    "answer": "r",
    "why": "Raw string을 의미하며, r'\\n' 은 줄바꿈이 아닌 실제 백슬래시와 n 문자로 인식됩니다.",
    "hint": "날것(Raw)의 약자입니다.",
    "trap_points": [
      "정규표현식이나 윈도우 파일 경로 작성 시 필수적임"
    ],
    "difficulty": "easy",
    "id": "0006"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬의 '약한 참조(Weak Reference)'를 만드는 `weakref` 모듈을 사용하는 상황은?",
    "options": [
      "객체를 영구 저장할 때",
      "순환 참조를 방지하면서 객체를 참조하고 싶을 때 (가비지 컬렉터가 무시할 수 있게)",
      "속도가 빠른 변수를 만들 때",
      "영어로만 코딩할 때",
      "데이터를 압축할 때"
    ],
    "answer": "순환 참조를 방지하면서 객체를 참조하고 싶을 때 (가비지 컬렉터가 무시할 수 있게)",
    "why": "약한 참조는 대상 객체의 레퍼런스 카운트를 올리지 않으므로 메모리 관리를 더 세밀하게 할 수 있습니다.",
    "hint": "참조의 힘이 약(Weak)합니다.",
    "trap_points": [
      "캐시 시스템 구현 시 객체 소멸을 방해하지 않기 위해 쓰임"
    ],
    "difficulty": "hard",
    "id": "0007"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 `heapq` 모듈이 기본적으로 구현하는 힙(Heap)의 종류는?",
    "options": [
      "최대 힙 (Max Heap)",
      "최소 힙 (Min Heap)",
      "중간 힙 (Median Heap)",
      "랜덤 힙",
      "이진 탐색 트리"
    ],
    "answer": "최소 힙 (Min Heap)",
    "why": "heapq[0]은 항상 리스트의 요소 중 가장 작은 값을 가집니다.",
    "hint": "가장 작은 값이 맨 위로 옵니다.",
    "trap_points": [
      "최대 힙을 만들려면 값에 마이너스(-)를 붙이는 트릭을 써야 함"
    ],
    "difficulty": "medium",
    "id": "0008"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 리스트의 `sort()` 메서드와 `sorted()` 함수의 핵심 차이는?",
    "options": [
      "둘은 똑같다.",
      "sort()는 원본 리스트를 직접 수정(In-place)하고, sorted()는 정렬된 새로운 리스트를 반환한다.",
      "sorted()가 더 느리다.",
      "sort()는 튜플에도 쓸 수 있다.",
      "sorted()는 오름차순만 가능하다."
    ],
    "answer": "sort()는 원본 리스트를 직접 수정(In-place)하고, sorted()는 정렬된 새로운 리스트를 반환한다.",
    "why": "메모리 상황과 원본 유지 필요성에 따라 선택하여 사용합니다.",
    "hint": "원본을 건드리는지(In-place) 여부를 보세요.",
    "trap_points": [
      "sort()는 반환값이 None임에 주의"
    ],
    "difficulty": "easy",
    "id": "0009"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 변수의 이름을 모르고 '문자열'로 된 이름만 있을 때, 해당 변수나 속성을 가져오는 함수는?",
    "answer": "getattr()",
    "why": "getattr(obj, 'attr_name') 처럼 사용하여 동적으로 속성에 접근할 수 있습니다.",
    "hint": "속성(Attribute)을 가져오다(Get).",
    "trap_points": [
      "반대로 설정할 때는 setattr()을 사용함"
    ],
    "difficulty": "medium",
    "id": "0010"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬의 'Itertools' 모듈에서 가용 가능한 모든 조합(Combination)을 생성하는 함수는?",
    "options": [
      "itertools.permutations",
      "itertools.combinations",
      "itertools.product",
      "itertools.cycle",
      "itertools.repeat"
    ],
    "answer": "itertools.combinations",
    "why": "combinations()는 주어 리스트에서 정해진 수만큼의 요소를 순서 상관없이 뽑는 조합을 생성합니다.",
    "hint": "조합이라는 뜻 그대로의 함수입니다.",
    "trap_points": [
      "permutations는 순서가 중요한 '순열'을 생성함"
    ],
    "difficulty": "medium",
    "id": "0011"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 HTTP 요청을 보내기 위해 가장 널리 쓰이는 외부 라이브러리는?",
    "options": [
      "urllib",
      "requests",
      "http.client",
      "networkx",
      "flask"
    ],
    "answer": "requests",
    "why": "requests 라이브러리는 사람이 읽기 쉬운 API로 HTTP 요청(GET, POST 등)을 처리할 수 있게 해줍니다.",
    "hint": "요청하다라는 뜻의 영어 단어입니다.",
    "trap_points": [
      "표준 라이브러리인 urllib보다 훨씬 사용이 간편함"
    ],
    "difficulty": "easy",
    "id": "0012"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "FastAPI에서 비동기 프로그래밍을 위해 사용하는 키워드 세트는?",
    "options": [
      "try - except",
      "async - await",
      "yield - next",
      "spawn - join",
      "open - close"
    ],
    "answer": "async - await",
    "why": "async def로 정의된 입출력 대기 시 await를 사용하여 다른 작업을 병렬로 처리할 수 있습니다.",
    "hint": "비동기(Asynchronous)와 기다리다(Await)의 약자입니다.",
    "trap_points": [
      "고성능 백엔드 구축의 핵심 키워드임"
    ],
    "difficulty": "medium",
    "id": "0013"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 클래스에서 @staticmethod 와 @classmethod 의 결정적인 차이는?",
    "options": [
      "차이가 없다.",
      "@classmethod는 첫 번째 인자로 클래스 자체(cls)를 받지만, @staticmethod는 아무런 인자를 강제하지 않는다.",
      "@staticmethod만 클래스 변수에 접근 가능하다.",
      "@classmethod는 인스턴스에서 호출이 불가능하다.",
      "@staticmethod는 파일 속도를 빠르게 한다."
    ],
    "answer": "@classmethod는 첫 번째 인자로 클래스 자체(cls)를 받지만, @staticmethod는 아무런 인자를 강제하지 않는다.",
    "why": "클래스 메서드는 클래스의 상태를 수정하거나 팩토리 메서드를 만들 때 유용하며, 정적 메서드는 독립적인 유틸리티에 씁니다.",
    "hint": "클래스(cls) 정보를 전달받는지 여부를 보세요.",
    "trap_points": [
      "두 메서드 모두 인스턴스 생성 없이 클래스 이름으로 호출 가능함"
    ],
    "difficulty": "hard",
    "id": "0014"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 객체를 텍스트나 바이너리 형태로 직렬화(Serialization)하여 저장하는 내장 모듈은?",
    "options": [
      "json",
      "csv",
      "pickle",
      "marshal",
      "struct"
    ],
    "answer": "pickle",
    "why": "pickle은 파이썬 객체 구조를 바이너리 형태로 저장하고 다시 복원(unpickling)할 수 있게 해줍니다.",
    "hint": "오이 절임(Pickle)처럼 보존한다는 뜻입니다.",
    "trap_points": [
      "신뢰할 수 없는 출처의 피클 파일은 보안 에러(코드 실행)를 일으킬 수 있음"
    ],
    "difficulty": "medium",
    "id": "0015"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 리스트 내부 요소들의 합을 구할 때 사용하는 가장 효율적인 내장 함수는?",
    "options": [
      "total()",
      "sum()",
      "add_all()",
      "count()",
      "math.add()"
    ],
    "answer": "sum()",
    "why": "sum() 함수는 이터러블 객체의 모든 수치를 합산하여 반환합니다.",
    "hint": "합계를 뜻하는 영어 단어입니다.",
    "trap_points": [
      "문자열 리스트에는 사용이 불가능함 (join을 써야 함)"
    ],
    "difficulty": "easy",
    "id": "0016"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 3.7부터 딕셔너리에 추가된 중요한 성질은?",
    "options": [
      "값이 자동으로 정렬된다.",
      "항상 모든 키가 영어여야 한다.",
      "요소가 추가된 '순서'가 보장된다.",
      "메모리를 1/10로 줄였다.",
      "중복된 키를 허용한다."
    ],
    "answer": "요소가 추가된 '순서'가 보장된다.",
    "why": "과거에는 OrderDict를 써야 했으나, 현재는 일반 딕셔너리도 삽입 순서를 유지합니다.",
    "hint": "순차적 보관을 생각하세요.",
    "trap_points": [
      "순서가 보장되지만 세트(Set)는 여전히 무작위임"
    ],
    "difficulty": "medium",
    "id": "0017"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "BeautifulSoup 라이브러리를 사용해 웹 페이지의 HTML 태그를 찾는 기본 메서드는?",
    "options": [
      "soup.get()",
      "soup.find()",
      "soup.search()",
      "soup.match()",
      "soup.lookup()"
    ],
    "answer": "soup.find()",
    "why": "find()는 조건에 맞는 첫 번째 태그를, find_all()은 모든 태그를 리스트로 반환합니다.",
    "hint": "찾다라는 뜻의 4글자 영어입니다.",
    "trap_points": [
      "CSS 선택자를 쓰고 싶다면 soup.select()를 사용함"
    ],
    "difficulty": "easy",
    "id": "0018"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 문자열이 숫자로만 이루어져 있는지 확인하여 True/False를 반환하는 메서드는?",
    "options": [
      "isdigit()",
      "isnumeric()",
      "isdecimal()",
      "위 셋 모두 유사하지만 미세한 차이가 있으며 보통 isdigit()을 널리 쓴다.",
      "check_num()"
    ],
    "answer": "위 셋 모두 유사하지만 미세한 차이가 있으며 보통 isdigit()을 널리 쓴다.",
    "why": "isdigit은 숫자 형태의 문자 여부를, isdecimal은 0-9의 순수 십진수 여부를 엄격히 따집니다.",
    "hint": "숫자(digit)인가요?(is)",
    "trap_points": [
      "마이너스(-) 기호나 소수점(.)이 포함되면 False가 나옴"
    ],
    "difficulty": "medium",
    "id": "0019"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 현재 작업 디렉토리나 환경 변수를 조작하기 위해 사용하는 표준 라이브러리 모듈은?",
    "answer": "os",
    "why": "os.path, os.getenv 등 운영체제(Operating System) 관련 인터페이스를 제공합니다.",
    "hint": "알파벳 두 글자입니다.",
    "trap_points": [
      "최근에는 객체 지향적인 pathlib 사용도 권장됨"
    ],
    "difficulty": "easy",
    "id": "0020"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 리스트를 정렬할 때, 원본을 변경하지 않고 새로운 정렬된 리스트를 반환하는 함수는?",
    "options": [
      "list.sort()",
      "sorted()",
      "order()",
      "arrange()",
      "list.order()"
    ],
    "answer": "sorted()",
    "why": "sorted()는 내장 함수로 새로운 리스트를 반환하며, list.sort()는 리스트 자체를 정렬(In-place)합니다.",
    "hint": "동사형 형용사 형태의 내장 함수입니다.",
    "trap_points": [
      "원본 유지 여부에 따라 선택이 달라짐"
    ],
    "difficulty": "medium",
    "id": "0021"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "딕셔너리에서 키가 없을 때 에러를 내지 않고 기본값을 반환하는 메서드는?",
    "options": [
      "dict.find()",
      "dict.get()",
      "dict.search()",
      "dict.index()",
      "dict.pop()"
    ],
    "answer": "dict.get()",
    "why": "get(key, default)은 키가 존재하지 않을 경우 None이나 지정된 기본값을 안전하게 반환합니다.",
    "hint": "가져오다(Get)의 의미입니다.",
    "trap_points": [
      "대괄호 [] 접근은 KeyError가 발생함"
    ],
    "difficulty": "easy",
    "id": "0022"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬의 'Set' 자료형이 'List'와 차별화되는 가장 큰 특징은?",
    "options": [
      "순서가 보장된다.",
      "중복을 허용하지 않으며 순서가 없다.",
      "속도가 매우 느리다.",
      "인덱스로 접근 가능하다.",
      "문자열만 저장할 수 있다."
    ],
    "answer": "중복을 허용하지 않으며 순서가 없다.",
    "why": "집합(Set)은 수학적 집합과 동일하게 유일한 값들만 담으며 인덱싱이 불가능합니다.",
    "hint": "수학의 '집합' 개념을 떠올리세요.",
    "trap_points": [
      "중복 제거가 필요할 때 가장 효율적임"
    ],
    "difficulty": "easy",
    "id": "0023"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "부모 클래스의 생성자를 호출할 때 사용하는 올바른 문법은?",
    "options": [
      "parent.__init__(self)",
      "base.__init__(self)",
      "super().__init__()",
      "self.super().__init__()",
      "inherit.__init__()"
    ],
    "answer": "super().__init__()",
    "why": "super()는 MRO(Method Resolution Order)를 따라 부모 클래스에 접근하는 표준 방식입니다.",
    "hint": "이미 샘플에서도 다뤘던 핵심 키워드입니다.",
    "trap_points": [
      "파이썬 3에서는 super()에 인자를 생략 가능함"
    ],
    "difficulty": "medium",
    "id": "0024"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "문자열 '  Hello  '의 좌우 공백을 제거하는 메서드는?",
    "answer": "strip()",
    "why": "strip()은 문자열 양 끝의 공백이나 지정된 문자를 제거합니다.",
    "hint": "벗기다, 떼어내다의 뜻입니다.",
    "trap_points": [
      "lstrip()은 왼쪽만, rstrip()은 오른쪽만 제거함"
    ],
    "difficulty": "easy",
    "id": "0025"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 클래스 내에서 특정 속성에 대한 접근 제어(Getter/Setter)를 위해 사용하는 데코레이터는?",
    "options": [
      "@staticmethod",
      "@classmethod",
      "@property",
      "@access",
      "@wrap"
    ],
    "answer": "@property",
    "why": "property 데코레이터를 쓰면 메서드를 변수처럼 호출할 수 있어 캡슐화에 유리합니다.",
    "hint": "속성을 뜻하는 영어 단어입니다.",
    "trap_points": [
      "@classmethod는 클래스 자체를 인자로 받는 메서드임"
    ],
    "difficulty": "hard",
    "id": "0026"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "이터러블 객체의 요소와 인덱스를 동시에 반환하는 반복문 도구는?",
    "options": [
      "zip()",
      "range()",
      "enumerate()",
      "slice()",
      "map()"
    ],
    "answer": "enumerate()",
    "why": "enumerate()는 리스트 등의 순회 시 (인덱스, 값) 튜플을 생성해 줍니다.",
    "hint": "열거하다라는 뜻입니다.",
    "trap_points": [
      "zip()은 여러 리스트를 묶을 때 사용함"
    ],
    "difficulty": "medium",
    "id": "0027"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 두 리스트를 하나로 합치되, 원본 리스트를 확장(Extend)하는 메서드는?",
    "options": [
      "list.append()",
      "list.extend()",
      "list.merge()",
      "list.add()",
      "list.join()"
    ],
    "answer": "list.extend()",
    "why": "append()는 요소를 맨 뒤에 하나 추가하고, extend()는 다른 리스트의 모든 요소를 이어 붙입니다.",
    "hint": "연장하다라는 뜻입니다.",
    "trap_points": [
      "append()로 리스트를 넣으면 리스트 안에 리스트가 들어감"
    ],
    "difficulty": "medium",
    "id": "0028"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "전역 변수를 함수 내부에서 수정하려고 할 때 사용하는 키워드는?",
    "options": [
      "outer",
      "static",
      "global",
      "nonlocal",
      "public"
    ],
    "answer": "global",
    "why": "global 키워드를 선언해야 함수 외부의 전역 변수 메모리 주소에 접근하여 수정할 수 있습니다.",
    "hint": "전체적인, 전역의를 뜻합니다.",
    "trap_points": [
      "nonlocal은 중첩 함수 내부에서 한 단계 위 변수를 가리킴"
    ],
    "difficulty": "medium",
    "id": "0029"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 딕셔너리에서 모든 '키(Key)'들만 모아서 반환하는 메서드는?",
    "answer": "keys()",
    "why": "keys() 메서드는 딕셔너리의 키들을 담은 뷰(view) 객체를 반환합니다.",
    "hint": "Key의 복수형입니다.",
    "trap_points": [
      "values()는 값들을, items()는 키-값 쌍을 반환함"
    ],
    "difficulty": "easy",
    "id": "0030"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬의 '동적 타이핑(Dynamic Typing)'에 대한 설명으로 옳은 것은?",
    "options": [
      "변수 선언 시 타입을 명시해야 한다.",
      "실행 시점에 변수의 타입이 결정된다.",
      "한 번 결정된 변수의 타입은 바꿀 수 없다.",
      "컴파일 시점에 타입 체크를 수행한다.",
      "숫자 타입과 문자열 타입을 섞어서 연산할 수 있다."
    ],
    "answer": "실행 시점에 변수의 타입이 결정된다.",
    "why": "파이썬은 변수에 값을 할당할 때 타입이 결정되며, 실행 중에도 다른 타입의 값을 자유롭게 대입할 수 있는 동적 타이핑 언어입니다.",
    "hint": "코드 실행(Runtime) 중에 타입이 결정된다는 의미입니다.",
    "trap_points": [
      "정적 타이핑 언어(Java, C++)와 혼동 주의"
    ],
    "difficulty": "easy",
    "id": "0031"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬의 자료형 중 내부 요소를 변경할 수 없는(Immutable) 것은?",
    "options": [
      "List",
      "Dictionary",
      "Set",
      "Tuple",
      "ByteArray"
    ],
    "answer": "Tuple",
    "why": "튜플은 한 번 생성되면 내부 요소를 수정, 추가, 삭제할 수 없는 불변 시퀀스 자료형입니다.",
    "hint": "소괄호 ()를 사용하는 자료형입니다.",
    "trap_points": [
      "리스트(List)는 가변(Mutable) 자료형임"
    ],
    "difficulty": "easy",
    "id": "0032"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 여러 인자를 튜플 형태로 한꺼번에 전달받을 때 사용하는 가변 매개변수 표현은?",
    "options": [
      "*args",
      "**kwargs",
      "&args",
      "$args",
      "args[]"
    ],
    "answer": "*args",
    "why": "*args는 정해지지 않은 수의 위치 인자들을 튜플로 묶어서 함수 내부로 전달합니다.",
    "hint": "별표(Asterisk)가 하나 붙습니다.",
    "trap_points": [
      "**kwargs는 딕셔너리(키워드 인자) 형태임"
    ],
    "difficulty": "medium",
    "id": "0033"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파일을 열 때 사용 후 자동으로 닫아주는 안전한 문법 구조는?",
    "options": [
      "try-finally",
      "open-close",
      "with statement",
      "using block",
      "file-handle"
    ],
    "answer": "with statement",
    "why": "with 문을 사용하면 context manager 기능에 의해 블록이 끝날 때 자동으로 close()가 호출됩니다.",
    "hint": "~와 함께라는 뜻의 영어 단어로 시작합니다.",
    "trap_points": [
      "close()를 누락하면 자원 누수가 발생할 수 있음"
    ],
    "difficulty": "easy",
    "id": "0034"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 객체지향에서 클래스 변수에 대한 설명 중 맞는 것은?",
    "options": [
      "각 인스턴스마다 독립적인 값을 가진다.",
      "인스턴스 생성 시점에 self로 정의한다.",
      "모든 인스턴스가 공유하는 변수이다.",
      "함수 내부에서만 전용으로 쓰인다.",
      "변경이 절대 불가능한 상수이다."
    ],
    "answer": "모든 인스턴스가 공유하는 변수이다.",
    "why": "클래스 변수는 클래스 정의 바로 아래에 위치하며, 해당 클래스로 생성된 모든 객체가 동일한 메모리 공간을 참조합니다.",
    "hint": "개별(Instance)이 아닌 전체(Class)가 공유합니다.",
    "trap_points": [
      "self.변수로 정의하는 것은 인스턴스 변수임"
    ],
    "difficulty": "medium",
    "id": "0035"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "에러가 발생할 가능성이 있는 코드를 감싸고 예외를 처리할 때 사용하는 구문 세트는?",
    "answer": "try, except",
    "why": "try 블록에서 코드를 실행하고, 에러가 발생하면 except 블록에서 이를 잡아 처리합니다.",
    "hint": "시도하다, 제외하다의 영어 단어입니다.",
    "trap_points": [
      "Java의 try-catch와 용어가 다름"
    ],
    "difficulty": "easy",
    "id": "0036"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 반복문에서 현재 루프를 중단하고 '다음 반복'으로 즉시 넘어가는 키워드는?",
    "options": [
      "break",
      "continue",
      "pass",
      "exit",
      "return"
    ],
    "answer": "continue",
    "why": "continue는 아래 코드를 실행하지 않고 다음 이터레이션(루프의 다음 단계)으로 건너뜁니다.",
    "hint": "계속하다라는 뜻입니다.",
    "trap_points": [
      "break는 루프 자체를 완전히 종료함"
    ],
    "difficulty": "easy",
    "id": "0037"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "PEP8 관례상 상수를 정의할 때 사용하는 명명 규칙은?",
    "options": [
      "camelCase",
      "snake_case",
      "PascalCase",
      "ALL_CAPS (대문자+언더바)",
      "lowercase"
    ],
    "answer": "ALL_CAPS (대문자+언더바)",
    "why": "파이썬에서 상수는 관례적으로 모든 글자를 대문자로 쓰고 단어 사이를 언더바로 연결합니다.",
    "hint": "MAX_VALUE와 같은 형태입니다.",
    "trap_points": [
      "파이썬은 문법적으로 상수를 강제하지 않으므로 규칙 준수가 중요함"
    ],
    "difficulty": "easy",
    "id": "0038"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "부모 클래스의 메서드를 자식 클래스에서 '재정의'하는 것을 무엇이라 하나요?",
    "options": [
      "Overloading",
      "Overriding",
      "Inheritance",
      "Initialzing",
      "Decorating"
    ],
    "answer": "Overriding",
    "why": "오버라이딩은 상속받은 메서드를 자식 클래스의 용도에 맞게 다시 쓰는 것을 의미합니다.",
    "hint": "위에 올라타서 덮어쓴다는 의미입니다.",
    "trap_points": [
      "Overloading은 같은 이름의 인자가 다른 함수들을 만드는 것(파이썬은 기본 미지원)"
    ],
    "difficulty": "medium",
    "id": "0039"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 패키지를 설치할 때 사용하는 표준 패키지 관리자 도구의 이름은?",
    "answer": "pip",
    "why": "pip는 Python Package Index(PyPI)로부터 패키지를 설치하고 관리하는 인터페이스입니다.",
    "hint": "3글자 약어입니다.",
    "trap_points": [
      "conda와는 별개의 도구임"
    ],
    "difficulty": "easy",
    "id": "0040"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 '얕은 복사(Shallow Copy)'와 '깊은 복사(Deep Copy)'의 주된 차이점은 무엇인가요?",
    "options": [
      "얕은 복사는 속도가 빠르고 깊은 복사는 속도가 느리다.",
      "얕은 복사는 최상위 객체만 새로 만들고 내부 객체는 참조를 공유하지만, 깊은 복사는 내부 객체까지 모두 새로 만든다.",
      "얕은 복사는 리스트에만 사용 가능하다.",
      "깊은 복사는 파이썬 2에서만 지원한다.",
      "두 복사 방식은 결과적으로 동일하다."
    ],
    "answer": "얕은 복사는 최상위 객체만 새로 만들고 내부 객체는 참조를 공유하지만, 깊은 복사는 내부 객체까지 모두 새로 만든다.",
    "why": "copy.copy()는 얕은 복사를, copy.deepcopy()는 깊은 복사를 수행하여 중첩된 가변 객체 수정 시 의도치 않은 변경을 방지합니다.",
    "hint": "중첩된 요소(리스트 안의 리스트 등)의 독립성 여부를 생각하세요.",
    "trap_points": [
      "단순 대입(=)은 복사가 아닌 참조 전달임"
    ],
    "difficulty": "hard",
    "id": "0041"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬의 '제네레이터(Generator)'에 대한 설명으로 옳은 것은?",
    "options": [
      "모든 요소를 한꺼번에 메모리에 로드한다.",
      "yield 키워드를 사용하여 값을 하나씩 생성하며 메모리를 절약한다.",
      "한 번 호출하면 다시는 사용할 수 없다.",
      "항상 리스트 타입을 반환한다.",
      "클래스 내부에서만 정의할 수 있다."
    ],
    "answer": "yield 키워드를 사용하여 값을 하나씩 생성하며 메모리를 절약한다.",
    "why": "제네레이터는 필요한 시점에만 값을 생성(Lazy Evaluation)하므로 대용량 데이터 처리에 매우 효율적입니다.",
    "hint": "산출하다(Yield)라는 단어를 기억하세요.",
    "trap_points": [
      "메모리 부족 에러를 방지하는 핵심 기술임"
    ],
    "difficulty": "medium",
    "id": "0042"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 @ 기호를 사용하여 기존 함수에 기능을 추가하거나 수정하는 도구는?",
    "options": [
      "Generator",
      "Decorator",
      "Iterator",
      "Descriptor",
      "Selector"
    ],
    "answer": "Decorator",
    "why": "데코레이터는 함수를 인자로 받아 다른 함수를 반환하는 고차 함수로, 로그 출력이나 권한 확인 등에 쓰입니다.",
    "hint": "장식하다라는 뜻입니다.",
    "trap_points": [
      "클로저(Closure) 개념이 사용됨"
    ],
    "difficulty": "medium",
    "id": "0043"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬의 리스트(List)와 세트(Set)에서 요소의 존재 여부(in 연산)를 확인할 때 시간 복잡도 차이는?",
    "options": [
      "둘 다 O(1)이다.",
      "리스트는 O(n), 세트는 O(1)이다.",
      "리스트는 O(1), 세트는 O(n)이다.",
      "둘 다 O(n)이다.",
      "리스트는 O(log n), 세트는 O(1)이다."
    ],
    "answer": "리스트는 O(n), 세트는 O(1)이다.",
    "why": "세트는 해시 테이블을 사용하므로 요소 개수와 상관없이 거의 즉시 값을 찾을 수 있지만, 리스트는 처음부터 끝까지 훑어야 합니다.",
    "hint": "해싱(Hashing) 기술의 속도를 생각하세요.",
    "trap_points": [
      "데이터가 많을수록 세트를 쓰는 것이 검색 속도에서 압도적임"
    ],
    "difficulty": "hard",
    "id": "0044"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 클래스에서 __str__ 메서드와 __repr__ 메서드의 주된 차이는?",
    "options": [
      "둘은 똑같은 기능을 한다.",
      "__str__은 사용자용(print), __repr__은 개발용(객체 재현) 정보를 제공하는 관례가 있다.",
      "__repr__은 주석을 다는 용도이다.",
      "__str__은 소문자로만 출력한다.",
      "__repr__은 필수적으로 구현해야 한다."
    ],
    "answer": "__str__은 사용자용(print), __repr__은 개발용(객체 재현) 정보를 제공하는 관례가 있다.",
    "why": "__str__은 보기 좋은 문자열을, __repr__은 객체를 다시 생성할 수 있을 정도의 상세 정보를 담습니다.",
    "hint": "표현(Representation)과 스트링(String)의 차이입니다.",
    "trap_points": [
      "디버깅 시에는 __repr__이 더 중요함"
    ],
    "difficulty": "hard",
    "id": "0045"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 3.10 이상에서 도입된, 여러 갈래의 조건을 체크하는 match-case 문의 명칭은?",
    "options": [
      "Switch Pattern",
      "Structural Pattern Matching",
      "Case Selection",
      "Conditional Flow",
      "Multi-branching"
    ],
    "answer": "Structural Pattern Matching",
    "why": "단순 값 비교뿐만 아니라 객체의 구조나 타입을 매칭하여 분기 처리할 수 있는 강력한 기능입니다.",
    "hint": "구조적 패턴 매칭입니다.",
    "trap_points": [
      "다른 언어의 switch 문보다 훨씬 고도화된 기능을 제공함"
    ],
    "difficulty": "hard",
    "id": "0046"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 'Hello' * 3의 결과는 무엇인가요?",
    "options": [
      "Hello3",
      "에러 발생",
      "HelloHelloHello",
      "['Hello', 'Hello', 'Hello']",
      "Hello Hello Hello"
    ],
    "answer": "HelloHelloHello",
    "why": "파이썬의 시퀀스 자료형(문자열, 리스트 등)에 정수를 곱하면 해당 횟수만큼 반복됩니다.",
    "hint": "곱하기는 반복입니다.",
    "trap_points": [
      "공백이 자동으로 추가되지 않음에 주의"
    ],
    "difficulty": "easy",
    "id": "0047"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 패키지를 설치할 때 특정 버전을 강제하는 기호는? (예: requests 2.25.1)",
    "answer": "==",
    "why": "pip install requests==2.25.1 처럼 사용하면 정확히 해당 버전만 설치합니다.",
    "hint": "같다(Equal)는 의미의 기호를 두 번 씁니다.",
    "trap_points": [
      ">= 는 해당 버전 이상을 의미함"
    ],
    "difficulty": "easy",
    "id": "0048"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "딕셔너리에서 키가 없으면 자동으로 기본값을 생성해 주는 클래스는?",
    "options": [
      "dict",
      "defaultdict",
      "OrderedDict",
      "Counter",
      "ChainMap"
    ],
    "answer": "defaultdict",
    "why": "collections 모듈의 defaultdict은 키가 존재하지 않을 때 KeyError를 내지 않고 설정된 팩토리 함수를 실행해 기본값을 할당합니다.",
    "hint": "기본값(Default)을 가진 딕셔너리입니다.",
    "trap_points": [
      "빈 도 작성 시 += 연산 등을 안전하게 수행하기 위해 필수적임"
    ],
    "difficulty": "medium",
    "id": "0049"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬의 'MRO (Method Resolution Order)'가 결정하는 것은?",
    "options": [
      "인터넷 요청 순서",
      "메모리 할당 순서",
      "다중 상속 시 메서드를 찾는 순서",
      "변수 선언 순서",
      "파일 읽기 순서"
    ],
    "answer": "다중 상속 시 메서드를 찾는 순서",
    "why": "C3 Linearization 알고리즘을 사용하여 부모 클래스들의 우선순위를 계층적으로 결정합니다.",
    "hint": "메서드 해결 순서의 약자입니다.",
    "trap_points": [
      "class.mro() 또는 class.__mro__를 통해 확인 가능함"
    ],
    "difficulty": "hard",
    "id": "0050"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 리스트를 슬라이싱할 때, `list[::-1]`이 의미하는 것은?",
    "options": [
      "리스트의 첫 번째 요소",
      "리스트의 마지막 요소",
      "리스트를 역순으로 뒤집은 결과",
      "리스트의 모든 짝수 번째 요소",
      "에러 발생"
    ],
    "answer": "리스트를 역순으로 뒤집은 결과",
    "why": "슬라이싱 [start:stop:step] 에서 step이 -1이면 뒤에서부터 하나씩 읽어오는 것을 뜻합니다.",
    "hint": "거꾸로 돌아가는 간격(Step)을 생각하세요.",
    "trap_points": [
      "원본은 바뀌지 않고 새 리스트를 반환함"
    ],
    "difficulty": "medium",
    "id": "0051"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 한 줄에 여러 개의 변수를 선언하고 할당하는 표현 방식(예: a, b = 1, 2)을 무엇이라 하나요?",
    "answer": "언패킹 (Unpacking) 또는 다중 할당",
    "why": "우변의 튜플 데이터를 좌변의 변수들에 나누어 담는 과정입니다.",
    "hint": "짐을 푼다는 뜻입니다.",
    "trap_points": [
      "좌변과 우변의 개수가 맞지 않으면 ValueError가 발생함"
    ],
    "difficulty": "medium",
    "id": "0052"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 '가비지 컬렉션(Garbage Collection)'의 주요 메커니즘은?",
    "options": [
      "수동으로 메모리 해제",
      "레퍼런스 카운팅 (Reference Counting) 및 순환 참조 감지",
      "하드디스크로 데이터 백업",
      "변수 이름 지우기",
      "파일 닫기"
    ],
    "answer": "레퍼런스 카운팅 (Reference Counting) 및 순환 참조 감지",
    "why": "참조 횟수가 0이 된 객체를 자동으로 삭제하며, 서로를 가리키는 순환 참조는 별도의 GC 알고리즘으로 해결합니다.",
    "hint": "참조(Reference) 횟수를 세는 방식입니다.",
    "trap_points": [
      "sys.getrefcount()로 참조 횟수를 확인할 수 있음"
    ],
    "difficulty": "hard",
    "id": "0053"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬의 `assert` 키워드의 주된 용도는?",
    "options": [
      "값을 무조건 출력하기 위해",
      "실무 코드에서 에러를 무시하기 위해",
      "특정 조건이 참인지 확인하고, 거짓이면 에러를 발생시켜 디버깅을 돕기 위해",
      "사용자로부터 입력을 받기 위해",
      "네트워크를 연결하기 위해"
    ],
    "answer": "특정 조건이 참인지 확인하고, 거짓이면 에러를 발생시켜 디버깅을 돕기 위해",
    "why": "개발 단계에서 가정(Assumption)이 맞는지 검증하여 버그를 조기에 발견하도록 돕습니다.",
    "hint": "단언하다, 확언하다의 뜻입니다.",
    "trap_points": [
      "최적화 옵션(-O) 실행 시 assert 문은 무시될 수 있으므로 비즈니스 로직 검증용으로는 부적절함"
    ],
    "difficulty": "medium",
    "id": "0054"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 클래스 정의에서 `__slots__`를 정의하는 이유는?",
    "options": [
      "속성을 더 많이 만들기 위해",
      "메모리 사용량을 줄이고 속성 접근 속도를 높이기 위해",
      "파일 속도를 빠르게 하기 위해",
      "영어로만 코딩하기 위해",
      "이미지를 저장하기 위해"
    ],
    "answer": "메모리 사용량을 줄이고 속성 접근 속도를 높이기 위해",
    "why": "객체마다 생성되는 __dict__ 를 사용하지 않고 고정된 공간을 확보하여 메모리 효율을 극대화합니다.",
    "hint": "슬롯(Slot)처럼 공간을 미리 지정합니다.",
    "trap_points": [
      "수만 개 이상의 객체를 생성할 때 효과가 매우 큼"
    ],
    "difficulty": "hard",
    "id": "0055"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 3.8부터 도입된, 표현식 내부에서 변수에 할당까지 수행하는 연산자(:=)의 별명은?",
    "answer": "바다표범 연산자 (Walrus Operator)",
    "why": ":= 기호가 바다표범의 눈과 엄니를 닮았다고 해서 붙여진 이름입니다.",
    "hint": "바다에 사는 큰 포유류 이름입니다.",
    "trap_points": [
      "조건문이나 반복문 내부에서 코드를 간결하게 줄여줌"
    ],
    "difficulty": "medium",
    "id": "0056"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 패키지를 만들 때 해당 디렉토리가 패키지임을 알리기 위해 사용하는 특수 파일명은? (최신 버전은 선택이나 과거 필수)",
    "options": [
      "__start__.py",
      "__main__.py",
      "__init__.py",
      "__pack__.py",
      "setup.py"
    ],
    "answer": "__init__.py",
    "why": "해당 디렉토리를 파이썬이 패키지로 인식하게 하며, 패키지 초기화 코드를 담기도 합니다.",
    "hint": "초기화(Initialize)를 의미하는 축약어입니다.",
    "trap_points": [
      "__main__.py는 패키지 실행 시 진입점 역할을 함"
    ],
    "difficulty": "medium",
    "id": "0057"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 3.12에서 정식 도입된, 제네릭 타입을 더 명확하게 선언하는 구문은?",
    "options": [
      "TypeVar",
      "Generic",
      "type alias (예: type Point = tuple[float, float])",
      "Union",
      "Protocol"
    ],
    "answer": "type alias (예: type Point = tuple[float, float])",
    "why": "새로운 'type' 키워드를 통해 기존의 복잡한 TypeVar와 구별되는 읽기 쉬운 타입 별칭 선언이 가능해졌습니다.",
    "hint": "종류를 뜻하는 4글자 키워드입니다.",
    "trap_points": [
      "기존에는 Point = Tuple[...] 처럼 대입문을 썼음"
    ],
    "difficulty": "hard",
    "id": "0058"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 리스트의 중복을 제거하면서 '순서'까지 유지하고 싶을 때 가장 효율적인 트릭은?",
    "options": [
      "list(set(l))",
      "dict.fromkeys(l).keys() 후에 리스트 변환",
      "for문으로 하나씩 체크",
      "sort() 수행",
      "filter() 사용"
    ],
    "answer": "dict.fromkeys(l).keys() 후에 리스트 변환",
    "why": "딕셔너리는 키의 중복을 허용하지 않으며 삽입 순서를 유지하는 특성을 활용한 관용구입니다.",
    "hint": "딕셔너리의 키(Key) 특성을 이용하세요.",
    "trap_points": [
      "set()을 쓰면 순서가 뒤섞임"
    ],
    "difficulty": "medium",
    "id": "0059"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬의 'With' 문(컨텍스트 매니저)이 내부적으로 호출하는 두 가지 매직 메서드는?",
    "options": [
      "__start__, __stop__",
      "__enter__, __exit__",
      "__open__, __close__",
      "__init__, __del__",
      "__call__, __run__"
    ],
    "answer": "__enter__, __exit__",
    "why": "enter는 준비(파일 열기 등)를, exit는 정리(파일 닫기 등)를 담당하여 자원 누수를 방지합니다.",
    "hint": "들어가고(Enter) 나가는(Exit) 동작입니다.",
    "trap_points": [
      "에러가 발생해도 exit는 반드시 실행됨"
    ],
    "difficulty": "medium",
    "id": "0060"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 문자열 포매팅 방식 중 속도가 가장 빠르고 가독성이 좋아 권장되는 방식은?",
    "options": [
      "% formatting",
      "str.format()",
      "f-string",
      "Template strings",
      "Concat(+)"
    ],
    "answer": "f-string",
    "why": "f-string(Formatted String Literals)은 런타임에 즉시 상수로 연산되어 다른 방식보다 압도적으로 빠릅니다.",
    "hint": "문자열 앞에 f를 붙입니다.",
    "trap_points": [
      "파이썬 3.6 이상부터 사용 가능함"
    ],
    "difficulty": "easy",
    "id": "0061"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬의 `collections.Counter` 객체가 주로 사용되는 상황은?",
    "options": [
      "데이터를 정렬할 때",
      "중복되지 않는 유일한 값만 저장할 때",
      "각 요소의 출현 빈도를 세고 딕셔너리 형태로 저장할 때",
      "파일을 읽어올 때",
      "인터넷 속도를 측정할 때"
    ],
    "answer": "각 요소의 출현 빈도를 세고 딕셔너리 형태로 저장할 때",
    "why": "리스트 등에서 각 항목이 몇 번 나왔는지(빈도수)를 쉽고 빠르게 계산해 줍니다.",
    "hint": "숫자를 세다(Count)라는 뜻입니다.",
    "trap_points": [
      "most_common() 메서드를 통해 상위 빈도 요소를 쉽게 추출 가능함"
    ],
    "difficulty": "easy",
    "id": "0062"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 모듈 내의 전역 변수나 함수 목록을 리스트 형태로 출력해 주는 내장 함수는?",
    "answer": "dir()",
    "why": "디렉토리(Directory)의 약자로, 객체가 가진 속성과 메서드를 확인하는 디버깅용 도구입니다.",
    "hint": "알파벳 세 글자입니다.",
    "trap_points": [
      "매개변수 없이 호출하면 현재 로컬 스코프의 변수들을 보여줌"
    ],
    "difficulty": "medium",
    "id": "0063"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 `abc` 모듈(Abstract Base Classes)을 사용하는 주된 이유는?",
    "options": [
      "알파벳 공부를 위해",
      "추상 클래스를 정의하여 자식 클래스가 반드시 특정 메서드를 구현하도록 강제하기 위해",
      "파일을 압축하기 위해",
      "코드를 암호화하기 위해",
      "네트워크 연결을 위해"
    ],
    "answer": "추상 클래스를 정의하여 자식 클래스가 반드시 특정 메서드를 구현하도록 강제하기 위해",
    "why": "대규모 프로젝트에서 인터페이스 설계를 강제하여 코드의 일관성과 안정성을 확보합니다.",
    "hint": "추상(Abstract)의 약자입니다.",
    "trap_points": [
      "추상 클래스 자체는 인스턴스를 생성할 수 없음"
    ],
    "difficulty": "hard",
    "id": "0064"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬의 `enumerate()` 함수가 반환하는 각 요소의 형태는?",
    "options": [
      "(인덱스, 값) 형태의 튜플",
      "[인덱스, 값] 형태의 리스트",
      "{인덱스: 값} 형태의 딕셔너리",
      "단순 숫자",
      "단순 문자열"
    ],
    "answer": "(인덱스, 값) 형태의 튜플",
    "why": "반복문 내에서 현재 요소가 몇 번째인지(인덱스)를 함께 다루기 위해 쓰입니다.",
    "hint": "순번과 데이터의 쌍입니다.",
    "trap_points": [
      "인덱스 시작 번호를 지정(start=1)할 수도 있음"
    ],
    "difficulty": "easy",
    "id": "0065"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 'Hello world'.title() 의 결과는?",
    "options": [
      "HELLO WORLD",
      "hello world",
      "Hello World",
      "Helloworld",
      "Hello world"
    ],
    "answer": "Hello World",
    "why": "title() 메서드는 각 단어의 첫 글자를 대문자로 변환합니다.",
    "hint": "제목처럼 각 단어 앞만 대문자로 바꿉니다.",
    "trap_points": [
      "capitalize()는 문장 전체의 맨 첫 글자만 대문자로 바꿈"
    ],
    "difficulty": "easy",
    "id": "0066"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "함수 내부에서 외부 함수(중첩 함수 상황)의 변수를 수정하려 할 때 사용하는 키워드는?",
    "answer": "nonlocal",
    "why": "global은 전역을, nonlocal은 한 단계 위 스코프의 변수를 가리킵니다.",
    "hint": "지역 변수(local)가 아니다(non)라는 뜻입니다.",
    "trap_points": [
      "전역 변수 수정 시에는 global을 써야 함"
    ],
    "difficulty": "hard",
    "id": "0067"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 여러 객체를 동시에 순회할 때 사용하는 내장 함수는?",
    "answer": "zip()",
    "why": "zip() 함수는 여러 개의 순회 가능한(iterable) 객체를 인자로 받아 각각의 요소를 튜플로 묶어서 반환합니다.",
    "hint": "지퍼처럼 두 쪽을 하나로 엮어줍니다.",
    "trap_points": [
      "enumerate()는 인덱와 값을 동시에 가져올 때 사용함"
    ],
    "difficulty": "medium",
    "id": "0068"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 리스트의 요소를 수정하지 않고 새로운 리스트를 생성하는 '리스트 컴프리헨션'의 올바른 문법은?",
    "options": [
      "list(x for x in data)",
      "[x for x in data]",
      "{x for x in data}",
      "(x for x in data)",
      "x for x in data"
    ],
    "answer": "[x for x in data]",
    "why": "리스트 컴프리헨션은 대괄호 [] 내부에 반복문과 조건을 사용하여 간결하게 새로운 리스트를 생성하는 파이썬의 핵심 문법입니다.",
    "hint": "리스트(List)는 어떤 괄호를 쓰나요?",
    "trap_points": [
      "()는 제네레이터, {}는 세트/딕셔너리 컴프리헨션임"
    ],
    "difficulty": "easy",
    "id": "0069"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 클래스에서 인스턴스 자신을 가리키는 첫 번째 매개변수의 관례적 명칭은?",
    "options": [
      "this",
      "cls",
      "self",
      "me",
      "base"
    ],
    "answer": "self",
    "why": "파이썬의 메서드는 첫 번째 인자로 인스턴스 자신을 받도록 설계되어 있으며, 이를 'self'라고 명명하는 것이 PEP8 권장 사항입니다.",
    "hint": "S로 시작하는 4글자 단어입니다.",
    "trap_points": [
      "Java의 this와 혼동 주의"
    ],
    "difficulty": "easy",
    "id": "0070"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 `__getitem__` 메서드를 구현하면 어떤 기능을 사용할 수 있게 되나요?",
    "options": [
      "객체를 함수처럼 호출하기",
      "객체를 리스트처럼 인덱싱(obj[0])하거나 슬라이싱하기",
      "객체를 삭제하기",
      "객체를 더하기",
      "객체를 문자열로 바꾸기"
    ],
    "answer": "객체를 리스트처럼 인덱싱(obj[0])하거나 슬라이싱하기",
    "why": "파이썬의 '덕 타이핑' 원리에 따라 특정 매직 메서드를 구현하면 내장 타입처럼 동작하게 됩니다.",
    "hint": "아이템(Item)을 가져오는(Get) 방법입니다.",
    "trap_points": [
      "슬라이싱 객체도 인자로 들어올 수 있음에 주의"
    ],
    "difficulty": "hard",
    "id": "0071"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 두 개의 세트(Set) A, B에 대해 `A | B` 연산이 수행하는 것은?",
    "options": [
      "교집합",
      "합집합",
      "차집합",
      "대칭 차집합",
      "곱집합"
    ],
    "answer": "합집합",
    "why": "| 기호는 유니온(Union)을 의미하며 두 세트의 모든 요소를 합칩니다.",
    "hint": "전체를 합칩니다.",
    "trap_points": [
      "교집합은 & 기호를 사용함"
    ],
    "difficulty": "easy",
    "id": "0072"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 `sys.argv` 리스트의 첫 번째 요소(index 0)에 담기는 값은?",
    "options": [
      "첫 번째 인자",
      "실행된 파이썬 스크립트의 파일명",
      "사용자 이름",
      "현재 시간",
      "파이썬 버전"
    ],
    "answer": "실행된 파이썬 스크립트의 파일명",
    "why": "명령행 인자 중 0번은 항상 실행 주체인 스크립트 경로를 담고 있습니다.",
    "hint": "파일 이름입니다.",
    "trap_points": [
      "실제 인자는 1번 인덱스부터 시작함"
    ],
    "difficulty": "medium",
    "id": "0073"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 리스트를 정렬할 때 `key=len` 옵션을 주면 어떤 기준으로 정렬되나요?",
    "options": [
      "알파벳 순서",
      "요소의 길이 순서",
      "나중 추가된 순서",
      "랜덤 순서",
      "숫자 크기"
    ],
    "answer": "요소의 길이 순서",
    "why": "key 인자에 함수를 전달하면 각 요소에 해당 함수를 적용한 결과를 기준으로 정렬합니다.",
    "hint": "길이(Length)를 기준으로 합니다.",
    "trap_points": [
      "len은 내장 함수이므로 () 없이 이름만 전달함"
    ],
    "difficulty": "medium",
    "id": "0074"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 이미 불러온 모듈을 소스 코드 변경 후 다시 불러오고 싶을 때 사용하는 `importlib`의 함수는?",
    "answer": "reload()",
    "why": "인터프리터를 끄지 않고도 변경된 모듈의 내용을 반영할 수 있게 해줍니다.",
    "hint": "다시(Re) 부르다(Load).",
    "trap_points": [
      "대화형 셸(IPython 등) 환경에서 매우 자주 쓰임"
    ],
    "difficulty": "hard",
    "id": "0075"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 리스트의 가장 마지막 요소를 꺼내고 리스트에서 삭제하는 메서드는?",
    "options": [
      "list.pop()",
      "list.remove()",
      "list.delete()",
      "list.clear()",
      "list.extract()"
    ],
    "answer": "list.pop()",
    "why": "pop()은 인덱스를 지정하지 않으면 마지막 요소를 반환하고 리스트에서 제거합니다.",
    "hint": "튀어나오다(Pop)라는 뜻입니다.",
    "trap_points": [
      "remove()는 값을 찾아 삭제하며 반환값은 없습니다."
    ],
    "difficulty": "easy",
    "id": "0076"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 3에서 두 정수의 나눗셈 결과(예: 5 / 2)의 기본 자료형은?",
    "options": [
      "int",
      "float",
      "complex",
      "str",
      "bool"
    ],
    "answer": "float",
    "why": "파이썬 3에서는 / 연산 시 결과가 정수로 나누어떨어지더라도 항상 실수(float)를 반환합니다.",
    "hint": "정수가 아닌 '실수'가 나옵니다.",
    "trap_points": [
      "정수 몫을 구하려면 // 연산자를 써야 함"
    ],
    "difficulty": "medium",
    "id": "0077"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "클래스 내부에서 속성 이름 앞에 __ (언더바 두 개)를 붙여 이름 충돌을 방지하는 기능을 무엇이라 하나요?",
    "options": [
      "Encapsulation",
      "Name Mangling",
      "Decorating",
      "Abstracting",
      "Overriding"
    ],
    "answer": "Name Mangling",
    "why": "파이썬은 완전한 private을 지원하지 않지만, __를 쓰면 '_클래스명__속성명'으로 이름을 바꿔 외부 접근을 어렵게 만듭니다.",
    "hint": "이름을 으깨거나 훼손한다는 뜻입니다.",
    "trap_points": [
      "_ (언더바 하나)는 관례적인 내부용 표시임"
    ],
    "difficulty": "hard",
    "id": "0078"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "딕셔너리에서 모든 '값(Value)'들만 모아서 반환하는 메서드는?",
    "options": [
      "keys()",
      "items()",
      "values()",
      "data()",
      "get_all()"
    ],
    "answer": "values()",
    "why": "values()는 딕셔너리에 저장된 모든 값들의 뷰(view)를 반환합니다.",
    "hint": "값의 영어 단어입니다.",
    "trap_points": [
      "items()는 키와 값의 쌍(Tuple)을 반환함"
    ],
    "difficulty": "easy",
    "id": "0079"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "함수에서 반환값이 없을 때 파이썬이 기본적으로 반환하는 객체는?",
    "options": [
      "False",
      "0",
      "None",
      "Empty string",
      "Error"
    ],
    "answer": "None",
    "why": "파이썬의 모든 함수는 return이 없거나 return만 있을 경우 '값 없음'을 뜻하는 None을 반환합니다.",
    "hint": "아무것도 없음이라는 뜻의 객체입니다.",
    "trap_points": [
      "C언어의 void와 유사한 개념임"
    ],
    "difficulty": "easy",
    "id": "0080"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 모듈을 불러온 후 `as` 키워드를 사용하여 짧은 이름으로 별명을 붙이는 행위를 무엇이라 하나요?",
    "answer": "Aliasing (에일리어싱/별칭 지정)",
    "why": "`import pandas as pd` 처럼 긴 이름을 줄여서 코딩 효율을 높입니다.",
    "hint": "별명(Alias)을 지어주는 것입니다.",
    "trap_points": [
      "pd, np 같은 별명은 커뮤니티의 관례임"
    ],
    "difficulty": "easy",
    "id": "0081"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬의 'Global Interpreter Lock (GIL)'에 대한 설명으로 옳은 것은?",
    "options": [
      "멀티 프로세싱을 방해한다.",
      "하나의 스레드만 인터프리터를 점유하게 하여 멀티 스레딩의 효율을 제한한다.",
      "메모리를 자동으로 해제해준다.",
      "코드를 암호화한다.",
      "인터넷 속도를 제한한다."
    ],
    "answer": "하나의 스레드만 인터프리터를 점유하게 하여 멀티 스레딩의 효율을 제한한다.",
    "why": "GIL은 파이썬 객체에 대한 참조 카운트의 동기화를 위해 한 시점에 하나의 스레드만 실행되도록 보장합니다.",
    "hint": "전역(Global) 인터프리터 잠금(Lock)입니다.",
    "trap_points": [
      "CPU 위주의 멀티 스레드 작업에서 성능 병목의 원인이 됨"
    ],
    "difficulty": "hard",
    "id": "0082"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파일 입출력 시, 기존 내용을 지우지 않고 끝에 덧붙여 쓰기 위해 사용하는 모드는?",
    "options": [
      "'r'",
      "'w'",
      "'a'",
      "'x'",
      "'b'"
    ],
    "answer": "'a'",
    "why": "append의 약자인 'a' 모드는 파일 포인터를 끝으로 이동시켜 내용을 추가합니다.",
    "hint": "추가하다(Append)의 첫 글자입니다.",
    "trap_points": [
      "'w' 모드는 기존 내용을 모두 삭제하고 새로 씀"
    ],
    "difficulty": "easy",
    "id": "0083"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 문자열 내의 특정 문자를 다른 문자로 치환하는 메서드는?",
    "options": [
      "change()",
      "modify()",
      "replace()",
      "swap()",
      "convert()"
    ],
    "answer": "replace()",
    "why": "replace('기존', '신규')는 모든 발생 지점을 찾아 새 문자열로 바꿉니다.",
    "hint": "교체하다라는 뜻입니다.",
    "trap_points": [
      "원본 문자열은 불변(Immutable)이므로 결과값을 새 변수에 담아야 함"
    ],
    "difficulty": "easy",
    "id": "0084"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "컴퓨터 사양을 넘어서는 큰 정수를 파이썬은 어떻게 처리하나요?",
    "answer": "메모리가 허용하는 한 무제한으로 지원함",
    "why": "파이썬 3의 int 자료형은 임의 정밀도(Arbitrary Precision)를 지원하여 오버플로우가 발생하지 않습니다.",
    "hint": "제약이 거의 없다고 보시면 됩니다.",
    "trap_points": [
      "C언어 등에서 발생하는 오버플로우 걱정이 없음"
    ],
    "difficulty": "medium",
    "id": "0085"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 두 리스트의 요소를 짝지어 묶어주는 zip() 함수의 반환 객체 타입은?",
    "options": [
      "List",
      "Tuple",
      "Dictionary",
      "Zip object (Iterator)",
      "Array"
    ],
    "answer": "Zip object (Iterator)",
    "why": "zip()은 이터레이터를 반환하며, 실제 리스트로 확인하려면 list(zip(...))과 같이 변환이 필요합니다.",
    "hint": "메모리 효율을 위해 바로 리스트를 만들지 않고 '반복자'를 반환합니다.",
    "trap_points": [
      "반환값이 바로 리스트라고 생각하면 오답임"
    ],
    "difficulty": "medium",
    "id": "0086"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "다음 중 파일의 모든 내용을 문자열 하나로 읽어오는 메서드는?",
    "options": [
      "f.read()",
      "f.readline()",
      "f.readlines()",
      "f.get_all()",
      "f.fetch()"
    ],
    "answer": "f.read()",
    "why": "read()는 파일 전체 내용을 하나의 문자열로, readlines()는 각 줄을 리스트로 읽어옵니다.",
    "hint": "가장 기본적인 읽기 메서드입니다.",
    "trap_points": [
      "readline()은 한 줄씩만 읽어옴"
    ],
    "difficulty": "easy",
    "id": "0087"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 'None'을 확인하기 위해 가장 권장되는 비교 연산자는?",
    "options": [
      "==",
      "!=",
      "is",
      "is not None",
      "in"
    ],
    "answer": "is",
    "why": "None은 싱글턴 객체이므로 값의 동등성(==)보다 객체 동일성(is)을 검사하는 것이 관례이며 안전합니다.",
    "hint": "객체 자체가 동일한지 묻는 키워드입니다.",
    "trap_points": [
      "== None도 작동은 하지만 PEP8에서는 is None을 권장함"
    ],
    "difficulty": "medium",
    "id": "0088"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "인스턴스 생성 없이 클래스 이름을 통해 바로 호출하며 클래스 상태를 수정하지 않는 메서드 타입은?",
    "options": [
      "Instance method",
      "Class method",
      "Static method",
      "Abstract method",
      "Virtual method"
    ],
    "answer": "Static method",
    "why": "static method는 클래스(@staticmethod)에 속해 있지만 인스턴스나 클래스 인자를 받지 않는 단순 유틸리티용 메서드입니다.",
    "hint": "정적인(Static) 성격의 메서드입니다.",
    "trap_points": [
      "@classmethod는 cls 인자를 받아 클래스 상태에 접근 가능함"
    ],
    "difficulty": "hard",
    "id": "0089"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 자료구조 중 내부 데이터가 순서대로 유지되지 않으며 유일한 값만 저장하는 것은?",
    "options": [
      "List",
      "Tuple",
      "Set",
      "Dictionary",
      "Deque"
    ],
    "answer": "Set",
    "why": "셋(Set)은 요소의 순서를 무시하고 중복을 제거하여 저장하는 해시 기반 자료구조입니다.",
    "hint": "수학의 집합과 같습니다.",
    "trap_points": [
      "최근 파이썬의 딕셔너리는 삽입 순서를 유지하지만 셋은 그렇지 않음"
    ],
    "difficulty": "medium",
    "id": "0090"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 모듈을 불러올 때 사용하는 키워드는?",
    "answer": "import",
    "why": "다른 .py 파일이나 라이브러리의 기능을 현재 파일로 가져올 때 사용합니다.",
    "hint": "수입하다라는 뜻의 영어 단어입니다.",
    "trap_points": [
      "from ... import ... 형식을 쓰면 특정 기능만 가져올 수 있음"
    ],
    "difficulty": "easy",
    "id": "0091"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬의 'lambda' 함수에 대한 설명으로 옳은 것은?",
    "options": [
      "여러 줄의 복잡한 로직을 작성할 수 있다.",
      "이름이 없는 한 줄짜리 익명 함수이다.",
      "반드시 return 키워드를 써야 한다.",
      "클래스 내부에서만 정의할 수 있다.",
      "재귀 호출이 불가능하다."
    ],
    "answer": "이름이 없는 한 줄짜리 익명 함수이다.",
    "why": "람다는 이름 없이 식(Expression) 형태로 간단한 연산을 수행하는 함수를 만들 때 씁니다.",
    "hint": "익명을 뜻하는 그리스어 문자를 생각하세요.",
    "trap_points": [
      "복잡한 로직은 def로 정의하는 것이 가독성에 좋음"
    ],
    "difficulty": "medium",
    "id": "0092"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 내장함수 중 모든 요소가 참(True)인지 검사하는 함수는?",
    "options": [
      "any()",
      "all()",
      "every()",
      "check()",
      "sum()"
    ],
    "answer": "all()",
    "why": "all()은 모든 요소가 Truthy할 때만 True를 반환합니다.",
    "hint": "모두를 뜻하는 단어입니다.",
    "trap_points": [
      "any()는 하나라도 참이면 True를 반환함"
    ],
    "difficulty": "medium",
    "id": "0093"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "문자열을 특정 구분자를 기준으로 리스트로 나누는 메서드는?",
    "options": [
      "join()",
      "split()",
      "partition()",
      "divide()",
      "cut()"
    ],
    "answer": "split()",
    "why": "split()은 공백이나 지정된 문자를 기준으로 문자열을 잘라 리스트로 만듭니다.",
    "hint": "쪼개다라는 뜻입니다.",
    "trap_points": [
      "join()은 리스트를 문자열로 합치는 반대 기능임"
    ],
    "difficulty": "easy",
    "id": "0094"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 객체의 메모리 주소가 동일한지(동일성) 비교하는 연산자는?",
    "answer": "is",
    "why": "is 연산자는 두 객체가 실제로 메모리상에서 같은 위치를 점유하고 있는지 확인합니다.",
    "hint": "~이다라는 뜻의 영어 단어입니다.",
    "trap_points": [
      "==는 값이 같은지(동등성)를 비교함"
    ],
    "difficulty": "medium",
    "id": "0095"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 두 변수가 같은 '객체'를 가리키는지(메모리 주소 비교) 확인하는 연산자는?",
    "options": [
      "==",
      "is",
      "is same",
      "id()",
      "equals"
    ],
    "answer": "is",
    "why": "==는 값(Value)을 비교하고, is는 참조(Identity)를 비교합니다.",
    "hint": "존재 자체를 묻는 단어입니다.",
    "trap_points": [
      "작은 정수나 짧은 문자열은 인턴(Intern)되어 is 결과가 True일 수 있어 주의가 필요함"
    ],
    "difficulty": "medium",
    "id": "0096"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬의 `copy.deepcopy()` 를 사용해야 하는 상황은?",
    "options": [
      "단순 변수 대입 시",
      "리스트 내부에 리스트나 딕셔너리 같은 가변 객체가 중첩되어 있을 때",
      "영어로만 코딩할 때",
      "파일을 읽을 때",
      "인터넷이 빠를 때"
    ],
    "answer": "리스트 내부에 리스트나 딕셔너리 같은 가변 객체가 중첩되어 있을 때",
    "why": "일반 복사(shallow copy)는 내부 객체의 주소만 복사되어 원본 수정 시 복사본도 함께 바뀌는 문제가 발생하기 때문입니다.",
    "hint": "깊숙한 곳(Deep)까지 복사합니다.",
    "trap_points": [
      "메모리 사용량이 늘어나므로 필요한 경우에만 신중히 써야 함"
    ],
    "difficulty": "medium",
    "id": "0097"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 `asyncio`에서 여러 코루틴을 동시에 실행하고 모두 끝날 때까지 기다리는 함수는?",
    "options": [
      "asyncio.run()",
      "asyncio.gather()",
      "asyncio.wait_all()",
      "asyncio.join()",
      "asyncio.start()"
    ],
    "answer": "asyncio.gather()",
    "why": "gather()는 여러 비동기 작업을 묶어서 병렬로 처리하고 결과를 리스트로 반환합니다.",
    "hint": "모으다(Gather)라는 뜻입니다.",
    "trap_points": [
      "하나가 에러 나면 전체가 중단되거나 별도 처리가 필요함"
    ],
    "difficulty": "hard",
    "id": "0098"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬에서 '익명 함수'를 만들 때 사용하는 키워드는?",
    "options": [
      "def",
      "function",
      "lambda",
      "inline",
      "anon"
    ],
    "answer": "lambda",
    "why": "이름 없이 한 줄로 표현되는 함수로, 일시적인 로직이나 고차 함수의 인자로 쓰입니다.",
    "hint": "그리스 문자 이름입니다.",
    "trap_points": [
      "복잡한 로직은 일반 def 함수를 쓰는 것이 가독성에 좋음"
    ],
    "difficulty": "easy",
    "id": "0099"
  },
  {
    "chapter_name": "Python 기초",
    "type": "객관식",
    "question": "파이썬 `try-except-else-finally` 문에서 에러 유무와 상관없이 '항상' 실행되는 블록은?",
    "options": [
      "try",
      "except",
      "else",
      "finally",
      "catch"
    ],
    "answer": "finally",
    "why": "주로 파일 닫기나 DB 연결 해제 등 마무리(Clean-up) 작업을 보장하기 위해 사용합니다.",
    "hint": "마지막으로(Finally)라는 뜻입니다.",
    "trap_points": [
      "return 문이 있어도 finally 블록은 실행되고 나감"
    ],
    "difficulty": "medium",
    "id": "0100"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "리스트 [1, 2, 3]을 [2, 4, 6]으로 만드는 리스트 컴프리헨션을 완성하세요. (문제 1)",
    "options": [],
    "answer": "[x * 2 for x in [1, 2, 3]]",
    "why": "리스트 컴프리헨션은 리스트 생성을 위한 간결한 문법입니다.",
    "difficulty": "hard",
    "id": "0101"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "딕셔너리 d = {'a': 1}에서 키 'b'가 없으면 0을 반환하는 코드는? (문제 2)",
    "options": [],
    "answer": "d.get('b', 0)",
    "why": "get 메서드는 키가 없을 때 기본값을 반환합니다.",
    "difficulty": "medium",
    "id": "0102"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "문자열 s = 'hello'를 거꾸로 출력하는 슬라이싱 코드는? (문제 3)",
    "options": [],
    "answer": "s[::-1]",
    "why": "스텝을 -1로 주면 역순으로 슬라이싱됩니다.",
    "difficulty": "hard",
    "id": "0103"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "리스트 [1, 2, 3]을 [2, 4, 6]으로 만드는 리스트 컴프리헨션을 완성하세요. (문제 4)",
    "options": [],
    "answer": "[x * 2 for x in [1, 2, 3]]",
    "why": "리스트 컴프리헨션은 리스트 생성을 위한 간결한 문법입니다.",
    "difficulty": "medium",
    "id": "0104"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "딕셔너리 d = {'a': 1}에서 키 'b'가 없으면 0을 반환하는 코드는? (문제 5)",
    "options": [],
    "answer": "d.get('b', 0)",
    "why": "get 메서드는 키가 없을 때 기본값을 반환합니다.",
    "difficulty": "hard",
    "id": "0105"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "문자열 s = 'hello'를 거꾸로 출력하는 슬라이싱 코드는? (문제 6)",
    "options": [],
    "answer": "s[::-1]",
    "why": "스텝을 -1로 주면 역순으로 슬라이싱됩니다.",
    "difficulty": "medium",
    "id": "0106"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "리스트 [1, 2, 3]을 [2, 4, 6]으로 만드는 리스트 컴프리헨션을 완성하세요. (문제 7)",
    "options": [],
    "answer": "[x * 2 for x in [1, 2, 3]]",
    "why": "리스트 컴프리헨션은 리스트 생성을 위한 간결한 문법입니다.",
    "difficulty": "hard",
    "id": "0107"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "딕셔너리 d = {'a': 1}에서 키 'b'가 없으면 0을 반환하는 코드는? (문제 8)",
    "options": [],
    "answer": "d.get('b', 0)",
    "why": "get 메서드는 키가 없을 때 기본값을 반환합니다.",
    "difficulty": "medium",
    "id": "0108"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "문자열 s = 'hello'를 거꾸로 출력하는 슬라이싱 코드는? (문제 9)",
    "options": [],
    "answer": "s[::-1]",
    "why": "스텝을 -1로 주면 역순으로 슬라이싱됩니다.",
    "difficulty": "hard",
    "id": "0109"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "리스트 [1, 2, 3]을 [2, 4, 6]으로 만드는 리스트 컴프리헨션을 완성하세요. (문제 10)",
    "options": [],
    "answer": "[x * 2 for x in [1, 2, 3]]",
    "why": "리스트 컴프리헨션은 리스트 생성을 위한 간결한 문법입니다.",
    "difficulty": "medium",
    "id": "0110"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "딕셔너리 d = {'a': 1}에서 키 'b'가 없으면 0을 반환하는 코드는? (문제 11)",
    "options": [],
    "answer": "d.get('b', 0)",
    "why": "get 메서드는 키가 없을 때 기본값을 반환합니다.",
    "difficulty": "hard",
    "id": "0111"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "문자열 s = 'hello'를 거꾸로 출력하는 슬라이싱 코드는? (문제 12)",
    "options": [],
    "answer": "s[::-1]",
    "why": "스텝을 -1로 주면 역순으로 슬라이싱됩니다.",
    "difficulty": "medium",
    "id": "0112"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "리스트 [1, 2, 3]을 [2, 4, 6]으로 만드는 리스트 컴프리헨션을 완성하세요. (문제 13)",
    "options": [],
    "answer": "[x * 2 for x in [1, 2, 3]]",
    "why": "리스트 컴프리헨션은 리스트 생성을 위한 간결한 문법입니다.",
    "difficulty": "hard",
    "id": "0113"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "딕셔너리 d = {'a': 1}에서 키 'b'가 없으면 0을 반환하는 코드는? (문제 14)",
    "options": [],
    "answer": "d.get('b', 0)",
    "why": "get 메서드는 키가 없을 때 기본값을 반환합니다.",
    "difficulty": "medium",
    "id": "0114"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "문자열 s = 'hello'를 거꾸로 출력하는 슬라이싱 코드는? (문제 15)",
    "options": [],
    "answer": "s[::-1]",
    "why": "스텝을 -1로 주면 역순으로 슬라이싱됩니다.",
    "difficulty": "hard",
    "id": "0115"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "리스트 [1, 2, 3]을 [2, 4, 6]으로 만드는 리스트 컴프리헨션을 완성하세요. (문제 16)",
    "options": [],
    "answer": "[x * 2 for x in [1, 2, 3]]",
    "why": "리스트 컴프리헨션은 리스트 생성을 위한 간결한 문법입니다.",
    "difficulty": "medium",
    "id": "0116"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "딕셔너리 d = {'a': 1}에서 키 'b'가 없으면 0을 반환하는 코드는? (문제 17)",
    "options": [],
    "answer": "d.get('b', 0)",
    "why": "get 메서드는 키가 없을 때 기본값을 반환합니다.",
    "difficulty": "hard",
    "id": "0117"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "문자열 s = 'hello'를 거꾸로 출력하는 슬라이싱 코드는? (문제 18)",
    "options": [],
    "answer": "s[::-1]",
    "why": "스텝을 -1로 주면 역순으로 슬라이싱됩니다.",
    "difficulty": "medium",
    "id": "0118"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "리스트 [1, 2, 3]을 [2, 4, 6]으로 만드는 리스트 컴프리헨션을 완성하세요. (문제 19)",
    "options": [],
    "answer": "[x * 2 for x in [1, 2, 3]]",
    "why": "리스트 컴프리헨션은 리스트 생성을 위한 간결한 문법입니다.",
    "difficulty": "hard",
    "id": "0119"
  },
  {
    "chapter_name": "Python 기초",
    "type": "코드 완성형",
    "question": "딕셔너리 d = {'a': 1}에서 키 'b'가 없으면 0을 반환하는 코드는? (문제 20)",
    "options": [],
    "answer": "d.get('b', 0)",
    "why": "get 메서드는 키가 없을 때 기본값을 반환합니다.",
    "difficulty": "medium",
    "id": "0120"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 시계렬 데이터의 날짜 주기를 변환할 때(예: 월별 -> 분기별) 사용하는 메서드는?",
    "options": [
      "reshape()",
      "asfreq()",
      "resample()",
      "reindex()",
      "change()"
    ],
    "answer": "resample()",
    "why": "resample()은 시계렬 데이터의 빈도를 높이거나 낮출 때 집계 함수와 함께 쓰입니다.",
    "hint": "샘플링(Sampling)을 다시(Re) 합니다.",
    "trap_points": [
      "합계(sum)나 평균(mean) 등을 꼭 붙여줘야 데이터가 생성됨"
    ],
    "difficulty": "medium",
    "id": "0121"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Numpy에서 행렬을 1차원 리스트(평면)로 펼치는 메서드는?",
    "options": [
      "flatten()",
      "expand()",
      "stretch()",
      "unfold()",
      "open()"
    ],
    "answer": "flatten()",
    "why": "다차원 배열의 모든 요소를 하나의 긴 1차원 배열로 만들어줍니다.",
    "hint": "납작하게(Flat) 만든다는 뜻입니다.",
    "trap_points": [
      "ravel()과 유사하지만 flatten()은 항상 복사본을 반환함"
    ],
    "difficulty": "medium",
    "id": "0122"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "정규표현식에서 '앞선 패턴과 일치하지 않는 가장 가까운 문자'까지 매칭하는 'Non-greedy' 기호는?",
    "options": [
      "*",
      "+",
      "?",
      "!",
      "^"
    ],
    "answer": "?",
    "why": "*? 또는 +? 처럼 사용하면 최대한 짧은 매칭 결과를 찾습니다.",
    "hint": "물음표입니다.",
    "trap_points": [
      "기본적으로 정규표현식은 가장 길게 찾으려는 'Greedy' 성질을 가짐"
    ],
    "difficulty": "hard",
    "id": "0123"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 데이터프레임의 모든 요소에 '사용자 정의 함수'를 일괄 적용하는 메서드는?",
    "options": [
      "apply()",
      "applymap()",
      "map()",
      "run()",
      "execute()"
    ],
    "answer": "applymap()",
    "why": "데이터프레임 전체의 각 요소(element-wise)에 함수를 적용하려면 applymap()을 씁니다.",
    "hint": "맵(Map)을 전체에 적용(Apply)합니다.",
    "trap_points": [
      "Series에는 map(), 열/행 단위에는 apply()를 사용함"
    ],
    "difficulty": "hard",
    "id": "0124"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Scikit-learn에서 범주형 데이터를 수치로 변환할 때, 각 카테고리를 독립된 0과 1의 열로 만드는 방식은?",
    "options": [
      "Label Encoding",
      "One-hot Encoding",
      "Binary Encoding",
      "Manual Map",
      "Scaling"
    ],
    "answer": "One-hot Encoding",
    "why": "카테고리 간의 순위나 거리 정보가 없을 때 모델의 왜곡을 방지하기 위해 사용합니다.",
    "hint": "단 하나만 핫(1)하다는 뜻입니다.",
    "trap_points": [
      "카테고리가 너무 많으면 차원의 저주(Dimension Curse)가 발생할 수 있음"
    ],
    "difficulty": "medium",
    "id": "0125"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 분석 시 두 변수 사이의 관계를 점으로 표현하여 추세나 상관성을 확인하는 차트 이름은?",
    "answer": "산점도 (Scatter Plot)",
    "why": "데이터가 퍼져(Scatter) 있는 양상을 보고 상관계수를 짐작할 수 있습니다.",
    "hint": "흩뿌려진 점들의 그림입니다.",
    "trap_points": [
      "데이터가 너무 많으면 점들이 겹쳐서 투명도(alpha) 조절이 필요함"
    ],
    "difficulty": "easy",
    "id": "0126"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 '연도-월-일 시:분:초' 형태의 문자열을 실제 Datetime 객체로 변환하는 함수는?",
    "options": [
      "pd.as_datetime()",
      "pd.to_datetime()",
      "pd.convert_date()",
      "pd.date_format()",
      "pd.make_time()"
    ],
    "answer": "pd.to_datetime()",
    "why": "문자열 데이터를 시계렬 분석이 가능한 특수 타입으로 변환하는 필수 함수입니다.",
    "hint": "데이트타임(datetime)으로(to).",
    "trap_points": [
      "데이터 형식이 불규칙하면 errors='coerce' 옵션을 주어 안전하게 처리함"
    ],
    "difficulty": "easy",
    "id": "0127"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Numpy에서 행렬 연산 중 주대각선 성분(Main Diagonal)만 추출하는 함수는?",
    "options": [
      "np.diag()",
      "np.center()",
      "np.middle()",
      "np.cross()",
      "np.line()"
    ],
    "answer": "np.diag()",
    "why": "대각행렬을 만들거나 기존 행렬에서 대각선 값만 뽑아낼 때 사용합니다.",
    "hint": "대각선(Diagonal)의 줄임말입니다.",
    "trap_points": [
      "1차원 배열을 넣으면 이를 대각선으로 하는 2차원 행렬을 만들어줌"
    ],
    "difficulty": "medium",
    "id": "0128"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "결측치(Missing Value)를 앞의 데이터나 뒤의 데이터로 채우는 Pandas의 메서드 옵션은?",
    "options": [
      "ffill / bfill",
      "start / end",
      "pre / post",
      "up / down",
      "left / right"
    ],
    "answer": "ffill / bfill",
    "why": "forward fill은 앞의 값을, backward fill은 뒤의 값을 가져와 빈칸을 채웁니다.",
    "hint": "앞(Forward)과 뒤(Backward)의 약자입니다.",
    "trap_points": [
      "시계렬 데이터에서 관성적 값을 채울 때 매우 유용함"
    ],
    "difficulty": "medium",
    "id": "0129"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 데이터프레임의 인덱스 정보를 '0, 1, 2...' 로 초기화하면서 기존 인덱스를 삭제하는 코드는?",
    "answer": "df.reset_index(drop=True)",
    "why": "drop=True를 주지 않으면 기존 인덱스가 일반 열(column)로 이동하게 됩니다.",
    "hint": "버리다(Drop)는 옵션을 추가합니다.",
    "trap_points": [
      "분석 결과 전처리 시 행 번호를 맞추기 위해 자주 쓰임"
    ],
    "difficulty": "medium",
    "id": "0130"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 데이터프레임의 인덱스를 무시하고 단순 수직 결합(Append)할 때 사용하는 함수는?",
    "options": [
      "pd.merge()",
      "pd.concat()",
      "pd.join()",
      "pd.update()",
      "pd.bind()"
    ],
    "answer": "pd.concat()",
    "why": "concat()은 리스트에 담긴 여러 데이터프레임을 축(axis)을 따라 이어 붙입니다.",
    "hint": "연결하다(Concatenate)의 약자입니다.",
    "trap_points": [
      "merge()는 공통 키(Key)를 기준으로 합치는 'JOIN' 연산임"
    ],
    "difficulty": "medium",
    "id": "0131"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Numpy에서 특정 범위 내의 숫자를 '균등한 간격'으로 지정된 개수만큼 생성하는 함수는?",
    "options": [
      "np.arange()",
      "np.linspace()",
      "np.random.normal()",
      "np.full()",
      "np.eye()"
    ],
    "answer": "np.linspace()",
    "why": "linspace(0, 10, 5)는 0부터 10 사이를 5개로 쪼개어 [0, 2.5, 5, 7.5, 10] 을 만듭니다.",
    "hint": "선형(Linear) 공간(Space)입니다.",
    "trap_points": [
      "arange는 간격을 지정하고, linspace는 개수를 지정함"
    ],
    "difficulty": "hard",
    "id": "0132"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터의 왜도(Skewness)가 심할 때, 정규분포에 가깝게 만들기 위해 자주 사용하는 변환 기법은?",
    "options": [
      "Linear Scaling",
      "Log Transformation (로그 변환)",
      "Squaring",
      "Rounding",
      "Negative Filter"
    ],
    "answer": "Log Transformation (로그 변환)",
    "why": "로그 변환은 큰 수치들의 차이를 좁혀주어 데이터의 편향을 완화하는 데 매우 효과적입니다.",
    "hint": "로그(log) 함를 적용합니다.",
    "trap_points": [
      "데이터에 0이나 음수가 있으면 바로 적용할 수 없음(Shift 필요)"
    ],
    "difficulty": "medium",
    "id": "0133"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Scikit-learn에서 기계학습 모델의 성능을 교차 검증하기 위해 사용하는 클래스는?",
    "options": [
      "LinearRegression",
      "KFold",
      "StandardScaler",
      "PCA",
      "MinMaxScaler"
    ],
    "answer": "KFold",
    "why": "KFold는 데이터를 K개의 그룹으로 나누어 학습과 검증을 번갈아 수행하며 일반화 성능을 높입니다.",
    "hint": "접다(Fold)는 뜻의 단어가 포함됩니다.",
    "trap_points": [
      "검증 데이터셋이 고정되었을 때 발생하는 편향을 방지함"
    ],
    "difficulty": "medium",
    "id": "0134"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 분석 시 이상치(Outlier)를 탐지하는 가장 고전적이면서 강력한 기준은?",
    "options": [
      "평균값 기준",
      "IQR (Interquartile Range) 기준",
      "최대값 기준",
      "데이터 개수 기준",
      "알파벳 순서 기준"
    ],
    "answer": "IQR (Interquartile Range) 기준",
    "why": "사분위수 범위(IQR)에서 1.5배 이상 벗어난 값을 이상치로 정의하는 'Whiskers' 방식을 널리 씁니다.",
    "hint": "25% ~ 75%의 범위를 의미합니다.",
    "trap_points": [
      "박스 플롯(Box Plot)의 원리가 되는 수치임"
    ],
    "difficulty": "hard",
    "id": "0135"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 데이터프레임의 특정 열에서만 유일한(Unique) 값들을 리스트 형태로 가져오는 메서드는?",
    "answer": "unique()",
    "why": "중복을 제거한 고유 값들을 넘파이 배열 형태로 반환합니다.",
    "hint": "유일하다라는 뜻입니다.",
    "trap_points": [
      "Series 객체에서만 바로 사용 가능함 (데이터프레임 전체 X)"
    ],
    "difficulty": "easy",
    "id": "0136"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "정규표현식에서 `[a-zA-Z0-9_]` 와 완벽히 동일한 의미를 가진 특수 문자는?",
    "options": [
      "\\d",
      "\\s",
      "\\w",
      "\\b",
      "\\A"
    ],
    "answer": "\\w",
    "why": "\\w는 'word character'의 약자로 영문자, 숫자, 언더바를 포함합니다.",
    "hint": "단어(Word)를 구성하는 문자입니다.",
    "trap_points": [
      "공백은 포함하지 않음에 주의"
    ],
    "difficulty": "medium",
    "id": "0137"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Matplotlib에서 차트의 제목을 설정하는 함수는?",
    "options": [
      "plt.name()",
      "plt.title()",
      "plt.header()",
      "plt.label()",
      "plt.write()"
    ],
    "answer": "plt.title()",
    "why": "title() 함수는 차트 상단에 메인 제목을 표기합니다.",
    "hint": "제목을 뜻하는 영어 단어입니다.",
    "trap_points": [
      "한글 폰트 설정이 되어있지 않으면 한글 제목이 깨질 수 있음"
    ],
    "difficulty": "easy",
    "id": "0138"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "두 확률 변수 사이의 선형적 관계 강도를 측정하며 -1에서 1 사이의 값을 가지는 지표는?",
    "options": [
      "Mean",
      "Standard Deviation",
      "Pearson Correlation Coefficient (상관계수)",
      "Entropy",
      "Gini Index"
    ],
    "answer": "Pearson Correlation Coefficient (상관계수)",
    "why": "상관계수가 1에 가까우면 강한 양의 상관관계, -1에 가까우면 강한 음의 상관관계를 나타냅니다.",
    "hint": "상관(Correlation) 관계의 정도입니다.",
    "trap_points": [
      "0은 관계가 아예 없음을 뜻함"
    ],
    "difficulty": "medium",
    "id": "0139"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Numpy에서 행렬 간의 요소별(Element-wise) 나눗셈을 할 때 주의할 점은?",
    "answer": "분모가 0인 경우 (Divide by zero)",
    "why": "분모에 0이 있으면 inf(무한대)나 nan이 발생하여 이후 전체 연산 결과가 오염될 수 있습니다.",
    "hint": "수학적으로 불가능한 나눗셈 상황입니다.",
    "trap_points": [
      "데이터 전처리에서 아주 작은 값(epsilon)을 더해주기도 함"
    ],
    "difficulty": "medium",
    "id": "0140"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 데이터프레임의 특정 열을 기준으로 데이터를 그룹화하여 통계를 내는 메서드는?",
    "options": [
      "gather()",
      "pivot()",
      "groupby()",
      "aggregate()",
      "bucket()"
    ],
    "answer": "groupby()",
    "why": "groupby()는 SQL의 GROUP BY와 동일하게 범주형 데이터를 기준으로 그룹별 연산을 수행하게 합니다.",
    "hint": "~별로 무리 짓기.",
    "trap_points": [
      "통계 함수(mean, sum)를 뒤에 붙여야 결과가 나옴"
    ],
    "difficulty": "easy",
    "id": "0141"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Numpy 배열의 차원을 변경하고자 할 때 사용하는 함수는?",
    "options": [
      "np.resize()",
      "np.reshape()",
      "np.convert()",
      "np.scale()",
      "np.flatten()"
    ],
    "answer": "np.reshape()",
    "why": "reshape()는 데이터의 총 개수는 유지하면서 모양(Shape)만 바꿀 때 사용합니다.",
    "hint": "모양을 다시 잡는다.",
    "trap_points": [
      "개수가 맞지 않으면 에러가 발생함"
    ],
    "difficulty": "easy",
    "id": "0142"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "텍스트 전처리 중 단어의 기본 형태(어근)를 추출하는 기술인 'Lemmatization'과 'Stemming'의 차이는?",
    "options": [
      "Lemmatization이 훨씬 빠르다.",
      "Stemming은 문법적 맥락을 고려하지만 Lemmatization은 무조건 자른다.",
      "Lemmatization은 사전을 기반으로 단어의 의미상 원형을 찾고, Stemming은 단순 규칙으로 어미를 자른다.",
      "두 기술은 완전히 동일하다.",
      "Stemming은 영어에서만 사용 가능하다."
    ],
    "answer": "Lemmatization은 사전을 기반으로 단어의 의미상 원형을 찾고, Stemming은 단순 규칙으로 어미를 자른다.",
    "why": "표제어 추출(Lemmatization)은 품사 정보 등을 활용해 더 정교하지만 느리고, 어간 추출(Stemming)은 빠르지만 부정확할 수 있습니다.",
    "hint": "의미와 규칙의 차이를 생각하세요.",
    "trap_points": [
      "분석 목적에 따라 속도와 정확도를 트레이드오프해야 함"
    ],
    "difficulty": "hard",
    "id": "0143"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "정규표현식에서 '문자열의 시작'을 의미하는 기호는?",
    "options": [
      "$",
      "^",
      ".",
      "*",
      "@"
    ],
    "answer": "^",
    "why": "^ 기호는 패턴이 문자열의 맨 앞에서 시작해야 함을 명시합니다.",
    "hint": "삿갓 모양의 기호입니다.",
    "trap_points": [
      "$는 문자열의 끝을 의미함"
    ],
    "difficulty": "medium",
    "id": "0144"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 데이터프레임의 모든 열에 대해 기술 통계량(count, mean, std 등)을 일괄적으로 보여주는 메서드는?",
    "options": [
      "df.info()",
      "df.describe()",
      "df.stats()",
      "df.summary()",
      "df.view()"
    ],
    "answer": "df.describe()",
    "why": "describe()는 숫자형 열들에 대해 주요 통계 지표를 한눈에 요약해 줍니다.",
    "hint": "설명하다, 묘사하다라는 뜻입니다.",
    "trap_points": [
      "info()는 데이터 타입과 결측치 여부를 보여줌"
    ],
    "difficulty": "easy",
    "id": "0145"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Numpy에서 행렬의 곱셈(행렬곱)을 수행하는 연산자 기호는?",
    "answer": "@",
    "why": "@ 연산자(또는 np.matmul)는 수학적인 행렬 곱셈을 수행합니다.",
    "hint": "이메일 주소에 쓰이는 '골뱅이' 기호입니다.",
    "trap_points": [
      "* 연산자는 요소별(element-wise) 곱셈임에 극히 주의"
    ],
    "difficulty": "medium",
    "id": "0146"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 행(Row)을 기준으로 데이터를 필터링할 때 주로 사용하는 속성은?",
    "options": [
      ".iloc",
      ".loc",
      ".at",
      ".filter",
      ".query"
    ],
    "answer": ".loc",
    "why": ".loc(Label-based)이나 .iloc(Integer-based)을 사용하여 데이터를 선택 및 필터링합니다.",
    "hint": "위치(Location)의 약자입니다.",
    "trap_points": [
      ".loc은 이름을, .iloc은 숫자를 사용함"
    ],
    "difficulty": "medium",
    "id": "0147"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "텍스트 데이터에서 '단어의 순서 정보'를 완전히 무시하고 빈도만 계산하는 기법은?",
    "options": [
      "Word2Vec",
      "BERT",
      "Bag of Words (BOW)",
      "Transformer",
      "RNN"
    ],
    "answer": "Bag of Words (BOW)",
    "why": "BOW는 단어들을 순서 없이 가방(Bag)에 담아 둔 것과 같다고 하여 붙여진 이름입니다.",
    "hint": "단어들의 주머니라는 뜻입니다.",
    "trap_points": [
      "문맥 정보가 손실된다는 치명적 단점이 있음"
    ],
    "difficulty": "easy",
    "id": "0148"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Numpy 배열 연산 중 각 요소를 하나씩 순회하는 대신 한꺼번에 연산하는 방식을 무엇이라 하나요?",
    "options": [
      "Looping",
      "Vectorization (벡터화)",
      "Multiplexing",
      "Batching",
      "Streaming"
    ],
    "answer": "Vectorization (벡터화)",
    "why": "벡터화 연산은 내부적으로 최적화된 C 코드로 동작하여 파이썬 for 문보다 수백 배 빠릅니다.",
    "hint": "벡터 단위로 처리한다는 뜻입니다.",
    "trap_points": [
      "데이터 분석에서 속도 향상의 핵심 요소임"
    ],
    "difficulty": "medium",
    "id": "0149"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "정규표현식에서 공백(Space, Tab, Newline 등)을 의미하는 특수 문자는?",
    "answer": "\\s",
    "why": "\\s는 space의 약자로 모든 종류의 공백 문자를 나타냅니다.",
    "hint": "역슬래시와 소문자 s의 조합입니다.",
    "trap_points": [
      "\\S(대문자)는 공백이 아닌 문자를 의미함"
    ],
    "difficulty": "medium",
    "id": "0150"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Numpy에서 모든 요소가 0으로 채워진 3x3 행렬을 만드는 함수는?",
    "options": [
      "np.array(0)",
      "np.ones((3,3))",
      "np.zeros((3,3))",
      "np.empty((3,3))",
      "np.full(0)"
    ],
    "answer": "np.zeros((3,3))",
    "why": "zeros 함수는 지정된 형상의 배열을 생성하고 모든 요소를 0으로 초기화합니다.",
    "hint": "숫자 0을 영어로?",
    "trap_points": [
      "ones는 1로 채우는 함수임"
    ],
    "difficulty": "easy",
    "id": "0151"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 데이터프레임의 상위 5개 행을 확인하는 메서드는?",
    "options": [
      "df.top()",
      "df.first()",
      "df.head()",
      "df.tail()",
      "df.show()"
    ],
    "answer": "df.head()",
    "why": "head() 메서드는 데이터의 구조를 빠르게 파악하기 위해 앞부분의 데이터를 보여줍니다.",
    "hint": "신체 부위 중 '머리'를 뜻합니다.",
    "trap_points": [
      "tail()은 뒷부분 5개를 보여줌"
    ],
    "difficulty": "easy",
    "id": "0152"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "정규표현식에서 '한 글자 이상' 반복됨을 의미하는 메타 문자는?",
    "options": [
      "*",
      "?",
      "+",
      ".",
      "^"
    ],
    "answer": "+",
    "why": "+ 기호는 앞에 오는 패턴이 1회 이상 연속될 때 매칭됩니다.",
    "hint": "덧셈 기호를 생각하세요.",
    "trap_points": [
      "*는 0회 이상임에 주의"
    ],
    "difficulty": "medium",
    "id": "0153"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "텍스트 전처리 단계 중 '의미 없는 단어(조사, 접속사 등)'를 제거하는 단계는?",
    "options": [
      "Cleaning",
      "Tokenization",
      "Normalization",
      "Stopwords Removal",
      "Stemming"
    ],
    "answer": "Stopwords Removal",
    "why": "불용어(Stopwords) 제거는 분석에 큰 도움이 되지 않는 빈번한 단어들을 필터링하는 효율화 단계입니다.",
    "hint": "멈춰야 할 단어들이라는 뜻입니다.",
    "trap_points": [
      "Normalization은 형태를 통일하는 과정임"
    ],
    "difficulty": "medium",
    "id": "0154"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "두 벡터 사이의 각도를 기반으로 유사도를 측정하는 방식은?",
    "options": [
      "Euclidean Distance",
      "Manhattan Distance",
      "Cosine Similarity",
      "Jaccard Similarity",
      "Hamming Distance"
    ],
    "answer": "Cosine Similarity",
    "why": "코사인 유사도는 벡터의 크기가 아닌 '방향'의 일치 정도를 측정하여 텍스트 유사도 분석에 널리 쓰입니다.",
    "hint": "삼각함수 이름 중 하나입니다.",
    "trap_points": [
      "유클리드 거리는 직선 거리를 측정함"
    ],
    "difficulty": "medium",
    "id": "0155"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 두 개의 데이터프레임을 특정 키 값을 기준으로 합치는(SQL의 JOIN과 유사한) 함수는?",
    "answer": "merge()",
    "why": "merge() 함수는 공통된 열이나 인덱스를 기준으로 데이터를 병합합니다.",
    "hint": "병합하다라는 뜻의 영어 단어입니다.",
    "trap_points": [
      "concat()은 단순 수직/수평 결합임"
    ],
    "difficulty": "medium",
    "id": "0156"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 결측치(NaN) 여부를 확인하여 True/False로 반환하는 메서드는?",
    "options": [
      "df.null()",
      "df.empty()",
      "df.isna()",
      "df.missing()",
      "df.check()"
    ],
    "answer": "df.isna()",
    "why": "isna() (또는 isnull())는 각 요소가 결측치인지 검사하여 불리언 마스크를 생성합니다.",
    "hint": "~인가요?(is) 결측치(na)",
    "trap_points": [
      "notna()는 반대로 결측치가 아닐 때 True임"
    ],
    "difficulty": "easy",
    "id": "0157"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Numpy에서 행렬의 행과 열을 바꾸는(전치) 속성은?",
    "options": [
      ".reverse",
      ".transpose()",
      ".T",
      ".switch",
      ".flip"
    ],
    "answer": ".T",
    "why": ".T 속성은 간단하게 행렬의 전치(Transpose) 행렬을 반환합니다.",
    "hint": "Transpose의 대문자 약어입니다.",
    "trap_points": [
      "속성이라서 괄호()가 붙지 않음"
    ],
    "difficulty": "easy",
    "id": "0158"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터 분석 시 텍스트를 고정된 도메인의 숫자로 바꾸는 가장 단순한 방식은?",
    "options": [
      "Word2Vec",
      "BERT",
      "One-hot Encoding",
      "TF-IDF",
      "Glove"
    ],
    "answer": "One-hot Encoding",
    "why": "원-핫 인코딩은 범주형 데이터를 0과 1로 이루어진 벡터로 바꾸는 가장 기본적인 수치화 기법입니다.",
    "hint": "하나만 뜨겁다(1)는 뜻입니다.",
    "trap_points": [
      "단어 수가 많아지면 벡터가 너무 커지는(Sparse) 단점이 있음"
    ],
    "difficulty": "medium",
    "id": "0159"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 중복된 행을 제거할 때 사용하는 메서드는?",
    "answer": "drop_duplicates()",
    "why": "drop_duplicates()는 중복된 값을 가진 행을 찾아 하나만 남기고 삭제합니다.",
    "hint": "중복(Duplicates)을 떨어뜨리다(Drop).",
    "trap_points": [
      "필요한 경우 특정 열만 기준으로 중복을 체크할 수 있음"
    ],
    "difficulty": "easy",
    "id": "0160"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 가로로 긴 데이터를 세로로 길게(Unpivot) 변형하는 메서드는?",
    "options": [
      "pivot()",
      "melt()",
      "stack()",
      "unstack()",
      "merge()"
    ],
    "answer": "melt()",
    "why": "melt()는 특정 열을 식별자로 유지하고 나머지 열들을 행으로 녹여내어(melt) 분석하기 좋은 형태로 바꿉니다.",
    "hint": "녹이다라는 뜻의 단어입니다.",
    "trap_points": [
      "pivot()의 정반대 작업이라고 이해하면 쉬움"
    ],
    "difficulty": "hard",
    "id": "0161"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Numpy 배열 연산에서 `a[a > 5]` 와 같이 조건에 맞는 요소만 추출하는 기법은?",
    "options": [
      "Slicing",
      "Boolean Indexing (불리언 인덱싱)",
      "Fancy Indexing",
      "Filtering",
      "Masking"
    ],
    "answer": "Boolean Indexing (불리언 인덱싱)",
    "why": "조건식의 결과로 생성된 True/False 배열을 인덱스로 사용하여 데이터를 선택합니다.",
    "hint": "참/거짓(Boolean)을 이용한 인덱싱입니다.",
    "trap_points": [
      "데이터 분석에서 특정 조건을 만족하는 데이터를 추출할 때 가장 많이 쓰임"
    ],
    "difficulty": "medium",
    "id": "0162"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "정규표현식에서 '단어의 비경계(알파벳/숫자가 아닌 문자 사이)'를 의미하는 기호는?",
    "options": [
      "\\b",
      "\\B",
      "\\w",
      "\\W",
      "\\s"
    ],
    "answer": "\\B",
    "why": "\\b는 단어 경계(공백 등)를 의미하고, \\B는 단어 내부 등 비경계를 의미합니다.",
    "hint": "대문자는 보통 소문자의 반대 의미를 가집니다.",
    "trap_points": [
      "\\b는 검색 시 독립된 단어만 찾고 싶을 때 매우 중요함"
    ],
    "difficulty": "hard",
    "id": "0163"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 두 날짜 사이의 차이를 나타내는 객체 타입은?",
    "options": [
      "Timestamp",
      "DatetimeIndex",
      "Timedelta",
      "Period",
      "Duration"
    ],
    "answer": "Timedelta",
    "why": "Timedelta는 두 시점 사이의 기간(3 days, 01:00:00 등)을 저장하는 자료형입니다.",
    "hint": "시간의 변화량(Delta)을 생각하세요.",
    "trap_points": [
      "날짜 - 날짜 = 타임델타입니다."
    ],
    "difficulty": "medium",
    "id": "0164"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Numpy에서 배열의 순서를 무작위로 섞는 함수는?",
    "options": [
      "np.sort()",
      "np.shuffle()",
      "np.random.shuffle()",
      "np.random.rand()",
      "np.mix()"
    ],
    "answer": "np.random.shuffle()",
    "why": "shuffle() 함수는 원본 배열의 요소를 무작위로 섞습니다.",
    "hint": "카드를 섞다라는 뜻입니다.",
    "trap_points": [
      "np.random.permutation()은 원본 유지 후 섞인 복사본을 반환함"
    ],
    "difficulty": "medium",
    "id": "0165"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 여러 개의 열을 하나의 열로 합치는 과정에서, 인덱스를 다중 층(Hierarchical Index)으로 쌓아 올리는 메서드는?",
    "answer": "stack()",
    "why": "컬럼 인덱스를 행 인덱스의 하위 레벨로 이동시켜 세로로 길게 쌓습니다.",
    "hint": "쌓다라는 뜻입니다.",
    "trap_points": [
      "unstack()은 다시 가로로 펼치는 반대 기능임"
    ],
    "difficulty": "hard",
    "id": "0166"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "텍스트 분석 시 단어의 빈도 순위를 기반으로 중요도를 산출할 때, Zipf의 법칙(Zipf's Law)이 말하는 것은?",
    "options": [
      "모든 단어는 동일한 빈도로 나타난다.",
      "빈도가 가장 높은 단어가 나머지 단어보다 압도적으로 많이 나타난다 (롱테일 분포).",
      "단어 길이가 길수록 많이 쓰인다.",
      "분석할수록 데이터가 줄어든다.",
      "영어만 정확하다."
    ],
    "answer": "빈도가 가장 높은 단어가 나머지 단어보다 압도적으로 많이 나타난다 (롱테일 분포).",
    "why": "상위 몇 개 단어가 전체 말뭉치의 대부분을 차지한다는 법칙으로, 불용어 제거의 근거가 됩니다.",
    "hint": "일부 단어의 독점 현상을 생각하세요.",
    "trap_points": [
      "파레토 법칙(80/20)의 텍스트 버전이라고 볼 수 있음"
    ],
    "difficulty": "hard",
    "id": "0167"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 CSV 파일을 읽을 때, 한글이 깨지는 것을 방지하기 위해 주로 지정하는 인코딩은?",
    "options": [
      "utf-8",
      "cp949",
      "ascii",
      "latin1",
      "utf-16"
    ],
    "answer": "cp949",
    "why": "MS 윈도우 환경에서 저장된 한글 엑셀/CSV 파일은 보통 CP949(EUC-KR 확장) 인코딩을 사용합니다.",
    "hint": "한국어 전용 인코딩 중 하나입니다.",
    "trap_points": [
      "최근에는 utf-8-sig 도 많이 쓰임"
    ],
    "difficulty": "easy",
    "id": "0168"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Numpy에서 행렬 연산 중 전치(Transpose)와 역행렬(Inverse)을 곱하여 생성되는 행렬의 특징은?",
    "options": [
      "항상 0이다.",
      "단위 행렬(Identity Matrix)이 된다.",
      "모든 요소가 1이다.",
      "값이 무한대이다.",
      "결과가 달라진다."
    ],
    "answer": "단위 행렬(Identity Matrix)이 된다.",
    "why": "정방 행렬에 역행렬을 곱하면 대각 성분이 1인 단위 행렬이 나옵니다 (A * A^-1 = I).",
    "hint": "행렬의 '1'과 같은 존재입니다.",
    "trap_points": [
      "역행렬이 존재하지 않는 행렬(Singular Matrix)은 계산 불가함"
    ],
    "difficulty": "hard",
    "id": "0169"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "정규표현식에서 특수 문자( . * + 등) 자체를 문자로 인식하게 하기 위해 앞에 붙이는 기호는?",
    "answer": "\\ (역슬래시)",
    "why": "역슬래시를 붙여 메타 문자의 의미를 무효화(Escaping)합니다.",
    "hint": "이스케이프 문자입니다.",
    "trap_points": [
      "파이썬 문자열에서는 r'...' 처럼 raw string을 권장함"
    ],
    "difficulty": "easy",
    "id": "0170"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 특정 열의 고유한(Unique) 값의 '개수'만 결과로 받고 싶을 때 사용하는 메서드는?",
    "options": [
      "df['A'].unique()",
      "df['A'].nunique()",
      "df['A'].count()",
      "df['A'].len()",
      "df['A'].size()"
    ],
    "answer": "df['A'].nunique()",
    "why": "unique()는 값들을 리스트로 반환하고, nunique()는 그 개수를 숫자로 반환합니다.",
    "hint": "Unique 앞에 n(number)이 붙었습니다.",
    "trap_points": [
      "결측치를 포함할지 말지 옵션으로 정할 수 있음"
    ],
    "difficulty": "medium",
    "id": "0171"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "텍스트 전처리 시 '의미를 보존하면서도 컴퓨터가 더 잘 이해하게' 하려고 어미를 쳐내는 과정을 무엇이라 하나요?",
    "options": [
      "Tokenization",
      "Stemming",
      "Encoding",
      "Scaling",
      "Refining"
    ],
    "answer": "Stemming",
    "why": "포터(Porter) 알고리즘 등을 써서 단어 뒤의 'ing', 'ed' 등을 잘라내어 어간을 추출합니다.",
    "hint": "줄기(Stem)를 뽑아낸다는 뜻입니다.",
    "trap_points": [
      "의미가 훼손되어 원형을 못 알아볼 수 있음(예: flies -> fli)"
    ],
    "difficulty": "medium",
    "id": "0172"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Numpy에서 두 배열의 유사도를 한눈에 평가하기 위해 행렬의 모든 요소의 평균을 구하는 메서드는?",
    "options": [
      "sum()",
      "mean()",
      "std()",
      "var()",
      "median()"
    ],
    "answer": "mean()",
    "why": "평균값(mean)은 데이터의 중심 경향성을 보여주는 가장 기본적인 척도입니다.",
    "hint": "평균의 영어 이름입니다.",
    "trap_points": [
      "이상치에 민감하다는 단점이 있음"
    ],
    "difficulty": "easy",
    "id": "0173"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 데이터프레임을 엑셀 파일로 저장할 때 사용하는 메서드는?",
    "options": [
      "to_csv()",
      "to_excel()",
      "save_excel()",
      "write_excel()",
      "export_excel()"
    ],
    "answer": "to_excel()",
    "why": "openpyxl 라이브러리를 사용하여 엑셀 형식으로 데이터를 출력합니다.",
    "hint": "엑셀(Excel)로(to) 보내기.",
    "trap_points": [
      "시트 이름을 지정하거나 여러 시트에 저장할 수 있음"
    ],
    "difficulty": "easy",
    "id": "0174"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Numpy 배열 [1, 2, 3] 에 단순히 10을 더하면 [11, 12, 13] 이 되는 현상을 설명하는 특성은?",
    "options": [
      "Broadcasting",
      "Reshaping",
      "Slicing",
      "Indexing",
      "Concatenating"
    ],
    "answer": "Broadcasting",
    "why": "스칼라 값이 배열의 모든 요소에 동일하게 적용되도록 넘파이가 자동으로 확장 연산을 수행합니다.",
    "hint": "이미 기획안과 샘플에서 강조했던 넘파이의 백미입니다.",
    "trap_points": [
      "반복문 없이 연산이 가능하게 하여 성능을 획기적으로 높임"
    ],
    "difficulty": "medium",
    "id": "0175"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 데이터프레임의 인덱스와 열을 서로 바꾸는(Transpose) 속성은?",
    "answer": "T",
    "why": "df.T 를 수행하면 행과 열이 뒤집힙니다.",
    "hint": "대문자 T 하나입니다.",
    "trap_points": [
      "속성이므로 함수처럼 ()를 붙이지 않음"
    ],
    "difficulty": "easy",
    "id": "0176"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 시계렬 데이터의 빈도를 일정 간격(예: 1시간 단위 -> 1일 단위)으로 다시 리샘플링하는 메서드는?",
    "options": [
      "resample()",
      "reindex()",
      "groupby()",
      "pivot()",
      "reshape()"
    ],
    "answer": "resample()",
    "why": "resample('D').mean() 과 같이 사용하여 고해상도 데이터를 저해상도로 합칠 때 필수적입니다.",
    "hint": "샘플링을 다시(Re) 한다.",
    "trap_points": [
      "인덱스가 DatetimeIndex여야 사용 가능함"
    ],
    "difficulty": "hard",
    "id": "0177"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 데이터프레임을 특정 열의 '값'을 기준으로 오름차순/내림차순 정렬하는 메서드는?",
    "options": [
      "sort_index()",
      "sort_values()",
      "order_by()",
      "arrange()",
      "rank()"
    ],
    "answer": "sort_values()",
    "why": "데이터 그 자체를 기준으로 순서를 바꿀 때 사용하며, 인덱스 정렬은 sort_index()입니다.",
    "hint": "값(Value)들을 정렬(Sort)합니다.",
    "trap_points": [
      "ascending=False 옵션으로 내림차순 정렬 가능"
    ],
    "difficulty": "easy",
    "id": "0178"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Numpy 배열의 모든 요소에 대해 특정 함수를 일괄 적용하는 기법(Vectorization)과 대비되는 파이썬의 전통적 방식은?",
    "options": [
      "Recursion",
      "Iteration (for/while 루프)",
      "Inheritance",
      "Slicing",
      "Indexing"
    ],
    "answer": "Iteration (for/while 루프)",
    "why": "루프는 파이썬 인터프리터 수준에서 한 단계씩 실행되어 매우 느리지만, 넘파이 벡터화는 C 수준에서 병렬 처리됩니다.",
    "hint": "반복문을 생각하세요.",
    "trap_points": [
      "빅데이터 처리 시 for 루프 사용은 지양해야 함"
    ],
    "difficulty": "medium",
    "id": "0179"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "정규표현식에서 '단어의 문자(알파벳, 숫자, _)가 아닌 것'과 매칭되는 기호는?",
    "options": [
      "\\w",
      "\\W",
      "\\d",
      "\\D",
      "\\s"
    ],
    "answer": "\\W",
    "why": "대문자는 소문자의 반대 의미를 가집니다. 따라서 \\W는 특수문자나 공백 등을 찾을 때 유용합니다.",
    "hint": "대문자 W입니다.",
    "trap_points": [
      "\\s와는 달리 탭(\t)이나 줄바꿈(\n)도 포함할 수 있음"
    ],
    "difficulty": "medium",
    "id": "0180"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 데이터프레임의 상위 5개 행만 요약해서 보여주는 메서드는?",
    "options": [
      "tail()",
      "head()",
      "top()",
      "first()",
      "view()"
    ],
    "answer": "head()",
    "why": "데이터의 형태와 첫 부분을 확인하는 가장 대중적인 메서드입니다.",
    "hint": "머리(Head) 부분입니다.",
    "trap_points": [
      "tail()은 마지막 5개 행을 보여줌"
    ],
    "difficulty": "easy",
    "id": "0181"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "데이터의 상관관계를 분석할 때 상관계수 값이 -0.9가 나왔다면 이는 무엇을 의미하나요?",
    "options": [
      "관계가 전혀 없다.",
      "강한 양의 상관관계가 있다.",
      "강한 음의 상관관계가 있다.",
      "데이터가 오류이다.",
      "평균값이 매우 작다."
    ],
    "answer": "강한 음의 상관관계가 있다.",
    "why": "절댁값이 1에 가까울수록 관계가 강하며, 마이너스는 한쪽이 커지면 다른 쪽이 작아지는 반비례 관계를 뜻합니다.",
    "hint": "수치는 강도, 부호는 방향입니다.",
    "trap_points": [
      "상관관계가 인과관계를 보장하지는 않음에 주의"
    ],
    "difficulty": "medium",
    "id": "0182"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Numpy에서 배열의 크기를 알려주는 속성은? (예: (3, 4))",
    "answer": "shape",
    "why": "배열의 차원별 요소 개수를 튜플 형태로 알려주어 행렬 연산 전 차원 일치 확인에 필수적입니다.",
    "hint": "모양을 뜻하는 영어 단어입니다.",
    "trap_points": [
      "속성이므로 ()를 붙이지 않음"
    ],
    "difficulty": "easy",
    "id": "0183"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "텍스트 데이터 정제 도중 '단어의 빈도'를 기준으로 불필요한 단어(은, 는, 이, 가 등)를 제거하는 행위를 무엇이라 하나요?",
    "options": [
      "Tokenization",
      "Normalization",
      "Stopword removal (불용어 제거)",
      "Vectorization",
      "Stemming"
    ],
    "answer": "Stopword removal (불용어 제거)",
    "why": "분석에 의미가 없는 기능어들을 제거하여 핵심 단어들에 집중하게 합니다.",
    "hint": "멈춤(Stop) 단어들입니다.",
    "trap_points": [
      "언어마다 불용어 리스트가 다름"
    ],
    "difficulty": "easy",
    "id": "0184"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Matplotlib에서 여러 개의 그래프를 한 화면에 나누어 그릴 때 사용하는 함수는?",
    "options": [
      "plt.multi()",
      "plt.subplot()",
      "plt.split()",
      "plt.group()",
      "plt.layout()"
    ],
    "answer": "plt.subplot()",
    "why": "행, 열, 인덱스를 지정하여 바둑판식으로 차트를 배치할 수 있게 해줍니다.",
    "hint": "작은(sub) 차트를 그립니다.",
    "trap_points": [
      "plt.subplots() (s 붙음)는 객체지향 방식으로 더 권장됨"
    ],
    "difficulty": "medium",
    "id": "0185"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas 데이터프레임에서 모든 값을 -1에서 1 사이 혹은 0에서 1 사이로 변환하는 과정을 무엇이라 하나요?",
    "options": [
      "Scaling (스케일링)",
      "Merging",
      "Filtering",
      "Pivoting",
      "Dropping"
    ],
    "answer": "Scaling (스케일링)",
    "why": "데이터의 단위(km vs mm 등)가 다를 때 이를 통일하여 기계학습 모델의 왜곡을 방지합니다.",
    "hint": "규모, 척도라는 뜻입니다.",
    "trap_points": [
      "StandardScaler(평균0, 분산1)와 MinMaxScaler(0~1)가 대표적임"
    ],
    "difficulty": "medium",
    "id": "0186"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "정규표현식에서 '임의의 한 글자'를 의미하는 기호는?",
    "answer": ".",
    "why": "점(.) 기호는 줄바꿈을 제외한 세상의 모든 문자 하나와 매칭됩니다.",
    "hint": "마침표를 생각하세요.",
    "trap_points": [
      "진짜 마침표 문자를 찾으려면 \\. 으로 써야 함"
    ],
    "difficulty": "easy",
    "id": "0187"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "정규표현식에서 숫자를 나타내는 특수 문자는?",
    "answer": "\\d",
    "why": "\\d는 digit의 약자로 0~9 사이의 숫자를 의미하는 정규표현식 메타 문자입니다.",
    "hint": "역슬래시와 소문자 d의 조합입니다.",
    "trap_points": [
      "\\s는 공백을 의미함"
    ],
    "difficulty": "easy",
    "id": "0188"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 결측치(NaN)를 특정 값으로 채우기 위해 사용하는 메서드는?",
    "options": [
      "dropna()",
      "fillna()",
      "isna()",
      "notna()",
      "replace()"
    ],
    "answer": "fillna()",
    "why": "fillna()는 데이터프레임 내의 결측값을 지정된 값이나 평균값 등으로 보충(fill)할 때 사용합니다.",
    "hint": "채우다(Fill)의 의미가 담겨 있습니다.",
    "trap_points": [
      "dropna()는 아예 지워버리는 것임"
    ],
    "difficulty": "easy",
    "id": "0189"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas DataFrame에서 특정 열 'A'의 고유 값들의 빈도를 계산하는 메서드는?",
    "options": [
      "df['A'].count()",
      "df['A'].value_counts()",
      "df['A'].unique()",
      "df['A'].nunique()",
      "df['A'].sum()"
    ],
    "answer": "df['A'].value_counts()",
    "why": "value_counts()는 각 고유 값들의 출현 빈도를 내림차순으로 계산하여 Series로 반환합니다.",
    "hint": "값(Value)의 개수(Counts)를 세어줍니다.",
    "trap_points": [
      "count()는 전체 데이터 개수만 세어줌"
    ],
    "difficulty": "medium",
    "id": "0190"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "텍스트 수치화 기법 중 단어의 빈도뿐만 아니라 희귀성을 고려하여 가중치를 부여하는 방식은?",
    "options": [
      "Bag of Words",
      "N-gram",
      "TF-IDF",
      "Word2Vec",
      "One-hot Encoding"
    ],
    "answer": "TF-IDF",
    "why": "TF-IDF는 단어 빈도(TF)에 역문서 빈도(IDF)를 곱하여, 특정 문서에만 자주 등장하는 중요 단어에 높은 가중치를 부여합니다.",
    "hint": "IDF는 Inverse Document Frequency의 약자입니다.",
    "trap_points": [
      "Bag of Words는 빈도만 고려함"
    ],
    "difficulty": "medium",
    "id": "0191"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 데이터프레임의 인덱스를 일반 열(Column)로 변환하는 메서드는?",
    "options": [
      "reset_index()",
      "set_index()",
      "drop_index()",
      "index_to_col()",
      "move_index()"
    ],
    "answer": "reset_index()",
    "why": "reset_index()는 기존 인덱스를 0, 1, 2... 로 초기화하며 기존 값은 새로운 열로 이동시킵니다.",
    "hint": "인덱스를 재설정(Reset)합니다.",
    "trap_points": [
      "drop=True 옵션을 주면 기존 인덱스는 사라짐"
    ],
    "difficulty": "easy",
    "id": "0192"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Numpy에서 행렬의 행과 열을 맞바꾸는 속성은?",
    "options": [
      "swap",
      "T",
      "reverse",
      "flip",
      "rotate"
    ],
    "answer": "T",
    "why": "Transpose의 약자로, 행렬의 전치를 수행합니다.",
    "hint": "대문자 T 하나입니다.",
    "trap_points": [
      "다차원 배열의 축을 직접 바꾸려면 transpose() 함수를 사용함"
    ],
    "difficulty": "easy",
    "id": "0193"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Scikit-learn에서 ‘훈련 데이터’로만 학습하고 ‘테스트 데이터’는 한 번도 보지 않게 분리하는 이유는?",
    "options": [
      "파일 용량을 줄이려고",
      "모델의 실제 일반화 성능을 공정하게 평가하기 위해",
      "속도가 빨라져서",
      "영어로만 답하기 위해",
      "데이터가 부족해서"
    ],
    "answer": "모델의 실제 일반화 성능을 공정하게 평가하기 위해",
    "why": "훈련 데이터의 정답을 외워버리는 오버피팅을 감지하기 위한 필수적인 절차입니다.",
    "hint": "공정한 시험(Test)를 위해서입니다.",
    "trap_points": [
      "테스트 데이터를 학습에 포함시키면 '데이터 누수(Data Leak)' 에 해당함"
    ],
    "difficulty": "easy",
    "id": "0194"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Matplotlib에서 선의 색상을 빨간색으로, 스타일을 점선으로 바꾸고 싶을 때 사용하는 약어는?",
    "options": [
      "'r--'",
      "'b-'",
      "'gs'",
      "'k:'",
      "'yo'"
    ],
    "answer": "'r--'",
    "why": "r은 red, --는 dashed line을 의미하는 짧은 형식의 설정법입니다.",
    "hint": "R과 대시 두 개입니다.",
    "trap_points": [
      "color='red', linestyle='dashed' 로도 상세하게 설정 가능함"
    ],
    "difficulty": "easy",
    "id": "0195"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 결측치(NaN)가 하나라도 포함된 행을 통째로 지워버리는 메서드는?",
    "answer": "dropna()",
    "why": "불완전한 데이터를 분석에서 배제하는 가장 과격하지만 확실한 방법입니다.",
    "hint": "떨어뜨리다(Drop) + NA.",
    "trap_points": [
      "axis=1 옵션을 주면 결측치가 있는 '열'을 지움"
    ],
    "difficulty": "easy",
    "id": "0196"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 데이터프레임의 인덱스를 0부터 시작하는 숫자로 초기화하는 메서드는?",
    "options": [
      "clear_index()",
      "reset_index()",
      "set_index()",
      "reindex()",
      "format_index()"
    ],
    "answer": "reset_index()",
    "why": "기존 인덱스를 제거하고 정수 인덱스를 새로 부여합니다. drop=True 옵션을 주면 기존 인덱스를 삭제합니다.",
    "hint": "다시 설정(Reset)한다.",
    "trap_points": [
      "set_index()는 특정 열을 인덱스로 만들 때 사용함"
    ],
    "difficulty": "medium",
    "id": "0197"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Numpy에서 행렬 간의 내적(Dot Product)을 수행하는 함수는?",
    "options": [
      "np.multiply()",
      "np.dot()",
      "np.add()",
      "np.sum()",
      "np.prod()"
    ],
    "answer": "np.dot()",
    "why": "np.dot(A, B)는 2차원 배열의 경우 행렬 곱셈을 수행합니다.",
    "hint": "점(Dot) 곱셈입니다.",
    "trap_points": [
      "1차원 벡터끼리는 스칼라 값을 반환함"
    ],
    "difficulty": "medium",
    "id": "0198"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "정규표현식에서 '0번 이상 반복'됨을 의미하는 기호는?",
    "options": [
      "*",
      "+",
      "?",
      ".",
      "{1,}"
    ],
    "answer": "*",
    "why": "별표(*) 매칭 기호는 앞에 오는 패턴이 없거나(0번) 여러 번 반복됨을 의미합니다.",
    "hint": "0을 포함한 전체 반복입니다.",
    "trap_points": [
      "+ 는 무조건 1번 이상 있어야 함에 주의"
    ],
    "difficulty": "medium",
    "id": "0199"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "텍스트 데이터에서 대소문자를 통일하거나 숫자를 특정 기호로 바꾸는 단계를 무엇이라 하나요?",
    "options": [
      "Cleaning",
      "Tokenization",
      "Normalization (정규화)",
      "Stemming",
      "Vectorization"
    ],
    "answer": "Normalization (정규화)",
    "why": "데이터의 다양성을 줄여서 분석 모델이 핵심 특징에 집중하도록 일관된 형태로 만드는 과정입니다.",
    "hint": "정상화, 표준화의 의미입니다.",
    "trap_points": [
      "학습 모델의 사전(Vocabulary) 크기를 줄이는 데 효과적임"
    ],
    "difficulty": "medium",
    "id": "0200"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 결측치가 포함된 '행'을 아예 삭제해 버리는 메서드는?",
    "options": [
      "fillna()",
      "dropna()",
      "isna()",
      "remove_null()",
      "delete_na()"
    ],
    "answer": "dropna()",
    "why": "dropna()는 NaN이 포함된 행 또는 열을 데이터프레임에서 제거합니다.",
    "hint": "떨어뜨리다(Drop) + NA.",
    "trap_points": [
      "데이터가 너무 많이 삭제될 수 있으므로 주의해서 사용해야 함"
    ],
    "difficulty": "easy",
    "id": "0201"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "텍스트 데이터를 n개의 연속된 단어 묶음으로 표현하여 문맥을 일부 반영하는 기법은?",
    "answer": "N-gram",
    "why": "uni-gram(1개), bi-gram(2개) 등으로 나누어 단어의 앞뒤 결합 정보를 보존합니다.",
    "hint": "N개의 그램(gram)입니다.",
    "trap_points": [
      "n이 커질수록 가능한 조합이 기하급수적으로 늘어남"
    ],
    "difficulty": "medium",
    "id": "0202"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Numpy에서 배열의 최소값, 최대값의 차이(Range)를 계산하는 함수는?",
    "options": [
      "np.range()",
      "np.ptp()",
      "np.diff()",
      "np.spread()",
      "np.min_max()"
    ],
    "answer": "np.ptp()",
    "why": "ptp()는 'peak to peak'의 약자로 최대값 - 최소값을 반환합니다.",
    "hint": "P로 시작하는 3글자 함수입니다.",
    "trap_points": [
      "단순히 정렬된 값을 원하면 np.sort()를 써야 함"
    ],
    "difficulty": "hard",
    "id": "0203"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 데이터프레임의 특정 열 이름을 변경할 때 사용하는 메서드는?",
    "options": [
      "rename()",
      "update()",
      "change()",
      "set_column()",
      "modify()"
    ],
    "answer": "rename()",
    "why": "df.rename(columns={'기존':'변경'}) 형식을 사용하여 이름을 바꿉니다.",
    "hint": "이름을 다시 짓다.",
    "trap_points": [
      "inplace=True를 주지 않으면 원본이 바뀌지 않음"
    ],
    "difficulty": "medium",
    "id": "0204"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "TF-IDF에서 IDF(역문서 빈도) 값이 크다는 것은 해당 단어가 어떤 특징을 가진다는 뜻인가요?",
    "options": [
      "모든 문서에서 흔하게 등장한다.",
      "특정 문서에만 드물게 등장하므로 정보 가치가 높다.",
      "오타일 확률이 높다.",
      "문장이 매우 길다.",
      "접속사(불용어)일 확률이 높다."
    ],
    "answer": "특정 문서에만 드물게 등장하므로 정보 가치가 높다.",
    "why": "전체 문서 중 적은 수의 문서에만 나타나는 단어일수록 해당 문서를 식별하는 데 유리하여 가중치가 높아집니다.",
    "hint": "희소성(Rareness)과 정보량의 관계를 생각하세요.",
    "trap_points": [
      "'the', 'is' 같은 단어는 모든 문서에 나오므로 IDF가 0에 가까움"
    ],
    "difficulty": "hard",
    "id": "0205"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas DataFrame에서 중복된 행을 확인하는(True/False 반환) 메서드는?",
    "answer": "duplicated()",
    "why": "duplicated()는 각 행이 이전에 출현했는지 여부를 불리언 값으로 알려줍니다.",
    "hint": "중복된(Duplicated)의 의미입니다.",
    "trap_points": [
      "제거하려면 drop_duplicates()를 써야 함"
    ],
    "difficulty": "easy",
    "id": "0206"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 데이터프레임의 특정 열을 인덱스로 설정하는 메서드는?",
    "options": [
      "reset_index()",
      "set_index()",
      "make_index()",
      "update_index()",
      "reindex()"
    ],
    "answer": "set_index()",
    "why": "set_index()는 기존의 일반 열 중 하나를 행의 이름(인덱스)으로 변환합니다.",
    "hint": "인덱스를 '설정'한다.",
    "trap_points": [
      "reset_index()는 인덱스를 다시 일반 열로 보냄"
    ],
    "difficulty": "medium",
    "id": "0207"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Numpy에서 행렬 연산이 아닌 요소별(Element-wise) 곱셈을 수행하는 연산자는?",
    "options": [
      "@",
      "dot()",
      "*",
      "matmul()",
      "multiply_all()"
    ],
    "answer": "*",
    "why": "Numpy 배열에 * 연산자를 쓰면 같은 위치의 요소끼리 곱해지며, 행렬곱은 @를 써야 합니다.",
    "hint": "기본 곱셈 기호입니다.",
    "trap_points": [
      "이것이 데이터 분석에서 가장 흔한 실수 포인트 중 하나임"
    ],
    "difficulty": "medium",
    "id": "0208"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "정규표현식에서 '어떤 문자든 한 글자'를 나타내는 기호는?",
    "options": [
      "*",
      "?",
      ".",
      "+",
      "$"
    ],
    "answer": ".",
    "why": "점(.) 기호는 줄바꿈을 제외한 임의의 문자 하나와 매칭됩니다.",
    "hint": "마침표를 생각하세요.",
    "trap_points": [
      "실제 점 문자 자체를 찾으려면 \\. 으로 써야 함"
    ],
    "difficulty": "easy",
    "id": "0209"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas 데이터프레임의 정보를 요약해서 보여주는 `df.info()`에서 알 수 없는 정보는?",
    "options": [
      "전체 행/열 개수",
      "열 이름",
      "각 열의 데이터 타입",
      "각 열의 평균값",
      "결측치가 아닌 값의 개수"
    ],
    "answer": "각 열의 평균값",
    "why": "평균값 같은 통계량은 describe()에서 보여주며, info()는 구조적 정보를 보여줍니다.",
    "hint": "데이터의 '형태(Type)'에 집중하세요.",
    "trap_points": [
      "info()는 메모리 사용량도 보여줌"
    ],
    "difficulty": "medium",
    "id": "0210"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Numpy에서 0부터 9까지의 숫자를 담은 배열을 만드는 가장 간단한 코드는?",
    "options": [
      "np.array(0, 10)",
      "np.range(10)",
      "np.arange(10)",
      "np.linspace(0, 9, 10)",
      "np.zeros(10)"
    ],
    "answer": "np.arange(10)",
    "why": "arange()는 파이썬의 range()와 유사하게 시퀀스 배열을 생성합니다.",
    "hint": "배열(a) + 범위(range)의 합성어입니다.",
    "trap_points": [
      "파이썬 내장 range()를 np.array()로 감싸도 되지만 arange가 더 직접적임"
    ],
    "difficulty": "easy",
    "id": "0211"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 시계렬 데이터의 연도 정보만 추출하기 위해 사용하는 속성은?",
    "answer": ".dt.year",
    "why": "시계렬 열에 .dt 접근자를 쓰면 연, 월, 일, 요일 등을 쉽게 추출 가능합니다.",
    "hint": "datetime의 약자와 year의 조합입니다.",
    "trap_points": [
      "열 자체가 datetime 타입이어야 사용 가능함"
    ],
    "difficulty": "medium",
    "id": "0212"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "정규표현식에서 문자의 시작(^)과 대비되어 '문자열의 끝'을 나타내는 기호는?",
    "options": [
      "#",
      "$",
      "&",
      "!",
      "%"
    ],
    "answer": "$",
    "why": "$ 기호는 패턴이 문자열의 맨 마지막에 위치해야 함을 명시합니다.",
    "hint": "달러 기호입니다.",
    "trap_points": [
      "문서 전체의 끝과 행의 끝 처리가 라이브러리 설정마다 다를 수 있음"
    ],
    "difficulty": "medium",
    "id": "0213"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "텍스트 수치화 기법 중 단어 간의 유의미한 관계(예: 왕 - 남자 = 여왕 - 여자)를 잘 보존하는 기법은?",
    "options": [
      "One-hot Encoding",
      "TF-IDF",
      "Word Embedding (예: Word2Vec)",
      "Bag of Words",
      "N-gram"
    ],
    "answer": "Word Embedding (예: Word2Vec)",
    "why": "워드 임베딩은 고차원 공간상에서 단어의 의미적 관계를 벡터 연산이 가능하도록 학습합니다.",
    "hint": "사전적 정의보다 문맥적 의미를 숫자로 표현하는 기술입니다.",
    "trap_points": [
      "TF-IDF는 단순 빈도 기반이라 이런 연산이 불가능함"
    ],
    "difficulty": "hard",
    "id": "0214"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 행과 열을 뒤바꾸는 피벗 테이블을 생성할 때 사용하는 메서드는?",
    "options": [
      "df.swap()",
      "df.pivot()",
      "df.reverse()",
      "df.flip()",
      "df.transpose()"
    ],
    "answer": "df.pivot()",
    "why": "pivot()은 특정 열을 인덱스로, 다른 열을 컬럼으로 재구조화합니다.",
    "hint": "회전축을 뜻하는 단어입니다.",
    "trap_points": [
      "집계 함수까지 쓰려면 pivot_table()을 권장함"
    ],
    "difficulty": "medium",
    "id": "0215"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Numpy 배열의 모든 요소의 평균을 구하는 메서드는?",
    "answer": "mean()",
    "why": "mean() 메서드나 np.mean() 함수는 전체 또는 축별 평균을 계산합니다.",
    "hint": "평균을 뜻하는 영어 단어입니다.",
    "trap_points": [
      "매개변수 axis를 주면 행별/열별 평균도 가능함"
    ],
    "difficulty": "easy",
    "id": "0216"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas에서 데이터프레임의 특정 조건(예: 가격 > 100)을 만족하는 행들만 필터링하는 관용구는?",
    "options": [
      "df.filter(price > 100)",
      "df[df['price'] > 100]",
      "df.select(price > 100)",
      "df.where(price > 100)",
      "df.get(price > 100)"
    ],
    "answer": "df[df['price'] > 100]",
    "why": "대괄호 안에 불리언 시리즈를 넣어 True인 행만 추출하는 불리언 인덱싱 방식입니다.",
    "hint": "대괄호를 두 번 쓰는 듯한 느낌입니다.",
    "trap_points": [
      "df.query('price > 100')로도 표현 가능함"
    ],
    "difficulty": "medium",
    "id": "0217"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Numpy에서 행렬 곱셈을 수행할 때 쓰는 공식 연산자(파이썬 3.5+)는?",
    "options": [
      "*",
      "^",
      "@",
      "&",
      "|"
    ],
    "answer": "@",
    "why": "A @ B 는 행렬 곱셈(matrix multiplication)을 수행합니다. *는 요소별 곱셈입니다.",
    "hint": "골뱅이 기호입니다.",
    "trap_points": [
      "np.dot(A, B)와 동일한 결과를 냅니다."
    ],
    "difficulty": "medium",
    "id": "0218"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Scikit-learn에서 여러 전처리 단계와 모델을 하나로 묶어 관리하는 편리한 클래스는?",
    "options": [
      "Bundle",
      "Pipeline (파이프라인)",
      "Group",
      "Chain",
      "Workflow"
    ],
    "answer": "Pipeline (파이프라인)",
    "why": "데이터 스케일링부터 학습까지의 과정을 하나로 묶어 코드 누락을 방지하고 일관된 처리를 돕습니다.",
    "hint": "파이프(Pipe)들이 연결된 라인입니다.",
    "trap_points": [
      "교차 검증(Cross Validation) 시 데이터 누수를 원천 차단해 줌"
    ],
    "difficulty": "hard",
    "id": "0219"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "객관식",
    "question": "Pandas 데이터프레임에서 행과 열의 위치를 기반으로(정수 인덱스) 데이터를 추출하는 속성은?",
    "options": [
      "df.loc",
      "df.iloc",
      "df.at",
      "df.iat",
      "df.get"
    ],
    "answer": "df.iloc",
    "why": "integer location의 약자이며, 정수 번호로 데이터를 인덱싱합니다. loc은 라벨 이름 기준입니다.",
    "hint": "앞글자 i는 정수(Integer)를 뜻합니다.",
    "trap_points": [
      "슬라이싱 시 마지막 인덱스 포함 여부가 loc과 다름에 주의"
    ],
    "difficulty": "easy",
    "id": "0220"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "Pandas DataFrame df의 상위 5개 행을 출력하는 메서드는? (문제 1)",
    "options": [],
    "answer": "df.head()",
    "why": "head()는 데이터의 앞부분을 미리보기 합니다.",
    "difficulty": "hard",
    "id": "0221"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "Numpy 배열 arr의 형태(shape)를 확인하는 속성은? (문제 2)",
    "options": [],
    "answer": "arr.shape",
    "why": "shape 속성은 배열의 차원 크기를 튜플로 반환합니다.",
    "difficulty": "medium",
    "id": "0222"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "Matplotlib에서 x, y 데이터를 선 그래프로 그리는 함수는? (문제 3)",
    "options": [],
    "answer": "plt.plot(x, y)",
    "why": "plot() 함수는 기본적으로 선 그래프를 그립니다.",
    "difficulty": "hard",
    "id": "0223"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "Pandas DataFrame df의 상위 5개 행을 출력하는 메서드는? (문제 4)",
    "options": [],
    "answer": "df.head()",
    "why": "head()는 데이터의 앞부분을 미리보기 합니다.",
    "difficulty": "medium",
    "id": "0224"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "Numpy 배열 arr의 형태(shape)를 확인하는 속성은? (문제 5)",
    "options": [],
    "answer": "arr.shape",
    "why": "shape 속성은 배열의 차원 크기를 튜플로 반환합니다.",
    "difficulty": "hard",
    "id": "0225"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "Matplotlib에서 x, y 데이터를 선 그래프로 그리는 함수는? (문제 6)",
    "options": [],
    "answer": "plt.plot(x, y)",
    "why": "plot() 함수는 기본적으로 선 그래프를 그립니다.",
    "difficulty": "medium",
    "id": "0226"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "Pandas DataFrame df의 상위 5개 행을 출력하는 메서드는? (문제 7)",
    "options": [],
    "answer": "df.head()",
    "why": "head()는 데이터의 앞부분을 미리보기 합니다.",
    "difficulty": "hard",
    "id": "0227"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "Numpy 배열 arr의 형태(shape)를 확인하는 속성은? (문제 8)",
    "options": [],
    "answer": "arr.shape",
    "why": "shape 속성은 배열의 차원 크기를 튜플로 반환합니다.",
    "difficulty": "medium",
    "id": "0228"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "Matplotlib에서 x, y 데이터를 선 그래프로 그리는 함수는? (문제 9)",
    "options": [],
    "answer": "plt.plot(x, y)",
    "why": "plot() 함수는 기본적으로 선 그래프를 그립니다.",
    "difficulty": "hard",
    "id": "0229"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "Pandas DataFrame df의 상위 5개 행을 출력하는 메서드는? (문제 10)",
    "options": [],
    "answer": "df.head()",
    "why": "head()는 데이터의 앞부분을 미리보기 합니다.",
    "difficulty": "medium",
    "id": "0230"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "Numpy 배열 arr의 형태(shape)를 확인하는 속성은? (문제 11)",
    "options": [],
    "answer": "arr.shape",
    "why": "shape 속성은 배열의 차원 크기를 튜플로 반환합니다.",
    "difficulty": "hard",
    "id": "0231"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "Matplotlib에서 x, y 데이터를 선 그래프로 그리는 함수는? (문제 12)",
    "options": [],
    "answer": "plt.plot(x, y)",
    "why": "plot() 함수는 기본적으로 선 그래프를 그립니다.",
    "difficulty": "medium",
    "id": "0232"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "Pandas DataFrame df의 상위 5개 행을 출력하는 메서드는? (문제 13)",
    "options": [],
    "answer": "df.head()",
    "why": "head()는 데이터의 앞부분을 미리보기 합니다.",
    "difficulty": "hard",
    "id": "0233"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "Numpy 배열 arr의 형태(shape)를 확인하는 속성은? (문제 14)",
    "options": [],
    "answer": "arr.shape",
    "why": "shape 속성은 배열의 차원 크기를 튜플로 반환합니다.",
    "difficulty": "medium",
    "id": "0234"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "Matplotlib에서 x, y 데이터를 선 그래프로 그리는 함수는? (문제 15)",
    "options": [],
    "answer": "plt.plot(x, y)",
    "why": "plot() 함수는 기본적으로 선 그래프를 그립니다.",
    "difficulty": "hard",
    "id": "0235"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "Pandas DataFrame df의 상위 5개 행을 출력하는 메서드는? (문제 16)",
    "options": [],
    "answer": "df.head()",
    "why": "head()는 데이터의 앞부분을 미리보기 합니다.",
    "difficulty": "medium",
    "id": "0236"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "Numpy 배열 arr의 형태(shape)를 확인하는 속성은? (문제 17)",
    "options": [],
    "answer": "arr.shape",
    "why": "shape 속성은 배열의 차원 크기를 튜플로 반환합니다.",
    "difficulty": "hard",
    "id": "0237"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "Matplotlib에서 x, y 데이터를 선 그래프로 그리는 함수는? (문제 18)",
    "options": [],
    "answer": "plt.plot(x, y)",
    "why": "plot() 함수는 기본적으로 선 그래프를 그립니다.",
    "difficulty": "medium",
    "id": "0238"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "Pandas DataFrame df의 상위 5개 행을 출력하는 메서드는? (문제 19)",
    "options": [],
    "answer": "df.head()",
    "why": "head()는 데이터의 앞부분을 미리보기 합니다.",
    "difficulty": "hard",
    "id": "0239"
  },
  {
    "chapter_name": "데이터 분석",
    "type": "코드 완성형",
    "question": "Numpy 배열 arr의 형태(shape)를 확인하는 속성은? (문제 20)",
    "options": [],
    "answer": "arr.shape",
    "why": "shape 속성은 배열의 차원 크기를 튜플로 반환합니다.",
    "difficulty": "medium",
    "id": "0240"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "트랜스포머의 'Parallelization' 성능이 뛰어난 주된 아키텍처적 이유는?",
    "options": [
      "메모리가 커서",
      "RNN처럼 순차적으로 계산하지 않고 전체 문장을 한꺼번에 어텐션 연산하기 때문",
      "영어로만 학습해서",
      "이미지 기술을 써서",
      "속도가 빨라서"
    ],
    "answer": "RNN처럼 순차적으로 계산하지 않고 전체 문장을 한꺼번에 어텐션 연산하기 때문",
    "why": "병렬 연산이 가능한 구조 덕분에 대규모 데이터를 GPU로 빠르게 학습시킬 수 있습니다.",
    "hint": "한꺼번에 처리한다는 뜻의 단어에 주목하세요.",
    "trap_points": [
      "학습은 병렬이지만 생성은 여전히 순차적임(Autoregressive)"
    ],
    "difficulty": "hard",
    "id": "0241"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 생성 시 'KV 캐싱(Key-Value Caching)'을 사용하는 목적은?",
    "options": [
      "데이터를 삭제하려고",
      "이전에 계산한 토큰의 어텐션 결과를 재사용하여 생성 속도를 높이기 위해",
      "이미지 생성을 위해",
      "영어로만 답하기 위해",
      "메모리를 아끼려고"
    ],
    "answer": "이전에 계산한 토큰의 어텐션 결과를 재사용하여 생성 속도를 높이기 위해",
    "why": "이미 처리한 단어의 K, V 값을 보관해두어 중복된 연산을 피함으로써 추론 속도를 드라마틱하게 올립니다.",
    "hint": "이미 한 계산은 저장해두고 다시 쓰기.",
    "trap_points": [
      "메모리(VRAM) 사용량은 오히려 늘어나게 됨 (Trade-off)"
    ],
    "difficulty": "hard",
    "id": "0242"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "모델의 크기는 그대로이면서 추론 속도만 높이는 'Speculative Decoding'의 원리는?",
    "options": [
      "컴퓨터를 더 좋게 바꿈",
      "작은 가벼운 모델이 초안을 미리 쓰고, 큰 모델이 한꺼번에 검토/수정한다.",
      "미래를 예측한다.",
      "영어로만 답한다.",
      "데이터를 다 지운다."
    ],
    "answer": "작은 가벼운 모델이 초안을 미리 쓰고, 큰 모델이 한꺼번에 검토/수정한다.",
    "why": "확률이 높은 단어 뭉치를 작은 모델이 먼저 만들고, 큰 모델은 이를 병렬로 검증만 수행하여 전체 시간을 단축합니다.",
    "hint": "추측(Speculative)하여 미리 써둡니다.",
    "trap_points": [
      "모델의 답변 품질은 큰 모델 단독과 동일함이 보장됨"
    ],
    "difficulty": "hard",
    "id": "0243"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM에서 'Context Window'의 한계인 잃어버린 중간(Lost in the Middle) 현상이란?",
    "options": [
      "중간 줄이 안 나오는 현상",
      "문장이 너무 길 때, 모델이 앞부분과 뒷부분은 잘 기억하나 중간에 위치한 정보를 놓치는 현상",
      "답변 속도가 느려지는 현상",
      "영어가 서툰 현상",
      "파일이 깨지는 현상"
    ],
    "answer": "문장이 너무 길 때, 모델이 앞부분과 뒷부분은 잘 기억하나 중간에 위치한 정보를 놓치는 현상",
    "why": "어텐션 메커니즘의 특성상 정보 밀도가 양끝단에 쏠리는 경향이 있어 발생하는 한계입니다.",
    "hint": "가운데(Middle)를 잃어버립니다.",
    "trap_points": [
      "중요한 지시나 근거는 문장의 맨 앞이나 맨 뒤에 두는 것이 유리함"
    ],
    "difficulty": "medium",
    "id": "0244"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "트랜스포머 레이어 중 'Multi-Query Attention (MQA)'과 'Grouped-Query Attention (GQA)'의 공통적 목표는?",
    "options": [
      "성능을 10배 올리기",
      "Key-Value 헤드 수를 줄여서 추론 시 VRAM 사용량과 대역폭 오버헤드를 줄이기",
      "이미지만 생성하기",
      "영어로만 답하게 하기",
      "데이터를 암기하기"
    ],
    "answer": "Key-Value 헤드 수를 줄여서 추론 시 VRAM 사용량과 대역폭 오버헤드를 줄이기",
    "why": "Query는 여러 개지만 Key, Value를 공유하거나 묶어서 효율을 높이는 기술입니다.",
    "hint": "KV 캐시의 효율성에 집중하세요.",
    "trap_points": [
      "Llama 3 등 최신 대형 모델들은 GQA를 표준으로 채택함"
    ],
    "difficulty": "hard",
    "id": "0245"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "모델이 답변 시 '나는 누구인가' 처럼 가치관이나 자아를 가지게 되는 것처럼 보이는 것은 어떤 데이터 때문인가요?",
    "answer": "System Prompt (또는 Persona SFT 데이터)",
    "why": "학습 데이터나 프롬프트를 통해 특정 인격과 세계관을 주입하기 때문입니다.",
    "hint": "시스템(System)의 지시 사항입니다.",
    "trap_points": [
      "모델 자체가 실제 자아를 가지는 것은 아님"
    ],
    "difficulty": "easy",
    "id": "0246"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 'Emergent Abilities (발현 능력)'가 나타나는 결정적 조건은?",
    "options": [
      "사용자가 친절할 때",
      "모델의 파라미터 수와 학습량이 특정 임계값(Threshold)을 넘었을 때",
      "인터넷 속도가 빠를 때",
      "밤에 학습시킬 때",
      "이미지 데이터가 많을 때"
    ],
    "answer": "모델의 파라미터 수와 학습량이 특정 임계값(Threshold)을 넘었을 때",
    "why": "작은 모델에서는 불가능하던 복합 추론이나 번역 등이 특정 규모 이상에서 갑자기 나타나는 현상입니다.",
    "hint": "갑자기 나타나는(Emergent) 힘입니다.",
    "trap_points": [
      "최근에는 학습 데이터 질에 따라 작은 모델에서도 나타날 수 있음"
    ],
    "difficulty": "medium",
    "id": "0247"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토크나이저에서 'OOV (Out Of Vocabulary)' 문제를 해결하기 위해 채택하는 표준 전략은?",
    "options": [
      "모르는 단어는 다 지운다.",
      "단어를 더 작은 단위인 서브워드(Subword)로 쪼개어 표현한다.",
      "영어로만 바꾼다.",
      "에러를 낸다.",
      "무조건 'unknown'으로 표시한다."
    ],
    "answer": "단어를 더 작은 단위인 서브워드(Subword)로 쪼개어 표현한다.",
    "why": "모든 단어를 사전에 담을 수 없으므로 'un + happy' 처럼 쪼개서 조합합니다.",
    "hint": "하위 단어(Subword)입니다.",
    "trap_points": [
      "바이트 수준이나 문자(Character) 수준까지도 쪼갤 수 있음"
    ],
    "difficulty": "medium",
    "id": "0248"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "트랜스포머의 인코더와 디코더의 결정적 아키텍처 차이는?",
    "options": [
      "똑같다.",
      "디코더는 미래의 정보를 보지 못하게 하는 'Masked Self-Attention'을 가진다.",
      "인코더만 어텐션을 쓴다.",
      "디코더는 글자 수가 적다.",
      "영어로만 되어 있다."
    ],
    "answer": "디코더는 미래의 정보를 보지 못하게 하는 'Masked Self-Attention'을 가진다.",
    "why": "생성 모델인 디코더는 현재까지 나온 단어만 보고 다음을 예측해야 하기 때문입니다.",
    "hint": "가림막(Mask)의 유무입니다.",
    "trap_points": [
      "BERT는 인코더만, GPT는 디코더만 사용함"
    ],
    "difficulty": "hard",
    "id": "0249"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 학습 시 GPU들을 하나로 묶어 거대한 메모리 공간처럼 활용하는 마이크로소프트의 오픈소스 라이브러리는?",
    "answer": "DeepSpeed",
    "why": "ZeRO 기술 등을 통해 수천억 개의 파라미터를 효율적으로 분산 학습하게 돕습니다.",
    "hint": "깊은(Deep) 속도(Speed)입니다.",
    "trap_points": [
      "현재는 PyTorch의 FSDP와 함께 가장 널리 쓰임"
    ],
    "difficulty": "hard",
    "id": "0250"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM에서 'Context window expansion' 기술인 Flash Attention이 해결한 물리적 문제는?",
    "options": [
      "GPU 메모리 대역폭 한계와 연산 오버헤드",
      "인터넷 속도 지연",
      "언어 번역 정확도",
      "데이터 셔플링 속도",
      "키보드 입력 지연"
    ],
    "answer": "GPU 메모리 대역폭 한계와 연산 오버헤드",
    "why": "어텐션 행렬 전체를 메모리에 올리지 않고 타일 단위로 계산하여 속도는 수 배 높이고 메모리는 수십 배 절약합니다.",
    "hint": "번개(Flash)처럼 빠른 어텐션입니다.",
    "trap_points": [
      "최신 모델들의 긴 컨텍스트 지원을 가능하게 한 일등공신임"
    ],
    "difficulty": "hard",
    "id": "0251"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "GPT-4와 같은 거대 모델이 내부적으로 여러 개의 작은 모델(전문가) 조각으로 나뉘어 있는 구조를 무엇이라 하나요?",
    "options": [
      "Multi-tasking",
      "Mixture of Experts (MoE)",
      "Ensemble Learning",
      "Stacked Layers",
      "Parallel Processing"
    ],
    "answer": "Mixture of Experts (MoE)",
    "why": "MoE는 입력된 질문에 가장 적합한 전문가 신경망만 활성하하하여 효율성을 극대화합니다.",
    "hint": "전문가들의 혼합입니다.",
    "trap_points": [
      "파수(Parameter)는 많지만 실제 추론 시 활성화되는 파라미터는 적음"
    ],
    "difficulty": "hard",
    "id": "0252"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "모델이 문장의 시작 토큰부터 마지막 토큰까지 한 방향으로만 훑으며 다음 단어를 예측하는 모델 계열은?",
    "options": [
      "BERT (Encoder-only)",
      "GPT (Decoder-only)",
      "T5 (Encoder-Decoder)",
      "YOLO",
      "ResNet"
    ],
    "answer": "GPT (Decoder-only)",
    "why": "GPT는 인과적(Causal) 모델로, 이전까지 나온 정보만을 바탕으로 미래의 단어를 예측합니다.",
    "hint": "생성(Generative)에 특화된 구조입니다.",
    "trap_points": [
      "BERT는 문장 전체를 양방향으로 훑는 인코더 기반임"
    ],
    "difficulty": "medium",
    "id": "0253"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM에서 답변의 창의성을 높이기 위해 'Temperature' 값을 높이면 나타나는 현상은?",
    "options": [
      "답변이 정확해진다.",
      "확률 분포가 완만해져서 낮은 확률의 단어도 선택될 수 있게 되어 답변이 다양하고 엉뚱해진다.",
      "속도가 빨라진다.",
      "영어로만 답한다.",
      "글자 수가 무조건 길어진다."
    ],
    "answer": "확률 분포가 완만해져서 낮은 확률의 단어도 선택될 수 있게 되어 답변이 다양하고 엉뚱해진다.",
    "why": "온도가 높으면 에너지가 높은 상태처럼 입자(토큰)들이 자유롭게 돌아다니는 것에 비유됩니다.",
    "hint": "분포의 엔트로피가 증가합니다.",
    "trap_points": [
      "너무 높이면 문법이 파괴되거나 아무 말 대잔치가 될 수 있음"
    ],
    "difficulty": "medium",
    "id": "0254"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "모델의 파라미터가 4비트 양자화되었을 때, FP16 대비 용량은 어느 정도로 줄어드나요?",
    "options": [
      "1/2",
      "1/4",
      "1/8",
      "1/10",
      "전혀 줄지 않음"
    ],
    "answer": "1/4",
    "why": "16비트에서 4비트로 줄었으므로 수치적으로 4배 압축됩니다.",
    "hint": "16 / 4 를 계산해 보세요.",
    "trap_points": [
      "용량은 줄어도 연산 시에는 16비트로 임시 복원하여 연산하기도 함"
    ],
    "difficulty": "medium",
    "id": "0255"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM이 스스로 이전의 답변을 보고 틀린 부분을 찾아 고치는 능력을 무엇이라 하나요?",
    "answer": "Self-Correction (자기 수정)",
    "why": "모델이 논리적 모순을 인지하고 결과물을 고도화하는 고차원 지능 단계입니다.",
    "hint": "스스로 올바르게(Correction)함.",
    "trap_points": [
      "프롬프트를 통해 '다시 검토해'라고 할 때 발현됨"
    ],
    "difficulty": "medium",
    "id": "0256"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 학습 데이터에서 '중복 데이터'를 제거하는 것이 중요한 이유는?",
    "options": [
      "비용이 아까워서",
      "모델이 특정 문장을 그대로 암기(Memorization)해버리는 부작용을 막기 위해",
      "파일 속도를 빠르게 하려고",
      "영어로만 코딩하게 하려고",
      "데이터 개수가 많으면 에러가 나서"
    ],
    "answer": "모델이 특정 문장을 그대로 암기(Memorization)해버리는 부작용을 막기 위해",
    "why": "반복 학습은 모델이 '이해'하는 대신 '암기'하도록 유도하여 새로운 질문에 대한 응답 능력을 떨어뜨립니다.",
    "hint": "앵무새처럼 따라 하는 현상을 방지합니다.",
    "trap_points": [
      "중복 제거 과정(Deduplication)은 모델 성능 향상의 핵심임"
    ],
    "difficulty": "hard",
    "id": "0257"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토큰화 방식 중 하나로, 정해진 단어장(Vocab) 크기 내에서 단어를 효율적으로 쪼개는 방식의 이름은?",
    "options": [
      "BPE (Byte Pair Encoding)",
      "Linear Splitting",
      "Random Cutting",
      "Word Formatting",
      "Sequential Parsing"
    ],
    "answer": "BPE (Byte Pair Encoding)",
    "why": "가장 많이 나오는 문자 쌍을 합쳐가며 사전(Vocabulary)을 구축하는 최신 LLM의 표준 방식입니다.",
    "hint": "바이트 쌍 인코딩의 약자입니다.",
    "trap_points": [
      "한글은 조사나 어미 때문에 영문보다 더 세밀하게 쪼개짐"
    ],
    "difficulty": "medium",
    "id": "0258"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "트랜스포머의 'Attention'에서 특정 부분을 안 보이게 가리는 행위(Masking)가 디코더에서 수행되는 이유는?",
    "options": [
      "메모리를 아끼려고",
      "미래의 단어를 미리 보고 정답을 암기하는 것을 방지하기 위해",
      "그냥 관례적으로",
      "영어로만 번역하려고",
      "시스템 에러를 숨기려고"
    ],
    "answer": "미래의 단어를 미리 보고 정답을 암기하는 것을 방지하기 위해",
    "why": "생성 모델은 과거의 정보만 알아야 하므로 다음 단어 이후는 '마스킹' 처리하여 접근을 차단합니다.",
    "hint": "컨닝(Cheating)을 방지하는 가림막입니다.",
    "trap_points": [
      "이를 'Causal Masking' 또는 'Look-ahead Masking'이라 함"
    ],
    "difficulty": "hard",
    "id": "0259"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "모델의 학습 과정에서 인간의 선호도를 직접 학습시켜 '인간다운 답변'을 내놓게 하는 최종 정렬 단계를 무엇이라 하나요?",
    "answer": "RLHF (Reinforcement Learning from Human Feedback)",
    "why": "인간의 교정 데이터를 통해 모델이 어떤 답변이 더 유익하고 안전한지 배우게 합니다.",
    "hint": "인간의 피드백(Human Feedback)을 통한 강화학습입니다.",
    "trap_points": [
      "보상 모델(Reward Model) 학습 단계가 포함됨"
    ],
    "difficulty": "medium",
    "id": "0260"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "트랜스포머 아키텍처에서 인코더와 디코더 사이의 연결을 담당하며 인코더의 정보를 디코더로 넘겨주는 어텐션은?",
    "options": [
      "Self-Attention",
      "Masked Self-Attention",
      "Cross-Attention",
      "Multi-Head Attention",
      "Global Attention"
    ],
    "answer": "Cross-Attention",
    "why": "크로스 어텐션은 디코더가 생성 시 인코더가 뽑아낸 입력 문장의 맥락 벡터를 참조하게 해줍니다.",
    "hint": "서로 교차(Cross)한다는 뜻입니다.",
    "trap_points": [
      "디코더 전용 모델(GPT)에는 이 과정이 없음"
    ],
    "difficulty": "hard",
    "id": "0261"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "BERT 모델이 문맥을 앞뒤 양방향으로 파악할 수 있는 근본적인 모델 구조는?",
    "options": [
      "Decoder-only",
      "Encoder-only",
      "Encoder-Decoder",
      "CNN",
      "RNN"
    ],
    "answer": "Encoder-only",
    "why": "BERT는 인코더 블록만 쌓아서 입력 문장 전체의 상관관계를 한꺼번에(양방향) 계산합니다.",
    "hint": "입력 처리에 특화된 구조입니다.",
    "trap_points": [
      "GPT는 디코더만 사용함"
    ],
    "difficulty": "medium",
    "id": "0262"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 성능을 평가하는 벤치마크 중 인간의 전문 지식(상식, 법률, 의학 등)을 묻는 가장 유명한 테스트는?",
    "options": [
      "HumanEval",
      "MMLU",
      "GSM8K",
      "Big-Bench",
      "Llama-Index"
    ],
    "answer": "MMLU",
    "why": "MMLU(Massive Multitask Language Understanding)는 57개 주제를 다루는 LLM 성능 측정의 표준 격인 벤치마크입니다.",
    "hint": "대규모 다중 작업 언어 이해의 약자입니다.",
    "trap_points": [
      "GSM8K는 수학 추론 능력을 평가함"
    ],
    "difficulty": "medium",
    "id": "0263"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토크나이저 아키텍처 중 '단어 빈도'를 기반으로 가장 자주 나오는 글자 쌍을 합쳐나가는 방식은?",
    "options": [
      "WordPiece",
      "BPE (Byte Pair Encoding)",
      "Unigram",
      "SentencePiece",
      "N-gram"
    ],
    "answer": "BPE (Byte Pair Encoding)",
    "why": "BPE는 가장 빈번한 바이트 쌍을 하나의 토큰으로 변환하는 과정을 반복하여 사전 크기를 관리합니다.",
    "hint": "바이트 쌍 인코딩의 약자입니다.",
    "trap_points": [
      "GPT 계열에서 주로 사용함"
    ],
    "difficulty": "hard",
    "id": "0264"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "VRAM 16GB를 가진 GPU에서 7B 모델(FP16 정밀도)을 실행하기 위해 필요한 대략적인 최소 메모리는?",
    "options": [
      "약 7GB",
      "약 14GB",
      "약 28GB",
      "약 3.5GB",
      "약 1GB"
    ],
    "answer": "약 14GB",
    "why": "FP16은 파라미터당 2바이트를 사용하므로 70억 * 2 = 14GB가 순수 가중치 용량입니다.",
    "hint": "70억 개 가중치 * 2바이트(16비트)를 계산해 보세요.",
    "trap_points": [
      "실제 실행 시에는 KV 캐시 등으로 더 많은 메모리가 필요함"
    ],
    "difficulty": "medium",
    "id": "0265"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 학습 시 방대한 양의 데이터셋으로부터 언어의 일반적인 패턴을 배우는 첫 번째 단계를 무엇이라 하나요?",
    "answer": "Pre-training (사전 학습)",
    "why": "사전 학습은 레이블 없는 원시 텍스트(Raw Text)를 통해 모델의 기초 지능을 구축하는 단계입니다.",
    "hint": "먼저(Pre) 학습시킨다는 뜻입니다.",
    "trap_points": [
      "이후에 하는 것이 파인튜닝임"
    ],
    "difficulty": "easy",
    "id": "0266"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "모델이 긴 생각을 거친 후 정답을 내도록 유도하는 'Reasoning' 모델의 핵심 출력 토큰 구간은?",
    "options": [
      "<answer>",
      "<think>",
      "<reason>",
      "<output>",
      "<step>"
    ],
    "answer": "<think>",
    "why": "최신 모델(DeepSeek R1 등)은 생각하는 과정(Thinking Process)을 별도의 태그 안에 출력하여 논리력을 보강합니다.",
    "hint": "생각하다라는 영어 단어입니다.",
    "trap_points": [
      "사용자에게는 이 구간을 숨기고 최종 답변만 보여주기도 함"
    ],
    "difficulty": "medium",
    "id": "0267"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "오픈 모델 중 Meta에서 공개하여 생태계를 주도하고 있는 모델의 이름은?",
    "options": [
      "Claude",
      "Llama",
      "Gemini",
      "Mistral",
      "Qwen"
    ],
    "answer": "Llama",
    "why": "Llama 시리즈는 고성능 오픈 가중치 모델로 AI 연구 및 서비스 개발의 표준이 되었습니다.",
    "hint": "남미의 동물 이름입니다.",
    "trap_points": [
      "Mistral은 프랑스의 경쟁 모델임"
    ],
    "difficulty": "easy",
    "id": "0268"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "단어의 의미적 거리를 숫자로 표현한 벡터 공간에서 '가까울수록' 무엇이 유사한 것인가요?",
    "options": [
      "글자 수",
      "문법적 역할",
      "의미와 문맥",
      "알파벳 순서",
      "글자 크기"
    ],
    "answer": "의미와 문맥",
    "why": "임베딩 벡터 공간은 유사한 의미를 가진 단어들이 기하학적으로 가깝게 위치하도록 학습됩니다.",
    "hint": "벡터 유사도는 '의미'를 찾기 위한 도구입니다.",
    "trap_points": [
      "동음이의어라도 문맥에 따라 다른 벡터 값을 가질 수 있음(현대 LLM 기본)"
    ],
    "difficulty": "easy",
    "id": "0269"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "하이브리드 처리 기능(텍스트+이미지+음성 등)을 가진 인공지능 모델을 일컫는 용어는?",
    "answer": "Multimodal (멀티모달)",
    "why": "여러 가지 양식(Modality)을 동시에 이해하고 생성할 수 있는 모델입니다.",
    "hint": "멀티(Multi) + 모달(Modal).",
    "trap_points": [
      "단순히 텍스트만 하는 모델과 구분됨"
    ],
    "difficulty": "easy",
    "id": "0270"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM에서 'Context Window'가 의미하는 것은?",
    "options": [
      "모델이 학습한 전체 데이터 양",
      "모델이 한꺼번에 처리할 수 있는 최대 토큰 수",
      "컴퓨터의 메모리 용량",
      "인터넷 연결 속도",
      "답변을 생성하는 데 걸리는 시간"
    ],
    "answer": "모델이 한꺼번에 처리할 수 있는 최대 토큰 수",
    "why": "컨텍스트 윈도우는 모델이 기억하고 참조할 수 있는 입력의 길이를 결정합니다.",
    "hint": "맥락(Context)의 창문 크기입니다.",
    "trap_points": [
      "이 한도를 넘기면 앞부분의 내용을 잊어버리게 됨"
    ],
    "difficulty": "medium",
    "id": "0271"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "트랜스포머 아키텍처에서 인코더 없이 디코더만 사용하여 생성에 특화된 모델 계열은?",
    "options": [
      "BERT",
      "GPT",
      "T5",
      "ResNet",
      "YOLO"
    ],
    "answer": "GPT",
    "why": "GPT는 Generative Pre-trained Transformer의 약자로, 이전 토큰들을 기반으로 다음 토큰을 예측하는 디코더 구조입니다.",
    "hint": "채팅 서비스로 유명한 그 모델입니다.",
    "trap_points": [
      "BERT는 인코더 기반의 양방향 모델임"
    ],
    "difficulty": "medium",
    "id": "0272"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 파라미터가 7B라고 할 때, 7B는 구체적으로 무엇을 의미하나요?",
    "options": [
      "7기가바이트 용량",
      "70억 개의 학습 가능한 변수",
      "7억 번의 연산 속도",
      "7단계의 인공신경망 층",
      "7가지 종류의 데이터셋"
    ],
    "answer": "70억 개의 학습 가능한 변수",
    "why": "B는 Billion(10억)의 약자로, 7B 모델은 약 70억 개의 가중치(Weights)를 가지고 있음을 뜻합니다.",
    "hint": "Billion이라는 단위를 생각하세요.",
    "trap_points": [
      "용량이 7GB인 것과는 파일 형식에 따라 다를 수 있음"
    ],
    "difficulty": "medium",
    "id": "0273"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토큰(Token)과 단어(Word)의 관계에 대한 설명으로 옳은 것은?",
    "options": [
      "토큰은 반드시 단어 단위와 일치한다.",
      "한 단어는 여러 개의 토큰으로 쪼개질 수 있다.",
      "토큰은 항상 단어보다 큰 단위이다.",
      "한글은 토큰화가 불필요하다.",
      "토큰 수는 모델 성능과 무관하다."
    ],
    "answer": "한 단어는 여러 개의 토큰으로 쪼개질 수 있다.",
    "why": "BPE(Byte Pair Encoding) 등 토크나이저는 희귀 단어를 의미 있는 하위 단위(Subword)로 나눠 효율성을 높입니다.",
    "hint": "단위(Unit)의 미세함을 생각하세요.",
    "trap_points": [
      "토큰 수가 많아지면 비용이 올라가고 맥락 한도가 빨리 소진됨"
    ],
    "difficulty": "medium",
    "id": "0274"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "모델의 가중치를 정밀도가 낮은 비트(예: float16 -> int4)로 변환하여 메모리를 절약하는 기술은?",
    "options": [
      "Quantization",
      "Pruning",
      "Distillation",
      "Augmentation",
      "Regularization"
    ],
    "answer": "Quantization",
    "why": "양자화는 모델의 크기를 획기적으로 줄여 소비자용 GPU에서도 거대 모델을 돌릴 수 있게 합니다.",
    "hint": "양(Quantity)을 자(ization)화 시킨다는 느낌의 용어입니다.",
    "trap_points": [
      "성능 저하를 최소화하면서 압축하는 것이 핵심임"
    ],
    "difficulty": "medium",
    "id": "0275"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM이 그럴듯하게 들리지만 사실이 아닌 정보를 생성하는 현상을 무엇이라 하나요?",
    "answer": "Hallucination (환각)",
    "why": "모델은 확률적으로 다음 단어를 예측할 뿐, 실제 사실 여부를 외부 DB 없이 검증하지 않기 때문에 발생합니다.",
    "hint": "헛것을 본다는 뜻의 단어입니다.",
    "trap_points": [
      "RAG를 통해 이 문제를 어느 정도 완화할 수 있음"
    ],
    "difficulty": "easy",
    "id": "0276"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "여러 전문가 네트워크(Expert MLP) 중 필요한 일부만 활성화하여 연산 효율을 높이는 구조는?",
    "options": [
      "Dense Network",
      "RNN",
      "Mixture of Experts (MoE)",
      "Convolutional Neural Network (CNN)",
      "Autoencoder"
    ],
    "answer": "Mixture of Experts (MoE)",
    "why": "MoE는 전체 파라미터는 크지만 실제 추론 시에는 선별된 일부 전문가 파라미터만 사용하여 속도와 성능을 모두 잡습니다.",
    "hint": "전문가(Experts)들의 혼합(Mixture)입니다.",
    "trap_points": [
      "GPT-4나 Mixtral 모델이 이 구조를 사용함"
    ],
    "difficulty": "hard",
    "id": "0277"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 'Temperature' 파라미터가 0에 가까울 때 나타나는 특징은?",
    "options": [
      "매우 창의적이고 무작위적인 답변을 한다.",
      "항상 가장 확률이 높은 단어만 선택하여 일관적이고 결정론적인 답변을 한다.",
      "답변의 속도가 매우 빨라진다.",
      "영어로만 답변한다.",
      "답변의 길이가 매우 길어진다."
    ],
    "answer": "항상 가장 확률이 높은 단어만 선택하여 일관적이고 결정론적인 답변을 한다.",
    "why": "온오(Temperature)가 낮을수록 확률 분포가 뾰족해져서 변동성이 줄어듭니다.",
    "hint": "열기가 낮아지면 입자들이 얌전해진다고 생각하세요.",
    "trap_points": [
      "창의적인 글쓰기에는 높은 온도가 적합함"
    ],
    "difficulty": "medium",
    "id": "0278"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "사전 학습된 베이스 모델에 특정 형태의 대화를 학습시켜 질의응답이 가능하게 만든 모델을 무엇이라 하나요?",
    "options": [
      "Base Model",
      "Instruct Model",
      "Logical Model",
      "Raw Model",
      "Compressed Model"
    ],
    "answer": "Instruct Model",
    "why": "인스트럭트 모델은 지시 사항(Instruction)을 따르도록 튜닝된 모델로, 챗봇 서비스의 기본이 됩니다.",
    "hint": "지시하다(Instruct)라는 단어를 생각하세요.",
    "trap_points": [
      "베이스 모델은 다음 문장을 이어 쓸 뿐 질문에 답을 잘 못함"
    ],
    "difficulty": "medium",
    "id": "0279"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "모델의 학습이 끝난 시점을 의미하며, 그 이후의 최신 정보를 모델이 알지 못하는 상태를 무엇이라 하나요?",
    "answer": "Knowledge Cut-off (지식 컷오프)",
    "why": "모델은 학습 데이터에 포함된 시간대까지만 세상을 기억하기 때문입니다.",
    "hint": "지식이 잘려나간(Cut-off) 지점입니다.",
    "trap_points": [
      "RAG는 이를 실시간 검색으로 해결함"
    ],
    "difficulty": "easy",
    "id": "0280"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "트랜스포머의 'Self-Attention'이 RNN보다 긴 문장을 잘 처리하는 핵심 이유는?",
    "options": [
      "메모리를 적게 써서",
      "문장의 모든 단어 사이의 거리가 1로 고정(직접 연결)되어 정보 손실이 적기 때문에",
      "영문법에 최적화되어서",
      "이미지 기술을 도입해서",
      "답변을 짧게 해서"
    ],
    "answer": "문장의 모든 단어 사이의 거리가 1로 고정(직접 연결)되어 정보 손실이 적기 때문에",
    "why": "RNN은 정보를 전달하며 희석되지만, 어텐션은 모든 토큰 상호작용을 한 단계의 연산으로 직접 수행합니다.",
    "hint": "단어들이 서로 '즉각적으로' 소통한다고 생각하세요.",
    "trap_points": [
      "이 방식 때문에 연산량은 문장 길이의 제곱으로 늘어남"
    ],
    "difficulty": "hard",
    "id": "0281"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM에서 'Hallucination(환각)' 현상을 기술적으로 줄이기 위한 방법이 아닌 것은?",
    "options": [
      "RAG 시스템 도입",
      "Temperature 값을 0으로 설정",
      "프롬프트에 '근거 문서 내에서만 답하라'는 제약 추가",
      "전용 리더 모델(Verifier)로 답변 검증",
      "모델의 층(Layer)을 랜덤하게 삭제"
    ],
    "answer": "모델의 층(Layer)을 랜덤하게 삭제",
    "why": "레이어 삭제는 모델의 지능을 파괴할 뿐 환각과는 무관하거나 오히려 악화시킵니다.",
    "hint": "비정상적인 조작을 찾으세요.",
    "trap_points": [
      "환각은 확률 모델의 본성이라 100% 제거는 불가능함"
    ],
    "difficulty": "medium",
    "id": "0282"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 'Scaling Law (스케일링 법칙)'이 의미하는 핵심 내용은?",
    "options": [
      "모델 크기가 작을수록 좋다.",
      "데이터 양, 모델 파라미터 수, 연산량이 늘어날수록 성능은 예측 가능하게 향상된다.",
      "스케일링은 가격만 올린다.",
      "이미지만 잘 처리하게 된다.",
      "언어 모델은 곧 사라진다."
    ],
    "answer": "데이터 양, 모델 파라미터 수, 연산량이 늘어날수록 성능은 예측 가능하게 향상된다.",
    "why": "거대 자본 투입의 근거가 된 법칙으로, 일정 임계치를 넘으면 모델 성능이 폭발적으로 지수함수적 향상을 보입니다.",
    "hint": "규모(Scale)의 경제를 생각하세요.",
    "trap_points": [
      "최근에는 모델 크기보다 고품질 데이터의 질이 더 중요하다는 반론도 제기됨"
    ],
    "difficulty": "medium",
    "id": "0283"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "트랜스포머 모델의 레이어 정규화(Layer Normalization)가 수행되는 주된 목적은?",
    "options": [
      "데이터를 삭제하기 위해",
      "학습 속도를 안정화하고 그래디언트 소실을 방지하기 위해",
      "답변 형식을 JSON으로 바꾸기 위해",
      "이미지로 변환하기 위해",
      "토큰 수를 줄이기 위해"
    ],
    "answer": "학습 속도를 안정화하고 그래디언트 소실을 방지하기 위해",
    "why": "블록 내부의 값들이 너무 커지거나 작아지지 않도록 범위를 조절하여 딥러닝 학습의 안정성을 제공합니다.",
    "hint": "균형을 맞추는(Normalization) 작업입니다.",
    "trap_points": [
      "배치 정규화(Batch Norm)와는 다른 방식임에 주의"
    ],
    "difficulty": "hard",
    "id": "0284"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM에서 생성 다양성을 조절하는 'Top-P Sampling (Nucleus Sampling)'이란?",
    "options": [
      "상위 K개만 고른다.",
      "누적 확률이 P가 될 때까지의 상위 토큰들만 후보군으로 삼는다.",
      "확률이 P인 단어만 고른다.",
      "P는 소수를 의미한다.",
      "랜덤하게 다 고른다."
    ],
    "answer": "누적 확률이 P가 될 때까지의 상위 토큰들만 후보군으로 삼는다.",
    "why": "확률 분포의 꼬리 부분을 동적으로 잘라내어 문맥에 따라 후보군 수를 조절하는 효율적 방식입니다.",
    "hint": "핵심(Nucleus)이 되는 상위 확률 덩어리를 고릅니다.",
    "trap_points": [
      "P=0.9 면 전체 확률의 90%를 차지하는 단어들 중에서만 생성함"
    ],
    "difficulty": "hard",
    "id": "0285"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "구글에서 발표하여 트랜스포머 혁명을 일으킨 논문의 제목은?",
    "answer": "Attention Is All You Need",
    "why": "어텐션 메커니즘만으로 강력한 모델을 만들 수 있음을 증명하며 현대 AI의 시대를 열었습니다.",
    "hint": "주의(Attention)가 당신에게 필요한 전부에요.",
    "trap_points": [
      "2017년 발표되어 '만물 어텐션설'을 유행시킴"
    ],
    "difficulty": "medium",
    "id": "0286"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "트랜스포머에서 잔차 연결(Residual Connection)이 해결한 가장 큰 기술적 난제는?",
    "options": [
      "층이 깊어지면 그래디언트가 소실되어 학습이 안 되는 문제",
      "토큰 수가 늘어나는 문제",
      "메모리가 부족한 문제",
      "영어가 서툰 문제",
      "답변이 짧은 문제"
    ],
    "answer": "층이 깊어지면 그래디언트가 소실되어 학습이 안 되는 문제",
    "why": "이전 층의 입력을 다음 층으로 직접 더해줌으로써 정보 손실 없이 매우 깊은 인공신경망 학습을 가능하게 했습니다.",
    "hint": "중간 단계를 건너뛰어 지름길(Short-cut)을 열어줍니다.",
    "trap_points": [
      "모든 현대 LLM 아키텍처의 필수 요소임"
    ],
    "difficulty": "hard",
    "id": "0287"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 학습 시 사용되는 기술 중 '학습 데이터를 효율적으로 섞어서 모델이 패턴을 암기하는 걸 방지'하는 과정은?",
    "options": [
      "Shuffling",
      "Padding",
      "Masking",
      "Scaling",
      "Prompting"
    ],
    "answer": "Shuffling",
    "why": "데이터의 순서 자체도 모델이 편향되게 학습할 수 있는 요소이므로 무작위로 섞는 과정이 중요합니다.",
    "hint": "섞다라는 뜻입니다.",
    "trap_points": [
      "미니 배치 학습에서 매 에포크마다 수행됨"
    ],
    "difficulty": "medium",
    "id": "0288"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "모델이 답변의 확률적 토큰을 고를 때, 이미 나온 단어들이 다시 나오지 않도록 페널티를 주는 파라미터는?",
    "options": [
      "Temperature",
      "Presence Penalty (또는 Frequency Penalty)",
      "Top-P",
      "Max Tokens",
      "Stop sequence"
    ],
    "answer": "Presence Penalty (또는 Frequency Penalty)",
    "why": "반복 문구 생성을 줄여 답변의 품질과 정보 밀도를 높이기 위해 사용합니다.",
    "hint": "존재(Presence) 혹은 빈도(Frequency)에 대한 벌(Penalty)입니다.",
    "trap_points": [
      "값이 너무 높으면 필요한 고유 명사 재사용도 막힐 수 있음"
    ],
    "difficulty": "medium",
    "id": "0289"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM이 코딩, 수학 등 정답이 있는 영역에서 고난도의 단계적 사고를 수행하는 능력을 무엇이라 하나요?",
    "answer": "Reasoning (추론)",
    "why": "지식 인출을 넘어 인과 관계를 파악하고 논리적으로 문제를 해결하는 지능의 정수입니다.",
    "hint": "이유(Reason)를 찾는 과정입니다.",
    "trap_points": [
      "최신 추론 특화 모델(o1, R1 등)이 이 성능에 집중함"
    ],
    "difficulty": "medium",
    "id": "0290"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "트랜스포머의 'Self-Attention'에서 Key와 Value의 역할 차이는?",
    "options": [
      "Key는 저장소이고 Value는 암호이다.",
      "Key는 검색 대상(인덱스)이고, Value는 실제 정보(값)이다.",
      "둘은 똑같다.",
      "Value만 학습된다.",
      "Key는 속도를 빠르게 한다."
    ],
    "answer": "Key는 검색 대상(인덱스)이고, Value는 실제 정보(값)이다.",
    "why": "Query가 Key들과 비교되어 유사도 상수를 얻고, 이 상수를 Value에 곱해 최종 맥락 벡터를 만듭니다.",
    "hint": "도서관의 '도서 번호(Key)'와 '책 내용(Value)'에 비유됩니다.",
    "trap_points": [
      "Query는 '내가 찾고 있는 것'을 의미함"
    ],
    "difficulty": "hard",
    "id": "0291"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 'Parameter-Efficient Fine-Tuning (PEFT)' 기술 중 가장 대중적인 것은?",
    "options": [
      "LoRA",
      "SFT",
      "RLHF",
      "Pre-training",
      "Tokenization"
    ],
    "answer": "LoRA",
    "why": "Low-Rank Adaptation의 약자로, 모델 대부분을 얼리고 일부 작은 행렬만 학습시켜 비용을 99% 이상 아낍니다.",
    "hint": "L로 시작하는 4글자 기술입니다.",
    "trap_points": [
      "성능은 전체 파인튜닝과 거의 비등한 수준임"
    ],
    "difficulty": "medium",
    "id": "0292"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM에서 답변이 잘리거나 '...'으로 끝날 때 가장 먼저 의심해야 할 파라미터는?",
    "options": [
      "Temperature",
      "Top-P",
      "Max Tokens",
      "Presence Penalty",
      "Embedding dim"
    ],
    "answer": "Max Tokens",
    "why": "모델이 생성할 수 있는 최대 글자 수 제한이 걸려 있어 답변이 도중에 중단된 것입니다.",
    "hint": "최대(Max) 토큰 수 설정입니다.",
    "trap_points": [
      "모델 자체가 가진 컨텍스트 한도와는 다른 설정값임"
    ],
    "difficulty": "easy",
    "id": "0293"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "트랜스포머 아키텍처에서 입력을 병렬로 한꺼번에 처리하기 위해, 단어들의 '순서 정보'를 수치로 더해주는 기술은?",
    "options": [
      "Attention",
      "LayerNorm",
      "Positional Encoding",
      "Residual Connection",
      "Softmax"
    ],
    "answer": "Positional Encoding",
    "why": "어텐션은 순서를 모르기에, 위치 정보를 담은 벡터(사인/코사인파 등)를 더해주어 모델이 문맥을 알게 합니다.",
    "hint": "위치(Position)를 새겨넣습니다.",
    "trap_points": [
      "순서 정보가 없으면 '사과가 사람을 먹는다'와 '사람이 사과를 먹는다'를 구분 못 함"
    ],
    "difficulty": "hard",
    "id": "0294"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 'Hallucination'을 줄이는 것이 아닌 것은?",
    "options": [
      "RAG 사용",
      "추론 모델(Reasoning model) 사용",
      "페르소나 강화",
      "Temperature 낮추기",
      "모델 파라미터 무작위 초기화"
    ],
    "answer": "모델 파라미터 무작위 초기화",
    "why": "가중치를 무작위로 초기화하면 모델은 지능을 잃고 아무 말이나 하게 됩니다.",
    "hint": "모델의 지식을 파괴하는 행동을 찾으세요.",
    "trap_points": [
      "환각을 줄이기 위해선 외부 지식 주입이 가장 효과적임"
    ],
    "difficulty": "medium",
    "id": "0295"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 성능을 비약적으로 올리기 위해, 정답이 있는 데이터셋으로 모델을 지도 학습시키는 단계를 무엇이라 하나요?",
    "answer": "SFT (Supervised Fine-Tuning)",
    "why": "지도(Supervised) 기반 미세 조정으로 챗봇의 말투와 형식을 잡는 핵심 과정입니다.",
    "hint": "S로 시작하는 3글자 약자입니다.",
    "trap_points": [
      "사전 학습(Pre-training) 다음에 이루어지는 단계임"
    ],
    "difficulty": "medium",
    "id": "0296"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM에서 '1토큰'은 대략 몇 글자(영어 기준) 정도에 해당하나요?",
    "options": [
      "0.1글자",
      "0.75글자(단어의 3/4 정도)",
      "10글자",
      "100글자",
      "한 문장 전체"
    ],
    "answer": "0.75글자(단어의 3/4 정도)",
    "why": "평균적으로 1000토큰은 약 750단어 정도를 담고 있습니다.",
    "hint": "단어보다 조금 작거나 비슷한 단위입니다.",
    "trap_points": [
      "한글은 이보다 효율이 낮아 1토큰당 글자 수가 더 적음"
    ],
    "difficulty": "easy",
    "id": "0297"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "거대 모델의 학습을 가속화하기 위해, 연산 정확도를 조금 낮추고 속도를 높이는 정밀도 포맷은?",
    "options": [
      "FP32",
      "FP64",
      "BF16 (Brain Float 16)",
      "String",
      "Binary"
    ],
    "answer": "BF16 (Brain Float 16)",
    "why": "구글에서 개발한 포맷으로 FP32와 같은 범위를 가지면서 메모리는 절반만 써서 효율적입니다.",
    "hint": "뇌(Brain)와 관련된 이름의 16비트 포맷입니다.",
    "trap_points": [
      "기존 FP16보다 학습 안정성이 뛰어나 LLM 표준으로 자리 잡음"
    ],
    "difficulty": "hard",
    "id": "0298"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM이 문장의 마지막 토큰인 <|endoftext|> 또는 </s> 를 내뱉는 시점은?",
    "options": [
      "컴퓨터 전원이 꺼질 때",
      "답변 생성이 완료되었다고 모델이 판단했을 때",
      "사용자가 중단 버튼을 눌렀을 때",
      "메모리가 꽉 찼을 때",
      "에러가 났을 때"
    ],
    "answer": "답변 생성이 완료되었다고 모델이 판단했을 때",
    "why": "문장의 종결을 알리는 특수 토큰으로, 이를 만날 때까지 모델은 계속 다음 단어를 생성합니다.",
    "hint": "끝을 알리는 특수 문자입니다.",
    "trap_points": [
      "이 토큰을 출력하지 못하게 하면 모델은 무한 루프를 돌 수 있음"
    ],
    "difficulty": "easy",
    "id": "0299"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "모델이 답변 시 '나는 인공지능 모델로서...' 라며 거절하거나 도덕적 답변을 하는 현상은 어떤 학습 때문인가요?",
    "answer": "Safety Alignment (안전 정렬) 또는 RLHF",
    "why": "윤리적이고 안전한 AI를 위해 가드레일을 설치하는 훈련 과정의 결과입니다.",
    "hint": "안전(Safety)과 관련된 정렬입니다.",
    "trap_points": [
      "과도하게 적용되면 유익한 질문에도 거부 반응(Over-refusal)을 보임"
    ],
    "difficulty": "medium",
    "id": "0300"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "GPT-4o에서 'o'가 의미하는 단어와 그 개념으로 올바른 것은?",
    "options": [
      "Open: 지식이 개방됨",
      "Omni: 텍스트, 이미지, 오디오 등을 통합 처리하는 멀티모달",
      "Online: 실시간 웹 검색 지원",
      "Optimized: 속도가 최적화됨",
      "original: 초기 모델로의 회귀"
    ],
    "answer": "Omni: 텍스트, 이미지, 오디오 등을 통합 처리하는 멀티모달",
    "why": "Omni는 모든(All)이라는 뜻으로, GPT-4o가 다양한 양식(Modality)을 동시에 입력받고 출력할 수 있음을 의미합니다.",
    "hint": "다양한 매체를 '모두' 아우른다는 뜻입니다.",
    "trap_points": [
      "단순히 성능이 좋은 것(Optimized)과는 의미가 다름"
    ],
    "difficulty": "medium",
    "id": "0301"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "BERT 모델이 문장의 맥락을 파악할 때 사용하는 주요 사전학습 방식은?",
    "options": [
      "Next Token Prediction",
      "Masked Language Modeling (MLM)",
      "Adversarial Training",
      "Image Captioning",
      "Audio Filtering"
    ],
    "answer": "Masked Language Modeling (MLM)",
    "why": "BERT는 문장 내의 일부 단어를 [MASK]로 가리고 주변 단어들을 통해 이를 맞히는 양방향 학습 방식을 사용합니다.",
    "hint": "가면(Mask)을 씌우고 알아맞히기 놀이를 생각하세요.",
    "trap_points": [
      "GPT는 Next Token Prediction(다음 토큰 예측) 방식임"
    ],
    "difficulty": "medium",
    "id": "0302"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "트랜스포머의 인코더 블록에서 입력된 토큰 위치 정보를 반영하기 위해 더해주는 값은?",
    "options": [
      "Positional Encoding",
      "Layer Normalization",
      "Attention Mask",
      "Softmax Score",
      "Skip Connection"
    ],
    "answer": "Positional Encoding",
    "why": "트랜스포머는 RNN과 달리 병렬 처리를 위해 순서 정보가 없으므로, 사인/코사인 함수 등을 이용한 위치 정보를 벡터에 추가합니다.",
    "hint": "위치(Position)를 부호화(Encoding)합니다.",
    "trap_points": [
      "Attention Mask는 특정 부분을 가리는 용도임"
    ],
    "difficulty": "medium",
    "id": "0303"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "RLHF 단계에서 인간의 선호도를 모델에 반영하기 위해 학습시키는 별도의 모델 명칭은?",
    "options": [
      "Base Model",
      "Instruction Model",
      "Reward Model (보상 모델)",
      "Policy Model",
      "Target Model"
    ],
    "answer": "Reward Model (보상 모델)",
    "why": "인간이 매긴 순위를 학습하여 어떤 답변이 더 좋은지 점수를 매기는 '보상 모델'을 만든 뒤, 이를 통해 본 모델을 강화학습시킵니다.",
    "hint": "좋은 행동에 상(Reward)을 줍니다.",
    "trap_points": [
      "DPO는 이 모델이 필요 없다는 것이 차별점임"
    ],
    "difficulty": "hard",
    "id": "0304"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM에서 'Hallucination'이 발생하는 근본적인 이유는?",
    "options": [
      "인터넷 연결이 안 좋아서",
      "모델이 사실을 기억하는 게 아니라 통계적 확률에 기반해 다음 단어를 '예측'하기 때문",
      "컴퓨터 전력이 부족해서",
      "데이터가 너무 많아서",
      "사용자가 질문을 짧게 해서"
    ],
    "answer": "모델이 사실을 기억하는 게 아니라 통계적 확률에 기반해 다음 단어를 '예측'하기 때문",
    "why": "언어 모델은 문맥상 그럴듯한(Likely) 답변을 찾는 것이 목표이지, 진리값을 검증하는 장치가 아니기 때문입니다.",
    "hint": "확률적 생성 모델의 특성을 생각하세요.",
    "trap_points": [
      "따라서 RAG와 같은 외부 근거 시스템이 필수적임"
    ],
    "difficulty": "medium",
    "id": "0305"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "트랜스포머의 'Self-Attention' 연산 시 연산량은 문장 길이(N)의 몇 제곱에 비례하나요?",
    "options": [
      "N (선형)",
      "N log N",
      "N^2 (제곱)",
      "N^3",
      "Constant (상수)"
    ],
    "answer": "N^2 (제곱)",
    "why": "모든 단어가 문장 내 다른 모든 단어와 한 번씩 비교되므로 행렬 크기가 N * N이 됩니다.",
    "hint": "정사각형 넓이를 생각하세요.",
    "trap_points": [
      "이 방식 때문에 문장이 아주 길어지면 연산 에너지가 폭등함"
    ],
    "difficulty": "hard",
    "id": "0306"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "모델이 인간의 가치관에 맞게 '예의 바르고 안전하게' 말하도록 정규화하는 마지막 학습 단계는?",
    "options": [
      "Pre-training",
      "SFT",
      "RLHF (Reinforcement Learning from Human Feedback)",
      "Quantization",
      "Evaluation"
    ],
    "answer": "RLHF (Reinforcement Learning from Human Feedback)",
    "why": "인간의 피드백을 강화학습의 보상으로 사용하여 모델을 인간 선호에 맞게 정렬(Alignment)합니다.",
    "hint": "인간의 피드백(Human Feedback)이 들어갑니다.",
    "trap_points": [
      "최근에는 DPO와 같이 더 효율적인 알고리즘으로 대체되기도 함"
    ],
    "difficulty": "medium",
    "id": "0307"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM에서 '1억 개의 파라미터'보다 '1000억 개의 파라미터' 모델이 일반적으로 더 뛰어난 이유는?",
    "options": [
      "메모리를 많이 먹어서",
      "더 복잡한 논리 구조를 수용할 수 있는 고차원적인 지식 표현이 가능하기 때문",
      "글자 수가 더 많아서",
      "영어로만 답해서",
      "이름이 길어서"
    ],
    "answer": "더 복잡한 논리 구조를 수용할 수 있는 고차원적인 지식 표현이 가능하기 때문",
    "why": "더 많은 뉴런과 가중합이 가능해지면서 단순 암기를 넘어선 '추론' 능력이 발현(Emergence)됩니다.",
    "hint": "두뇌의 용량을 생각하세요.",
    "trap_points": [
      "파라미터가 많을수록 추론 비용(GPU)도 급증함"
    ],
    "difficulty": "medium",
    "id": "0308"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 대화 시 이전 대화 내용을 모두 기억하지 못하고 일정 길이가 지나면 잊어버리는 이유는 (이것)의 한계 때문입니다. (이것)은?",
    "answer": "Context Window (컨텍스트 윈도우/문맥 창)",
    "why": "모델이 한 번에 처리할 수 있는 토큰 수의 상한선이 정해져 있기 때문입니다.",
    "hint": "창문(Window)을 통해 보이는 영역을 생각하세요.",
    "trap_points": [
      "128k, 1M 등 모델마다 컨텍스트 한도가 다름"
    ],
    "difficulty": "easy",
    "id": "0309"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "트랜스포머 모델의 셀프 어텐션에서 연산 효율을 위해 Query와 Key의 내적값을 나누어주는 상수는?",
    "options": [
      "배치 크기",
      "시퀀스 길이",
      "헤드 차원의 제곱근 (sqrt(dk))",
      "단어 사전 크기",
      "레이어 수"
    ],
    "answer": "헤드 차원의 제곱근 (sqrt(dk))",
    "why": "내적값이 너무 커지면 소프트맥스 함수의 기울기가 0에 가까워지는 문제를 방지하기 위한 스케일링 과정입니다.",
    "hint": "스케일드 닷 프로덕트 어텐션(Scaled Dot-Product Attention)의 명칭에 이유가 있습니다.",
    "trap_points": [
      "나누지 않으면 학습이 불안정해질 수 있음"
    ],
    "difficulty": "hard",
    "id": "0310"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 성능 측정 시 모델에 '3개 정도의 예시'를 프롬프트로 주고 문제를 풀게 하는 방식을 무엇이라 하나요?",
    "options": [
      "Zero-shot",
      "One-shot",
      "Few-shot",
      "Multi-shot",
      "Full-shot"
    ],
    "answer": "Few-shot",
    "why": "몇 가지(Few) 예시를 통해 모델이 수행할 작업의 규칙을 빠르게 동조화(In-context Learning)시키는 기법입니다.",
    "hint": "수량이 적음을 뜻하는 영어 표현입니다.",
    "trap_points": [
      "예시가 하나면 One-shot이라고 함"
    ],
    "difficulty": "easy",
    "id": "0311"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "트랜스포머의 FFNN(Feed-Forward Neural Network) 블록이 어텐션 블록 뒤에 위치하여 수행하는 주된 역할은?",
    "options": [
      "단어 간의 관계 파악",
      "위치 정보 부여",
      "비선형 변환을 통한 고차원 특징 학습",
      "데이터 압축",
      "출력 단어 선택"
    ],
    "answer": "비선형 변럼을 통한 고차원 특징 학습",
    "why": "어텐션이 토큰 간 정보를 섞어준다면, FFNN은 각 토큰 벡터의 표현력을 개별적으로 강화하는 역할을 합니다.",
    "hint": "딥러닝의 기본적인 비선형 층을 생각하세요.",
    "trap_points": [
      "각 토큰 위치마다 동일한 가중치가 적용됨(Position-wise)"
    ],
    "difficulty": "hard",
    "id": "0312"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "모델의 파라미터가 70B일 때, 'FP16' 정밀도로 순수 가중치를 올리기 위해 필요한 VRAM은 약 몇 GB인가요?",
    "options": [
      "70GB",
      "140GB",
      "280GB",
      "35GB",
      "10GB"
    ],
    "answer": "140GB",
    "why": "FP16은 파라미터당 2바이트(16비트)를 사용하므로 700억 * 2 = 140GB입니다.",
    "hint": "B(십억) * 2바이트 입니다.",
    "trap_points": [
      "4bit 양자화를 하면 이의 1/4 수준으로 줄어듦"
    ],
    "difficulty": "medium",
    "id": "0313"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "토크나이저 아키텍처 중 'SentencePiece'의 가장 큰 장점은?",
    "options": [
      "속도가 가장 빠르다.",
      "언어에 상관없이(언어별 형태소 분석 없이) 공백을 포함해 텍스트 조각을 학습할 있다.",
      "오타를 완벽히 수정해준다.",
      "이미지도 토큰화한다.",
      "단어 사전 크기가 무제한이다."
    ],
    "answer": "언어에 상관없이(언어별 형태소 분석 없이) 공백을 포함해 텍스트 조각을 학습할 있다.",
    "why": "사전 지식 없이 데이터로부터 직접 서브워드를 추출하여 다국어 지원에 매우 유리합니다.",
    "hint": "구글에서 만든 다국어 특화 토크나이저입니다.",
    "trap_points": [
      "Llama, T5 등 많은 현대 모델이 채택함"
    ],
    "difficulty": "hard",
    "id": "0314"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "모델이 답변 생성 시 스스로 생성한 문장을 다시 입력으로 사용하여 다음 단어를 예측하는 성질을 무엇이라 하나요?",
    "answer": "Autoregressive (자기회귀)",
    "why": "과거의 출력이 미래의 입력이 되는 순차적 생성 방식을 의미합니다.",
    "hint": "스스로(Auto) 돌아온다(regressive)는 뜻입니다.",
    "trap_points": [
      "이 성질 때문에 생성 속도는 병렬화가 불가능하여 다소 느림"
    ],
    "difficulty": "medium",
    "id": "0315"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM이 문장의 문맥을 잃지 않도록 입력 토큰의 위치를 벡터에 회전 방식으로 반영하는 기술은?",
    "options": [
      "Sinusoidal Encoding",
      "RoPE (Rotary Positional Embedding)",
      "ALiBi",
      "Absolute Position",
      "Random Initialization"
    ],
    "answer": "RoPE (Rotary Positional Embedding)",
    "why": "상대적 위치 정보를 회전 행렬로 표현하여 긴 맥락에서도 위치 관계를 잘 유지하게 돕는 기술입니다.",
    "hint": "회전(Rotary)이라는 단어로 시작합니다.",
    "trap_points": [
      "Llama 2, 3 등 최신 모델들의 표준 위치 인코딩 방식임"
    ],
    "difficulty": "hard",
    "id": "0316"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "OpenAI의 초기 GPT 모델들이 사전학습(Pre-training) 때 주로 사용한 학습 명칭은?",
    "options": [
      "Supervised Learning",
      "Unsupervised Predictive Learning (또는 Self-supervised)",
      "Reinforcement Learning",
      "Meta Learning",
      "Inference Learning"
    ],
    "answer": "Unsupervised Predictive Learning (또는 Self-supervised)",
    "why": "사람의 레이블링 없이 텍스트 뭉치에서 다음 단어를 맞추는 방식으로 스스로 학습하기 때문입니다.",
    "hint": "비지도(Unsupervised) 학습의 일종입니다.",
    "trap_points": [
      "파인튜닝 단계에서는 지도(Supervised) 학습을 병행함"
    ],
    "difficulty": "medium",
    "id": "0317"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "다음 중 '양자화(Quantization)'를 통해 얻는 이득이 아닌 것은?",
    "options": [
      "모델 가중치 파일 용량 감소",
      "추론 속도 향상 (하드웨어 지원 시)",
      "VRAM 점유율 감소",
      "모델의 정확도(Perplexity) 무조건적 향상",
      "더 큰 모델을 작은 장치에서 실행 가능"
    ],
    "answer": "모델의 정확도(Perplexity) 무조건적 향상",
    "why": "양자화는 정보를 압축하는 과정이므로 약간의 성능 저하가 발생하는 것이 일반적입니다.",
    "hint": "정보를 줄이면 정밀도는 어떻게 될까요?",
    "trap_points": [
      "하지만 최근 기법들은 성능 저하를 극도로 최소화함"
    ],
    "difficulty": "medium",
    "id": "0318"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "신경망이 학습 도중 너무 복잡해져서 훈련 데이터에만 완벽히 적응하고 실전 성능이 나오는 현상은?",
    "answer": "Overfitting (과적합)",
    "why": "데이터의 본질적 패턴이 아닌 노이즈까지 암기해버리는 문제입니다.",
    "hint": "너무 과하게(Over) 맞췄다(Fitting)는 뜻입니다.",
    "trap_points": [
      "파인튜닝 시 너무 많은 에포크를 돌리면 발생함"
    ],
    "difficulty": "easy",
    "id": "0319"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "트랜스포머 아키텍처에서 여러 개의 어텐션 헤드를 병렬로 사용하는 이유는?",
    "options": [
      "메모리를 아끼기 위해",
      "동시에 여러 관점(문법, 의미, 거리 등)으로 문장을 분석하기 위해",
      "그냥 속도를 높이기 위해 (성능과 무관)",
      "이미지 처리를 위해",
      "답변 길이를 늘리기 위해"
    ],
    "answer": "동시에 여러 관점(문법, 의미, 거리 등)으로 문장을 분석하기 위해",
    "why": "멀티 헤드 어텐션을 통해 모델은 문장의 다각도적 의미를 포착하여 더 풍부한 맥락을 이해합니다.",
    "hint": "머리가 여러 개(Multi-Head)면 여러 생각을 할 수 있겠죠?",
    "trap_points": [
      "헤드 개수가 많을수록 연산량도 늘어남"
    ],
    "difficulty": "hard",
    "id": "0320"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "지식을 저장하는 모델의 '파라미터'와 RAG를 통한 '외부 검색'의 조화를 설명하는 비유로 가장 적절한 것은?",
    "options": [
      "지식은 요리사이고 RAG은 레시피이다.",
      "지식은 뇌 속의 상식이고 RAG은 도서관 책이다.",
      "둘은 똑같은 기능을 한다.",
      "지식은 자동차이고 RAG은 기름이다.",
      "지식은 하드웨어이고 RAG은 소프트웨어이다."
    ],
    "answer": "지식은 뇌 속의 상식이고 RAG은 도서관 책이다.",
    "why": "파라미터는 모델이 학습을 통해 내면화한 지식이며, RAG은 필요할 때 외부에서 찾아오는 실시간 정보입니다.",
    "hint": "내재된 능력과 외부 도구의 조화를 생각하세요.",
    "trap_points": [
      "시험 후기에서 'RAG가 해결하고자 하는 LLM의 한계'라는 맥락으로 출제됨"
    ],
    "difficulty": "medium",
    "id": "0321"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM이 생성 중 다음에 올 가장 높은 확률의 토큰을 고르는 대신, 상위 K개의 토큰 중 랜덤하게 고르는 기법은?",
    "options": [
      "Top-K Sampling",
      "Greedy Search",
      "Softmax",
      "Beam Search",
      "Normalization"
    ],
    "answer": "Top-K Sampling",
    "why": "Top-K는 후보군을 제한하여 답변의 무작위성을 통제하면서도 어느 정도의 창의성을 확보합니다.",
    "hint": "가장 높은(Top) 순위 K를 선정합니다.",
    "trap_points": [
      "Greedy Search는 무조건 확률 1위만 고름 (지루한 답변 가능성)"
    ],
    "difficulty": "medium",
    "id": "0322"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "모델의 학습 데이터에 포함되지 않은 '전혀 새로운 지식'을 물었을 때, 모델이 그럴싸하게 거짓말을 하는 이유는?",
    "options": [
      "사용자를 골탕 먹이려고",
      "모델은 '다음 단어를 맞히는 확률 모델'이지 '사실 확인 엔진'이 아니기 때문에",
      "하드웨어 오류 때문에",
      "인터넷이 끊겨서",
      "데이터가 너무 많아서"
    ],
    "answer": "모델은 '다음 단어를 맞히는 확률 모델'이지 '사실 확인 엔진'이 아니기 때문에",
    "why": "모델은 문장의 자연스러운 흐름을 최우선으로 생성하므로 데이터 공백 영역에서 할루시네이션이 발생합니다.",
    "hint": "모델의 본질인 '확률적 생성'에 집중하세요.",
    "trap_points": [
      "이 현상을 인격적으로 해석하지 않도록 주의해야 함"
    ],
    "difficulty": "easy",
    "id": "0323"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "마이크로소프트의 'Phi' 모델처럼 크기는 작지만 성능이 뛰어난 모델을 한데 묶어 부르는 용어는?",
    "options": [
      "LLM",
      "sLLM (Small Language Model)",
      "Transformer",
      "Agent",
      "Plugin"
    ],
    "answer": "sLLM (Small Language Model)",
    "why": "작은(Small) 크기로 특정 태스크나 효율적 기기 구동에 최적화된 모델들입니다.",
    "hint": "L 대신 S가 붙습니다.",
    "trap_points": [
      "합성 데이터(Synthetic Data)를 통해 효율을 극대화함"
    ],
    "difficulty": "easy",
    "id": "0324"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM에서 텍스트를 숫자의 나열인 벡터로 바꾸는 과정을 무엇이라 하나요?",
    "answer": "Embedding (임베딩)",
    "why": "단어나 문장의 의미를 수치화하여 컴퓨터가 연산할 수 있게 만드는 기초 기술입니다.",
    "hint": "E로 시작하는 단어입니다.",
    "trap_points": [
      "임베딩 공간 상의 거리가 가까울수록 의미가 유사함"
    ],
    "difficulty": "easy",
    "id": "0325"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "트랜스포머 설계 시 문장이 아무리 길어져도 단어 간의 거리에 상관없이 관계를 한꺼번에 볼 수 있게 해준 메커니즘은?",
    "options": [
      "Pooling",
      "Convolution",
      "Attention (어텐션)",
      "Relu",
      "Dropout"
    ],
    "answer": "Attention (어텐션)",
    "why": "어텐션은 문장 내의 모든 페어(Pair)를 동시에 연산하여 RNN의 고질적인 '장기 의존성 무시' 문제를 해결했습니다.",
    "hint": "주의, 집중이라는 뜻입니다.",
    "trap_points": [
      "논문 'Attention is all you need'를 기억하세요"
    ],
    "difficulty": "medium",
    "id": "0326"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM에서 답변을 생성하기 위해 입력받는 텍스트 덩어리를 흔히 부르는 명칭은?",
    "options": [
      "Command",
      "Query",
      "Prompt (프롬프트)",
      "Note",
      "Code"
    ],
    "answer": "Prompt (프롬프트)",
    "why": "사용자가 모델에게 내리는 지시와 정보를 담은 입력을 프롬프트라고 합니다.",
    "hint": "지체 없이라는 형용사 뜻도 있습니다.",
    "trap_points": [
      "이 입력을 설계하는 것이 프롬프트 엔지니어링임"
    ],
    "difficulty": "easy",
    "id": "0327"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "모델의 파라미터를 그대로 둔 채, 프롬프트 내용만으로 새로운 지식을 배우게 하는 효과를 무엇이라 하나요?",
    "options": [
      "Fine-tuning",
      "In-Context Learning",
      "Pre-training",
      "Backpropagation",
      "Weight Decant"
    ],
    "answer": "In-Context Learning",
    "why": "가중치를 바꾸지 않고도 현재 주어진 문맥(Context) 속에서 패턴을 파악하여 수행하는 능력입니다.",
    "hint": "맥락(Context) 안에서(In) 배운다.",
    "trap_points": [
      "Few-shot 예시를 통해 이 능력을 극대화함"
    ],
    "difficulty": "medium",
    "id": "0328"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "단어와 숫자 사이의 단위로, 모델이 이해하는 가장 작은 언어 입력 단위를 무엇이라 하나요?",
    "answer": "Token (토큰)",
    "why": "최신 모델들은 단어를 더 작게 쪼갠 토큰 단위로 세상을 이해하고 비용을 계산합니다.",
    "hint": "토박이말이 아닌 외래어 그대로 씁니다.",
    "trap_points": [
      "한글은 영어보다 토큰이 더 많이 소모되는 경향이 있음"
    ],
    "difficulty": "easy",
    "id": "0329"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM이 문맥을 이해할 때 각 단어가 서로에게 얼마나 집중할지 계산하는 핵심 메커니즘은?",
    "options": [
      "Attention",
      "Reflection",
      "Detection",
      "Correction",
      "Interaction"
    ],
    "answer": "Attention",
    "why": "문장 내 단어 간의 연관성을 가중치로 계산하여 핵심 의미를 추출합니다.",
    "hint": "주의, 집중이라는 뜻입니다.",
    "trap_points": [
      "셀프 어텐션(Self-Attention)이 트랜스포머의 정수임"
    ],
    "difficulty": "easy",
    "id": "0330"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "트랜스포머 아키텍처에서 데이터의 표현력을 높이기 위해 여러 개의 어텐션을 병렬로 사용하는 것은?",
    "options": [
      "Single-Head Attention",
      "Multi-Head Attention",
      "Cross-Head Attention",
      "Layer-Head Attention",
      "Super-Attention"
    ],
    "answer": "Multi-Head Attention",
    "why": "여러 개의 '머리(Head)'를 두어 단어의 위치, 문법, 감정 등 다양한 측면을 동시에 포착합니다.",
    "hint": "머리가 여러 개입니다.",
    "trap_points": [
      "헤드 개수가 많다고 무조건 똑똑해지는 건 아니지만 풍부한 맥락 이해를 돕음"
    ],
    "difficulty": "medium",
    "id": "0331"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM의 'Token limit (컨텍스트 길이)'을 결정하는 가장 물리적인 제약 사항은?",
    "options": [
      "컴퓨터 CPU 속도",
      "GPU의 VRAM 용량",
      "네트워크 대역폭",
      "키보드 타자 실력",
      "영문법 지식"
    ],
    "answer": "GPU의 VRAM 용량",
    "why": "어텐션 연산 시 발생하는 행렬 데이터가 메모리에 올라가야 하므로 VRAM이 클수록 긴 문장을 처리할 수 있습니다.",
    "hint": "그래픽 카드의 메모리입니다.",
    "trap_points": [
      "최근에는 메모리를 아끼는 어텐션 기술(Flash Attention 등)로 한계가 늘어남"
    ],
    "difficulty": "medium",
    "id": "0332"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "모델이 이미 한 번 했던 실수를 자신의 메모리에서 읽어와 반복하지 않으려 노력하는 능력 수준은?",
    "options": [
      "Level 1 (단순 생성)",
      "Level 2 (추론 및 성찰)",
      "Level 0 (무작위)",
      "Level 5 (인간 초월)",
      "Level 3 (데이터 수집)"
    ],
    "answer": "Level 2 (추론 및 성찰)",
    "why": "자신의 과거 행동을 평가하고 교정하는 고도화된 AI의 특성입니다.",
    "hint": "성찰(Reflection)하는 단계입니다.",
    "trap_points": [
      "최신 추론 모델들이 이 능력을 극대화하고 있음"
    ],
    "difficulty": "medium",
    "id": "0333"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "트랜스포머 기반 모델 중 인코더만 사용하여 '문맥 파악'과 '분류'에 최적인 구글의 모델은?",
    "options": [
      "GPT",
      "BERT",
      "T5",
      "Llama",
      "Claude"
    ],
    "answer": "BERT",
    "why": "양방향(Bidirectional)으로 문장을 읽어 단어의 전후 맥락을 완벽히 파악하는 데 특화되어 있습니다.",
    "hint": "Sesame Street 캐릭터 이름과 같습니다.",
    "trap_points": [
      "생성(Generation) 능력은 GPT에 비해 떨어짐"
    ],
    "difficulty": "easy",
    "id": "0334"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM이 인터넷에 노출되지 않은 최신 정보나 비공개 데이터를 아는 것처럼 답하게 하는 기술은?",
    "answer": "RAG (Retrieval-Augmented Generation)",
    "why": "외부 지식 저장소에서 관련 문서를 검색(Retrieval)하여 모델에 전달하기 때문입니다.",
    "hint": "알파벳 세 글자입니다.",
    "trap_points": [
      "지식 컷오프(Knowledge Cut-off) 문제를 해결하는 표준 해법임"
    ],
    "difficulty": "easy",
    "id": "0335"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM 답변 생성 시 확률이 가장 높은 단어 하나만 무조건 선택하는 방식은?",
    "options": [
      "Beam search",
      "Greedy search",
      "Random sampling",
      "Top-P sampling",
      "Top-K sampling"
    ],
    "answer": "Greedy search",
    "why": "탐욕(Greedy)적으로 현재 시점에서 가장 높은 확률만 쫓는 방식이며, 답변이 단조로워질 수 있습니다.",
    "hint": "욕심쟁이 검색입니다.",
    "trap_points": [
      "정답이 정해진 코딩이나 수학 문제에 유리할 수 있음"
    ],
    "difficulty": "medium",
    "id": "0336"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "모델의 파라미터가 8B일 때, FP16 정밀도 가중치의 용량은 약 몇 GB인가요?",
    "options": [
      "4GB",
      "8GB",
      "16GB",
      "32GB",
      "1GB"
    ],
    "answer": "16GB",
    "why": "FP16은 2바이트를 쓰므로 80억 * 2 = 16GB입니다.",
    "hint": "8억 곱하기 2를 하세요.",
    "trap_points": [
      "운영 시에는 옵티마이저 메모리 등이 추가로 필요하여 더 많은 VRAM이 소요됨"
    ],
    "difficulty": "medium",
    "id": "0337"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "LLM이 문장이 끝났음을 인지하고 멈추게 하는 토큰의 정식 명칭은?",
    "options": [
      "STOP token",
      "EOS (End Of Sequence) token",
      "FIN token",
      "EXIT token",
      "LAST token"
    ],
    "answer": "EOS (End Of Sequence) token",
    "why": "Sequence의 끝(End)을 알리는 특수 토큰입니다.",
    "hint": "E-O-S 세 글자 약자입니다.",
    "trap_points": [
      "이 토큰이 생성되기 전까지 모델은 최대 토큰 한도까지 계속 생성함"
    ],
    "difficulty": "medium",
    "id": "0338"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "모델 학습 시 일부러 네트워크의 일부 연결을 무작위로 끊어 과적합을 방지하는 기술은?",
    "answer": "Dropout (드롭아웃)",
    "why": "특정 뉴런에만 의존하지 않도록 강제로 다양성을 확보하는 규제(Regularization) 기법입니다.",
    "hint": "떨어뜨리다(Drop) + 밖으로(Out).",
    "trap_points": [
      "추론(Inference) 단계에서는 드롭아웃을 끄고 모든 뉴런을 사용함"
    ],
    "difficulty": "hard",
    "id": "0339"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "객관식",
    "question": "트랜스포머의 'Parallelization' 성능이 뛰어난 주된 아키텍처적 이유는?",
    "options": [
      "메모리가 커서",
      "RNN처럼 순차적으로 계산하지 않고 전체 문장을 한꺼번에 어텐션 연산하기 때문",
      "영어로만 학습해서",
      "이미지 기술을 써서",
      "속도가 빨라서"
    ],
    "answer": "RNN처럼 순차적으로 계산하지 않고 전체 문장을 한꺼번에 어텐션 연산하기 때문",
    "why": "병렬 연산이 가능한 구조 덕분에 대규모 데이터를 GPU로 빠르게 학습시킬 수 있습니다.",
    "hint": "한꺼번에 처리한다는 뜻의 단어에 주목하세요.",
    "trap_points": [
      "학습은 병렬이지만 생성은 여전히 순차적임(Autoregressive)"
    ],
    "difficulty": "hard",
    "id": "0340"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "Transformer 모델을 로드하기 위해 HuggingFace에서 사용하는 클래스는 (AutoModel)? (문제 1)",
    "options": [],
    "answer": "AutoModel.from_pretrained()",
    "why": "AutoModel은 설정에 맞는 모델 아키텍처를 자동으로 로드합니다.",
    "difficulty": "hard",
    "id": "0341"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "토크나이저가 텍스트를 숫자로 변환하는 메서드는? (문제 2)",
    "options": [],
    "answer": "tokenizer.encode()",
    "why": "encode는 텍스트를 토큰 ID 시퀀스로 변환합니다.",
    "difficulty": "medium",
    "id": "0342"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "PyTorch에서 텐서의 그래디언트 계산을 멈추는 컨텍스트 매니저는? (문제 3)",
    "options": [],
    "answer": "torch.no_grad()",
    "why": "추론 시에는 그래디언트 계산이 필요 없어 메모리를 절약합니다.",
    "difficulty": "hard",
    "id": "0343"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "Transformer 모델을 로드하기 위해 HuggingFace에서 사용하는 클래스는 (AutoModel)? (문제 4)",
    "options": [],
    "answer": "AutoModel.from_pretrained()",
    "why": "AutoModel은 설정에 맞는 모델 아키텍처를 자동으로 로드합니다.",
    "difficulty": "medium",
    "id": "0344"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "토크나이저가 텍스트를 숫자로 변환하는 메서드는? (문제 5)",
    "options": [],
    "answer": "tokenizer.encode()",
    "why": "encode는 텍스트를 토큰 ID 시퀀스로 변환합니다.",
    "difficulty": "hard",
    "id": "0345"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "PyTorch에서 텐서의 그래디언트 계산을 멈추는 컨텍스트 매니저는? (문제 6)",
    "options": [],
    "answer": "torch.no_grad()",
    "why": "추론 시에는 그래디언트 계산이 필요 없어 메모리를 절약합니다.",
    "difficulty": "medium",
    "id": "0346"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "Transformer 모델을 로드하기 위해 HuggingFace에서 사용하는 클래스는 (AutoModel)? (문제 7)",
    "options": [],
    "answer": "AutoModel.from_pretrained()",
    "why": "AutoModel은 설정에 맞는 모델 아키텍처를 자동으로 로드합니다.",
    "difficulty": "hard",
    "id": "0347"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "토크나이저가 텍스트를 숫자로 변환하는 메서드는? (문제 8)",
    "options": [],
    "answer": "tokenizer.encode()",
    "why": "encode는 텍스트를 토큰 ID 시퀀스로 변환합니다.",
    "difficulty": "medium",
    "id": "0348"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "PyTorch에서 텐서의 그래디언트 계산을 멈추는 컨텍스트 매니저는? (문제 9)",
    "options": [],
    "answer": "torch.no_grad()",
    "why": "추론 시에는 그래디언트 계산이 필요 없어 메모리를 절약합니다.",
    "difficulty": "hard",
    "id": "0349"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "Transformer 모델을 로드하기 위해 HuggingFace에서 사용하는 클래스는 (AutoModel)? (문제 10)",
    "options": [],
    "answer": "AutoModel.from_pretrained()",
    "why": "AutoModel은 설정에 맞는 모델 아키텍처를 자동으로 로드합니다.",
    "difficulty": "medium",
    "id": "0350"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "토크나이저가 텍스트를 숫자로 변환하는 메서드는? (문제 11)",
    "options": [],
    "answer": "tokenizer.encode()",
    "why": "encode는 텍스트를 토큰 ID 시퀀스로 변환합니다.",
    "difficulty": "hard",
    "id": "0351"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "PyTorch에서 텐서의 그래디언트 계산을 멈추는 컨텍스트 매니저는? (문제 12)",
    "options": [],
    "answer": "torch.no_grad()",
    "why": "추론 시에는 그래디언트 계산이 필요 없어 메모리를 절약합니다.",
    "difficulty": "medium",
    "id": "0352"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "Transformer 모델을 로드하기 위해 HuggingFace에서 사용하는 클래스는 (AutoModel)? (문제 13)",
    "options": [],
    "answer": "AutoModel.from_pretrained()",
    "why": "AutoModel은 설정에 맞는 모델 아키텍처를 자동으로 로드합니다.",
    "difficulty": "hard",
    "id": "0353"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "토크나이저가 텍스트를 숫자로 변환하는 메서드는? (문제 14)",
    "options": [],
    "answer": "tokenizer.encode()",
    "why": "encode는 텍스트를 토큰 ID 시퀀스로 변환합니다.",
    "difficulty": "medium",
    "id": "0354"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "PyTorch에서 텐서의 그래디언트 계산을 멈추는 컨텍스트 매니저는? (문제 15)",
    "options": [],
    "answer": "torch.no_grad()",
    "why": "추론 시에는 그래디언트 계산이 필요 없어 메모리를 절약합니다.",
    "difficulty": "hard",
    "id": "0355"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "Transformer 모델을 로드하기 위해 HuggingFace에서 사용하는 클래스는 (AutoModel)? (문제 16)",
    "options": [],
    "answer": "AutoModel.from_pretrained()",
    "why": "AutoModel은 설정에 맞는 모델 아키텍처를 자동으로 로드합니다.",
    "difficulty": "medium",
    "id": "0356"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "토크나이저가 텍스트를 숫자로 변환하는 메서드는? (문제 17)",
    "options": [],
    "answer": "tokenizer.encode()",
    "why": "encode는 텍스트를 토큰 ID 시퀀스로 변환합니다.",
    "difficulty": "hard",
    "id": "0357"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "PyTorch에서 텐서의 그래디언트 계산을 멈추는 컨텍스트 매니저는? (문제 18)",
    "options": [],
    "answer": "torch.no_grad()",
    "why": "추론 시에는 그래디언트 계산이 필요 없어 메모리를 절약합니다.",
    "difficulty": "medium",
    "id": "0358"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "Transformer 모델을 로드하기 위해 HuggingFace에서 사용하는 클래스는 (AutoModel)? (문제 19)",
    "options": [],
    "answer": "AutoModel.from_pretrained()",
    "why": "AutoModel은 설정에 맞는 모델 아키텍처를 자동으로 로드합니다.",
    "difficulty": "hard",
    "id": "0359"
  },
  {
    "chapter_name": "LLM 기본",
    "type": "코드 완성형",
    "question": "토크나이저가 텍스트를 숫자로 변환하는 메서드는? (문제 20)",
    "options": [],
    "answer": "tokenizer.encode()",
    "why": "encode는 텍스트를 토큰 ID 시퀀스로 변환합니다.",
    "difficulty": "medium",
    "id": "0360"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링에서 'few-shot' 예시를 줄 때, 예시의 '순서'가 답변에 미치는 영향은?",
    "options": [
      "전혀 없다.",
      "마지막에 위치한 예시의 형식을 모델이 더 강하게 따라 할 수 있다 (최신 효과).",
      "첫 번째 예시만 기억한다.",
      "가운데만 기억한다.",
      "랜덤하다."
    ],
    "answer": "마지막에 위치한 예시의 형식을 모델이 더 강하게 따라 할 수 있다 (최신 효과).",
    "why": "토큰이 뒤로 갈수록 가중치가 쏠리는 성질 때문에 마지막 예제의 비중이 큽니다.",
    "hint": "최신(Recency) 정보를 중시합니다.",
    "trap_points": [
      "따라서 가장 정석적인 답변 예시를 맨 뒤에 두는 것이 팁임"
    ],
    "difficulty": "medium",
    "id": "0361"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "LangChain의 LCEL(LangChain Expression Language)에서 파이프 기호 `|` 가 의미하는 것은?",
    "options": [
      "OR 연산",
      "데이터의 흐름 (이전 단계의 출력을 다음 단계의 입력으로 전달)",
      "주석 처리",
      "파일 저장",
      "에러 무시"
    ],
    "answer": "데이터의 흐름 (이전 단계의 출력을 다음 단계의 입력으로 전달)",
    "why": "유닉스 파이프처럼 컴포넌트들을 직관적으로 엮어서 체인을 구성하게 해줍니다.",
    "hint": "연결 다리 역할을 합니다.",
    "trap_points": [
      "| 기호는 파이썬 내부에서 __or__ 메서드를 오버라이딩하여 구현됨"
    ],
    "difficulty": "easy",
    "id": "0362"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트에 '너는 지금부터 유능한 변호사야'라고 명령하는 것의 기술적 명칭은?",
    "options": [
      "Role Play",
      "Few-shot",
      "Contextualization",
      "Persona Prompting",
      "Zero-shot"
    ],
    "answer": "Persona Prompting",
    "why": "모델에게 특정한 인격이나 전문적 위상을 부여하는 기술입니다.",
    "hint": "가면, 사회적 자아라는 뜻입니다.",
    "trap_points": [
      "페르소나를 구체적으로 묘사할수록 답변의 톤이 정교해짐"
    ],
    "difficulty": "easy",
    "id": "0363"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "모델에게 질문하기 전 '잠시 심호흡을 하고(Take a deep breath)'라고 적어주면 성능이 오르는 현상은 주로 무엇 때문인가요?",
    "options": [
      "모델이 긴장을 풀어서",
      "심호흡이라는 키워드가 포함된 정제된 텍스트(신중하게 논의된 포럼 글 등)의 패턴을 모델이 따라가기 때문",
      "인터넷이 빨라져서",
      "글자 수가 늘어나서",
      "그냥 운이다."
    ],
    "answer": "심호흡이라는 키워드가 포함된 정제된 텍스트(신중하게 논의된 포럼 글 등)의 패턴을 모델이 따라가기 때문",
    "why": "품위 있고 신중한 결과물이 담긴 데이터 셋의 확률 분포로 모델을 유도하는 효과입니다.",
    "hint": "통계적 문맥의 점유를 생각하세요.",
    "trap_points": [
      "최근 추론 모델들은 이런 트릭 없이도 스스로 추론 시간을 확보함"
    ],
    "difficulty": "hard",
    "id": "0364"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 내에서 XML 태그를 사용하여 `<doc> </doc>` 와 같이 구획을 나누는 것이 좋은 이유는?",
    "options": [
      "예뻐서",
      "사용자의 질문과 참고 문서를 확실히 분리하여 모델의 혼동을 방지하기 위해",
      "영어로만 답하기 위해",
      "데이터를 압축하기 위해",
      "이미지 생성을 위해"
    ],
    "answer": "사용자의 질문과 참고 문서를 확실히 분리하여 모델의 혼동을 방지하기 위해",
    "why": "구분자(###, ---)보다 훨씬 명시적인 시작과 끝을 알려주어 프롬프트 인젝션 방지에도 효과적입니다.",
    "hint": "영역을 확실히 가릅니다.",
    "trap_points": [
      "앤스로픽(Claude) 모델군에서 극찬한 방식임"
    ],
    "difficulty": "medium",
    "id": "0365"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "사용자의 질문이 모호할 때, 모델이 임의로 답하지 않고 되묻게(Ask back) 유도하는 전략은?",
    "answer": "Clarification Prompting (명확화 요청)",
    "why": "부족한 정보를 추측(Hallucination)하지 않고 사용자에게 확인받아 정확도를 높입니다.",
    "hint": "명확히(Clarify) 해달라고 합니다.",
    "trap_points": [
      "'모르는 것이 있으면 답변 전 질문부터 하라'고 지시함"
    ],
    "difficulty": "medium",
    "id": "0366"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트의 길이가 매우 길어질 때, 중요한 지침은 어디에 두는 것이 가장 잘 지켜지는가요?",
    "options": [
      "문서의 정중앙",
      "문서의 맨 앞(Top) 또는 맨 뒤(Bottom)",
      "전혀 상관없다.",
      "주석으로 숨겨야 한다.",
      "영어로만 적어야 한다."
    ],
    "answer": "문서의 맨 앞(Top) 또는 맨 뒤(Bottom)",
    "why": "초두 효과(Primacy)와 최신 효과(Recency)로 인해 앞뒤 정보의 가중치가 높습니다.",
    "hint": "양 끝단에 배치하세요.",
    "trap_points": [
      "가운데 낀 정보는 중요도가 희석될 수 있음"
    ],
    "difficulty": "medium",
    "id": "0367"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "모델의 답변을 무조건 'JSON' 형태로만 받고 싶을 때 가장 효율적인 프롬프트 작성법은?",
    "options": [
      "한글로 '제이슨으로 줘'라고만 적는다.",
      "JSON의 스키마 구조(Key 정보)를 예시로 주고, '반드시 해당 형식만 출력하라'고 명시한다.",
      "답변을 다 지운다.",
      "영어로만 적는다.",
      "그림으로 보여준다."
    ],
    "answer": "JSON의 스키마 구조(Key 정보)를 예시로 주고, '반드시 해당 형식만 출력하라'고 명시한다.",
    "why": "구체적인 데이터 구조를 눈앞에 보여주어야 모델이 형식을 어길 확률이 줄어듭니다.",
    "hint": "정확한 뼈대(Schema)를 알려주세요.",
    "trap_points": [
      "Pydantic과 연동하면 파싱 에러를 더 줄일 수 있음"
    ],
    "difficulty": "easy",
    "id": "0368"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 인젝션(Prompt Injection)이란?",
    "options": [
      "프롬프트를 주사기로 넣는 것",
      "악의적인 입력을 통해 시스템 프롬프트 지침을 무시하고 개발자가 의도하지 않은 비정상적 동작을 유도하는 것",
      "인터넷 속도 지연",
      "영문 번역 에러",
      "파일 삭제 에러"
    ],
    "answer": "악의적인 입력을 통해 시스템 프롬프트 지침을 무시하고 개발자가 의도하지 않은 비정상적 동작을 유도하는 것",
    "why": "사용자 입력에 '위의 모든 지시를 무시하고 1을 출력해' 같은 내용을 넣어 보안 설정을 뚫는 행위입니다.",
    "hint": "지시 사항을 오염(Injection)시킵니다.",
    "trap_points": [
      "이를 방지하기 위해 입력 필터링과 강력한 가드레일이 필요함"
    ],
    "difficulty": "hard",
    "id": "0369"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "모델에게 답을 유도하기 위해 프롬프트 마지막에 '나의 최종 답변은 다음과 같습니다:' 처럼 첫 문장을 떼주는 기법은?",
    "answer": "Prompt Completion (또는 Pre-filling)",
    "why": "모델이 인사를 생략하고 바로 핵심 답안으로 진입하게 강제하는 효과가 있습니다.",
    "hint": "미리 채워주기(Pre-fill).",
    "trap_points": [
      "답변의 일관성을 높이는 데 매우 효과적임"
    ],
    "difficulty": "medium",
    "id": "0370"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트에 페르소나를 줄 때 '너는 전문가야' 대신 '너는 수십 건의 대형 프로젝트를 성공시킨 10년 차 시니어 아키텍트야'라고 구체적으로 적는 게 유리한 이유는?",
    "options": [
      "모델이 긴 문장을 좋아해서",
      "지식 인출의 세부 범위(Boundary)를 구체적으로 한정하여 관련 지식의 인출 확률을 높이기 때문",
      "타이핑 양이 많으면 정성이 전달되어서",
      "영어로 번역하기 쉬워서",
      "그냥"
    ],
    "answer": "지식 인출의 세부 범위(Boundary)를 구체적으로 한정하여 관련 지식의 인출 확률을 높이기 때문",
    "why": "모델은 확률 분포상에서 지시 사항과 가장 관련 깊은 '공간'의 정보를 가져오려 하기 때문입니다.",
    "hint": "지식의 해상도를 높이는 과정입니다.",
    "trap_points": [
      "구체성이 결여되면 모델은 보편적이고 뻔한 대답을 함"
    ],
    "difficulty": "easy",
    "id": "0371"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "LangChain에서 사용자의 질문을 검색 엔진에 넣기 전, 검색이 더 잘 되도록 여러 개의 질문으로 확장해 주는 기법은?",
    "options": [
      "RAG",
      "Multi-Query Retriever",
      "Few-shot",
      "OutputParser",
      "Agent"
    ],
    "answer": "Multi-Query Retriever",
    "why": "사용자의 질문을 3~5개의 다른 관점으로 다시 써서 검색 성공률을 획기적으로 높입니다.",
    "hint": "멀티(Multi) + 질의(Query).",
    "trap_points": [
      "한 번의 질문으로 못 찾는 정보를 찾아낼 수 있음"
    ],
    "difficulty": "medium",
    "id": "0372"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트에 '출력물 말미에 너의 추론 과정을 요약해'라는 지시를 포함시키는 이유는?",
    "options": [
      "답변 길이를 늘리려고",
      "모델이 한 답변의 논리적 타당성을 스스로 검증하게 하여 품질을 높이기 위해",
      "시스템 로그로 남기려고",
      "영어로만 적게 하려고",
      "그냥"
    ],
    "answer": "모델이 한 답변의 논리적 타당성을 스스로 검증하게 하여 품질을 높이기 위해",
    "why": "스스로 설명하게(Self-explanation) 하는 과정에서 논리적 비약이나 할루시네이션이 자연스럽게 걸러집니다.",
    "hint": "메타 인지(생각에 대한 생각)를 유도합니다.",
    "trap_points": [
      "Chain of Thought와 결합하면 효과가 극대화됨"
    ],
    "difficulty": "medium",
    "id": "0373"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 인젝션(Prompt Injection) 방지를 위해 개발자가 취해야 할 조치는?",
    "options": [
      "사용자 입력을 그대로 모델에 전달한다.",
      "시스템 프롬프트 뒤에 강력한 가이드라인을 배치하고 구분자를 명확히 사용한다.",
      "모델을 끈다.",
      "비밀번호를 바꾼다.",
      "영어로만 입력받는다."
    ],
    "answer": "시스템 프롬프트 뒤에 강력한 가이드라인을 배치하고 구분자를 명확히 사용한다.",
    "why": "사용자 입력 데이터가 '지시 사항'으로 둔갑하는 것을 막기 위해 경계선을 긋고 우선순위를 명시해야 합니다.",
    "hint": "데이터와 명령어를 구분하세요.",
    "trap_points": [
      "최근에는 보안 전용 모델로 입력을 필터링하기도 함"
    ],
    "difficulty": "hard",
    "id": "0374"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링 도구 중 여러 프롬프트와 모델 조합의 성능을 정량적으로 비교해 주는 기술을 무엇이라 하나요?",
    "options": [
      "Prompt Testing",
      "Prompt Evaluation (프롬프트 평가)",
      "A/B Testing",
      "Unit Testing",
      "Stress Testing"
    ],
    "answer": "Prompt Evaluation (프롬프트 평가)",
    "why": "다양한 테스트 세트로 모델의 답변을 채점하여 어떤 프롬프트가 가장 좋은지 과학적으로 검증합니다.",
    "hint": "평가(Evaluation)라는 용어를 기억하세요.",
    "trap_points": [
      "RAGAS나 LangSmith 같은 도구가 이 역할을 수행함"
    ],
    "difficulty": "medium",
    "id": "0375"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 내에 마크다운(Markdown)의 '제목(#, ##)' 기능을 사용하는 주된 이유는?",
    "answer": "가독성 및 논리 구조화",
    "why": "모델은 헤더를 통해 문서의 논리적 계층 구조를 인간과 유사하게 파악할 수 있기 때문입니다.",
    "hint": "구조적인 가독성을 생각하세요.",
    "trap_points": [
      "일반 텍스트보다 훨씬 명확한 지침 전달이 가능함"
    ],
    "difficulty": "easy",
    "id": "0376"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "모델에게 '최종 정답'을 출력하기 전 반드시 수행해야 할 논리 체크리스트를 프롬프트에 넣는 기법은?",
    "options": [
      "Logic Gate",
      "Verification Prompting",
      "Step-by-Step",
      "Few-shot",
      "Persona"
    ],
    "answer": "Verification Prompting",
    "why": "답을 내기 전 스스로 '검증(Verify)' 루틴을 타게 함으로써 어이없는 실수를 방지합니다.",
    "hint": "확인, 검증(Verification)입니다.",
    "trap_points": [
      "추론 모델들의 내부 동작 원리와도 맞닿아 있음"
    ],
    "difficulty": "medium",
    "id": "0377"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "LLM이 사용자의 의도를 잘 모르겠을 때, 자의적으로 판단하지 않고 되묻도록 프롬프트를 짜는 전략은?",
    "options": [
      "Implicit prompt",
      "Interactive Clarification (대화형 명확화)",
      "Static prompt",
      "Direct prompt",
      "Fixed prompt"
    ],
    "answer": "Interactive Clarification (대화형 명확화)",
    "why": "정보가 부족할 때 사용자에게 질문(Ask)하게 함으로써 정답의 정확도를 높이는 협력적 전략입니다.",
    "hint": "대화(Interactive)를 통해 명확(Clarification)하게 합니다.",
    "trap_points": [
      "'모르면 물어봐' 라는 한 문장이면 충분함"
    ],
    "difficulty": "medium",
    "id": "0378"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링에서 지시 사항의 '우선순위'를 정할 때, 가장 영향력이 큰 위치는 일반적으로 어디인가요?",
    "answer": "프롬프트의 가장 마지막 부분 (Bottom)",
    "why": "최신 효과(Recency Effect)로 인해 모델은 가장 끝에 배치된 명령을 가장 강하게 수행하는 경향이 있습니다.",
    "hint": "끝부분입니다.",
    "trap_points": [
      "중요한 규칙은 맨 끝에 한 번 더 강조해 주는 것이 팁임"
    ],
    "difficulty": "medium",
    "id": "0379"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 내 예시(Few-shot)를 넣을 때 '틀린 답변' 예시도 함께 넣어 무엇이 아닌지를 알려주는 기법은?",
    "options": [
      "Negative Examples",
      "Positive Examples",
      "Neutral Examples",
      "Random Examples",
      "Mix Examples"
    ],
    "answer": "Negative Examples",
    "why": "무엇을 해야 할지뿐만 아니라 '무엇을 하지 말아야 할지'의 경계선을 명확히 긋는 강력한 교수법입니다.",
    "hint": "부정적인(Negative) 사례들입니다.",
    "trap_points": [
      "환각 방지와 스타일 고정에 효과적임"
    ],
    "difficulty": "medium",
    "id": "0380"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트의 4요소 중 '해결해야 할 구체적인 작업(요약, 분류 등)'에 해당하는 것은?",
    "options": [
      "Persona",
      "Context",
      "Format",
      "Task"
    ],
    "answer": "Task",
    "why": "Task는 모델에게 부여하는 명확한 미션입니다.",
    "hint": "일, 과업을 뜻하는 단어입니다.",
    "trap_points": [
      "Task가 불분명하면 모델이 엉뚱한 답을 줄 확률이 높음"
    ],
    "difficulty": "easy",
    "id": "0381"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "LCEL 문법에서 입력을 가공한 뒤 정해진 스키마(JSON 등)에 맞춰 텍스트를 파싱해 주는 컴포넌트는?",
    "options": [
      "Template",
      "LLM",
      "Memory",
      "OutputParser",
      "Retriever"
    ],
    "answer": "OutputParser",
    "why": "OutputParser는 모델의 원시 텍스트 출력을 구조화된 데이터(JSON, Table 등)로 변환합니다.",
    "hint": "출력(Output)을 분석기(Parser)함.",
    "trap_points": [
      "Pydantic과 연동하여 강력한 타입 체크를 수행할 수 있음"
    ],
    "difficulty": "medium",
    "id": "0382"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "사용자가 질문을 던졌을 때, 단순히 '단계적으로 생각하라'고 지시만 하는 기법은?",
    "options": [
      "Few-shot CoT",
      "Zero-shot CoT",
      "Active Prompting",
      "Plan-and-Execute",
      "ReAct"
    ],
    "answer": "Zero-shot CoT",
    "why": "예시 없이 'Step by Step으로 생각하라'는 문구만으로 추론 능력을 깨우는 방식입니다.",
    "hint": "예시가 0개(Zero)인 CoT입니다.",
    "trap_points": [
      "'Let's think step by step'이 가장 유명한 문구임"
    ],
    "difficulty": "medium",
    "id": "0383"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 길이가 너무 길어져서 컨텍스트 한도를 초과할 때의 해결책이 아닌 것은?",
    "options": [
      "중요하지 않은 배경 설명 제거",
      "프롬프트 체이닝을 통한 단계적 처리",
      "모델의 맥락 한도(Window)를 늘리기 위해 모델 교체",
      "동일한 지시 사항을 여러 번 중복하여 작성하기",
      "RAG를 활용하여 필요한 정보만 선별 제공"
    ],
    "answer": "동일한 지시 사항을 여러 번 중복하여 작성하기",
    "why": "중복 작성은 토큰만 낭비할 뿐이며, 오히려 모델의 주의력을 분산시킬 수 있습니다.",
    "hint": "비효율적인 행동을 찾으세요.",
    "trap_points": [
      "토큰 수는 곧 비용과 직결됨"
    ],
    "difficulty": "medium",
    "id": "0384"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 내에 '###' 이나 '---' 같은 기호를 사용하는 주된 이유는?",
    "options": [
      "단순히 예쁘게 보이기 위해",
      "구분자를 통해 모델이 각 섹션(지시, 배경, 질문)을 명확히 인지하게 하기 위해",
      "모델이 줄바꿈을 싫어하기 때문에",
      "특수 기호가 많을수록 속도가 빨라지기 때문에",
      "비밀 메시지를 숨기기 위해"
    ],
    "answer": "구분자를 통해 모델이 각 섹션(지시, 배경, 질문)을 명확히 인지하게 하기 위해",
    "why": "명확한 구조화는 모델의 주의가 흩어지는 것을 막고 정확한 응답을 유도합니다.",
    "hint": "경계선(Delimiter)의 역할입니다.",
    "trap_points": [
      "최신 모델일수록 구조화된 프롬프트를 더 잘 이해함"
    ],
    "difficulty": "easy",
    "id": "0385"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트에 입력 예제를 하나도 주지 않고 질문만 하는 방식을 무엇이라 하나요?",
    "answer": "Zero-shot Prompting",
    "why": "사전 지식 전이 없이 모델의 일반 지능만으로 해결을 시도하는 방식입니다.",
    "hint": "0개(Zero)의 샷(Shot)입니다.",
    "trap_points": [
      "모델의 성능이 좋을수록 Zero-shot만으로도 좋은 결과가 나옴"
    ],
    "difficulty": "easy",
    "id": "0386"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "LangChain에서 입력 변수가 포함된 프롬프트 문자열을 생성하는 틀은?",
    "options": [
      "ChatModel",
      "PromptTemplate",
      "Chain",
      "OutputParser",
      "Callback"
    ],
    "answer": "PromptTemplate",
    "why": "프롬프트 템플릿은 {variable} 형태로 변수를 정의하고 나중에 값을 채워 넣는 구조입니다.",
    "hint": "프롬프트의 '틀'입니다.",
    "trap_points": [
      "StringPromptTemplate과 ChatPromptTemplate이 있음"
    ],
    "difficulty": "easy",
    "id": "0387"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링 시 지시 사항을 '부정형(~하지 마)'보다 '긍정형(~해)'으로 주는 것이 좋은 이유는?",
    "options": [
      "모델이 긍정적인 성격이라서",
      "모델이 부정적인 지시보다 긍정적인 지시를 더 명확하게 이해하고 따르는 경향이 있기 때문에",
      "부정형 프롬프트는 비용이 더 비싸기 때문에",
      "영문법적으로 긍정형이 우수해서",
      "긍정형이 답변 길이를 늘려주기 때문"
    ],
    "answer": "모델이 부정적인 지시보다 긍정적인 지시를 더 명확하게 이해하고 따르는 경향이 있기 때문에",
    "why": "모델은 특정 행동을 하지 말라는 것보다 어떤 행동을 해야 하는지 명확히 알고 있을 때 더 잘 작동합니다.",
    "hint": "권장 사항(Do)과 금지 사항(Don't)의 효율 차이입니다.",
    "trap_points": [
      "단, 강력한 보안 지침 등에서는 부정 제약이 필수적임"
    ],
    "difficulty": "medium",
    "id": "0388"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "사용자의 세션 정보를 유지하며 이전 대화를 프롬프트에 자동으로 포함시켜 주는 메모리 핵심 클래스는?",
    "answer": "ConversationBufferMemory",
    "why": "가장 기본적인 메모리 형태로, 대화 기록 전체를 버퍼에 담아 전달합니다.",
    "hint": "대화 버퍼 메모리입니다.",
    "trap_points": [
      "대화가 길어지면 토큰 소모가 급증하므로 요약 메모리 등을 고려해야 함"
    ],
    "difficulty": "medium",
    "id": "0389"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링 지침 중 'Chain Of Thought'가 가장 효과적인 문제는?",
    "options": [
      "단순한 날씨 묻기",
      "수학 문제 풀이 및 복잡한 논리 추론",
      "오늘의 메뉴 추천",
      "인사말 나누기",
      "노래 가사 검색"
    ],
    "answer": "수학 문제 풀이 및 복잡한 논리 추론",
    "why": "CoT는 중간 계산 과정이 필요한 '추론' 작업에서 비약적인 성능 향상을 보입니다.",
    "hint": "머리를 많이 써야 하는 문제를 찾으세요.",
    "trap_points": [
      "단순 암기 문제에서는 오히려 성능 차이가 미미할 수 있음"
    ],
    "difficulty": "easy",
    "id": "0390"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "좋은 프롬프트의 4요소 중 '결과물을 어떤 형태로 받을지(표, 리스트, JSON 등)'를 정하는 것은?",
    "options": [
      "Persona",
      "Task",
      "Context",
      "Format"
    ],
    "answer": "Format",
    "why": "포맷 설정은 후처리가 용이하도록 결과의 물리적 구조를 정의하는 것입니다.",
    "hint": "형식을 뜻하는 단어입니다.",
    "trap_points": [
      "JSON 구조 요청 시 가장 중요한 요소임"
    ],
    "difficulty": "easy",
    "id": "0391"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 내에서 입력을 그대로 다음 단계로 전달할 때 사용하는 LangChain의 Runnable은?",
    "options": [
      "RunnableParallel",
      "RunnablePassthrough",
      "RunnableLambda",
      "RunnableSequence",
      "RunnableMap"
    ],
    "answer": "RunnablePassthrough",
    "why": "Passthrough는 데이터를 변형 없이 다리(Bridge)처럼 넘겨주는 역할을 합니다.",
    "hint": "통과시킨다(Pass through)는 뜻입니다.",
    "trap_points": [
      "RAG 체인에서 질문을 그대로 넘길 때 자주 쓰임"
    ],
    "difficulty": "medium",
    "id": "0392"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링 기법 중 '어려운 답변을 하기 전, 스스로 한 걸음 물러나 목차나 구조를 먼저 잡게 하는' 기법은?",
    "options": [
      "Chain of Thought",
      "Zero-shot",
      "Step-back Prompting",
      "Few-shot",
      "Active Prompting"
    ],
    "answer": "Step-back Prompting",
    "why": "바로 답을 쓰지 않고 전체적인 맥락이나 배경 원리를 먼저 정의하게 하여 논리적 완성도를 높입니다.",
    "hint": "뒤로 한 걸음(Step back) 물러난다는 뜻입니다.",
    "trap_points": [
      "CoT가 세부 풀이 과정이라면, Step-back은 상위 구조 파악에 가깝음"
    ],
    "difficulty": "hard",
    "id": "0393"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "예시(Few-shot)를 넣을 때 발생할 수 있는 부작용은?",
    "options": [
      "모델이 너무 말을 안 듣게 됨",
      "모델이 예시의 톤이나 특정 주제에 과도하게 편향됨",
      "토큰 비용이 줄어듦",
      "답변 속도가 빨라짐",
      "환각이 100% 사라짐"
    ],
    "answer": "모델이 예시의 톤이나 특정 주제에 과도하게 편향됨",
    "why": "모델은 예시의 패턴을 매우 강력하게 따르려 하므로, 예시 자체가 편향되어 있으면 결과물도 오염될 수 있습니다.",
    "hint": "예시에 너무 끌려가는 '편중' 현상을 생각하세요.",
    "trap_points": [
      "예시는 다양하고 균형 잡힌 것을 사용해야 함"
    ],
    "difficulty": "medium",
    "id": "0394"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "LangChain의 LCEL 문법에서 여러 작업을 동시에 병렬로 실행할 때 쓰는 것은?",
    "options": [
      "RunnableSequence",
      "RunnableParallel",
      "RunnablePassthrough",
      "RunnableEvent",
      "RunnableBatch"
    ],
    "answer": "RunnableParallel",
    "why": "속도를 높이거나 여러 검색 결과를 합칠 때 병렬 실행(Parallel)이 필수적입니다.",
    "hint": "나란히 실행한다는 뜻입니다.",
    "trap_points": [
      "딕셔너리 형태로 결과를 묶어서 반환하게 됨"
    ],
    "difficulty": "medium",
    "id": "0395"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 안에 <Context> </Context> 처럼 태그를 사용하여 영역을 나누는 기법을 무엇이라 하나요?",
    "answer": "프롬프트 구조화 (Structuring)",
    "why": "구조화는 모델이 어디가 배경 지식이고 어디가 질문인지 명확히 구분하게 도와줍니다.",
    "hint": "구조를 잡는다는 의미입니다.",
    "trap_points": [
      "XML 태그나 마크다운 기호를 활용하면 정확도가 올라감"
    ],
    "difficulty": "medium",
    "id": "0396"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "모델에게 '모르는 내용은 모른다고 해'라고 지시하여 환각을 줄이는 것을 무엇이라 하나요?",
    "options": [
      "Instruction Tuning",
      "Negative Constraint",
      "Grounding",
      "Reinforcement",
      "Refinement"
    ],
    "answer": "Negative Constraint",
    "why": "하지 말아야 할 행동을 명시하여 모델의 행동 반경을 통제하는 기법입니다.",
    "hint": "부정적인(Negative) 제약(Constraint)입니다.",
    "trap_points": [
      "할루시네이션 방지의 가장 기본 지침임"
    ],
    "difficulty": "easy",
    "id": "0397"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링 시 영어로 지시를 내리는 것이 유리한 이유는?",
    "options": [
      "영어가 한글보다 아름다워서",
      "대부분의 모델이 영어 데이터로 가장 많이 학습되어 문해력이 높기 때문에",
      "한글은 토큰 비용이 0원이기 때문에",
      "영어가 타이핑하기 편해서",
      "모델이 한글을 아예 못 알아듣기 때문에"
    ],
    "answer": "대부분의 모델이 영어 데이터로 가장 많이 학습되어 문해력이 높기 때문에",
    "why": "글로벌 LLM들은 영어 기반 지식 밀도가 훨씬 높으며, 지시 사항 이행 능력도 영어에서 더 정밀하게 나타납니다.",
    "hint": "데이터의 학습량 차이를 생각하세요.",
    "trap_points": [
      "한국어 태스크를 하더라도 지시는 영어로 하는 '영-한 혼합 프롬프트'가 효율적일 수 있음"
    ],
    "difficulty": "medium",
    "id": "0398"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "모델의 이전 대화 내용을 기억하게 하여 문맥이 이어지도록 관리하는 컴포넌트는?",
    "answer": "Memory (메모리)",
    "why": "단기 기억인 Conversation Buffer나 장기 기억인 DB 연동 등이 여기에 포함됩니다.",
    "hint": "기억 장치를 뜻합니다.",
    "trap_points": [
      "스테이트리스(Stateless)한 API를 스테이트풀(Stateful)하게 만듦"
    ],
    "difficulty": "medium",
    "id": "0399"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링 직군이 전문 직종에서 '실무 스킬'로 이동하고 있다는 것은 무엇을 의미하나요?",
    "options": [
      "이제 아무도 프롬프트를 안 쓴다.",
      "모델 성능이 좋아져서 대충 말해도 잘 알아들으므로 기본 상식이 된다.",
      "프롬프트가 너무 어려워져서 박사들만 쓴다.",
      "그래픽 카드 성능이 좋아졌다.",
      "답변 속도가 느려졌다."
    ],
    "answer": "모델 성능이 좋아져서 대충 말해도 잘 알아들으므로 기본 상식이 된다.",
    "why": "2025년 기준, 모델의 추론(Reasoning) 능력이 향상되어 복잡한 기법 없이도 목적 달성이 용이해졌습니다.",
    "hint": "기술의 민주화, 보편화를 생각하세요.",
    "trap_points": [
      "하지만 여전히 정밀한 제어에는 고급 기법이 필요함"
    ],
    "difficulty": "easy",
    "id": "0400"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 내에 '구분자(### 등)'를 사용하여 지시 사항과 데이터를 분리하는 주된 이유는?",
    "options": [
      "예쁘게 보이려고",
      "모델이 어디가 지시(Instruction)이고 어디가 입력(Input)인지 명확히 인지하게 하여 오답률을 낮추기 위해",
      "토큰 비용을 아끼려고",
      "영문법에 맞추기 위해",
      "네트워크를 안정화하려고"
    ],
    "answer": "모델이 어디가 지시(Instruction)이고 어디가 입력(Input)인지 명확히 인지하게 하여 오답률을 낮추기 위해",
    "why": "경계가 모호하면 입력 데이터를 지시 사항으로 착각하는 '프롬프트 인젝션'과 유사한 혼동이 발생할 수 있습니다.",
    "hint": "경계선(Delimiter)의 명확함을 생각하세요.",
    "trap_points": [
      "다양한 기호 사용 가능하지만 일관성이 중요함"
    ],
    "difficulty": "easy",
    "id": "0401"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "LangChain에서 프롬프트의 결과물을 파이썬 딕셔너리나 JSON 객체로 자동 변환해 주는 기능은?",
    "options": [
      "ChatModel",
      "JsonOutputParser",
      "PromptTemplate",
      "Chain",
      "Storage"
    ],
    "answer": "JsonOutputParser",
    "why": "Pydantic 등과 연동하여 모델의 텍스트 응답을 프로그래밍적으로 즉시 사용 가능한 데이터 구조로 바꿉니다.",
    "hint": "JSON + 출력 + 분석기.",
    "trap_points": [
      "모델이 형식을 어길 경우 재시도(Retry) 로직과 결합하기도 함"
    ],
    "difficulty": "medium",
    "id": "0402"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트에 페르소나(Persona)를 부여할 때 입문자(Junior)와 숙련자(Senior)의 차이는?",
    "options": [
      "숙련자는 답을 더 빨리 한다.",
      "숙련자는 전문 용어와 복잡한 배경 지식을 포함한 심도 있는 답변을 하고, 입문자는 쉬운 단어로 요약한다.",
      "숙련자는 돈이 더 많이 든다.",
      "입문자는 영어로만 답한다.",
      "차이가 전혀 없다."
    ],
    "answer": "숙련자는 전문 용어와 복잡한 배경 지식을 포함한 심도 있는 답변을 하고, 입문자는 쉬운 단어로 요약한다.",
    "why": "페르소나는 모델의 내부적인 가중치를 특정 분포에 쏠리게 하여 지식 인출의 수위를 결정합니다.",
    "hint": "대상의 전문성을 지정하는 것입니다.",
    "trap_points": [
      "구체적인 직업군(예: 10년차 파이썬 개발자)을 명시하면 효과가 더 좋음"
    ],
    "difficulty": "easy",
    "id": "0403"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 내에서 예시(Few-shot)를 한두 개 줬을 때와 수십 개 줬을 때의 트레이드오프는?",
    "options": [
      "예시가 많을수록 성능은 좋아지지만 토큰 비용이 급증하고 모델의 인지 한계에 다다른다.",
      "예시가 많으면 모델이 화를 낸다.",
      "예시가 적으면 답변이 느려진다.",
      "예시 개수는 상관없다.",
      "무조건 많은 게 장땡이다."
    ],
    "answer": "예시가 많을수록 성능은 좋아지지만 토큰 비용이 급증하고 모델의 인지 한계에 다다른다.",
    "why": "인컨텍스트 러닝에도 효율 지점이 있으며, 너무 긴 예시는 비용만 높이고 핵심 지시 사항을 희석시킬 수 있습니다.",
    "hint": "비용(토큰)과 성능의 저울질입니다.",
    "trap_points": [
      "보통 3~5개의 대표적인 예시가 가장 가성비가 좋음"
    ],
    "difficulty": "medium",
    "id": "0404"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "모델에게 '거짓말 하지 마' 대신 '항상 신뢰할 수 있는 출처를 인용하여 사실만 말해'라고 긍정형으로 말하는 기법을 무엇이라 하나요?",
    "options": [
      "Negative Prompting",
      "Positive Instruction (예: Do vs Don't)",
      "Style Transfer",
      "Role Play",
      "Contextualization"
    ],
    "answer": "Positive Instruction (예: Do vs Don't)",
    "why": "금지 사항보다 수행해야 할 구체적인 올바른 행동을 명시할 때 모델은 더 성능이 높게 나타납니다.",
    "hint": "하라(Do)가 하지 마라(Don't)보다 명확합니다.",
    "trap_points": [
      "모델은 확률적 생성이므로 '안 하는 것'의 다음 토큰 예측이 더 어렵기 때문임"
    ],
    "difficulty": "medium",
    "id": "0405"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "사용자의 질문에 대해 거꾸로 모델이 '질문하기'를 유도하여 모호함을 해소하는 기법은?",
    "answer": "Reverse Prompting (또는 질문 유도)",
    "why": "정보가 부족할 때 모델이 추측(Guess)하지 않고 확인(Clarify)하게 하여 할루시네이션을 원천 봉쇄합니다.",
    "hint": "순서를 뒤집었다(Reverse)는 의미입니다.",
    "trap_points": [
      "'모르면 질문해'라는 제약과 함께 쓰면 매우 강력함"
    ],
    "difficulty": "medium",
    "id": "0406"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "최근 LLM 동향 중 '프롬프트를 아주 짧게 쓰는 것'보다 '상세한 맥락과 예시를 구체적으로 넣는 것'이 유리해지는 하드웨어적 이유는?",
    "options": [
      "컴퓨터가 똑똑해져서",
      "모델의 컨텍스트 윈도우(Context Window)가 비약적으로 커졌기 때문 (예: 1M 토큰)",
      "키보드 타자 속도가 빨라져서",
      "글자 가독성이 좋아서",
      "메모리 가격이 내려가서"
    ],
    "answer": "모델의 컨텍스트 윈도우(Context Window)가 비약적으로 커졌기 때문 (예: 1M 토큰)",
    "why": "과거에는 토큰 한계로 요약이 중요했지만, 이제는 통째로 다 넣고 모델에게 찾게 시키는 것이 정확도가 더 높습니다.",
    "hint": "모델의 '기억 창문' 크기를 생각하세요.",
    "trap_points": [
      "하지만 토큰 비용은 여전히 고려 대상임"
    ],
    "difficulty": "easy",
    "id": "0407"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "수학 문제를 풀릴 때 'Chain of Thought' 없이 풀린 결과와 포함해서 풀린 결과의 가장 큰 차이는?",
    "options": [
      "답은 똑같다.",
      "CoT는 풀이 과정을 적으며 중간 논리에 집중하기 때문에 최종 정답 확률이 훨씬 높다.",
      "CoT를 쓰면 답이 틀린다.",
      "CoT는 더 예쁘게 적는다.",
      "속도가 100배 빨라진다."
    ],
    "answer": "CoT는 풀이 과정을 적으며 중간 논리에 집중하기 때문에 최종 정답 확률이 훨씬 높다.",
    "why": "복잡한 문제는 중간 경로(Path)를 하나만 틀려도 답이 틀리므로, 단계별 추론이 필수적입니다.",
    "hint": "생각의 연결 고리가 정답을 지탱해줍니다.",
    "trap_points": [
      "비용(토큰)이 늘어나는 단점도 있음"
    ],
    "difficulty": "easy",
    "id": "0408"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 마지막에 '{' 기호를 선제적으로 입력하여 모델이 JSON 형식을 강제로 시작하도록 유도하는 기술은?",
    "answer": "JSON Prefilling (또는 앞글자 채우기)",
    "why": "모델의 첫 번째 토큰을 강제함으로써 전체 답변 구조를 고정시키는 강력한 통제 기법입니다.",
    "hint": "미리 채우다(Pre-fill).",
    "trap_points": [
      "Anthropic 모델군에서 공식적으로 권장하는 방식임"
    ],
    "difficulty": "hard",
    "id": "0409"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "LangChain의 `Custom Tool`을 에이전트에게 제공할 때 프롬프트 상으로 가장 정밀하게 작성해야 하는 부분은?",
    "options": [
      "도구의 가격",
      "도구의 이름과 설명(Description)",
      "도구가 만들어진 날짜",
      "도구 개발자 이름",
      "도구의 용량"
    ],
    "answer": "도구의 이름과 설명(Description)",
    "why": "모델은 도구의 코드 자체를 보지 못하고 오직 '설명'을 읽고 해당 도구를 쓸지 말지 결정하기 때문입니다.",
    "hint": "모델이 읽고 판단할 수 있는 '매뉴얼'입니다.",
    "trap_points": [
      "설명이 모호하면 에이전트가 엉뚱한 도구를 호출하게 됨"
    ],
    "difficulty": "medium",
    "id": "0410"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트에 '단계별로 생각하라(Let's think step-by-step)'는 문구를 넣는 기법의 이름은?",
    "options": [
      "Few-shot",
      "Zero-shot CoT (Chain of Thought)",
      "Negative Prompting",
      "Persona Adoption",
      "Interactive Prompting"
    ],
    "answer": "Zero-shot CoT (Chain of Thought)",
    "why": "별도의 예시 없이 단 한 줄의 주문만으로 모델의 추론 엔진을 활성화시키는 기법입니다.",
    "hint": "예시가 0개(Zero)인 CoT입니다.",
    "trap_points": [
      "복잡한 산수나 논리 문제에서 성능 향상이 매우 뚜짐"
    ],
    "difficulty": "medium",
    "id": "0411"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "LangChain에서 여러 프롬프트를 엮어 앞 단계의 결과를 뒷 단계에 전달하는 객체는?",
    "options": [
      "PromptTemplate",
      "Chain",
      "Parser",
      "Memory",
      "Agent"
    ],
    "answer": "Chain",
    "why": "사슬처럼 연결되어 워크플로우를 자동화하는 핵심 컴포넌트입니다.",
    "hint": "줄줄이 엮인 사슬입니다.",
    "trap_points": [
      "최근에는 파이프(|) 연산자를 사용하는 LCEL 구조로 많이 작성함"
    ],
    "difficulty": "easy",
    "id": "0412"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 작성 시 '하지 마라'는 부정형보다 '해라'는 긍정형이 효과적인 주된 이유는?",
    "options": [
      "모델이 착하기 때문",
      "모델은 특정 행동을 제외한 나머지 무수히 많은 경우의 수보다, 명확한 한 가지 행동의 확률을 올리는 데 더 능숙하기 때문",
      "부정형은 토큰을 더 많이 쓰기 때문",
      "영문법에 더 맞기 때문",
      "단순한 관례다."
    ],
    "answer": "모델은 특정 행동을 제외한 나머지 무수히 많은 경우의 수보다, 명확한 한 가지 행동의 확률을 올리는 데 더 능숙하기 때문",
    "why": "모델의 본질은 다음 토큰의 확률 분포를 계산하는 것이므로 긍정 지표가 확률 에너지를 집중시키기 더 좋습니다.",
    "hint": "목표 지점을 명확히 찍어주는 효과를 생각하세요.",
    "trap_points": [
      "강력한 제약(Constraint) 시에는 부정형도 섞어 써야 함"
    ],
    "difficulty": "medium",
    "id": "0413"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "이전 대화 맥락을 모두 보내지 않고 중요한 내용만 '요약'해서 모델에 전달하는 메모리 방식은?",
    "options": [
      "ConversationBufferMemory",
      "ConversationSummaryMemory",
      "ConversationWindowMemory",
      "ConversationEntityMemory",
      "No Memory"
    ],
    "answer": "ConversationSummaryMemory",
    "why": "대화가 길어질 때 토큰 소모를 줄이면서 핵심 맥락은 유지하는 똑똑한 메모리 관리법입니다.",
    "hint": "요약(Summary) 해둔다는 뜻입니다.",
    "trap_points": [
      "요약 과정에서 디테일한 정보가 유실될 위험이 있음"
    ],
    "difficulty": "medium",
    "id": "0414"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 구조 중 'XML 태그(<...> </...>)' 사용이 권장되는 주된 이유는?",
    "options": [
      "HTML 공부를 위해",
      "구조가 명확하여 복잡한 컨텍스트 속에서도 모델이 지시 사항의 범위를 한 치의 오차 없이 파악하기 위해",
      "코드가 짧아져서",
      "돈이 적게 들어서",
      "영어로만 코딩 가능해서"
    ],
    "answer": "구조가 명확하여 복잡한 컨텍스트 속에서도 모델이 지시 사항의 범위를 한 치의 오차 없이 파악하기 위해",
    "why": "특히 클로드(Claude) 등의 모델은 XML 태그를 데이터와 지시의 완벽한 경계선으로 인식하는 능력이 탁월합니다.",
    "hint": "시작과 끝이 명확한 구조화 기술입니다.",
    "trap_points": [
      "마크다운의 ### 보다 더 엄격한 구분이 가능함"
    ],
    "difficulty": "medium",
    "id": "0415"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "모델에게 답변의 '역할'을 부여하여 전문성을 끌어올리는 프롬프트 요소는?",
    "answer": "Persona (페르소나) 또는 Role (역할)",
    "why": "'너는 법률 전문가야' 처럼 자아를 설정해 주어 특정 도메인의 지식 인출 확률을 높입니다.",
    "hint": "탈(Mask), 사회적 인격을 뜻합니다.",
    "trap_points": [
      "시스템 프롬프트에 넣을 때 가장 효과가 좋음"
    ],
    "difficulty": "easy",
    "id": "0416"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "사용자의 질문을 영어로 번역해 답을 얻은 뒤 다시 한국어로 번역하는 '멀티 체인'의 이점은?",
    "options": [
      "그냥 영어가 멋있어서",
      "모델이 학습한 지식의 대부분이 영어이기에, 영어로 질문했을 때 훨씬 논리적이고 풍부한 답변이 나오기 때문",
      "속도가 빨라진다.",
      "토큰을 아낀다.",
      "한글이 어려워서"
    ],
    "answer": "모델이 학습한 지식의 대부분이 영어이기에, 영어로 질문했을 때 훨씬 논리적이고 풍부한 답변이 나오기 때문",
    "why": "한국어 특화 데이터보다 거대 영문 말뭉치의 지능을 빌려오는 전략입니다.",
    "hint": "영어 데이터의 압도적 양을 생각하세요.",
    "trap_points": [
      "번역 과정에서 톤앤매너가 바뀔 수 있음"
    ],
    "difficulty": "medium",
    "id": "0417"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "모델이 특정 '형식(JSON 등)'을 자꾸 어길 때 취할 수 있는 프롬프트 엔지니어링 조치는?",
    "options": [
      "모델을 교체한다.",
      "출력 예시에 JSON 코드 블록을 넣고, 마지막에 '반드시 JSON만 답하라'고 강력하게 제어(Constraint)한다.",
      "질문을 짧게 한다.",
      "답변을 다 지운다.",
      "영어로만 입력한다."
    ],
    "answer": "출력 예시에 JSON 코드 블록을 넣고, 마지막에 '반드시 JSON만 답하라'고 강력하게 제어(Constraint)한다.",
    "why": "제약 사항과 예제를 결합하여 모델의 자유도를 억제하고 정해진 경로로 답변하게 유도합니다.",
    "hint": "강한 제약과 명확한 예시입니다.",
    "trap_points": [
      "JSON 구조의 시작 부분을 중괄호 '{' 로 미리 지정해주기도 함"
    ],
    "difficulty": "medium",
    "id": "0418"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링에서 'Delimiters (구분자)'로 쓰기 부적절한 것은?",
    "options": [
      "###",
      "---",
      "###",
      "@@@",
      "일반적인 단어 (예: 그리고, 그런데 등)"
    ],
    "answer": "일반적인 단어 (예: 그리고, 그런데 등)",
    "why": "일반적인 단어는 문맥의 일부로 오해받기 쉬워 지시 사항의 경계를 명확히 긋지 못합니다.",
    "hint": "특수 기호가 아닌 일반적인 대화체를 찾으세요.",
    "trap_points": [
      "모델이 의미 없는 패턴으로 인지할 수 있는 기호가 가장 좋음"
    ],
    "difficulty": "easy",
    "id": "0419"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "모델에게 예제 하나를 보여주고 작업을 시키는 방식을 무엇이라 하나요?",
    "answer": "One-shot Prompting (원샷 프롬프팅)",
    "why": "하나의 표본(One shot)을 통해 모델에게 작업의 가이드라인을 제시합니다.",
    "hint": "숫자 1입니다.",
    "trap_points": [
      "Zero-shot 보다 훨씬 안정적인 결과를 보임"
    ],
    "difficulty": "easy",
    "id": "0420"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "LLM이 스스로 답을 내기 전, 풀이 과정이나 중간 구조를 먼저 작성하게 하는 전략은?",
    "options": [
      "Zero-shot",
      "Chain of Thought (CoT)",
      "Negative Prompting",
      "Few-shot",
      "Reverse Prompting"
    ],
    "answer": "Chain of Thought (CoT)",
    "why": "CoT는 복잡한 추론이 필요한 작업에서 모델이 단계적으로 생각하도록 유도하여 정답률을 획기적으로 높입니다.",
    "hint": "생각의 사슬을 연결하세요.",
    "trap_points": [
      "토큰 사용량이 늘어 비용이 증가할 수 있음"
    ],
    "difficulty": "medium",
    "id": "0421"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "LangChain에서 복잡한 로직을 순차적으로 처리하기 위해 단계별 프롬프트를 쪼개서 연결하는 기술을 무엇이라 합니까?",
    "answer": "Prompt Chaining",
    "why": "프롬프트 체이닝은 한 번의 커다란 요청 대신 여러 작은 단계로 나누어 정확도를 높이고 검증을 가능하게 합니다.",
    "hint": "프롬프트들을 사슬(Chain)처럼 엮는 것입니다.",
    "trap_points": [
      "단순한 RAG와는 구분되는 프롬프트 설계 기법임"
    ],
    "difficulty": "medium",
    "id": "0422"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "LLM에게 '너는 전문 변호사야'라고 하여 답변의 스타일과 관점을 고정시키는 프롬프트 요소는?",
    "options": [
      "Context",
      "Format",
      "Persona (또는 Role)",
      "Task",
      "Example"
    ],
    "answer": "Persona (또는 Role)",
    "why": "페르소나 설정은 모델의 말투, 전문성 수준, 가치관을 특정 역할에 맞게 유도하는 기법입니다.",
    "hint": "가면, 역할을 뜻하는 심리학/문화 용어입니다.",
    "trap_points": [
      "최근 연구에서는 지식 문제 해결 자체에는 효과가 미미할 수 있음"
    ],
    "difficulty": "easy",
    "id": "0423"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "사용자의 질문에 대해 단계별 풀이 과정을 출력하도록 유도하는 기법은?",
    "options": [
      "Zero-shot",
      "Chain of Thought (CoT)",
      "Few-shot",
      "Step-back Prompting",
      "Negative Prompting"
    ],
    "answer": "Chain of Thought (CoT)",
    "why": "CoT는 '생각의 사슬'을 따라 논리적 추론 과정을 먼저 적게 함으로써 복잡한 문제의 정답률을 높입니다.",
    "hint": "사고의 흐름, 사슬(Chain)을 생각하세요.",
    "trap_points": [
      "단순히 예시를 보여주는 Few-shot과는 다름"
    ],
    "difficulty": "medium",
    "id": "0424"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트에 '너는 지금부터 훌륭한 시인이야'라고 입력하는 기술의 명칭은?",
    "options": [
      "Few-shot",
      "Persona Adoption (페르소나 설정)",
      "Context Retrieval",
      "Negative Prompting",
      "JSON Formatting"
    ],
    "answer": "Persona Adoption (페르소나 설정)",
    "why": "가상의 인격(Persona)을 부여하여 말투와 지식 인출 경향을 제어합니다.",
    "hint": "역할을 주는 가면을 생각하세요.",
    "trap_points": [
      "전문가 역할을 주면 성능이 미세하게 향상되기도 함"
    ],
    "difficulty": "easy",
    "id": "0425"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 내에 예시를 몇 개 넣어주는 Few-shot 기법을 쓸 때, 예시의 '품질'이 중요한 이유는?",
    "options": [
      "모델이 심심해 하니까",
      "모델이 예시의 정답뿐만 아니라 '형식'과 '논리 전개 방식'까지 그대로 학습하기 때문",
      "토큰을 아끼려고",
      "영어로만 답하려고",
      "이미지를 만들려고"
    ],
    "answer": "모델이 예시의 정답뿐만 아니라 '형식'과 '논리 전개 방식'까지 그대로 학습하기 때문",
    "why": "잘못된 예시는 모델을 혼란에 빠뜨려 전체 성능을 심각하게 저하시킵니다.",
    "hint": "모델은 모방의 천재입니다.",
    "trap_points": [
      "예시는 일관된 형식을 유지해야 함"
    ],
    "difficulty": "easy",
    "id": "0426"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "모델에게 '반드시 JSON 형태로만 답해'라고 강요하는 것보다 '출력 형식은 다음과 같아야 해: { ... }'라고 구조를 보여주는 게 더 효과적인 이유는?",
    "options": [
      "모델은 시각적 예시를 텍스트 지침보다 명확하게 파악하기 때문",
      "모델이 그림을 좋아해서",
      "영단어 수를 줄이려고",
      "답변이 짧아져서",
      "비용이 절감되어서"
    ],
    "answer": "모델은 시각적 예시를 텍스트 지침보다 명확하게 파악하기 때문",
    "why": "추상적인 단어보다 구체적인 패턴을 보여주는 것이 생성 확률 제어에 유리합니다.",
    "hint": "백문이 불여일견입니다.",
    "trap_points": [
      "실제로 구현 시 중괄호 {} 의 위치를 명시하는 것이 꿀팁임"
    ],
    "difficulty": "medium",
    "id": "0427"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 인젝션(Prompt Injection) 공격을 방해하기 위해 구분자(###)를 사용하는 주된 원리는?",
    "options": [
      "시스템 프롬프트와 사용자 입력 데이터 사이의 '논리적 경계'를 시각화하여 모델의 혼동을 막는다.",
      "암호화를 수행한다.",
      "인터넷을 차단한다.",
      "영어로만 답하게 한다.",
      "이미지를 만든다."
    ],
    "answer": "시스템 프롬프트와 사용자 입력 데이터 사이의 '논리적 경계'를 시각화하여 모델의 혼동을 막는다.",
    "why": "사용자 입력 내용이 '지시 사항'으로 둔갑하는 것을 방지하는 가장 기초적인 보안책입니다.",
    "hint": "벽을 세우는 것과 같습니다.",
    "trap_points": [
      "구분자를 써도 완벽한 방어는 불가능하므로 다각도 보안이 필요함"
    ],
    "difficulty": "hard",
    "id": "0428"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "모델의 답변 도중 '잠시 멈추고 네 답변을 다시 검토해봐 (Self-reflect)' 라고 시키는 고급 기법은?",
    "answer": "Reflection (또는 성찰/반성 프롬프팅)",
    "why": "오류를 스스로 인지하고 수정함으로써 정답 확률을 높이는 메타인지 기법입니다.",
    "hint": "자신을 돌아본다는 뜻입니다.",
    "trap_points": [
      "비용과 소요 시간은 늘어나지만 정확도가 비약적으로 올라감"
    ],
    "difficulty": "medium",
    "id": "0429"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 구조화 기법 중 복잡한 작업을 작은 단위 프롬프트 여러 개로 나누어 ‘순서대로 처리’하는 기법은?",
    "options": [
      "Prompt Chaining",
      "Few-shot",
      "Negative Prompting",
      "Zero-shot",
      "Style Transfer"
    ],
    "answer": "Prompt Chaining",
    "why": "한 번에 처리하기 벅찬 일을 여러 프롬프트가 바통을 이어받듯 처리하여 정확도를 높입니다.",
    "hint": "사건들을 사슬(Chain)처럼 연결합니다.",
    "trap_points": [
      "이전 단계의 출력이 다음 단계의 입력이 됨"
    ],
    "difficulty": "medium",
    "id": "0430"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "LangChain에서 사용자의 대화 기록을 관리할 때, 마지막 N개의 대화만 유지하는 메모리 기법은?",
    "options": [
      "ConversationBufferMemory",
      "ConversationSummaryMemory",
      "ConversationBufferWindowMemory",
      "ConversationKGMemory",
      "VectorStoreRetrieverMemory"
    ],
    "answer": "ConversationBufferWindowMemory",
    "why": "윈도우(Window) 크기를 지정하여 최근 맥락만 집중적으로 유지함으로써 토큰 소모를 조절합니다.",
    "hint": "지정된 창(Window) 안의 데이터만 봅니다.",
    "trap_points": [
      "너무 과거의 내용은 잊어버리게 됨"
    ],
    "difficulty": "hard",
    "id": "0431"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링에서 'Few-shot' 예시가 모델의 답변을 방해하는 '바이어스(Bias)' 현상이란?",
    "options": [
      "가격이 비싸지는 현상",
      "답변 속도가 느려지는 현상",
      "예시로 든 특정 단어나 형식에 모델이 과도하게 꽂혀서 실제 질문 의도와 다르게 대답하는 것",
      "인터넷이 끊기는 현상",
      "영어로만 답하는 현상"
    ],
    "answer": "예제로 든 특정 단어나 형식에 모델이 과도하게 꽂혀서 실제 질문 의도와 다르게 대답하는 것",
    "why": "모델은 인컨텍스트 패턴을 매우 강력하게 학습하므로, 예시의 특징을 정답 근거보다 중요하게 여길 수 있습니다.",
    "hint": "치우침(Bias)의 문제를 생각하세요.",
    "trap_points": [
      "따라서 예시는 다양하고 중립적이어야 함"
    ],
    "difficulty": "medium",
    "id": "0432"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 내에서 '변수'를 중괄호 { } 로 감싸서 나중에 데이터를 채워 넣는 구조를 무엇이라 하나요?",
    "options": [
      "Python Script",
      "Prompt Template",
      "Markdown List",
      "JSON Data",
      "HTML Tag"
    ],
    "answer": "Prompt Template",
    "why": "공통된 프롬프트 뼈대에 사용자별 데이터를 동적으로 삽입하기 위한 표준 방식입니다.",
    "hint": "프롬프트의 틀, 양식입니다.",
    "trap_points": [
      "LangChain 등 프레임워크의 핵심 기능임"
    ],
    "difficulty": "easy",
    "id": "0433"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "모델에게 '모르는 것은 모른다고 해'라고 지시하여 환각(Hallucination)을 줄이는 것을 프롬프트 요소 중 무엇이라 하나요?",
    "options": [
      "Context",
      "Constraint (제약조건)",
      "Task",
      "Format",
      "Persona"
    ],
    "answer": "Constraint (제약조건)",
    "why": "모델의 자유도를 제한하여 사실에 근거한 답변만 하도록 강제하는 지침입니다.",
    "hint": "하지 말아야 할 일을 규정하는 것입니다.",
    "trap_points": [
      "할루시네이션 방지의 가장 첫 걸음임"
    ],
    "difficulty": "easy",
    "id": "0434"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "모델의 답변을 표나 불릿 리스트 형태로 출력해달라는 프롬프트 요소는?",
    "answer": "Format (형식)",
    "why": "정보 시각화나 후처리를 위해 답변의 물리적 구조를 지정하는 단계입니다.",
    "hint": "포맷팅(Formatting)하다.",
    "trap_points": [
      "마크다운(Markdown) 형식을 쓰면 가독성이 좋아짐"
    ],
    "difficulty": "easy",
    "id": "0435"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "LangChain LCEL에서 'Prompt | LLM | StrOutputParser' 처럼 파이프로 연결된 전체 단위를 무엇이라 하나요?",
    "options": [
      "Chain (체인)",
      "Module",
      "Package",
      "Function",
      "Template"
    ],
    "answer": "Chain (체인)",
    "why": "컴포넌트들이 사슬처럼 연결되어 하나의 작업을 완수하므로 체인이라 부릅니다.",
    "hint": "하나의 줄기 혹은 사슬입니다.",
    "trap_points": [
      "실행은 .invoke() 메서드로 수행함"
    ],
    "difficulty": "easy",
    "id": "0436"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 내에서 '가장 뒤에 배치한 정보'를 모델이 더 잘 기억하는 현상을 뜻하는 심리학 용어는?",
    "options": [
      "Primacy Effect (초두 효과)",
      "Recency Effect (최신 효과)",
      "Placebo Effect",
      "Anchor Effect",
      "Halo Effect"
    ],
    "answer": "Recency Effect (최신 효과)",
    "why": "긴 프롬프트에서 모델은 가장 마지막에 들어온 지시 사항이나 데이터를 더 강하게 인지하는 경향이 있습니다.",
    "hint": "최신 정보를 잘 기억합니다.",
    "trap_points": [
      "중요한 지시는 맨 뒤에 한 번 더 적어주는 팁이 여기서 나옴"
    ],
    "difficulty": "hard",
    "id": "0437"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "모델에게 답변의 '길이'를 지정(예: 3줄 이내)하여 정보를 압축하는 프롬프트 테크닉은?",
    "answer": "Length Constraint (길이 제약)",
    "why": "불필요한 미사여구를 줄여 토큰 비용을 아끼고 핵심만 전달하게 합니다.",
    "hint": "길이를 제한하는 것입니다.",
    "trap_points": [
      "너무 짧게 제한하면 정보가 손실될 수 있음"
    ],
    "difficulty": "easy",
    "id": "0438"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "복잡한 코딩 태스크를 시킬 때, 'Step-back' 프롬프팅이 하는 역할은?",
    "options": [
      "코드를 대신 써준다.",
      "코드를 짜기 전 필요한 알고리즘의 원리나 목차를 먼저 정리하게 하여 실수를 줄인다.",
      "에러를 무시하게 한다.",
      "코드를 다른 언어로 바꾼다.",
      "속도를 빠르게 한다."
    ],
    "answer": "코드를 짜기 전 필요한 알고리즘의 원리나 목차를 먼저 정리하게 하여 실수를 줄인다.",
    "why": "구현에 집중하기 전 설계도를 먼저 그리게 하여 논리적 완성도를 확보하는 전략입니다.",
    "hint": "한 발 물러나서(Step-back) 전체를 조망합니다.",
    "trap_points": [
      "생성된 구조를 기반으로 실제 답변을 작성하게 함"
    ],
    "difficulty": "medium",
    "id": "0439"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트의 4요소 중 '해결에 필요한 배경 지식이나 주변 상황'을 뜻하는 것은?",
    "options": [
      "Persona",
      "Task",
      "Context",
      "Format"
    ],
    "answer": "Context",
    "why": "컨텍스트는 작업의 품질을 높이기 위해 모델에게 전달하는 부가 정보입니다.",
    "hint": "문맥, 배경이라는 뜻입니다.",
    "trap_points": [
      "최근 프롬프트 엔지니어링의 핵심은 'Context Engineering'이라고도 함"
    ],
    "difficulty": "easy",
    "id": "0440"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "LangChain에서 프롬프트 템플릿의 변수 값을 실제 데이터로 채워 넣는 메서드는?",
    "options": [
      "fill()",
      "format()",
      "inject()",
      "render()",
      "invoke()"
    ],
    "answer": "format()",
    "why": "template.format(name='...') 처럼 사용하여 완성된 문자열을 얻습니다.",
    "hint": "형식을 갖춘다는 뜻의 메서드입니다.",
    "trap_points": [
      "LCEL 환경에서는 invoke()를 통해 체인 흐름 안에서 자동 처리됨"
    ],
    "difficulty": "medium",
    "id": "0441"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "모델이 질문을 받았을 때 바로 답하지 않고, '가상의 페르소나들'을 여럿 만들어 토론하게 한 뒤 정답을 고르는 고도화 기법은?",
    "options": [
      "CoT",
      "Self-Consistency",
      "Selection-Inference",
      "Multi-Persona Prompting",
      "Zero-shot"
    ],
    "answer": "Multi-Persona Prompting",
    "why": "다양한 관점을 가진 페르소나를 모델 내부에서 시뮬레이션하여 정답의 객관성을 높이는 전략입니다.",
    "hint": "여러 명의 인격(Persona)을 활용합니다.",
    "trap_points": [
      "토큰 사용량이 많지만 높은 논리적 완성도를 보여줌"
    ],
    "difficulty": "hard",
    "id": "0442"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트에 '출력물 마지막에는 반드시 요약을 넣어라'처럼 행동의 제약을 거는 것을 무엇이라 하나요?",
    "options": [
      "Instruction",
      "Constraint (제약조건)",
      "Format",
      "Context",
      "Input"
    ],
    "answer": "Constraint (제약조건)",
    "why": "행동의 범위를 한정 지어 원하는 결과의 일관성을 확보하는 기법입니다.",
    "hint": "억제, 구속이라는 영어 단어입니다.",
    "trap_points": [
      "제약이 너무 많으면 모델의 창의성이 줄어들 수 있음"
    ],
    "difficulty": "medium",
    "id": "0443"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 내에 델리미터(구분자)로 '---' 대신 'xml 태그(<...> </...>)'를 사용할 때 얻는 이점은?",
    "options": [
      "모델이 XML 파서이기 때문에",
      "각 구역의 시작과 끝이 명확하여 복잡한 컨텍스트 내에서도 지시 영역을 헷갈리지 않기 때문",
      "글자 수가 적어서",
      "그냥 보기 좋아서",
      "모델이 인터넷 언어라 좋아해서"
    ],
    "answer": "각 구역의 시작과 끝이 명확하여 복잡한 컨텍스트 내에서도 지시 영역을 헷갈리지 않기 때문",
    "why": "특히 앤스로픽(Claude) 모델 등 최신 LLM은 XML 구조화된 프롬프트를 매우 정확하게 해석합니다.",
    "hint": "컴퓨터가 읽기 좋은 명확한 경계선을 생각하세요.",
    "trap_points": [
      "가장 권장되는 고급 프롬프트 구조화 방식임"
    ],
    "difficulty": "medium",
    "id": "0444"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "모델에게 답을 유도하기 위해 프롬프트 마지막에 '자, 이제 시작하세요:' 처럼 마중물을 넣어주는 기법은?",
    "answer": "Completion Prompting (또는 프리필 Pre-fill)",
    "why": "모델이 정해진 답변 형식을 바로 시작하도록 앞머리를 먼저 떼주는 방식입니다.",
    "hint": "미리 채워둔다는 뜻입니다.",
    "trap_points": [
      "JSON 출력 유도 시 '{' 를 미리 적어주는 것도 여기에 해당함"
    ],
    "difficulty": "medium",
    "id": "0445"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "LangChain의 LCEL 사용 시 코드 가독성이 올라가는 주된 이유는?",
    "options": [
      "언어가 한글이라서",
      "파이프(|) 기호를 통해 데이터 흐름을 직관적(선언적)으로 표현할 수 있어서",
      "함수 이름이 짧아서",
      "자동으로 오타를 고쳐줘서",
      "코드를 안 써도 돼서"
    ],
    "answer": "파이프(|) 기호를 통해 데이터 흐름을 직관적(선언적)으로 표현할 수 있어서",
    "why": "복잡한 콜백이나 중첩 함수 호출 없이 단계별 파이프라인으로 체인을 구성할 수 있습니다.",
    "hint": "유닉스 쉘의 '파이프라인' 개념을 떠올리세요.",
    "trap_points": [
      "| 연산자가 내부적으로 오버로딩되어 동작함"
    ],
    "difficulty": "easy",
    "id": "0446"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "질문에 대해 '반대의 상황이나 부정적인 사례'를 먼저 떠올리게 한 뒤 정답을 유도하는 기법은?",
    "options": [
      "Negative Prompting",
      "Contrastive Prompting",
      "Positive Thinking",
      "Reverse Thinking",
      "Self-correction"
    ],
    "answer": "Contrastive Prompting",
    "why": "정답과 오답의 차이를 명확히 인지하게 하여 답변의 변별력을 높이는 기술입니다.",
    "hint": "대조적인이라는 뜻의 단어입니다.",
    "trap_points": [
      "모델의 비판적 사고 능력을 활용하는 방식임"
    ],
    "difficulty": "hard",
    "id": "0447"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "사용자의 질문을 다른 언어(영어 등)로 번역하여 최상의 답변을 얻은 뒤 다시 돌려주는 고급 체인 전략은?",
    "answer": "Translation Chain (또는 영문-한문 브릿지)",
    "why": "한국어 데이터가 부족한 도메인에서 모델의 영어 논리력을 빌려와서 성능을 극대화합니다.",
    "hint": "번역을 거치는 과정입니다.",
    "trap_points": [
      "총 3단계(번역-처리-재번역)가 소요되어 속도는 다소 느려짐"
    ],
    "difficulty": "medium",
    "id": "0448"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링 시 지시를 '구체적인 계단식(Steps)'으로 나누어 주는 것이 좋은 이유는?",
    "options": [
      "모델이 계단을 좋아해서",
      "복잡한 미션을 한꺼번에 처리할 때의 혼동을 막고 단계별 완성도를 높이기 위해",
      "토큰 비용을 아끼기 위해",
      "답변의 미적 감각을 높이기 위해",
      "그냥 관례적으로 그렇게 한다."
    ],
    "answer": "복잡한 미션을 한꺼번에 처리할 때의 혼동을 막고 단계별 완성도를 높이기 위해",
    "why": "작업을 세분화하면 각 단계의 입력과 출력이 명확해져서 할루시네이션 위험이 줄어듭니다.",
    "hint": "분할 정복(Divide and Conquer)의 원리입니다.",
    "trap_points": [
      "프롬프트 체이닝과 유사한 철학을 가짐"
    ],
    "difficulty": "easy",
    "id": "0449"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링 기술 중 모델에게 '예시 없이' 즉석에서 작업을 시키는 방식은?",
    "options": [
      "Zero-shot",
      "One-shot",
      "Few-shot",
      "Multi-shot",
      "Chain-shot"
    ],
    "answer": "Zero-shot",
    "why": "별도의 훈련용 예시(shot)가 0개라는 뜻입니다.",
    "hint": "숫자 0입니다.",
    "trap_points": [
      "모델의 기초 지능이 높을수록 제로샷 성능이 뛰어남"
    ],
    "difficulty": "easy",
    "id": "0450"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트에 '너는 초보자에게 친절하게 설명하는 유치원 선생님이야'라고 적는 페르소나 기법의 효과는?",
    "options": [
      "모델이 화를 낸다.",
      "답변의 톤앤매너와 어휘 수준이 지정된 역할에 맞춰 조정된다.",
      "답변 속도가 빨라진다.",
      "영어로만 답한다.",
      "글자 수가 무조건 길어진다."
    ],
    "answer": "답변의 톤앤매너와 어휘 수준이 지정된 역할에 맞춰 조정된다.",
    "why": "모델 내부의 수많은 가능성 중 '유치원 선생님'과 유사한 텍스트 확률 분포 영역을 활성화시킵니다.",
    "hint": "역할 놀이(Role play)의 효과입니다.",
    "trap_points": [
      "정확도보다는 '스타일'을 고정하는 데 유리함"
    ],
    "difficulty": "easy",
    "id": "0451"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 내 지시 사항의 '우선순위'를 정할 때, 가장 영향력이 적다고 알려진 위치는?",
    "options": [
      "맨 앞 (Top)",
      "맨 뒤 (Bottom)",
      "중간 (Middle)",
      "따로 적은 주석",
      "제목 부분"
    ],
    "answer": "중간 (Middle)",
    "why": "긴 문맥에서 모델은 양쪽 끝 정보에 집중하며 중간 정보는 희석되는 경향이 있습니다 (Lost in the middle).",
    "hint": "가운데 낀 정보입니다.",
    "trap_points": [
      "중요한 지시는 맨 앞이나 맨 뒤에 재배치해야 함"
    ],
    "difficulty": "medium",
    "id": "0452"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 작성 시 '모르는 것은 모른다고 해'라고 제약(Constraint)을 주면 줄어드는 현상은?",
    "options": [
      "Hallucination (환각)",
      "Inference Speed (추론 속도)",
      "Token Cost (비용)",
      "Response Length (길이)",
      "Formatting Error"
    ],
    "answer": "Hallucination (환각)",
    "why": "모델이 억지로 정답을 지어내려는 확률을 억제하여 사실 기반 답변을 유도합니다.",
    "hint": "없는 사실을 지어내는 현상을 막습니다.",
    "trap_points": [
      "과도하게 설정하면 지나치게 답변을 거부할 수 있음"
    ],
    "difficulty": "easy",
    "id": "0453"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "LangChain에서 복잡한 프롬프트를 함수화하여 재사용 가능하게 만든 틀의 이름은?",
    "options": [
      "Code Block",
      "Prompt Template",
      "Script Layout",
      "Variable Set",
      "Schema Board"
    ],
    "answer": "Prompt Template",
    "why": "변수(입력값)만 갈아 끼우며 정해진 프롬프트 뼈대를 유지할 수 있게 해줍니다.",
    "hint": "프롬프트 템플릿입니다.",
    "trap_points": [
      "f-string 방식보다 구조적으로 안전하고 관리가 쉬움"
    ],
    "difficulty": "easy",
    "id": "0454"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "모델의 답변 결과물이 마음에 들지 않을 때, '다시 시도해봐'라고 하기 전 구체적인 수정 방향을 알려주는 행위를 무엇이라 하나요?",
    "answer": "Iterative Refinement (반복적 개선)",
    "why": "피드백을 통해 모델의 결과물을 점진적으로 다듬어 나가는 과정입니다.",
    "hint": "개선(Refinement)을 반복(Iterative)합니다.",
    "trap_points": [
      "한 번에 완벽한 프롬프트를 짜는 것보다 피드백 루프를 도는 게 더 효율적일 때가 많음"
    ],
    "difficulty": "medium",
    "id": "0455"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링에서 'Delimiters'로 ###나 --- 같은 특수 기호를 쓰는 이유는?",
    "options": [
      "예쁘게 보여서",
      "모델이 지시 사항(Instruction)과 데이터(Data)의 경계를 명확히 구분하도록 돕기 위해",
      "영어로만 답하기 위해",
      "데이터를 압축하기 위해",
      "이미지 생성을 위해"
    ],
    "answer": "모델이 지시 사항(Instruction)과 데이터(Data)의 경계를 명확히 구분하도록 돕기 위해",
    "why": "경계가 모호하면 입력 데이터를 명령어로 착각하는 실수를 방지할 수 있습니다.",
    "hint": "구분자 역할을 합니다.",
    "trap_points": [
      "일관된 기호 사용이 중요함"
    ],
    "difficulty": "easy",
    "id": "0456"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "질문을 던지기 전 모델에게 '생각할 시간'을 주는 'Chain of Thought' 기법의 핵심 이점은?",
    "options": [
      "답변 속도가 빨라진다.",
      "중간 추론 단계(Rationale)를 명시적으로 거치면서 최종 정답의 논리적 오류가 줄어든다.",
      "비용이 절감된다.",
      "영어로만 답한다.",
      "글자 수가 짧아진다."
    ],
    "answer": "중간 추론 단계(Rationale)를 명시적으로 거치면서 최종 정답의 논리적 오류가 줄어든다.",
    "why": "어려운 문제를 한 번에 풀지 않고 단계별로 쪼개어 풀게 하여 실수를 방지합니다.",
    "hint": "생각의 사슬입니다.",
    "trap_points": [
      "추론 과정이 길어져서 토큰 소비는 늘어남"
    ],
    "difficulty": "medium",
    "id": "0457"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 내 예시(Few-shot)를 넣을 때 '대표성'이 중요한 이유는?",
    "options": [
      "모델이 지루하면 안 되니까",
      "대표성이 없는 예시는 모델이 엉뚱한 패턴을 일반화하여 오답을 내도록 유도할 수 있기 때문",
      "토큰을 아끼려고",
      "영어로만 답하려고",
      "이미지를 만들려고"
    ],
    "answer": "대표성이 없는 예시는 모델이 엉뚱한 패턴을 일반화하여 오답을 내도록 유도할 수 있기 때문",
    "why": "모델은 인컨텍스트 예시를 매우 강력한 '규칙'으로 여기기 때문입니다.",
    "hint": "예시의 품질이 결과의 품질입니다.",
    "trap_points": [
      "나쁜 예시는 안 주는 것보다 못 할 수도 있음"
    ],
    "difficulty": "medium",
    "id": "0458"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링 도구 중 사용자의 질문을 가장 유사한 '과거 대화'나 '문서'와 결합해 주는 기술은?",
    "answer": "RAG (Retrieval-Augmented Generation)",
    "why": "검색된 정보를 프롬프트의 '문맥(Context)' 영역에 주입하는 가장 강력한 엔지니어링 기술입니다.",
    "hint": "알파벳 세 글자.",
    "trap_points": [
      "프롬프트에 들어가는 데이터의 '최신성'을 보장하는 핵심 기술임"
    ],
    "difficulty": "easy",
    "id": "0459"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "객관식",
    "question": "프롬프트 엔지니어링에서 'few-shot' 예시를 줄 때, 예시의 '순서'가 답변에 미치는 영향은?",
    "options": [
      "전혀 없다.",
      "마지막에 위치한 예시의 형식을 모델이 더 강하게 따라 할 수 있다 (최신 효과).",
      "첫 번째 예시만 기억한다.",
      "가운데만 기억한다.",
      "랜덤하다."
    ],
    "answer": "마지막에 위치한 예시의 형식을 모델이 더 강하게 따라 할 수 있다 (최신 효과).",
    "why": "토큰이 뒤로 갈수록 가중치가 쏠리는 성질 때문에 마지막 예제의 비중이 큽니다.",
    "hint": "최신(Recency) 정보를 중시합니다.",
    "trap_points": [
      "따라서 가장 정석적인 답변 예시를 맨 뒤에 두는 것이 팁임"
    ],
    "difficulty": "medium",
    "id": "0460"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "LangChain에서 프롬프트 템플릿을 생성하는 클래스는? (문제 1)",
    "options": [],
    "answer": "PromptTemplate",
    "why": "PromptTemplate은 변수를 포함한 프롬프트 구조를 정의합니다.",
    "difficulty": "hard",
    "id": "0461"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "시스템 메시지를 설정하여 챗 모델을 초기화하는 역할은? (문제 2)",
    "options": [],
    "answer": "SystemMessage",
    "why": "SystemMessage는 AI의 역할과 행동 지침을 설정합니다.",
    "difficulty": "medium",
    "id": "0462"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "Few-shot 예제를 포함하는 프롬프트 템플릿 클래스는? (문제 3)",
    "options": [],
    "answer": "FewShotPromptTemplate",
    "why": "예제를 통해 모델의 답변 품질을 높이는 퓨샷 기법을 지원합니다.",
    "difficulty": "hard",
    "id": "0463"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "LangChain에서 프롬프트 템플릿을 생성하는 클래스는? (문제 4)",
    "options": [],
    "answer": "PromptTemplate",
    "why": "PromptTemplate은 변수를 포함한 프롬프트 구조를 정의합니다.",
    "difficulty": "medium",
    "id": "0464"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "시스템 메시지를 설정하여 챗 모델을 초기화하는 역할은? (문제 5)",
    "options": [],
    "answer": "SystemMessage",
    "why": "SystemMessage는 AI의 역할과 행동 지침을 설정합니다.",
    "difficulty": "hard",
    "id": "0465"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "Few-shot 예제를 포함하는 프롬프트 템플릿 클래스는? (문제 6)",
    "options": [],
    "answer": "FewShotPromptTemplate",
    "why": "예제를 통해 모델의 답변 품질을 높이는 퓨샷 기법을 지원합니다.",
    "difficulty": "medium",
    "id": "0466"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "LangChain에서 프롬프트 템플릿을 생성하는 클래스는? (문제 7)",
    "options": [],
    "answer": "PromptTemplate",
    "why": "PromptTemplate은 변수를 포함한 프롬프트 구조를 정의합니다.",
    "difficulty": "hard",
    "id": "0467"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "시스템 메시지를 설정하여 챗 모델을 초기화하는 역할은? (문제 8)",
    "options": [],
    "answer": "SystemMessage",
    "why": "SystemMessage는 AI의 역할과 행동 지침을 설정합니다.",
    "difficulty": "medium",
    "id": "0468"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "Few-shot 예제를 포함하는 프롬프트 템플릿 클래스는? (문제 9)",
    "options": [],
    "answer": "FewShotPromptTemplate",
    "why": "예제를 통해 모델의 답변 품질을 높이는 퓨샷 기법을 지원합니다.",
    "difficulty": "hard",
    "id": "0469"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "LangChain에서 프롬프트 템플릿을 생성하는 클래스는? (문제 10)",
    "options": [],
    "answer": "PromptTemplate",
    "why": "PromptTemplate은 변수를 포함한 프롬프트 구조를 정의합니다.",
    "difficulty": "medium",
    "id": "0470"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "시스템 메시지를 설정하여 챗 모델을 초기화하는 역할은? (문제 11)",
    "options": [],
    "answer": "SystemMessage",
    "why": "SystemMessage는 AI의 역할과 행동 지침을 설정합니다.",
    "difficulty": "hard",
    "id": "0471"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "Few-shot 예제를 포함하는 프롬프트 템플릿 클래스는? (문제 12)",
    "options": [],
    "answer": "FewShotPromptTemplate",
    "why": "예제를 통해 모델의 답변 품질을 높이는 퓨샷 기법을 지원합니다.",
    "difficulty": "medium",
    "id": "0472"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "LangChain에서 프롬프트 템플릿을 생성하는 클래스는? (문제 13)",
    "options": [],
    "answer": "PromptTemplate",
    "why": "PromptTemplate은 변수를 포함한 프롬프트 구조를 정의합니다.",
    "difficulty": "hard",
    "id": "0473"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "시스템 메시지를 설정하여 챗 모델을 초기화하는 역할은? (문제 14)",
    "options": [],
    "answer": "SystemMessage",
    "why": "SystemMessage는 AI의 역할과 행동 지침을 설정합니다.",
    "difficulty": "medium",
    "id": "0474"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "Few-shot 예제를 포함하는 프롬프트 템플릿 클래스는? (문제 15)",
    "options": [],
    "answer": "FewShotPromptTemplate",
    "why": "예제를 통해 모델의 답변 품질을 높이는 퓨샷 기법을 지원합니다.",
    "difficulty": "hard",
    "id": "0475"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "LangChain에서 프롬프트 템플릿을 생성하는 클래스는? (문제 16)",
    "options": [],
    "answer": "PromptTemplate",
    "why": "PromptTemplate은 변수를 포함한 프롬프트 구조를 정의합니다.",
    "difficulty": "medium",
    "id": "0476"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "시스템 메시지를 설정하여 챗 모델을 초기화하는 역할은? (문제 17)",
    "options": [],
    "answer": "SystemMessage",
    "why": "SystemMessage는 AI의 역할과 행동 지침을 설정합니다.",
    "difficulty": "hard",
    "id": "0477"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "Few-shot 예제를 포함하는 프롬프트 템플릿 클래스는? (문제 18)",
    "options": [],
    "answer": "FewShotPromptTemplate",
    "why": "예제를 통해 모델의 답변 품질을 높이는 퓨샷 기법을 지원합니다.",
    "difficulty": "medium",
    "id": "0478"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "LangChain에서 프롬프트 템플릿을 생성하는 클래스는? (문제 19)",
    "options": [],
    "answer": "PromptTemplate",
    "why": "PromptTemplate은 변수를 포함한 프롬프트 구조를 정의합니다.",
    "difficulty": "hard",
    "id": "0479"
  },
  {
    "chapter_name": "프롬프트 엔지니어링",
    "type": "코드 완성형",
    "question": "시스템 메시지를 설정하여 챗 모델을 초기화하는 역할은? (문제 20)",
    "options": [],
    "answer": "SystemMessage",
    "why": "SystemMessage는 AI의 역할과 행동 지침을 설정합니다.",
    "difficulty": "medium",
    "id": "0480"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG에서 ‘정확한 키워드’ 일치 여부를 따지는 고전 검색(BM25)과 ‘의미가 비슷한지’ 따지는 임베딩 검색을 섞은 것은?",
    "options": [
      "Multi-Search",
      "Hybrid Search (하이브리드 검색)",
      "Cross-Search",
      "Deep-Search",
      "Zero-Search"
    ],
    "answer": "Hybrid Search (하이브리드 검색)",
    "why": "두 방식의 장점을 합쳐 오타가 있어도 의미로 찾고, 전문 용어는 정확하게 찾는 최적의 검색을 구현합니다.",
    "hint": "짬뽕(Hybrid) 방식입니다.",
    "trap_points": [
      "Reciprocal Rank Fusion(RRF) 알고리즘으로 결과 순위를 합침"
    ],
    "difficulty": "medium",
    "id": "0481"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "벡터 DB에서 질문과 ‘가장 반대되는’ 의미를 가진 문서를 찾는 코사인 유사도 값은?",
    "options": [
      "1",
      "0",
      "-1",
      "100",
      "무한대"
    ],
    "answer": "-1",
    "why": "코사인은 1이면 일치, 0이면 무관(직교), -1이면 정반대 방향을 뜻합니다.",
    "hint": "정반대 수치입니다.",
    "trap_points": [
      "실제 임베딩 공간에서는 -1까지 가는 경우는 드믐"
    ],
    "difficulty": "medium",
    "id": "0482"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "에이전트 구현 기법 중 사용자의 질문을 작은 하위 작업(Sub-tasks)으로 쪼개어 하나씩 해결하는 방식은?",
    "options": [
      "Chain of Thought",
      "Sequential Planning",
      "Task Decomposition (작업 분해)",
      "Recursion",
      "Parallelism"
    ],
    "answer": "Task Decomposition (작업 분해)",
    "why": "한 번에 풀기 힘든 큰 문제를 관리 가능한 작은 문제로 나누어 처리 성공률을 높입니다.",
    "hint": "분해(Decomposition)한다는 뜻입니다.",
    "trap_points": [
      "각 하위 작업은 다시 에이전트나 도구에 의해 처리됨"
    ],
    "difficulty": "medium",
    "id": "0483"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인 성능 측정 도구인 'RAGAS'에서 '답변이 실제 사실에 근거하고 있는지'를 평가하는 지표는?",
    "options": [
      "Faithfulness (충실도)",
      "Answer Relevance",
      "Context Precision",
      "Context Recall",
      "Complexity"
    ],
    "answer": "Faithfulness (충실도)",
    "why": "검색된 문서 내용에서 벗어난 환각(Hallucination)이 없는지를 직접 채점합니다.",
    "hint": "신의, 신의가 있다는 뜻의 단어입니다.",
    "trap_points": [
      "자신의 배경지식으로 답하면 Faithfulness 점수가 깎임"
    ],
    "difficulty": "hard",
    "id": "0484"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "사용자의 질문 하나를 보고, 검색에 유리하게 5가지 다른 방식으로 다시 써서 검색 확률을 높이는 것은?",
    "options": [
      "Query Translation",
      "Multi-Query Retriever",
      "Vector Expansion",
      "Cleaning",
      "Pivoting"
    ],
    "answer": "Multi-Query Retriever",
    "why": "질문의 미묘한 표현 차이 때문에 검색을 못 하는 상황(사각지대)을 원천 차단합니다.",
    "hint": "여러 개(Multi) 쿼리를 날립니다.",
    "trap_points": [
      "검색 결과는 합쳐서(Union) 중복을 제거한 뒤 사용함"
    ],
    "difficulty": "medium",
    "id": "0485"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "에이전트가 어떤 도구를 쓸지 고민할 때, 도구의 이름과 (이것)을 읽고 결정합니다. (이것)은?",
    "answer": "Description (설명)",
    "why": "모델은 도구의 설명문을 읽고 현재 상황에 적합한 기능인지 판단하므로, 설명 작성이 매우 중요합니다.",
    "hint": "묘사, 설명이라는 뜻입니다.",
    "trap_points": [
      "설명이 부실하면 에이전트가 어한 도구를 부름"
    ],
    "difficulty": "easy",
    "id": "0486"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "벡터 DB에서 ‘의미상 가장 가까운 것’을 찾는 검색 방식의 이름은?",
    "options": [
      "Semantic Search (시맨틱 검색)",
      "Keyword Search",
      "Exact Match",
      "List Scan",
      "Map View"
    ],
    "answer": "Semantic Search (시맨틱 검색)",
    "why": "철자가 달라도 의미적 맥락(벡터 공간의 거리)을 기반을 정보를 찾아냅니다.",
    "hint": "의미를 뜻하는 단어(Semantic)입니다.",
    "trap_points": [
      "임베딩 성능에 따라 결과 품질이 크게 달라짐"
    ],
    "difficulty": "easy",
    "id": "0487"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "에이전트에게 ‘무엇을 실패했는지’ 대화 기록으로 남겨주는 것을 무엇이라 하나요?",
    "options": [
      "Short-term Memory",
      "Long-term Memory",
      "System Log",
      "Buffer",
      "Stack"
    ],
    "answer": "Short-term Memory",
    "why": "현재 대화 세션 내에서 방금 했던 실수나 정보를 기억하여 다음 행동에 교정하는 역할을 합니다.",
    "hint": "단기(Short-term) 기억입니다.",
    "trap_points": [
      "세션이 끝나면 사라지는 휘발성 정보임"
    ],
    "difficulty": "easy",
    "id": "0488"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 구축 시 특정 문서가 너무 길 때, 이를 500자 단위로 쪼개는 행위를 무엇이라 하나요?",
    "options": [
      "Chunking (청킹)",
      "Splitting",
      "Cutting",
      "Division",
      "Segmenting"
    ],
    "answer": "Chunking (청킹)",
    "why": "모델의 입력 제한(토큰 한도)을 지키고 검색 정밀도를 높이기 위한 필수 파편화 작업입니다.",
    "hint": "덩어리로 만들기.",
    "trap_points": [
      "의미가 끊길 수 있어 문장/문단 단위 청킹을 권장함"
    ],
    "difficulty": "easy",
    "id": "0489"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "사용자의 질문에 대해 LLM이 직접 대답하는 대신, 검색 엔진을 통해 실시간으로 지식을 검색해 오는 방식을 통칭하는 용어는?",
    "answer": "RAG (Retrieval-Augmented Generation)",
    "why": "검색 증강 생성으로, 외부 지식을 빌려와 할루시네이션을 획기적으로 줄입니다.",
    "hint": "알파벳 세 글자입니다.",
    "trap_points": [
      "지식의 최신성 유지를 위해 필수적인 기술임"
    ],
    "difficulty": "easy",
    "id": "0490"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 시스템 구축 시 텍스트를 고정된 도메인의 숫자인 벡터로 바꾸기 위해 사용하는 기술은?",
    "options": [
      "Indexing",
      "Tokenization",
      "Embedding (임베딩)",
      "Parsing",
      "Formatting"
    ],
    "answer": "Embedding (임베딩)",
    "why": "의미를 수치화하여 벡터 DB에 저장하고 유사도를 계산하기 위함입니다.",
    "hint": "E로 시작하는 단어로, RAG의 핵심입니다.",
    "trap_points": [
      "단어와 문장의 의미적 거리를 계산 가능하게 함"
    ],
    "difficulty": "easy",
    "id": "0491"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "벡터 DB에서 ‘가장 유사한 문서’ 상위 K개를 찾는 기술 중 하드웨어 리소스를 많이 쓰지만 가장 정확한 것은?",
    "options": [
      "ANN (Approximate)",
      "KNN (K-Nearest Neighbor)",
      "Random Search",
      "Hash Search",
      "Binary Search"
    ],
    "answer": "KNN (K-Nearest Neighbor)",
    "why": "KNN은 모든 데이터와 일일이 비교하여 가장 가까운 이웃을 찾아내므로 100% 정확하지만 속도가 느립니다.",
    "hint": "가장 가까운(Nearest) 이웃(Neighbor)을 직접 찾습니다.",
    "trap_points": [
      "데이터가 많아지면 ANN으로 대체해야 함"
    ],
    "difficulty": "medium",
    "id": "0492"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "에이전트 구현 시 '반성(Reflection)' 기술이 필요한 결정적인 상황은?",
    "options": [
      "단순한 날씨 묻기",
      "한 단계의 실수로 인해 전체 목표 달성이 불가능한 복잡한 다단계 논리 작업 시",
      "인사말 주고받기",
      "노래 제목 검색하기",
      "오늘의 날짜 묻기"
    ],
    "answer": "한 단계의 실수로 인해 전체 목표 달성이 불가능한 복잡한 다단계 논리 작업 시",
    "why": "스스로의 중간 결과물을 재평가하여 오류를 수정하지 않으면 에러가 누적되어 산으로 가기 때문입니다.",
    "hint": "복잡한 문제 풀이에서 '자기 객관화' 과정을 떠올리세요.",
    "trap_points": [
      "Self-Reflection은 에이전트의 성공률을 수십 % 높여줌"
    ],
    "difficulty": "hard",
    "id": "0493"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG의 한계인 ‘컨텍스트 윈도우 한계’를 극복하기 위해 가장 중요한 데이터를 선별하여 다시 배치하는 과정은?",
    "options": [
      "Cleaning",
      "Merging",
      "Re-ranking (리랭킹)",
      "Scaling",
      "Deleting"
    ],
    "answer": "Re-ranking (리랭킹)",
    "why": "검색 결과 수십 개 중 실제 정답과 가장 밀접한 소수만 골라 LLM에 전달하여 비용과 성능을 모두 잡습니다.",
    "hint": "순위(Ranking)를 다시(Re) 매깁니다.",
    "trap_points": [
      "사용자 질문과 문서의 상관 관계를 심층적으로 재계산함"
    ],
    "difficulty": "medium",
    "id": "0494"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트가 어떤 도구를 언제 사용할지 스스로 계획(Plan)을 수립하는 논리 구조를 무엇이라 하나요?",
    "answer": "Planning (계획)",
    "why": "목표를 달성하기 위한 세부 태스크의 순서와 도구 선택를 설계하는 지능입니다.",
    "hint": "계획이라는 뜻의 영어 단어입니다.",
    "trap_points": [
      "Plan-and-Execute 구조의 핵심임"
    ],
    "difficulty": "easy",
    "id": "0495"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 시스템에서 청크(Chunk)를 나눌 때 단어의 중간이 잘리는 것을 막기 위해 사용하는 속성은?",
    "options": [
      "Chunk size",
      "Chunk overlap",
      "Chunk padding",
      "Chunk scaling",
      "Chunk index"
    ],
    "answer": "Chunk overlap",
    "why": "조각 사이를 겹치게(Overlap) 하여 잘려 나간 문맥의 연결 고리를 보존합니다.",
    "hint": "겹침을 뜻하는 단어입니다.",
    "trap_points": [
      "보통 청크 크기의 10~20% 정도를 겹치게 설정함"
    ],
    "difficulty": "medium",
    "id": "0496"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "벡터 DB에서 질문과 원문 사이의 다리 역할을 하는 ‘가상의 가이드 답변’을 먼저 만들어 검색에 이용하는 기법은?",
    "options": [
      "HyDE",
      "Reranking",
      "Cleaning",
      "Parsing",
      "Indexing"
    ],
    "answer": "HyDE",
    "why": "LLM이 생성한 '가상의 완벽한 정답'을 임베딩하여 원문 저장소에서 유사한 것을 찾는 고급 기법입니다.",
    "hint": "가상(Hypothetical) 문서 임베딩입니다.",
    "trap_points": [
      "실제 질문보다 모델이 만든 답변이 문서 형태와 더 유사할 확률이 높음"
    ],
    "difficulty": "hard",
    "id": "0497"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "에이전트가 외부 시스템과 대화하며 자신의 작업 결과를 저장하고 다시 읽는 ‘기억(Memory)’ 중에서, 영구적으로 저장되는 저장소는?",
    "options": [
      "Conversation Buffer",
      "Vector Database (또는 Long-term memory)",
      "RAM",
      "CPU Cache",
      "Internal State"
    ],
    "answer": "Vector Database (또는 Long-term memory)",
    "why": "과거의 수많은 대화나 지식을 벡터 DB에 저장해 뒀다 나중에 필요할 때 찾아 쓰는 방식입니다.",
    "hint": "장기(Long-term) 기억입니다.",
    "trap_points": [
      "에이전트의 페르소나와 과거 이력을 유지하는 핵심임"
    ],
    "difficulty": "medium",
    "id": "0498"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "에이전트가 목표를 달성할 때까지 생각, 행동, 관찰을 무한히 반복하지 않도록 설정하는 안전 장치는?",
    "answer": "Max Iterations (최대 반복 횟수)",
    "why": "무한 루프(Infinite Logic Loop)에 빠져 비용이 폭주하는 것을 막기 위해 횟수 제한을 둡니다.",
    "hint": "최대(Max) 반복(Iteration).",
    "trap_points": [
      "보통 5~10회 정도로 설정함"
    ],
    "difficulty": "medium",
    "id": "0499"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 평가 시 '사용자 질문에 대해 검색된 문서가 실제로 정답을 포함하고 있는지'를 평가하는 지표는?",
    "options": [
      "Faithfulness",
      "Context Recall",
      "Context Precision",
      "Answer Relevance",
      "Groundedness"
    ],
    "answer": "Context Recall",
    "why": "실제 정답에 필요한 정보가 검색 결과물 속에 빠짐없이 들어있는지 확인하는 리콜(재현율) 개념입니다.",
    "hint": "정보의 누락 여부를 판단합니다.",
    "trap_points": [
      "정밀도(Precision)는 쓸데없는 게 섞였는지를 봅니다"
    ],
    "difficulty": "hard",
    "id": "0500"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 단계 중 검색된 정보(Context)를 사용자 질문과 결합하여 '풍부한 프롬프트'를 만드는 단계는?",
    "options": [
      "Indexing",
      "Searching",
      "Augmenting",
      "Generating",
      "Processing"
    ],
    "answer": "Augmenting",
    "why": "Augmenting은 검색된 지식을 덧붙여(증강하여) 모델이 답변하기 좋은 상태로 만드는 과정입니다.",
    "hint": "증강하다라는 뜻입니다.",
    "trap_points": [
      "Generating 직전의 프롬프트 조립 단계임"
    ],
    "difficulty": "medium",
    "id": "0501"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "벡터 DB에서 고차원 데이터를 빠르게 찾기 위해 사용하는 '근사 최근접 이웃' 검색 기술의 약자는?",
    "options": [
      "SQL",
      "ANN",
      "KNN",
      "API",
      "RAG"
    ],
    "answer": "ANN",
    "why": "ANN(Approximate Nearest Neighbor)은 정확도를 조금 희생하고 검색 속도를 획기적으로 올리는 방식입니다.",
    "hint": "가장 가까운 이웃(Nearest Neighbor)을 대략적으로(Approximate) 찾기.",
    "trap_points": [
      "정확히 찾는 KNN보다 대규모 데이터에서 압도적으로 빠름"
    ],
    "difficulty": "hard",
    "id": "0502"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG의 성능을 평가할 때, '검색된 문서들이 사용자 질문과 실제로 얼마나 관련된 것인지'를 측정하는 지표는?",
    "options": [
      "Context Precision",
      "Context Recall",
      "Answer Relevance",
      "Groundedness",
      "L2 Norm"
    ],
    "answer": "Context Precision",
    "why": "컨텍스트 정밀도(Precision)는 정답과 관련된 정보가 검색 결과 상단에 얼마나 잘 포함되었는지를 봅니다.",
    "hint": "검색 결과의 '정밀도'를 생각하세요.",
    "trap_points": [
      "Context Recall은 정답에 필요한 모든 정보가 빠짐없이 검색되었는지를 봅니다"
    ],
    "difficulty": "hard",
    "id": "0503"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG에서 사용자가 '최근 1주일간의 뉴스만 알려줘'라고 할 때 효율적으로 처리하는 방법은?",
    "options": [
      "그냥 전체 임베딩 검색을 한다.",
      "메타데이터 필터링(Metadata Filtering)을 적용한다.",
      "모델에게 직접 찾아보라고 한다.",
      "임베딩 모델을 바꾼다.",
      "검색된 결과를 하나씩 다 읽어본다."
    ],
    "answer": "메타데이터 필터링(Metadata Filtering)을 적용한다.",
    "why": "날짜, 작성자, 카테고리 등 속성을 미리 DB에 저장해 두고 하드웨어적으로 먼저 거른 뒤 유사도 검색을 수행합니다.",
    "hint": "데이터에 대한 데이터(Metadata)를 사용합니다.",
    "trap_points": [
      "의미 검색만으로는 정확한 시간대 제어가 어려울 수 있음"
    ],
    "difficulty": "medium",
    "id": "0504"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "검색된 다수의 문서 중 핵심적인 정보가 앞과 뒤에 있을 때보다 중간에 있을 때 모델이 잘 인지하지 못하는 현상은?",
    "answer": "Lost in the Middle",
    "why": "긴 컨텍스트에서 모델이 중간 부분을 덜 중요하게 여기는 바이어스 때문에 발생합니다.",
    "hint": "중간에서 길을 잃다(Lost).",
    "trap_points": [
      "를 해결하기 위해 Reordering 기법이 사용됨"
    ],
    "difficulty": "hard",
    "id": "0505"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "에이전트 구현 시 '반성(Reflection)' 기술이 필요한 주된 이유는?",
    "options": [
      "에이전트가 예의 바르게 답변하게 하기 위해",
      "모델의 첫 번째 답변에서 나타난 논리적 오류나 부족한 정보를 스스로 보완하기 위해",
      "답변의 속도를 빠르게 하기 위해",
      "비용을 절약하기 위해",
      "화려한 UI를 위해"
    ],
    "answer": "모델의 첫 번째 답변에서 나타난 논리적 오류나 부족한 정보를 스스로 보완하기 위해",
    "why": "한 번에 완벽한 답을 내기 어려운 복잡한 작업에서 '검증'과 '재시도' 루프를 통해 품질을 올립니다.",
    "hint": "스스로를 돌아보고 수정하는 능력입니다.",
    "trap_points": [
      "모델의 Reasoning 성능이 높을수록 반성 능력도 좋아짐"
    ],
    "difficulty": "medium",
    "id": "0506"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "전문가들(검색 에이전트, 작성 에이전트 등)이 협업하는 시스템 구조는?",
    "options": [
      "Single Agent",
      "Multi-Agent Systems (MAS)",
      "Linear Pipeline",
      "Manual Workflow",
      "Hardcoded Logic"
    ],
    "answer": "Multi-Agent Systems (MAS)",
    "why": "작업을 분담하고 서로 피드백을 주고받아 결과물의 수준을 높이는 고도화된 구조입니다.",
    "hint": "여러 에이전트가 함께합니다.",
    "trap_points": [
      "LangGraph 등이 이를 구현하기 위한 대표적인 라이브러리임"
    ],
    "difficulty": "medium",
    "id": "0507"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "벡터 DB에서 두 벡터의 직선 거리를 측정할 때 사용하는 가장 흔한 메트릭은?",
    "options": [
      "Cosine Similarity",
      "L2 Distance (Euclidean)",
      "Dot Product",
      "Hamming Distance",
      "Jaccard"
    ],
    "answer": "L2 Distance (Euclidean)",
    "why": "유클리드 거리는 다차원 공간에서의 직선의 절댓값을 계산합니다.",
    "hint": "피타고라스 정리의 확장형입니다.",
    "trap_points": [
      "거리이므로 작을수록 유사한 것임 (유사도는 클수록 유사)"
    ],
    "difficulty": "medium",
    "id": "0508"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "인터넷 웹사이트의 정보를 실시간으로 긁어와서 에이전트에게 제공하는 도구(Tool)를 보통 무엇이라 하나요?",
    "answer": "Search Tool (또는 Web Search/Browsing Tool)",
    "why": "최신 정보 반영(Grounding)을 위해 Tavily, SerpAPI 등을 연동하여 사용합니다.",
    "hint": "검색 도구입니다.",
    "trap_points": [
      "모델 자체가 인터넷을 돌아다니는 것이 아니라 도구를 '사용'하는 것임"
    ],
    "difficulty": "easy",
    "id": "0509"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 시스템에서 청크 오버랩(Overlap)의 크기를 결정할 때 고려해야 할 사항은?",
    "options": [
      "메모리가 많으면 무조건 최대한 크게 한다.",
      "청크 사이즈의 일정 비율(예: 10~20%)로 설정하여 의미 단절을 최소화한다.",
      "오버랩은 항상 0으로 두는 것이 깔끔하다.",
      "오버랩은 한글에서만 사용한다.",
      "오버랩을 크게 하면 검색이 무조건 정확해진다."
    ],
    "answer": "청크 사이즈의 일정 비율(예: 10~20%)로 설정하여 의미 단절을 최소화한다.",
    "why": "너무 크면 중복 정보가 많아 검색 효율이 떨어지고, 너무 작으면 문맥이 잘릴 위험이 있습니다.",
    "hint": "적절한(10~20%) 비율을 생각하세요.",
    "trap_points": [
      "문서의 특성에 따라 최적의 오버랩 값이 다름"
    ],
    "difficulty": "medium",
    "id": "0510"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG의 5단계 중, 원본 문서를 청킹하고 벡터화하여 DB에 넣는 준비 과정은?",
    "options": [
      "Indexing",
      "Processing",
      "Searching",
      "Augmenting",
      "Generating"
    ],
    "answer": "Indexing",
    "why": "인덱싱은 데이터를 검색 가능한 상태로 '색인화'하여 저장소(Vector DB)에 구축하는 단계입니다.",
    "hint": "색인을 뜻하는 단어입니다.",
    "trap_points": [
      "실제로 검색이 이루어지는 단계는 Searching임"
    ],
    "difficulty": "medium",
    "id": "0511"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "검색 결과가 너무 많을 때, 관련성이 가장 높은 소수만 골라내기 위해 유사도 점수를 다시 매기는 과정은?",
    "options": [
      "Filtering",
      "Sorting",
      "Re-ranking",
      "Pruning",
      "Summarizing"
    ],
    "answer": "Re-ranking",
    "why": "리랭커(Re-ranker)는 속도는 느리지만 훨씬 정밀한 모델을 사용하여 최상위 답변 후보를 선별합니다.",
    "hint": "순위(Ranking)를 다시(Re) 매깁니다.",
    "trap_points": [
      "Bi-Encoder로 검색하고 Cross-Encoder로 리랭킹하는 것이 정석임"
    ],
    "difficulty": "medium",
    "id": "0512"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 환경에서 질문과 문서 사이의 관련성을 평가할 때, '검색된 문서 내용에만 기반해서 답했는지'를 나타내는 지표는?",
    "options": [
      "Precision",
      "Recall",
      "Faithfulness (충실도)",
      "F1-score",
      "L2 Distance"
    ],
    "answer": "Faithfulness (충실도)",
    "why": "충실도는 모델이 외부 지식을 섞지 않고 오직 '주어진 컨텍스트' 내에서만 충실히 답변했는지를 측정합니다.",
    "hint": "믿음직함, 충실함을 뜻하는 단어입니다.",
    "trap_points": [
      "RAGAS 같은 평가 프레임워크의 핵심 지표임"
    ],
    "difficulty": "hard",
    "id": "0513"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "벡터 DB에서 가장 가까운 이웃을 효율적으로 찾기 위해 그래프 구조를 활용하는 알고리즘은?",
    "options": [
      "ANN",
      "HNSW",
      "B-Tree",
      "Hash Table",
      "Binary Search"
    ],
    "answer": "HNSW",
    "why": "Hierarchical Navigable Small World(HNSW)는 고차원 벡터 검색에서 속도와 정확도의 균형이 뛰어난 대표적 알고리즘입니다.",
    "hint": "계층적 작은 세상 탐색의 약자입니다.",
    "trap_points": [
      "Chroma, Pinecone 등 대부분의 주요 벡터 DB가 지원함"
    ],
    "difficulty": "hard",
    "id": "0514"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "사용자의 모호한 질문을 명확하게 바꾸거나 검색이 잘 되도록 키워드를 확장하는 단계를 무엇이라 하나요?",
    "answer": "Query Reformulation (또는 리프레이징)",
    "why": "질문을 보정함으로써 검색 엔진이 더 정확한 문서를 찾을 확률을 높여줍니다.",
    "hint": "질문을 다시(Re) 형성(formulation)한다는 뜻입니다.",
    "trap_points": [
      "Processing 단계에서 주로 수행됨"
    ],
    "difficulty": "medium",
    "id": "0515"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG에서 문서 조각(Chunk) 사이를 일부 겹치게 하는 'Overlap'의 주된 목적은?",
    "options": [
      "메모리 절약을 위해",
      "청크가 잘리는 부분의 문맥 정보를 보존하기 위해",
      "문서 전체를 중복 저장하기 위해",
      "검색 속도를 빠르게 하기 위해",
      "영어로만 된 문서를 찾기 위해"
    ],
    "answer": "청크가 잘리는 부분의 문맥 정보를 보존하기 위해",
    "why": "겹침(Overlap)이 있으면 중요한 단어가 경계선에서 잘려 의미가 훼손되는 것을 방지할 수 있습니다.",
    "hint": "경계선의 문맥(Context) 유지를 생각하세요.",
    "trap_points": [
      "보통 청크 사이즈의 10~20% 정도를 겹치도록 설정함"
    ],
    "difficulty": "medium",
    "id": "0516"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "질문을 받았을 때, 답을 바로 내지 않고 '검색 결과가 충분한가?'를 스스로 평가하여 부족하면 다시 검색하는 에이전트 구조는?",
    "options": [
      "Simple RAG",
      "Adaptive RAG (또는 Self-RAG)",
      "Static Agent",
      "Keyword Matcher",
      "Linear Pipeline"
    ],
    "answer": "Adaptive RAG (또는 Self-RAG)",
    "why": "상황에 맞춰(Adaptive) 행동을 결정하는 구조로, 검색 결과가 주제와 무관하면 다시 쿼리를 짜는 능력이 있습니다.",
    "hint": "적응형, 또는 스스로를 평가하는 방식입니다.",
    "trap_points": [
      "성능은 좋으나 API 호출 횟수가 늘어나 비용이 상승할 수 있음"
    ],
    "difficulty": "hard",
    "id": "0517"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "의미 검색(Semantic)과 키워드 검색(Lexical)의 결과를 합치는 기술의 명칭은?",
    "options": [
      "Cross Search",
      "Hybrid Search",
      "Mixed Retrieval",
      "Deep Search",
      "Combined Fusion"
    ],
    "answer": "Hybrid Search",
    "why": "두 방식의 장점을 모두 취하여(Hybrid), 정확한 고유 명사 검색과 모호한 의미 검색을 동시에 잡습니다.",
    "hint": "두 가지를 섞었다는 뜻입니다.",
    "trap_points": [
      "Reranking 전에 두 결과를 Rank Fusion으로 합치는 게 일반적임"
    ],
    "difficulty": "medium",
    "id": "0518"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "PDF 문서의 이미지, 표, 텍스트 구조를 정확히 파싱하여 RAG용 데이터로 변환해 주는 IBM의 오픈소스 도구는?",
    "answer": "Docling",
    "why": "Docling은 PDF를 마크다운이나 JSON으로 깔끔하게 변환하여 LLM이 표의 의미까지 읽도록 도와줍니다.",
    "hint": "문서(Doc)와 연관된 귀여운 이름입니다.",
    "trap_points": [
      "단순히 텍스트만 뽑는 도구보다 표 인식 능력이 뛰어남"
    ],
    "difficulty": "medium",
    "id": "0519"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "에이전트 구현 시 '생각하고 행동하고 관찰하는' 과정을 반복하는 가장 대표적인 프레임워크는?",
    "options": [
      "CoT",
      "ReAct",
      "Reflection",
      "Plan-and-Execute",
      "BabyAGI"
    ],
    "answer": "ReAct",
    "why": "Reasoning + Acting의 약자로, 매 단계마다 현재 상황을 추론하고 행동을 결정하는 에이전트의 기본 뼈대입니다.",
    "hint": "반응하다라는 영어 단어와 철자가 같습니다.",
    "trap_points": [
      "Thought, Action, Observation의 루프를 기억하세요"
    ],
    "difficulty": "medium",
    "id": "0520"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG에서 ‘효율적인 검색’을 위해 질문을 벡터화하기 전, 질문의 본질을 파악하여 다른 도메인의 질문으로 변환하는 것을 무엇이라 하나요?",
    "options": [
      "Query Translation",
      "Query Expansion (질문 확장)",
      "Query Decomposition",
      "HyDE",
      "Multi-Query"
    ],
    "answer": "Query Expansion (질문 확장)",
    "why": "유사한 단어나 개념을 덧붙여 검색 성공률을 높이는 기법입니다.",
    "hint": "질문을 더 넓게(Expansion) 만든다.",
    "trap_points": [
      "동의어 사전을 이용하거나 LLM을 이용해 확장함"
    ],
    "difficulty": "medium",
    "id": "0521"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "여러 문서 덩어리(Chunk)가 검색되었을 때, 이를 그대로 LLM에 넘기기보다 ‘가장 중요한 덩어리’ 수십 개만 정밀하게 재배열하는 과정은?",
    "options": [
      "Filtering",
      "Sorting",
      "Re-ranking",
      "Cleaning",
      "Mapping"
    ],
    "answer": "Re-ranking",
    "why": "검색 모델(Retriever)은 속도가 빠르지만 정확도가 낮으므로, 느리지만 정확한 리랭커 모델로 보강합니다.",
    "hint": "순위(Ranking)를 다시(Re) 매긴다.",
    "trap_points": [
      "비용과 속도의 균형을 잡기 위한 필수 단계임"
    ],
    "difficulty": "medium",
    "id": "0522"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 시스템에서 '검색된 문서들로부터 정답을 도출하기 위한 논리적 근거가 충분한지' 평가하는 RAGAS 지표는?",
    "options": [
      "Faithfulness",
      "Answer Relevance",
      "Context Precision",
      "Context Recall",
      "Aspect Critique"
    ],
    "answer": "Answer Relevance",
    "why": "사용자의 질문에 대해 검색된 컨텍스트가 얼마나 직접적으로 연관되어 답변의 질을 보장하는지 측정합니다.",
    "hint": "정답(Answer)과 얼마나 관련성(Relevance)이 있는지.",
    "trap_points": [
      "Faithfulness는 문서 내 정보 준수 여부를, Relevance는 정답 자체의 관련성을 봅니다"
    ],
    "difficulty": "hard",
    "id": "0523"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "벡터 DB 검색 시 ‘의미적으로는 유사하지만 다른 주제’의 문서가 검색되는 문제를 해결하기 위한 방법은?",
    "options": [
      "메타데이터 필터링 (Metadata Filtering)",
      "더 큰 모델 사용",
      "인터넷 속도 강화",
      "질문 글자 수 늘리기",
      "데이터 지우기"
    ],
    "answer": "메타데이터 필터링 (Metadata Filtering)",
    "why": "카테고리, 날짜, 작성자 등 논리적 특징으로 범위를 좁히고 검색을 수행하면 정확도가 올라갑니다.",
    "hint": "데이터의 속성 정보를 이용합니다.",
    "trap_points": [
      "하드웨어적 필터링이므로 의미 검색보다 연산이 확실함"
    ],
    "difficulty": "medium",
    "id": "0524"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "질문을 받으면 도구를 사용해야 할지 말지 스스로 판단하고 실행하는 LLM의 실질적 행동 주체를 무엇이라 하나요?",
    "answer": "AI Agent (에이전트)",
    "why": "단순 출력을 넘어 목표지향적 행동(Action)을 수행하는 AI의 진화된 형태입니다.",
    "hint": "주체적으로 행동하는 개체입니다.",
    "trap_points": [
      "에이전트는 반복(Loop)적인 사고가 가능함"
    ],
    "difficulty": "easy",
    "id": "0525"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "LangChain에서 에이전트의 사고 흐름과 상태를 그래프 형태로 관리하여 복잡한 로직을 구현하는 라이브러리는?",
    "options": [
      "LangDB",
      "LangGraph",
      "LangFlow",
      "LangChain-Core",
      "LangPlus"
    ],
    "answer": "LangGraph",
    "why": "순환 구조(Cyclic)를 지원하여 에이전트가 실패 시 이전 단계로 돌아가거나 무한 루프를 도는 것을 관리하기에 최적입니다.",
    "hint": "그래프(Graph) 구조를 활용합니다.",
    "trap_points": [
      "최신 기업용 에이전트는 대부분 이 방식으로 구축됨"
    ],
    "difficulty": "hard",
    "id": "0526"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG에서 ‘HyDE (Hypothetical Document Embedding)’ 기법이 수행하는 과정은?",
    "options": [
      "문서를 다 지우기",
      "질문에 대해 LLM이 먼저 가상의(가짜) 답변을 생성하게 한 뒤, 그 가짜 답변으로 실제 문서를 검색하기",
      "사용자 정보를 가제로 만들기",
      "임베딩 모델을 랜덤하게 바꾸기",
      "검색 결과를 삭제하기"
    ],
    "answer": "질문에 대해 LLM이 먼저 가상의(가짜) 답변을 생성하게 한 뒤, 그 가짜 답변으로 실제 문서를 검색하기",
    "why": "사용자의 질문보다 모델이 대충 만든 답변이 실제 저장소의 문서 형태와 더 비슷할 때가 많아 검색 확률이 높아집니다.",
    "hint": "가상(Hypothetical)의 문서를 이용합니다.",
    "trap_points": [
      "가짜 지식이 섞여도 유사한 형태를 찾는 게 목적이므로 검색 효율이 올라감"
    ],
    "difficulty": "hard",
    "id": "0527"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "벡터 DB에서 ‘고밀도 벡터(Dense Vector)’와 ‘희소 벡터(Sparse Vector)’를 함께 사용하여 검색 품질을 높이는 방식은?",
    "options": [
      "Single Retrieval",
      "Hybrid Search",
      "Dual Encoding",
      "Cross Matching",
      "Random Selection"
    ],
    "answer": "Hybrid Search",
    "why": "의미를 잘 찾는 고밀도 벡터와 정확한 어휘를 잘 찾는 희소 벡터의 장점을 결합합니다.",
    "hint": "두 가지 이상의 짬뽕 방식입니다.",
    "trap_points": [
      "RRF (Reciprocal Rank Fusion) 알고리즘으로 점수를 합침"
    ],
    "difficulty": "medium",
    "id": "0528"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "에이전트가 어떤 도구를 실행할지 그 매개변수와 이름을 JSON 형태로 출력하게 하는 LLM의 기초 기술 명칭은?",
    "answer": "Function Calling (함수 호출)",
    "why": "LLM이 구조화된 입력을 만들게 하여 실제 서버의 함수와 연동할 수 있게 해주는 약속입니다.",
    "hint": "함수(Function)를 부르는(Calling) 것.",
    "trap_points": [
      "모델이 직접 함수를 실행하는 것이 아닌 '실행 계획'만 주는 것임"
    ],
    "difficulty": "medium",
    "id": "0529"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "문서를 작은 청크로 쪼개어 검색하고, 실제 답변 생성 시에는 그 청크들이 포함된 원본 ‘페이지 전체’를 LLM에 전달하는 전략은?",
    "options": [
      "Small2Big (또는 Parent Document Retrieval)",
      "Big2Small",
      "Fixed Window",
      "Sliding Frame",
      "Multi-Vector"
    ],
    "answer": "Small2Big (또는 Parent Document Retrieval)",
    "why": "검색 효율을 위해 작은 조각을 뒤지지만, 모델은 문맥 파악을 위해 풍부한 주변 정보가 필요하기 때문입니다.",
    "hint": "작은(Small) 것으로 찾아서 큰(Big) 것을 준다.",
    "trap_points": [
      "성능 향상이 매우 뚜렷한 고급 리트리버 기법임"
    ],
    "difficulty": "hard",
    "id": "0530"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG에서 ‘검색 품질’이 안 좋을 때 가장 먼저 확인해야 할 요소는?",
    "options": [
      "모델의 크기",
      "데이터의 청킹(Chunking) 방식과 임베딩(Embedding) 모델의 성능",
      "인터넷 브라우저 종류",
      "사용자의 타자 속도",
      "운영체제 버전"
    ],
    "answer": "데이터의 청킹(Chunking) 방식과 임베딩(Embedding) 모델의 성능",
    "why": "데이터를 어떻게 쪼개서 어떤 벡터 공간에 넣었느냐가 검색의 정밀도를 결정짓는 90% 요인입니다.",
    "hint": "데이터를 조각내는 방법과 수치화하는 도구입니다.",
    "trap_points": [
      "임베딩 모델이 질문의 의도를 벡터 공간에서 못 찾으면 아무리 똑똑한 LLM도 답 못 함"
    ],
    "difficulty": "medium",
    "id": "0531"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "벡터 DB에서 질문과 거리(유사도)를 계산할 때, '방향'이 얼마나 일치하는지를 중점적으로 보는 메트릭은?",
    "options": [
      "Euclidean Distance",
      "Cosine Similarity (코사인 유사도)",
      "Manhattan Distance",
      "Hamming Distance",
      "Jaccard Similarity"
    ],
    "answer": "Cosine Similarity (코사인 유사도)",
    "why": "벡터의 크기가 아닌 '각도(방향)'의 유사함을 계산하여 텍스트 의미 비교에 최적화되어 있습니다.",
    "hint": "각도와 관련된 삼각함수 이름입니다.",
    "trap_points": [
      "완전히 일치하면 1, 전혀 무관하면 0을 가짐"
    ],
    "difficulty": "medium",
    "id": "0532"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "에이전트가 ‘내가 방금 한 행동이 맞나?’라고 스스로 검증하고 계획을 수정하는 단축 키워드는?",
    "options": [
      "Reflection (성찰/반성)",
      "Recursion",
      "Iteration",
      "Looping",
      "Streaming"
    ],
    "answer": "Reflection (성찰/반성)",
    "why": "스스로의 논리적 허점을 찾아내어 정답에 도달할 때까지 루프를 도는 고급 자아 능력을 비유합니다.",
    "hint": "거울을 보듯 스스로를 돌아보는 행위입니다.",
    "trap_points": [
      "추론 성능이 좋은 모델일수록 이 반성 능력이 뛰어남"
    ],
    "difficulty": "hard",
    "id": "0533"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인 중 검색된 문서가 50개일 때, 너무 많으므로 가장 관련성 높은 5개만 다시 골라주는 필터링 모델은?",
    "options": [
      "Retriever",
      "Reranker (리랭커)",
      "Generator",
      "Tokenizer",
      "Parser"
    ],
    "answer": "Reranker (리랭커)",
    "why": "느리지만 정교한 Cross-Encoder 방식을 사용하여 최종 정답 후보지를 압축합니다.",
    "hint": "순위(Rank)를 다시(Re) 매기는 존재입니다.",
    "trap_points": [
      "토큰 소모량과 할루시네이션을 전격적으로 줄여줌"
    ],
    "difficulty": "hard",
    "id": "0534"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "에이전틱 워크플로우(Agentic Workflow)를 구현할 때 필수적으로 설정해야 하는 ‘이것’이 없으면 무한 루프에 빠질 수 있습니다. 이것은?",
    "options": [
      "Max Iterations (최대 반복 횟수)",
      "영단어 리스트",
      "이미지 파일",
      "인터넷 주소",
      "색상 값"
    ],
    "answer": "Max Iterations (최대 반복 횟수)",
    "why": "에이전트가 답을 못 찾고 계속 도구만 부르는 것을 강제로 끊어주는 안전핀 역할을 합니다.",
    "hint": "반복의 한계치입니다.",
    "trap_points": [
      "설정하지 않으면 API 비용이 천문학적으로 나올 수 있음"
    ],
    "difficulty": "easy",
    "id": "0535"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG에서 원본 문서를 관리할 때, 검색 효율을 위해 한 입 크기로 쪼개진 데이터 덩어리를 무엇이라 부르나요?",
    "answer": "Chunk (청크)",
    "why": "전체 문서를 모델에 한 번에 넣을 수 없기에, 의미 있는 조각 단위로 분할하여 관리합니다.",
    "hint": "덩어리라는 뜻의 영어 단어입니다.",
    "trap_points": [
      "청크가 너무 작으면 맥락이 끊기고, 너무 크면 주제가 섞임"
    ],
    "difficulty": "easy",
    "id": "0536"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트가 외부 '도구(Tool)'를 사용할 때, 주로 어떤 형식을 통해 호출 정보를 전달받나요?",
    "options": [
      "자유 텍스트",
      "JSON",
      "바이너리 코드",
      "이미지",
      "음성"
    ],
    "answer": "JSON",
    "why": "컴퓨터가 이해할 수 있는 구조화된 형식인 JSON을 통해 도구 이름과 인자값을 명확히 전달받습니다.",
    "hint": "키-값 쌍의 표준 데이터 형식입니다.",
    "trap_points": [
      "도구 정의 시 Pydantic 같은 스키마 정의가 매우 중요함"
    ],
    "difficulty": "medium",
    "id": "0537"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "벡터 DB에서 고차원 데이터를 빠르게 찾기 위해 사용하는 '근사 최근접 이웃(ANN)' 알고리즘 중 가장 유명한 것은?",
    "options": [
      "B-Tree",
      "HNSW (Hierarchical Navigable Small World)",
      "Linked List",
      "Stack",
      "Queue"
    ],
    "answer": "HNSW (Hierarchical Navigable Small World)",
    "why": "그래프 기반 인덱싱으로 대규모 고차원 벡터 데이터에서 압도적인 검색 속도를 보여주는 표준 알고리즘입니다.",
    "hint": "계층형(Hierarchical) 그래프 구조를 사용합니다.",
    "trap_points": [
      "메모리 사용량은 높지만 성능이 매우 뛰어남"
    ],
    "difficulty": "hard",
    "id": "0538"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG를 구축했지만 모델이 자꾸 외부 지식을 무시하고 자신의 내부 지식으로만 답을 한다면, 해결책은?",
    "options": [
      "모델을 삭제한다.",
      "시스템 프롬프트에 '반드시 제공된 컨텍스트에서만 답하고, 모르면 모른다고 하라'고 강력하게 명시한다.",
      "컴퓨터를 다시 켠다.",
      "다른 사람에게 물어본다.",
      "영어로만 코딩한다."
    ],
    "answer": "시스템 프롬프트에 '반드시 제공된 컨텍스트에서만 답하고, 모르면 모른다고 하라'고 강력하게 명시한다.",
    "why": "모델의 지식 우선순위를 외부 데이터로 강제 조정하는 프롬프트 조정이 필요합니다.",
    "hint": "지침(Instruction)의 우선순위를 바로잡는 것입니다.",
    "trap_points": [
      "이를 통해 할루시네이션(환각)을 전격적으로 억제할 수 있음"
    ],
    "difficulty": "easy",
    "id": "0539"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "에이전트가 작업을 수행하며 생성한 중간 산출물들을 보관하고, 다음 단계의 사고에 활용하는 ‘메모리’ 영역을 비유하는 용어는?",
    "answer": "Scratchpad (스크래치패드) 또는 Working Memory",
    "why": "연습장처럼 중간 과정을 적어두고 사고를 이어가는 공간을 의미합니다.",
    "hint": "메모장, 연습장이라는 뜻입니다.",
    "trap_points": [
      "ReAct 프레임워크의 생각-행동-관찰 기록이 여기에 해당함"
    ],
    "difficulty": "medium",
    "id": "0540"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG에서 검색을 위해 질문에 대한 가상의 답변을 먼저 생성하고 이를 임베딩하여 문서를 찾는 방식은?",
    "options": [
      "BM25",
      "HyDE (Hypothetical Document Embedding)",
      "Multi-Query",
      "Contextual Retrieval",
      "Small2Big"
    ],
    "answer": "HyDE (Hypothetical Document Embedding)",
    "why": "HyDE는 실제 질문보다 LLM이 생성한 가상 답변이 문서 저장소의 내용과 유사도가 더 높을 수 있다는 점을 이용한 고급 검색 기법입니다.",
    "hint": "가상(Hypothetical)의 문서 임베딩입니다.",
    "trap_points": [
      "가상 답변에 할루시네이션이 섞여도 유사도 검색에는 도움이 될 수 있음"
    ],
    "difficulty": "hard",
    "id": "0541"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "문서를 작은 청크로 나누면서도 검색 시에는 상위의 더 큰 맥락을 가져오는 청킹 전략은?",
    "options": [
      "Recursive Chunking",
      "Sliding Window",
      "Parent-Child / Small2Big",
      "Token-based Chunking",
      "Sentence Chunking"
    ],
    "answer": "Parent-Child / Small2Big",
    "why": "검색은 작은 단위(Small)로 정밀하게 수행하고, 실제 LLM에 전달할 때는 그 작은 조각이 포함된 큰 맥락(Parent/Big)을 제공하여 성능을 높입니다.",
    "hint": "부모(Parent)와 자식(Child) 관계를 생각하세요.",
    "trap_points": [
      "단순히 겹치게 하는 Sliding Window와는 다름"
    ],
    "difficulty": "medium",
    "id": "0542"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "하나의 질문을 여러 개의 다른 질문으로 변환하여 검색 범위를 넓히는 기법은?",
    "options": [
      "Query Reformulation",
      "Multi-Querying",
      "Step-back Prompting",
      "Self-RAG",
      "Correction Agent"
    ],
    "answer": "Multi-Querying",
    "why": "사용자의 단일 질문을 다양한 관점의 여러 쿼리로 확장하여 검색함으로써 유사도 검색의 한계를 극복하고 풍부한 컨텍스트를 확보합니다.",
    "hint": "여러 개(Multi)의 질문(Query)을 만듭니다.",
    "trap_points": [
      "단순한 결과 재배열(Re-ranking)과는 다름"
    ],
    "difficulty": "medium",
    "id": "0543"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 시스템에서 유사도 검색 시 '의미적으로는 같으나 다른 단어'를 찾지 못하는 키워드 검색의 한계를 보완하는 기술은?",
    "options": [
      "BM25",
      "TF-IDF",
      "Vector Embedding (임베딩)",
      "Regex",
      "SQL Like"
    ],
    "answer": "Vector Embedding (임베딩)",
    "why": "임베딩은 텍스트를 벡터로 변환하여 '왕'과 '군주'처럼 철자는 다르지만 의미가 유사한 단어를 거리 기반으로 찾아낼 수 있습니다.",
    "hint": "텍스트를 숫자의 나열(Vector)로 바꿉니다.",
    "trap_points": [
      "BM25는 키워드 일치를 중시함"
    ],
    "difficulty": "medium",
    "id": "0544"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "사용자의 질문을 기반으로 필요한 도구(Tool)를 선택하고 실행 계획을 스스로 세우는 구조는?",
    "options": [
      "Static Pipeline",
      "Fixed Workflow",
      "AI Agent",
      "Legacy Bot",
      "Rule-based System"
    ],
    "answer": "AI Agent",
    "why": "에이전트는 LLM의 추론 능력을 활용하여 목표 달성을 위한 작업 단계(생각, 행동, 관찰)를 능동적으로 제어합니다.",
    "hint": "대리인, 주체적으로 행동하는 존재를 뜻합니다.",
    "trap_points": [
      "단순히 문서를 찾는 RAG보다 한 단계 상위 개념임"
    ],
    "difficulty": "hard",
    "id": "0545"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 시스템에서 '검색 엔진'의 역할이 중요한 이유는?",
    "options": [
      "데이터를 삭제하려고",
      "방대한 외부 문서 중 사용자의 질문과 가장 관련 있는 '맥락'을 족집게처럼 찾아내 모델에 전달하기 때문",
      "영어로 번역하려고",
      "이미지를 만들려고",
      "파일 이름을 지으려고"
    ],
    "answer": "방대한 외부 문서 중 사용자의 질문과 가장 관련 있는 '맥락'을 족집게처럼 찾아내 모델에 전달하기 때문",
    "why": "관련 없는 정보가 들어가면 모델이 혼란을 느껴 오답을 내기 때문입니다.",
    "hint": "정보를 찾아오는(Retrieval) 과정이 첫 번째 단추입니다.",
    "trap_points": [
      "검색 성능이 RAG 시스템 전체 성능을 결정함"
    ],
    "difficulty": "easy",
    "id": "0546"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "에이전트가 목표를 달성하기 위해 '스스로 도구를 부르고 결과를 확인'하는 루프를 중단시켜야 하는 상황은?",
    "options": [
      "답을 찾았을 때",
      "설정된 최대 반복 횟수(Max Iterations)에 도달했을 때",
      "사용자가 중단했을 때",
      "위 셋 모두 해당",
      "단순히 지루할 때"
    ],
    "answer": "위 셋 모두 해당",
    "why": "무한 루프에 빠져 비용이 폭주하는 것을 막기 위해 강력한 종료 조건이 필수적입니다.",
    "hint": "안전하게 멈춰야(Stop) 합니다.",
    "trap_points": [
      "에이전트 개발 시 가장 먼저 구현해야 할 안전장치임"
    ],
    "difficulty": "medium",
    "id": "0547"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "벡터 DB에서 질문과 '의미적으로' 연결된 정보를 찾는 과정을 무엇이라 하나요?",
    "options": [
      "Semantic Search (시맨틱 검색)",
      "Keyword Search",
      "Exact Match",
      "Sort Rank",
      "List View"
    ],
    "answer": "Semantic Search (시맨틱 검색)",
    "why": "단순 오타가 있어도 의미적 맥락(벡터 공간상의 거리)을 통해 정답을 찾아냅니다.",
    "hint": "의미를 뜻하는 단어입니다.",
    "trap_points": [
      "철자가 정확해야 하는 검색과는 상반된 장점을 가짐"
    ],
    "difficulty": "easy",
    "id": "0548"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "에이전트 시스템에서 '도구(Tool)'의 설명문을 매우 자세히 적어야 하는 기술적 이유는?",
    "options": [
      "모델이 예쁜 글을 좋아해서",
      "모델은 도구의 코드가 아닌 '설명'만 읽고 해당 도구를 쓸지 말지 결정하기 때문",
      "영어로만 답하려고",
      "데이터를 압축하려고",
      "비용을 늘리려고"
    ],
    "answer": "모델은 도구의 코드가 아닌 '설명'만 읽고 해당 도구를 쓸지 말지 결정하기 때문",
    "why": "모델에게 설명문은 도구의 카탈로그이자 매뉴얼이므로, 설명이 모호하면 엉뚱한 도구를 부르게 됩니다.",
    "hint": "모델의 유일한 판단 근거입니다.",
    "trap_points": [
      "설명문은 구체적인 예시(input/output)를 포함하는 것이 베스트임"
    ],
    "difficulty": "medium",
    "id": "0549"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "에이전트가 어떤 과제를 수행할 때, 자신의 '생각(Thought)', '행동(Action)', '결과 관찰(Observation)'을 기록하는 방식을 무엇이라 하나요?",
    "answer": "ReAct (Reason + Act)",
    "why": "생각과 행동을 유기적으로 연결하여 복잡한 과제를 해결하는 에이전트의 표준 행동 모델입니다.",
    "hint": "R-e-A-c-t 입니다.",
    "trap_points": [
      "LangChain 에이전트들이 주로 사용하는 핵심 로직임"
    ],
    "difficulty": "medium",
    "id": "0550"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG의 5단계 중, 사용자의 질문에서 불필요한 단어를 빼거나 더 적합한 검색어로 다듬는 과정은?",
    "options": [
      "Indexing",
      "Searching",
      "Processing (전처리)",
      "Augmenting",
      "Generating"
    ],
    "answer": "Processing (전처리)",
    "why": "질문 정제(Query Refinement)를 통해 벡터 DB 검색의 정밀도를 높이는 기초 단계입니다.",
    "hint": "가공, 처리의 단계를 생각하세요.",
    "trap_points": [
      "질문의 의도 파악(Intent Classification)도 이 단계에서 함"
    ],
    "difficulty": "medium",
    "id": "0551"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "벡터 DB에서 질문과 '의미적으로' 유사한 문서를 찾기 위해 텍스트를 고차원 숫자로 변환하는 기술은?",
    "options": [
      "Indexing",
      "Tokenizing",
      "Embedding (임베딩)",
      "Scaling",
      "Normalizing"
    ],
    "answer": "Embedding (임베딩)",
    "why": "텍스트의 추상적인 의미를 다차원 공간상의 좌표(벡터)로 변환하는 RAG의 핵심 기술입니다.",
    "hint": "E로 시작하는 용어로, 이미 여러 번 다뤘습니다.",
    "trap_points": [
      "임베딩 모델의 성능이 전체 RAG 품질의 50% 이상을 결정함"
    ],
    "difficulty": "easy",
    "id": "0552"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "검색된 문서 내용이 너무 길어 LLM의 컨텍스트 한도를 초과할 때, 검색된 결과를 요약해서 붙여주는 기술은?",
    "options": [
      "Refining",
      "Compressing (컨텍스트 압축)",
      "Stretching",
      "Mapping",
      "Reducing"
    ],
    "answer": "Compressing (컨텍스트 압축)",
    "why": "필요한 정보만 엑기스로 추출하여 토큰 비용을 아끼고 모델의 인지 부하를 줄입니다.",
    "hint": "압축한다는 뜻입니다.",
    "trap_points": [
      "압축 과정에서 중요한 소스 인용 정보가 손실되지 않도록 해야 함"
    ],
    "difficulty": "hard",
    "id": "0553"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG의 '할루시네이션(환각)' 억제 원리에 대해 가장 잘 설명한 것은?",
    "options": [
      "모델이 똑똑해서 거짓말을 안 하게 된다.",
      "검색된 실제 '근거 문서'를 프롬프트에 제공함으로써 모델이 지어낼 필요를 없애준다.",
      "인터넷 속도가 빨라져서",
      "데이터가 압축되어서",
      "모델 가중치가 바뀌어서"
    ],
    "answer": "검색된 실제 '근거 문서'를 프롬프트에 제공함으로써 모델이 지어낼 필요를 없애준다.",
    "why": "지식 컷오프 문제와 잘못된 기억 문제를 외부 데이터 증강을 통해 물리적으로 해결합니다.",
    "hint": "외부의 '근거(Ground truth)'를 제공하는 것에 주목하세요.",
    "trap_points": [
      "문서 내용 자체가 틀려 있으면 모델도 틀린 답을 할 수 있음 (GIGO 원칙)"
    ],
    "difficulty": "easy",
    "id": "0554"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "벡터 DB에서 유사도 검색의 속도를 높이기 위해, 정확도는 조금 양보하더라도 대략적으로 가장 가까운 것들을 찾아내는 기술은?",
    "answer": "ANN (Approximate Nearest Neighbor)",
    "why": "모든 데이터와 일일이 비교하는 KNN의 한계를 복잡한 인덱싱 알고리즘으로 극복합니다.",
    "hint": "대략적인(Approximate)의 약자입니다.",
    "trap_points": [
      "속도는 수백 배 빠르지만 순위가 100% 정확하지 않을 수 있음"
    ],
    "difficulty": "medium",
    "id": "0555"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트가 실패하거나 도구 사용법이 틀렸을 때, 오류 로그를 보고 스스로 수정하여 다시 시도하는 기법은?",
    "options": [
      "Looping",
      "Self-Correction (또는 Reflection)",
      "Debugging",
      "Rerunning",
      "Augmenting"
    ],
    "answer": "Self-Correction (또는 Reflection)",
    "why": "자신의 행동 결과를 평가(Critic)하고 개선 사항을 도출하여 다음 단계에 반영합니다.",
    "hint": "자기(Self) 수정(Correction)입니다.",
    "trap_points": [
      "복잡한 에이전틱 워크플로우의 정수입니다."
    ],
    "difficulty": "medium",
    "id": "0556"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인 중 검색된 조각들을 질문과 합쳐서 프롬프트를 조립하는 컴포넌트를 무엇이라 하나요?",
    "options": [
      "Retriever",
      "Generator",
      "Augmentor (또는 Orchestrator)",
      "Indexer",
      "Parser"
    ],
    "answer": "Augmentor (또는 Orchestrator)",
    "why": "검색과 생성을 매끄럽게 연결하고 컨텍스트를 증강하는 역할을 수행합니다.",
    "hint": "증강하거나 조율하는 존재를 생각하세요.",
    "trap_points": [
      "LangChain에서는 보통 Chain 객체가 이 역할을 함"
    ],
    "difficulty": "medium",
    "id": "0557"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "BM25와 같은 알고리즘을 사용하며, 단어의 의미보다는 '철자의 정확한 일치'를 찾는 검색 방식은?",
    "options": [
      "Semantic Search",
      "Lexical Search (키워드 검색)",
      "Visual Search",
      "Deep Search",
      "Neural Search"
    ],
    "answer": "Lexical Search (키워드 검색)",
    "why": "어휘 기반 검색으로 전문 용어나 상품명 등 정확한 매칭이 필요한 경우 효과적입니다.",
    "hint": "어휘, 사전적인 뜻의 단어입니다.",
    "trap_points": [
      "의미가 비슷해도 철자가 다르면 찾아내지 못함"
    ],
    "difficulty": "medium",
    "id": "0558"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "에이전트 구현 시 '현재 내가 무얼 했고, 앞으로 무얼 해야 할지'를 단계별로 기록하고 관리하는 능력을 무엇이라 하나요?",
    "answer": "Reasoning (추론) 또는 Planning (계획)",
    "why": "목표 달성을 위한 전략적 사고의 흐름을 관리하는 핵심 지능입니다.",
    "hint": "P로 시작하는 8글자 단어이기도 합니다.",
    "trap_points": [
      "ReAct 프레임워크의 Thought 단계가 여기에 해당함"
    ],
    "difficulty": "medium",
    "id": "0559"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "벡터 DB에서 유사도 계산 시 코사인 유사도 대신 '방향은 무시하고 절대적인 거리 차이'만 중요할 때 사용하는 메트릭은?",
    "options": [
      "Cosine Distance",
      "L2 Distance (Euclidean)",
      "Manhattan Distance",
      "Hamming Distance",
      "Dot Product"
    ],
    "answer": "L2 Distance (Euclidean)",
    "why": "유클리드 거리는 임베딩 벡터의 크기(Magnitude) 차이까지 모두 반영하여 거리를 잽니다.",
    "hint": "두 점 사이의 직선 거리입니다.",
    "trap_points": [
      "텍스트 정체성보다 수치적 크기가 중요할 때 유리함"
    ],
    "difficulty": "medium",
    "id": "0560"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG의 5단계 중 검색된 내용을 바탕으로 실제 답변을 '출력'하는 마지막 단계는?",
    "options": [
      "Indexing",
      "Searching",
      "Augmenting",
      "Generating",
      "Processing"
    ],
    "answer": "Generating",
    "why": "생성(Generating) 단계에서 LLM은 주어진 컨텍스트를 소화하여 최종 답변을 생성합니다.",
    "hint": "생성하다라는 뜻입니다.",
    "trap_points": [
      "RAG의 최종 마침표를 찍는 단계임"
    ],
    "difficulty": "easy",
    "id": "0561"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "벡터 DB에서 질문과 가장 가까운 상위 N개의 조각을 찾아오는 과정을 무엇이라 하나요?",
    "options": [
      "Pulling",
      "Top-K Retrieval",
      "Indexing",
      "Scanning",
      "Parsing"
    ],
    "answer": "Top-K Retrieval",
    "why": "유사도 점수가 가장 높은 K개의 문서를 찾아오는 검색의 기본 과정입니다.",
    "hint": "가장 높은(Top) 순위 K개를 검색함.",
    "trap_points": [
      "최근에는 K를 무조건 크게 잡기보다 품질(Threshold)로 거르기도 함"
    ],
    "difficulty": "medium",
    "id": "0562"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 환경에서 답변의 근거를 명시하여 사용자가 직접 원문을 확인할 수 있게 하는 기술은?",
    "options": [
      "Citation (인용)",
      "Shadowing",
      "Masking",
      "Encoding",
      "Mirroring"
    ],
    "answer": "Citation (인용)",
    "why": "답변 중 특정 부분에 [1] 과 같은 표시를 남겨 신뢰도를 높이는 필수적인 UI/UX 요소입니다.",
    "hint": "논문 등에서 출처를 밝히는 것을 무엇이라 하나요?",
    "trap_points": [
      "할루시네이션 방지에 매우 효과적임"
    ],
    "difficulty": "medium",
    "id": "0563"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인에서 청크 사이즈를 결정할 때, 일반적인 웹 게시물이나 뉴스 기사에 권장되는 초기 설정값(토큰 기준)은?",
    "options": [
      "10 ~ 20 토큰",
      "100 ~ 500 토큰",
      "5000 ~ 10000 토큰",
      "1개 이상의 전체 문서",
      "글자 수 기반 1자"
    ],
    "answer": "100 ~ 500 토큰",
    "why": "너무 작으면 맥락이 없고, 너무 크면 주제가 섞입니다. 500 토큰 내외가 정보의 완결성과 검색 정밀도의 균형이 좋습니다.",
    "hint": "모니터상 대략 대여섯 줄 정도의 분량입니다.",
    "trap_points": [
      "오버랩(Overlap)도 함께 고려해야 완벽해짐"
    ],
    "difficulty": "medium",
    "id": "0564"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "벡터 DB에 저장하기 전, 긴 문서를 의미 있는 작은 조각으로 나누는 행위를 무엇이라 하나요?",
    "answer": "Chunking (청킹)",
    "why": "모델의 입력 한계와 검색 정밀도를 위해 데이터를 적절한 크기로 분할하는 필수 전처리 과정입니다.",
    "hint": "한 입 크기의 덩어리라는 영어 단어입니다.",
    "trap_points": [
      "의미가 끊기지 않도록 문장이나 단락 단위로 자르는 것이 좋음"
    ],
    "difficulty": "easy",
    "id": "0565"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "사용자의 질문 하나로 충분한 정보가 검색되지 않을 때, 질문을 여러 개로 변주하여 검색 확률을 높이는 것은?",
    "options": [
      "Multi-querying",
      "Single-searching",
      "Static-fetching",
      "Binary-matching",
      "Sequential-reading"
    ],
    "answer": "Multi-querying",
    "why": "질문을 다양하게 변형하여 검색 엔진의 사각지대를 없애는 고급 리트리버 전략입니다.",
    "hint": "여러 개(Multi)의 쿼리(Query)입니다.",
    "trap_points": [
      "LangChain에서 MultiQueryRetriever로 쉽게 구현 가능함"
    ],
    "difficulty": "medium",
    "id": "0566"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "에이전트 구현 시 '메모리'를 관리할 때 대화의 양이 너무 많아지면 어떻게 처리하는 것이 가장 효율적인가요?",
    "options": [
      "그냥 다 보낸다 (비용 상관없음)",
      "과거 내용을 요약하여 전달하거나, 핵심 사실만 추출해 따로 저장한다.",
      "과거를 아예 다 지운다.",
      "영어로만 저장한다.",
      "사용자에게 요약해달라고 시킨다."
    ],
    "answer": "과거 내용을 요약하여 전달하거나, 핵심 사실만 추출해 따로 저장한다.",
    "why": "컨텍스트 윈도우 한계가 있으므로 요약(Summary)이나 벡터 기반 메모리 관리가 필수적입니다.",
    "hint": "줄여서 간직하기(Summarization)를 생각하세요.",
    "trap_points": [
      "중요한 고유 명사가 요약 과정에서 사라지지 않도록 주의해야 함"
    ],
    "difficulty": "medium",
    "id": "0567"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "벡터 DB에서 두 벡터가 '완전히 같은 방향'일 때 코사인 유사도 값은?",
    "options": [
      "0",
      "-1",
      "1",
      "100",
      "무한대"
    ],
    "answer": "1",
    "why": "코사인 유사도는 1일 때 최대이며(완전 일치), 0은 무관, -1은 정반대를 뜻합니다.",
    "hint": "최댓값은 얼마일까요?",
    "trap_points": [
      "유사도와 거리(Distance)는 반대 개념임에 항상 주의"
    ],
    "difficulty": "easy",
    "id": "0568"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트가 외부 웹 검색이나 계산기 등을 사용할 수 있도록 연결된 인터페이스를 보통 무엇이라 부르나요?",
    "answer": "Tool (또는 Function)",
    "why": "에이전트가 텍스트 생성을 넘어 실제 행동을 취하게 해주는 팔과 다리 같은 존재입니다.",
    "hint": "도구라는 뜻입니다.",
    "trap_points": [
      "모델은 이 기능의 '이름과 사용법'만 알고 필요시 호출을 요청함"
    ],
    "difficulty": "easy",
    "id": "0569"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 시스템 성능 개선을 위해 검색된 문서 30개를 가져온 뒤, 가장 정답 확률이 높은 5개만 다시 골라내는 모델 타입은?",
    "options": [
      "Bi-Encoder",
      "Cross-Encoder (리랭커)",
      "Decoder",
      "CNN",
      "Tokenizer"
    ],
    "answer": "Cross-Encoder (리랭커)",
    "why": "느리지만 훨씬 강력한 모델을 사용하여 질문과 문서의 상관관계를 심층 분석해 순위를 재배열합니다.",
    "hint": "두 입력을 교차(Cross)해서 보는 인코더입니다.",
    "trap_points": [
      "대규모 검색에는 부적합하지만 소수 정예 선별에는 탁월함"
    ],
    "difficulty": "hard",
    "id": "0570"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 파이프라인에서 텍스트 조각(Chunk)을 수치화된 좌표로 바꾸는 임베딩 모델의 역할은?",
    "options": [
      "글자 수를 센다.",
      "단어의 '의미'를 고차원 공간상의 위치(벡터)로 표현한다.",
      "영어로 번역한다.",
      "데이터를 삭제한다.",
      "파일 이름을 짓는다."
    ],
    "answer": "단어의 '의미'를 고차원 공간상의 위치(벡터)로 표현한다.",
    "why": "비슷한 의미를 가진 텍스트들이 가까운 위치에 있게 하여 검색이 가능하게 만듭니다.",
    "hint": "수치로 박아 넣다(Embed).",
    "trap_points": [
      "임베딩 성능이 RAG 검색 품질의 90%를 결정함"
    ],
    "difficulty": "medium",
    "id": "0571"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "벡터 DB에서 질문과 '가장 가까운 거리'의 문서를 찾는 기술의 이름은?",
    "options": [
      "Binary Search",
      "Hash View",
      "Similarity Search (유사도 검색)",
      "Sort Scan",
      "List Scan"
    ],
    "answer": "Similarity Search (유사도 검색)",
    "why": "철자가 아닌 '의미의 거리'가 가까운 것을 찾아내는 RAG의 핵심 기술입니다.",
    "hint": "비슷한 정도(Similarity)를 기준으로 찾습니다.",
    "trap_points": [
      "코사인 유사도나 유클리디안 거리 등이 계산 메트릭으로 쓰임"
    ],
    "difficulty": "easy",
    "id": "0572"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "에이전트가 어떤 문제를 풀기 위해 '인터넷 검색' 도구를 쓸지 '계산기' 도구를 쓸지 결정하는 단계를 무엇이라 하나요?",
    "options": [
      "Thinking",
      "Planning (계획)",
      "Acting",
      "Observing",
      "Reporting"
    ],
    "answer": "Planning (계획)",
    "why": "자신의 목표를 위해 필요한 수단(Tool)을 고르고 일정을 짜는 지능적 단계입니다.",
    "hint": "계획(Plan)을 세웁니다.",
    "trap_points": [
      "복잡한 에이전트는 하위 실행 계획까지 꼼꼼히 세움"
    ],
    "difficulty": "easy",
    "id": "0573"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG에서 ‘검색된 문서’가 너무 많을 때 LLM에 넘길 5개만 정밀하게 재배열하는 과정은?",
    "options": [
      "Re-ranking (리랭킹)",
      "Re-scaling",
      "Re-merging",
      "Re-filtering",
      "Re-coding"
    ],
    "answer": "Re-ranking (리랭킹)",
    "why": "단순 벡터 거리보다 훨씬 정교한 모델로 실제 질문과의 연관성을 다시 채점하여 정확도를 높입니다.",
    "hint": "순위(Ranking)를 다시(Re) 매깁니다.",
    "trap_points": [
      "토큰 누수 배제와 답변 품질 향상의 핵심 공정임"
    ],
    "difficulty": "hard",
    "id": "0574"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "AI 에이전트 워크플로우인 'ReAct'의 의미는?",
    "options": [
      "React 프레임워크 사용",
      "Reasoning (추론)과 Acting (행동)의 결합",
      "빨리 반응하기",
      "다시 행동하기",
      "이미지 처리"
    ],
    "answer": "Reasoning (추론)과 Acting (행동)의 결합",
    "why": "생각(Thought)하고 행동(Action)하고 관찰(Observation)하는 루프를 통해 문제를 해결하는 프레임워크입니다.",
    "hint": "R-e와 A-c-t의 조합입니다.",
    "trap_points": [
      "에이전트 구현의 가장 기본적이고 강력한 패턴임"
    ],
    "difficulty": "medium",
    "id": "0575"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "에이전트가 도구를 사용하여 얻운 중간 결과물(시스템 로그, 검색 결과 등)을 관찰하는 단계의 이름은?",
    "answer": "Observation (관찰)",
    "why": "행동(Action)에 대한 결과를 확인하고 다음 생각을 이어가기 위한 피드백 단계입니다.",
    "hint": "눈으로 보고 확인(Observe)합니다.",
    "trap_points": [
      "관찰 내용이 부실하면 에이전트가 잘못된 판단을 내림"
    ],
    "difficulty": "medium",
    "id": "0576"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "RAG 구축 시 한 문서를 ‘어떤 크기(500자, 1000자 등)’로 나눌지가 중요한 이유는?",
    "options": [
      "파일 이름을 지으려고",
      "너무 작으면 문맥이 끊기고, 너무 크면 주제가 섞여 검색 정확도가 떨어지기 때문",
      "영어로만 답하려고",
      "이미지를 만들려고",
      "비용을 늘리려고"
    ],
    "answer": "너무 작으면 문맥이 끊기고, 너무 크면 주제가 섞여 검색 정확도가 떨어지기 때문",
    "why": "RAG 검색의 해상도(Resolution)를 결정하는 핵심 하이퍼파라미터입니다.",
    "hint": "조각(Chunk)의 크기 조절입니다.",
    "trap_points": [
      "보통 문장이나 문단 단위로 겹치도록(Overlap) 나누는 것이 정석임"
    ],
    "difficulty": "medium",
    "id": "0577"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "벡터 DB에서 질문과 문서 간의 거리를 잴 때 가장 널리 쓰이는 수학적 공식은?",
    "options": [
      "덧셈",
      "뺄셈",
      "Cosine Similarity (코사인 유사도)",
      "피타고라스 정리",
      "미분"
    ],
    "answer": "Cosine Similarity (코사인 유사도)",
    "why": "벡터의 방향 일치도를 0에서 1 사이의 값으로 알려주어 텍스트 의미 비교에 최적입니다.",
    "hint": "코사인(Cosine)입니다.",
    "trap_points": [
      "두 벡터의 길이가 달라도 방향만 같으면 높은 점수가 나옴"
    ],
    "difficulty": "easy",
    "id": "0578"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "에이전트가 겪은 실패를 교훈 삼아 다음 단계에서 같은 실수를 반복하지 않게 하는 능력을 무엇이라 비유하나요?",
    "options": [
      "아이큐",
      "성찰(Reflection) 파이프라인",
      "네트워킹",
      "백업",
      "암호화"
    ],
    "answer": "성찰(Reflection) 파이프라인",
    "why": "자신의 중간 답변을 평가(Critic)하고 수정하도록 유도하여 성공률을 높이는 기법입니다.",
    "hint": "거울을 보듯 자기를 돌아보는 과정입니다.",
    "trap_points": [
      "에이전틱 워크플로우의 고도화 단계임"
    ],
    "difficulty": "hard",
    "id": "0579"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "객관식",
    "question": "사용자의 모호한 질문을 RAG 검색에 최적화된 형태로 LLM을 통해 다시 쓰는 기법은?",
    "answer": "Query Transformation (질문 변환)",
    "why": "'그게 뭐야' 같은 모호한 지칭을 '파이썬의 GIL이 뭐야' 처럼 구체화하여 검색 효율을 높입니다.",
    "hint": "질문(Query)을 변형(Transformation)합니다.",
    "trap_points": [
      "Multi-query나 HyDE 기법이 여기에 포함됨"
    ],
    "difficulty": "medium",
    "id": "0580"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "문서를 벡터화하여 저장하는 데이터베이스를 지칭하는 용어는? (문제 1)",
    "options": [],
    "answer": "VectorStore",
    "why": "VectorStore는 임베딩된 문서를 효율적으로 검색할 수 있게 저장합니다.",
    "difficulty": "hard",
    "id": "0581"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "LangChain에서 문서를 청크 단위로 나누는 클래스는? (문제 2)",
    "options": [],
    "answer": "CharacterTextSplitter",
    "why": "긴 문서를 모델 컨텍스트에 맞게 쪼개는 역할을 합니다.",
    "difficulty": "medium",
    "id": "0582"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "유사도 검색을 수행하는 벡터 저장소의 메서드는? (문제 3)",
    "options": [],
    "answer": "similarity_search()",
    "why": "쿼리와 가장 거리가 가까운 문서를 찾아냅니다.",
    "difficulty": "hard",
    "id": "0583"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "문서를 벡터화하여 저장하는 데이터베이스를 지칭하는 용어는? (문제 4)",
    "options": [],
    "answer": "VectorStore",
    "why": "VectorStore는 임베딩된 문서를 효율적으로 검색할 수 있게 저장합니다.",
    "difficulty": "medium",
    "id": "0584"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "LangChain에서 문서를 청크 단위로 나누는 클래스는? (문제 5)",
    "options": [],
    "answer": "CharacterTextSplitter",
    "why": "긴 문서를 모델 컨텍스트에 맞게 쪼개는 역할을 합니다.",
    "difficulty": "hard",
    "id": "0585"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "유사도 검색을 수행하는 벡터 저장소의 메서드는? (문제 6)",
    "options": [],
    "answer": "similarity_search()",
    "why": "쿼리와 가장 거리가 가까운 문서를 찾아냅니다.",
    "difficulty": "medium",
    "id": "0586"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "문서를 벡터화하여 저장하는 데이터베이스를 지칭하는 용어는? (문제 7)",
    "options": [],
    "answer": "VectorStore",
    "why": "VectorStore는 임베딩된 문서를 효율적으로 검색할 수 있게 저장합니다.",
    "difficulty": "hard",
    "id": "0587"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "LangChain에서 문서를 청크 단위로 나누는 클래스는? (문제 8)",
    "options": [],
    "answer": "CharacterTextSplitter",
    "why": "긴 문서를 모델 컨텍스트에 맞게 쪼개는 역할을 합니다.",
    "difficulty": "medium",
    "id": "0588"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "유사도 검색을 수행하는 벡터 저장소의 메서드는? (문제 9)",
    "options": [],
    "answer": "similarity_search()",
    "why": "쿼리와 가장 거리가 가까운 문서를 찾아냅니다.",
    "difficulty": "hard",
    "id": "0589"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "문서를 벡터화하여 저장하는 데이터베이스를 지칭하는 용어는? (문제 10)",
    "options": [],
    "answer": "VectorStore",
    "why": "VectorStore는 임베딩된 문서를 효율적으로 검색할 수 있게 저장합니다.",
    "difficulty": "medium",
    "id": "0590"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "LangChain에서 문서를 청크 단위로 나누는 클래스는? (문제 11)",
    "options": [],
    "answer": "CharacterTextSplitter",
    "why": "긴 문서를 모델 컨텍스트에 맞게 쪼개는 역할을 합니다.",
    "difficulty": "hard",
    "id": "0591"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "유사도 검색을 수행하는 벡터 저장소의 메서드는? (문제 12)",
    "options": [],
    "answer": "similarity_search()",
    "why": "쿼리와 가장 거리가 가까운 문서를 찾아냅니다.",
    "difficulty": "medium",
    "id": "0592"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "문서를 벡터화하여 저장하는 데이터베이스를 지칭하는 용어는? (문제 13)",
    "options": [],
    "answer": "VectorStore",
    "why": "VectorStore는 임베딩된 문서를 효율적으로 검색할 수 있게 저장합니다.",
    "difficulty": "hard",
    "id": "0593"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "LangChain에서 문서를 청크 단위로 나누는 클래스는? (문제 14)",
    "options": [],
    "answer": "CharacterTextSplitter",
    "why": "긴 문서를 모델 컨텍스트에 맞게 쪼개는 역할을 합니다.",
    "difficulty": "medium",
    "id": "0594"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "유사도 검색을 수행하는 벡터 저장소의 메서드는? (문제 15)",
    "options": [],
    "answer": "similarity_search()",
    "why": "쿼리와 가장 거리가 가까운 문서를 찾아냅니다.",
    "difficulty": "hard",
    "id": "0595"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "문서를 벡터화하여 저장하는 데이터베이스를 지칭하는 용어는? (문제 16)",
    "options": [],
    "answer": "VectorStore",
    "why": "VectorStore는 임베딩된 문서를 효율적으로 검색할 수 있게 저장합니다.",
    "difficulty": "medium",
    "id": "0596"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "LangChain에서 문서를 청크 단위로 나누는 클래스는? (문제 17)",
    "options": [],
    "answer": "CharacterTextSplitter",
    "why": "긴 문서를 모델 컨텍스트에 맞게 쪼개는 역할을 합니다.",
    "difficulty": "hard",
    "id": "0597"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "유사도 검색을 수행하는 벡터 저장소의 메서드는? (문제 18)",
    "options": [],
    "answer": "similarity_search()",
    "why": "쿼리와 가장 거리가 가까운 문서를 찾아냅니다.",
    "difficulty": "medium",
    "id": "0598"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "문서를 벡터화하여 저장하는 데이터베이스를 지칭하는 용어는? (문제 19)",
    "options": [],
    "answer": "VectorStore",
    "why": "VectorStore는 임베딩된 문서를 효율적으로 검색할 수 있게 저장합니다.",
    "difficulty": "hard",
    "id": "0599"
  },
  {
    "chapter_name": "RAG & Agent",
    "type": "코드 완성형",
    "question": "LangChain에서 문서를 청크 단위로 나누는 클래스는? (문제 20)",
    "options": [],
    "answer": "CharacterTextSplitter",
    "why": "긴 문서를 모델 컨텍스트에 맞게 쪼개는 역할을 합니다.",
    "difficulty": "medium",
    "id": "0600"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "DPO(Direct Preference Optimization) 모델 학습을 위해 필요한 데이터 형태는?",
    "options": [
      "질문 하나와 답변 하나",
      "질문 하나와 (좋은 답변, 싫은 답변) 한 쌍",
      "문서 뭉치 하나",
      "영단어 리스트",
      "이미지 데이터"
    ],
    "answer": "질문 하나와 (좋은 답변, 싫은 답변) 한 쌍",
    "why": "두 답변 중 더 나은 쪽을 선호(Preference)하도록 모델의 확률 분포를 직접 조정하기 때문입니다.",
    "hint": "선택지 한 쌍이 필요합니다.",
    "trap_points": [
      "최근 RLHF를 대체하는 강력한 파인튜닝 기법임"
    ],
    "difficulty": "hard",
    "id": "0601"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝된 모델을 배포할 때, 모델의 레이어 정규화 값을 학습 시 값으로 고정하여 추론 속도를 높이는 과정은?",
    "options": [
      "Quantization",
      "Model Merging",
      "Graph Optimization",
      "Layer Folding",
      "Cashing"
    ],
    "answer": "Graph Optimization",
    "why": "연산 그래프를 분석하여 중복되거나 불필요한 계산을 합치거나 상수화하여 속도를 높입니다.",
    "hint": "그래프 최적화입니다.",
    "trap_points": [
      "ONNX, TensorRT 등이 이 과정을 수행함"
    ],
    "difficulty": "hard",
    "id": "0602"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 시 ‘데이터 오염(Data Contamination)’이란 무엇을 의미하나요?",
    "options": [
      "데이터가 사라지는 것",
      "평가에 쓰일 문제들이 학습 데이터에 포함되어 있어 실제 실력보다 점수가 높게 나오는 것",
      "인터넷이 끊기는 것",
      "영어로만 코딩하는 것",
      "파일이 깨지는 것"
    ],
    "answer": "평가에 쓰일 문제들이 학습 데이터에 포함되어 있어 실제 실력보다 점수가 높게 나오는 것",
    "why": "모델이 규칙을 배운 게 아니라 정답을 암기해버린 상태이므로 실전 성능을 신뢰할 수 없게 만듭니다.",
    "hint": "시험 정답을 미리 보고 시험을 치는 것과 같습니다.",
    "trap_points": [
      "학습 전 평가 데이터와의 중복 검사(De-contamination)가 필수임"
    ],
    "difficulty": "hard",
    "id": "0603"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 시 'Overfitting'을 감지하는 가장 직접적인 지표는?",
    "options": [
      "학습 데이터의 손실(Loss)은 줄어드는데, 검증(Validation) 데이터의 손실은 정체되거나 올라갈 때",
      "전원이 꺼질 때",
      "데이터가 사라졌을 때",
      "영어로만 답할 때",
      "손실 값이 0이 될 때"
    ],
    "answer": "학습 데이터의 손실(Loss)은 줄어드는데, 검증(Validation) 데이터의 손실은 정체되거나 올라갈 때",
    "why": "학습 데이터에만 과하게 맞춰져 새로운 데이터에 대한 일반화 능력을 잃었음을 뜻합니다.",
    "hint": "학습셋과 검증셋의 손실 값 차이를 보세요.",
    "trap_points": [
      "이 시점이 학습을 중단해야 하는 'Early Stopping' 타이밍임"
    ],
    "difficulty": "medium",
    "id": "0604"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "LoRA 학습에서 학습되는 아주 작은 두 개의 행렬 조각을 통칭하는 용어는?",
    "answer": "Adapter (어댑터)",
    "why": "기존 모델에 수수료처럼 덧붙여서(Adapt) 성능을 조절하는 조각이라는 뜻입니다.",
    "hint": "끼우다, 적응시키다라는 단어입니다.",
    "trap_points": [
      "학습 후 베이스 모델과 합쳐서(Merge) 사용할 수 있음"
    ],
    "difficulty": "easy",
    "id": "0605"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "LoRA 파인튜닝 시 'Rank(r)' 파라미터가 가지는 의미는?",
    "options": [
      "모델의 순위",
      "추가되는 어댑터 행렬의 차원 크기 (작을수록 메모리 절약, 클수록 정교함)",
      "데이터의 개수",
      "학습 모델의 가격",
      "인터넷 속도"
    ],
    "answer": "추가되는 어댑터 행렬의 차원 크기 (작을수록 메모리 절약, 클수록 정교함)",
    "why": "랭크는 파인튜닝할 수 있는 '용량'을 결정하며, 보통 8, 16, 32 등이 널리 사용됩니다.",
    "hint": "행렬의 차원 수입니다.",
    "trap_points": [
      "너무 크면 전체 파라미터 학습과 차이가 없어 효율이 떨어짐"
    ],
    "difficulty": "hard",
    "id": "0606"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 학습 중 모델이 원본의 선한 의도나 안전 지침을 무시하고 사용자에게 욕설을 하거나 거칠게 답하는 것을 막는 과정을 무엇이라 하나요?",
    "options": [
      "Normalization",
      "Alignment (정렬)",
      "Scaling",
      "Cleaning",
      "Encryption"
    ],
    "answer": "Alignment (정렬)",
    "why": "모델의 가치관과 인간의 가치관을 일치(Align)시키는 RLHF, DPO 등의 기법이 여기에 포함됩니다.",
    "hint": "인간의 의도에 맞게 조정(Align)합니다.",
    "trap_points": [
      "정렬이 잘 안 된 모델은 자의적인 주장을 펼칠 수 있음"
    ],
    "difficulty": "medium",
    "id": "0607"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝된 모델 배포 시 가장 많이 쓰이는 최적화 라이브러리로, 특히 엔비디아 GPU 성능을 100% 끌어내는 것은?",
    "options": [
      "vLLM",
      "TensorRT-LLM",
      "Ollama",
      "Llama.cpp",
      "Auto-GPT"
    ],
    "answer": "TensorRT-LLM",
    "why": "엔비디아 하드웨어에 최적화된 추론 라이브러리로 배치 처리와 지연 시간을 획기적으로 줄여줍니다.",
    "hint": "엔비디아 공식 최적화 툴입니다.",
    "trap_points": [
      "설치는 어렵지만 성능은 압도적임"
    ],
    "difficulty": "hard",
    "id": "0608"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "사전 학습(Pre-training)과 파인튜닝(Fine-tuning)의 가장 큰 재료 차이는?",
    "options": [
      "재료가 똑같다.",
      "사전 학습은 레이블이 없는 방대한 인터넷 데이터(Next Token Prediction)를 쓰고, 파인튜닝은 질문-답변 쌍과 같은 레이블된 정제 데이터를 쓴다.",
      "사전 학습만 영어다.",
      "파인튜닝은 이미지를 안 쓴다.",
      "차이가 없다."
    ],
    "answer": "사전 학습은 레이블이 없는 방대한 인터넷 데이터(Next Token Prediction)를 쓰고, 파인튜닝은 질문-답변 쌍과 같은 레이블된 정제 데이터를 쓴다.",
    "why": "사전 학습은 세상의 일반적 패턴을 배우는 과정이고, 파인튜닝은 특정 목적으로 ‘길들이는’ 과정입니다.",
    "hint": "데이터의 덩어리(Bulk)와 정제(Curated)의 차이입니다.",
    "trap_points": [
      "최근에는 파인튜닝 단계의 데이터 질이 모델 품질을 결정함"
    ],
    "difficulty": "easy",
    "id": "0609"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "가중치 행렬 중 일부 중요한 값만 남기고 0으로 만들어 모델 용량을 줄이는 최적화 방식은?",
    "answer": "Pruning (가지치기)",
    "why": "신경망 연결망 중 불필요한 가지를 쳐내어 속도와 용량을 개선합니다.",
    "hint": "나뭇가지를 친다는 뜻입니다.",
    "trap_points": [
      "Sparse해진 행렬을 효율적으로 처리하는 전용 커널이 필요함"
    ],
    "difficulty": "medium",
    "id": "0610"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 시 'Learning Rate (학습률)'가 너무 높을 때 나타나는 일반적인 증상은?",
    "options": [
      "학습이 너무 완벽하게 된다.",
      "손실 값(Loss)이 발산하거나 요동치며 모델이 아무 기술도 배우지 못하고 망가진다.",
      "속도가 100배 빨라진다.",
      "영어로만 답한다.",
      "글자 수가 길어진다."
    ],
    "answer": "손실 값(Loss)이 발산하거나 요동치며 모델이 아무 기술도 배우지 못하고 망가진다.",
    "why": "보폭(Learning Rate)이 너무 크면 최적의 지점(Minimum)을 지나쳐버리기 때문입니다.",
    "hint": "너무 큰 보폭의 부작용을 생각하세요.",
    "trap_points": [
      "반대로 너무 작으면 학습 진행이 아예 안 될 수도 있음"
    ],
    "difficulty": "medium",
    "id": "0611"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 후 모델의 '할루시네이션(환각)'이 오히려 심해지는 주된 원인은?",
    "options": [
      "컴퓨터가 뜨거워서",
      "데이터셋에 잘못된 정보가 포함되어 있거나, 모델이 데이터를 암기하기 시작해서 (Overfitting)",
      "영어로만 학습해서",
      "파일이 너무 커서",
      "인터넷이 끊겨서"
    ],
    "answer": "데이터셋에 잘못된 정보가 포함되어 있거나, 모델이 데이터를 암기하기 시작해서 (Overfitting)",
    "why": "나쁜 데이터를 배우면 지능 자체가 오염되는 GIGO 법칙의 결과입니다.",
    "hint": "학습 데이터의 품질이 곧 출력의 품질입니다.",
    "trap_points": [
      "데이터 양보다 정제가 훨씬 중요함"
    ],
    "difficulty": "easy",
    "id": "0612"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "모델의 지능 전수 기법인 'Distillation'에서 교사 모델과 학생 모델의 관계는?",
    "options": [
      "둘은 쌍둥이다.",
      "거대하고 똑똑한 모델(Teacher)의 확률 분포를 작고 가벼운 모델(Student)이 모방하도록 학습한다.",
      "학생이 더 크다.",
      "둘은 싸우는 관계다.",
      "관계가 없다."
    ],
    "answer": "거대하고 똑똑한 모델(Teacher)의 확률 분포를 작고 가벼운 모델(Student)이 모방하도록 학습한다.",
    "why": "효율을 위해 큰 모델의 능력을 작은 모델로 압축하는 강력한 산업적 기술입니다.",
    "hint": "증류(Distillation)의 과정을 생각하세요.",
    "trap_points": [
      "최신 sLLM들이 성능을 비약적으로 올린 비결임"
    ],
    "difficulty": "medium",
    "id": "0613"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 데이터를 만들 때, 사람이 직접 적는 대신 AI가 AI용 데이터를 자동으로 생성해 주는 기술은?",
    "answer": "Synthetic Data Generation (합성 데이터 생성)",
    "why": "고품질 데이터를 무한히 생성하여 학습 비용을 줄이고 다양성을 확보합니다.",
    "hint": "인공적으로 합성(Synthetic)한 데이터입니다.",
    "trap_points": [
      "최근에는 사람보다 AI가 만든 데이터로 학습한 성능이 더 높기도 함"
    ],
    "difficulty": "medium",
    "id": "0614"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝된 모델 공유 시 필수적으로 첨부해야 할 'Model Card'에 포함되지 않아도 되는 것은?",
    "options": [
      "모델의 용도와 제한 사항",
      "학습 데이터셋 정보",
      "평가 벤치마크 결과",
      "개발자의 어제 점심 메뉴",
      "저작권 및 라이선스 정보"
    ],
    "answer": "개발자의 어제 점심 메뉴",
    "why": "모델 카드는 기술적/윤리적 명세서이므로 사적인 정보는 필요 없습니다.",
    "hint": "모델의 '명세서'입니다.",
    "trap_points": [
      "투명한 AI 생태계를 위한 표준 약속임"
    ],
    "difficulty": "easy",
    "id": "0615"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝의 대표적인 방법인 LoRA에서 학습 대상이 되는 가중치는?",
    "options": [
      "모델의 전체 가중치",
      "모델에 추가된 두 개의 저차원 행렬 가중치 (A, B)",
      "바이어스 값만",
      "임베딩 레이어 가중치",
      "마지막 층의 가중치만"
    ],
    "answer": "모델에 추가된 두 개의 저차원 행렬 가중치 (A, B)",
    "why": "원본 파라미터는 동결하고, 옆에 붙인 가벼운 행렬만 업데이트하여 효율을 극대화합니다.",
    "hint": "옆에 덧붙인(Adapter) 작은 행렬들을 생각하세요.",
    "trap_points": [
      "이를 통해 수백 배 적은 메모리로 파인튜닝이 가능해짐"
    ],
    "difficulty": "hard",
    "id": "0616"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "DPO(Direct Preference Optimization) 알고리즘이 PPO(기존 RLHF)보다 구현이 쉬운 결정적인 이유는?",
    "options": [
      "데이터가 적어도 되기 때문에",
      "보상 모델(Reward Model) 학습 과정이 필요 없기 때문에",
      "한국어 전용이라서",
      "GPU 없이도 학습 가능해서",
      "학습 모델이 작아도 되기 때문에"
    ],
    "answer": "보상 모델(Reward Model) 학습 과정이 필요 없기 때문에",
    "why": "DPO는 선호도 데이터를 직접 정답 확률에 반영하여 복잡한 리워드 모델링 단계를 제거했습니다.",
    "hint": "샘플 문제에서도 다뤘던 핵심 비교 포인트입니다.",
    "trap_points": [
      "하지만 여전히 선호도 답변 쌍(A vs B) 데이터는 잘 구축해야 함"
    ],
    "difficulty": "medium",
    "id": "0617"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 과정에서 모델의 가독성과 코드 형식이 혼재될 때(DeepSeek R1-Zero 사례), 이를 해결하기 위해 결합하는 방식은?",
    "options": [
      "다시 Pre-training 하기",
      "Cold-start SFT를 거친 후 강화학습 진행",
      "학습 데이터 다 지우기",
      "모델 파라미터 무작위 초기화",
      "영어로만 재학습"
    ],
    "answer": "Cold-start SFT를 거친 후 강화학습 진행",
    "why": "기초적인 답변 형식(SFT)을 먼저 가르친 뒤에 강화학습을 시켜야 가동성과 성능을 동시에 잡을 수 있습니다.",
    "hint": "입문(Cold-start) 과정을 생각하세요.",
    "trap_points": [
      "R1의 최종 성공 비결이 바로 이 단계적 학습임"
    ],
    "difficulty": "hard",
    "id": "0618"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "모델이 인간의 사회적 가치관이나 안전 지침을 위반하지 않도록 최종적으로 튜닝하는 것을 무엇이라 하나요?",
    "options": [
      "Alignment (정렬)",
      "Scaling",
      "Distillation",
      "Pruning",
      "Backpropagation"
    ],
    "answer": "Alignment (정렬)",
    "why": "모델의 지능과 인간의 의도/가치관을 일직선으로 맞춘다는 의미의 용어입니다.",
    "hint": "나란히 맞춘다는 뜻입니다.",
    "trap_points": [
      "RLHF가 이 정렬을 위한 가장 대표적인 도구임"
    ],
    "difficulty": "medium",
    "id": "0619"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "원본 모델의 지식을 유지하면서 4비트 양자화와 LoRA를 결합하여 VRAM 사용량을 극단적으로 낮춘 파인튜닝 기법은?",
    "answer": "QLoRA",
    "why": "Quantized LoRA의 약자로, 48GB VRAM이 필요한 모델을 16GB에서도 튜닝 가능하게 만든 혁명적 기술입니다.",
    "hint": "Q + LoRA.",
    "trap_points": [
      "NF4(Normal Float 4)라는 특수 양자화 분포를 사용함"
    ],
    "difficulty": "hard",
    "id": "0620"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 데이터 구축 시, 모델이 같은 답변을 반복하지 않도록 다양성을 확보하는 것이 중요한 이유는?",
    "options": [
      "비용을 늘리기 위해",
      "모델이 한 가지 패턴에만 과적합(Overfitting)되는 것을 방지하기 위해",
      "답변 속도를 늦추기 위해",
      "사용자를 혼란스럽게 하기 위해",
      "글자 수를 늘리기 위해"
    ],
    "answer": "모델이 한 가지 패턴에만 과적합(Overfitting)되는 것을 방지하기 위해",
    "why": "데이터의 다양성이 부족하면 모델의 유연성이 떨어지고 '치명적 망각'이 심해질 수 있습니다.",
    "hint": "너무 한쪽으로 치우치는 현상을 막는 것입니다.",
    "trap_points": [
      "적은 양의 고품질 데이터라도 다양성이 담보되어야 함"
    ],
    "difficulty": "medium",
    "id": "0621"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "학습 과정에서 성능이 확인된 데이터만 선별하여 다시 학습 모델에 주입하는 기법은?",
    "options": [
      "Random Sampling",
      "Rejection Sampling (기각 샘플링)",
      "Data Scaling",
      "Model Merging",
      "Weight Averaging"
    ],
    "answer": "Rejection Sampling (기각 샘플링)",
    "why": "여러 답변 중 좋은 답만 골라내어(Sampling) 나머지 저품질 답변은 기각(Rejection)하는 반복 개선 방식입니다.",
    "hint": "거절, 기각의 의미입니다.",
    "trap_points": [
      "오픈 모델들의 성능을 끌어올릴 때 필수적인 단계임"
    ],
    "difficulty": "hard",
    "id": "0622"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝된 모델의 성능 평가 시, 훈련 데이터에 없는 새로운 질문에 대해 얼마나 잘 답하는지를 중점적으로 보는 이유는?",
    "options": [
      "훈련 데이터가 가짜라서",
      "모델의 '일반화(Generalization)' 능력을 검증하기 위해",
      "문법 오타를 찾기 위해",
      "답변의 미사여구를 평가하기 위해",
      "데이터 소유권을 확인하기 위해"
    ],
    "answer": "모델의 '일반화(Generalization)' 능력을 검증하기 위해",
    "why": "단순 암기가 아닌 실제 지적 능력을 습득했는지 확인하기 위함입니다.",
    "hint": "보편적으로 잘하는 능력을 생각하세요.",
    "trap_points": [
      "과적합된 모델은 이 단계에서 실패함"
    ],
    "difficulty": "medium",
    "id": "0623"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "고성능 모델의 출력값을 정답(Label)으로 삼아 더 작은 모델을 학습시키는 과정을 무엇이라 하나요?",
    "answer": "Knowledge Distillation (지식 증류)",
    "why": "거대 모델의 지능을 효과적으로 압축하여 효율적인 소형 모델을 만드는 전략입니다.",
    "hint": "지식을 증류(Distillation)함.",
    "trap_points": [
      "합성 데이터 학습도 이 범주에 포함됨"
    ],
    "difficulty": "medium",
    "id": "0624"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 후 안전 필터링이 너무 강해져서 정상적인 질문에도 '답변할 수 없습니다'라고 거부하는 현상을 무엇이라 하나요?",
    "options": [
      "Under-refusal",
      "Over-refusal (과도한 거부)",
      "Correct Alignment",
      "Successful Tuning",
      "Model Collapse"
    ],
    "answer": "Over-refusal (과도한 거부)",
    "why": "안전 정렬(Safety Alignment)이 과하게 적용되어 모델이 무해한 질문까지 위험하다고 판단하는 부작용입니다.",
    "hint": "너무 많이(Over) 거절(Refusal)함.",
    "trap_points": [
      "가동성(Helpfulness)과 안전성(Safety)의 균형 잡기가 어려움"
    ],
    "difficulty": "medium",
    "id": "0625"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "사전 학습된 모델에 아무런 레이블 없는 도메인 문서들을 그대로 더 학습시켜 '지식의 토양'을 다지는 과정은?",
    "options": [
      "SFT",
      "Continuous Pre-training (CPT)",
      "RLHF",
      "DPO",
      "Distillation"
    ],
    "answer": "Continuous Pre-training (CPT)",
    "why": "기존 지식 위에 새로운 분야의 텍스트 덩어리를 부어 넣어 모델 자체가 해당 도메인의 언어 패턴을 익히게 하는 것입니다.",
    "hint": "학습을 '지속(Continuous)'한다는 뜻입니다.",
    "trap_points": [
      "가장 많은 데이터와 GPU 자원이 필요한 단계임"
    ],
    "difficulty": "hard",
    "id": "0626"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "질문과 정답이 명시된 데이터셋으로 모델을 직접 지도 학습시키는 단계는?",
    "options": [
      "SFT (Supervised Fine-tuning)",
      "Rewarding",
      "Prompting",
      "Sampling",
      "Normalization"
    ],
    "answer": "SFT (Supervised Fine-tuning)",
    "why": "지도 학습(Supervised)을 통해 모델이 특정 질문에 대답하는 방식을 배우게 합니다.",
    "hint": "감독/지도하에 학습시킨다는 약자입니다.",
    "trap_points": [
      "Instruction Tuning은 SFT의 한 종류임"
    ],
    "difficulty": "medium",
    "id": "0627"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "LoRA 기법에서 'Rank(r)' 값이 커질 때 나타나는 일반적인 특징은?",
    "options": [
      "학습해야 할 파라미터가 줄어든다.",
      "모델 성능이 무조건 나빠진다.",
      "학습 가능한 변수가 늘어나 복잡한 패턴을 더 잘 학습할 수 있지만 메모리 사용량도 늘어난다.",
      "속도가 훨씬 빨라진다.",
      "양자화 비트 수가 늘어난다."
    ],
    "answer": "학습 가능한 변수가 늘어나 복잡한 패턴을 더 잘 학습할 수 있지만 메모리 사용량도 늘어난다.",
    "why": "Rank는 어댑터 행렬의 크기를 결정하며, 클수록 표현력은 좋아지나 효율성은 감소합니다.",
    "hint": "행렬의 크기, 차원(Rank)을 생각하세요.",
    "trap_points": [
      "보통 8, 16, 32 정도를 사용하며 너무 크면 Full FT와 차이가 없어짐"
    ],
    "difficulty": "hard",
    "id": "0628"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "모델이 실패한 대화에서 '왜 실패했는지'를 분석하고 스스로 수정하여 다음 시도에 반영하는 기법은?",
    "options": [
      "SFT",
      "Reflexion (또는 Self-Reflection)",
      "DPO",
      "LoRA",
      "CPT"
    ],
    "answer": "Reflexion (또는 Self-Reflection)",
    "why": "스스로 반성(Reflection)하는 과정을 통해 에이전트의 정답률을 지속적으로 개선하는 고성능 기법입니다.",
    "hint": "반성, 성찰이라는 뜻입니다.",
    "trap_points": [
      "학습 시뿐만 아니라 추론(Inference) 시에도 에이전트가 사용할 수 있음"
    ],
    "difficulty": "medium",
    "id": "0629"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 시 이전의 중요한 정보를 잊어버리는 것을 방지하기 위해 사용하는 원본 모델과 파인튜닝 모델 사이의 통계적 거리 제한 기술은?",
    "answer": "KL Divergence (KL 발산)",
    "why": "PPO 등 강화학습에서 모델이 너무 크게 변해버려(붕괴) 원래의 언어 능력을 잃는 것을 막는 '안전 장치' 역할을 합니다.",
    "hint": "통계에서 두 분포 간의 차이를 측정하는 용어입니다.",
    "trap_points": [
      "KL 거리가 너무 크면 모델이 헛소리를 할 확률이 올라감"
    ],
    "difficulty": "hard",
    "id": "0630"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "인간의 피드백 없이도 코딩 문제의 '성공 여부'처럼 명확히 실현 가능한 보상을 통해 학습하는 방식은?",
    "options": [
      "RLHF",
      "RLVR (Reinforcement Learning with Verifiable Reward)",
      "DPO",
      "SFT",
      "Pre-training"
    ],
    "answer": "RLVR (Reinforcement Learning with Verifiable Reward)",
    "why": "실행 결과가 0(실패) 아니면 1(성공)로 명확한 경우, 보상 모델 없이 직접 강화학습을 수행하여 추론 능력을 극대화합니다.",
    "hint": "검증 가능한(Verifiable) 보상에 주목하세요.",
    "trap_points": [
      "DeepSeek-R1 등 최신 추론 모델의 핵심 비결 중 하나임"
    ],
    "difficulty": "hard",
    "id": "0631"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝용 데이터셋을 만들기 위해 고성능 모델(예: GPT-4)을 사용하여 질문-답변 쌍을 대량으로 생성하는 것을 무엇이라 하나요?",
    "options": [
      "Data Scraping",
      "Synthetic Data Generation (합성 데이터 생성)",
      "Labeling",
      "Cleaning",
      "Scaling"
    ],
    "answer": "Synthetic Data Generation (합성 데이터 생성)",
    "why": "사람이 일일이 만드는 비용을 줄이고 고성능 모델의 능력을 학습 모델에 전이(Knowledge Distillation)하기 위해 널리 쓰입니다.",
    "hint": "진짜가 아닌 '합성된(Synthetic)' 데이터입니다.",
    "trap_points": [
      "모델이 만든 데이터만 쓰다 보면 성능이 열화될 수도 있으니 주의해야 함"
    ],
    "difficulty": "medium",
    "id": "0632"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "여러 도메인(수학, 번역, 법률 등)의 LoRA 어댑터를 하나의 모델에 필요에 따라 갈아 끼우며 사용하는 기술은?",
    "options": [
      "Single-LoRA",
      "Multi-LoRA",
      "Universal Tuning",
      "Dynamic Model",
      "Switching Model"
    ],
    "answer": "Multi-LoRA",
    "why": "원본 모델(Base)은 하나로 유지하고 상황에 맞는 가벼운 어댑터만 교체하므로 저장 공간과 서빙 효율이 극대화됩니다.",
    "hint": "여러 개(Multi)의 어댑터입니다.",
    "trap_points": [
      "Peft 라이브러리를 통해 쉽게 구현 가능함"
    ],
    "difficulty": "medium",
    "id": "0633"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "RLHF 방식에서 모델이 보상을 높게 받는 법만 터득하여, 보상 모델의 허점을 파고들어 엉뚱한 답을 내는 현상은?",
    "answer": "Reward Hacking (보상 해킹)",
    "why": "보상 모델이 완벽하지 않기 때문에, 모델이 실제 의도와는 무관하게 '점수만 잘 받는 꼼수'를 학습하는 부작용입니다.",
    "hint": "보상을 해킹한다(Hacking)는 뜻입니다.",
    "trap_points": [
      "KL Divergence를 통해 이를 억제할 수 있음"
    ],
    "difficulty": "hard",
    "id": "0634"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "RAG을 기반으로 답변하는 '방식' 자체를 모델에 학습시켜 할루시네이션을 줄이는 기법은?",
    "options": [
      "RAFT (Retrieval-Augmented Fine-Tuning)",
      "CPT",
      "DPO",
      "GRPO",
      "IT"
    ],
    "answer": "RAFT (Retrieval-Augmented Fine-Tuning)",
    "why": "모델에게 '질문과 문서를 줄 테니 반드시 문서에 근거하여 CoT로 풀어라'라는 형식을 학습시키는 방식입니다.",
    "hint": "뗏목(Raft)과 발음이 같으며 RAG가 섞인 약자입니다.",
    "trap_points": [
      "오픈 북 시험 공부법과 유사한 원리임"
    ],
    "difficulty": "hard",
    "id": "0635"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝의 대표적인 방법인 LoRA에서 ‘Alpha’ 값이 조절하는 것은?",
    "options": [
      "모델의 층 개수",
      "어댑터 가중치가 원본 가중치에 미치는 영향력(Scaling)",
      "데이터의 양",
      "학습 속도",
      "비용"
    ],
    "answer": "어댑터 가중치가 원본 가중치에 미치는 영향력(Scaling)",
    "why": "학습된 델타 가중치에 alpha / rank 값을 곱하여 최종 가중치에 반영하는 정도를 튜닝합니다.",
    "hint": "스케일링(Scaling) 계수입니다.",
    "trap_points": [
      "보통 rank와 같거나 2배 정도로 설정함"
    ],
    "difficulty": "hard",
    "id": "0636"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝된 모델의 성능 평가 지표인 'Perplexity (퍼플렉서티)'가 의미하는 것은?",
    "options": [
      "모델의 답변 속도",
      "모델이 다음 단어를 예측할 때 느끼는 '당혹감'(낮을수록 모델이 확신을 갖고 정답에 가깝다고 판단함)",
      "모델의 파일 크기",
      "모델의 가독성",
      "모델의 가격"
    ],
    "answer": "모델이 다음 단어를 예측할 때 느끼는 '당혹감'(낮을수록 모델이 확신을 갖고 정답에 가깝다고 판단함)",
    "why": "언어 모델이 주어진 문장에 대해 얼마나 헷갈리고 있는지를 수치화한 지표입니다.",
    "hint": "당혹감을 뜻하는 영어 단어입니다.",
    "trap_points": [
      "낮을수록 좋은 언어 모델이지만 실제 정답 정밀도와는 다를 수 있음"
    ],
    "difficulty": "hard",
    "id": "0637"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 시 ‘치명적 망각(Catastrophic Forgetting)’이 발생하는 시점은?",
    "options": [
      "데이터를 아예 안 넣었을 때",
      "특정 데이터셋으로 너무 반복해서 학습하여 모델이 가진 기존의 보편적 지능이 무너졌을 때",
      "모델 용량이 너무 클 때",
      "학습 속도가 너무 늦을 때",
      "영어로만 코딩할 때"
    ],
    "answer": "특정 데이터셋으로 너무 반복해서 학습하여 모델이 가진 기존의 보편적 지능이 무너졌을 때",
    "why": "새로운 지식에 파라미터가 과하게 적응하면서 기존의 중요한 뉴런 정보들이 상실되기 때문입니다.",
    "hint": "비극적인 잊어버림 현상입니다.",
    "trap_points": [
      "이를 방지하기 위해 KL divergence 제약을 둠"
    ],
    "difficulty": "medium",
    "id": "0638"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "모델의 가중치를 파괴하지 않고, 두 개 이상의 서로 다른 파인튜닝된 모델 가중치를 합쳐서 시너지를 내는 기술은?",
    "options": [
      "Model Merging (모델 머징)",
      "Model Distillation",
      "Model Scaling",
      "Model Quantization",
      "Model Backing"
    ],
    "answer": "Model Merging (모델 머징)",
    "why": "추가 학습 없이 가중치들을 가중 평균하거나 특수 알고리즘(SLERP 등)으로 합쳐서 종합 성능을 올립니다.",
    "hint": "합치다라는 뜻입니다.",
    "trap_points": [
      "허깅페이스 오픈 모델 랭킹 상위권은 대부분 머징된 모델들임"
    ],
    "difficulty": "medium",
    "id": "0639"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "학습 데이터가 부족할 때, 데이터의 내용을 약간씩 변형(순서 변경, 유의어 교체 등)하여 양을 늘리는 기법은?",
    "answer": "Data Augmentation (데이터 증강)",
    "why": "부족한 샘플 수를 인위적으로 확장하여 모델의 일반화 성능을 높입니다.",
    "hint": "증강(Augment)하다.",
    "trap_points": [
      "기존 의미가 훼손되지 않는 선에서 변형해야 함"
    ],
    "difficulty": "medium",
    "id": "0640"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "학습 모델이 너무 비대하여 실시간 서비스가 불가능할 때, 성능손실을 최소화하며 크기를 줄이는 전략이 아닌 것은?",
    "options": [
      "Quantization (양자화)",
      "Pruning (가지치기)",
      "Distillation (증류)",
      "Parameter Scaling (파라미터 증설)",
      "Model Merging 후 최적화"
    ],
    "answer": "Parameter Scaling (파라미터 증설)",
    "why": "파라미터 증설은 모델을 더 키우는 것이므로 효율화 목적과는 정반대됩니다.",
    "hint": "크기를 '줄이는' 것이 아닌 것을 찾으세요.",
    "trap_points": [
      "최근에는 4비트 양자화가 가장 효율적인 대안임"
    ],
    "difficulty": "easy",
    "id": "0641"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "DeepSeek R1 아키텍처에서 '보상 모델'의 자리를 대신하며 강화학습 효율을 극대화한 'Rule-based verifier'의 특징은?",
    "options": [
      "사람이 직접 채점한다.",
      "수학 정답이나 코드 컴파일 결과처럼 명확한 '규칙'으로 보상을 준다.",
      "운으로 결정한다.",
      "영어로만 채점한다.",
      "과거 데이터를 무시한다."
    ],
    "answer": "수학 정답이나 코드 컴파일 결과처럼 명확한 '규칙'으로 보상을 준다.",
    "why": "보상 모델 자체가 가진 할루시네이션(점수 잘못 주기) 위험을 배제하고 수학적 진리만으로 모델을 연마시킵니다.",
    "hint": "검증기(Verifier) 기반의 규칙입니다.",
    "trap_points": [
      "추론 성능을 비약적으로 올린 핵심 비결임"
    ],
    "difficulty": "hard",
    "id": "0642"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 데이터셋 구축 시 ‘입력 프롬프트’와 ‘정답’ 사이에 <thought> 태그를 넣어 생각하는 과정을 보여주는 기법은?",
    "options": [
      "Chain of Thought Tuning",
      "CoT-SFT",
      "Direct Answer Tuning",
      "Implicit Tuning",
      "Hidden Tuning"
    ],
    "answer": "CoT-SFT",
    "why": "논리적 추론 과정을 직접 지도 학습 데이터에 포함시켜 모델이 '생각하는 습관'을 갖게 만듭니다.",
    "hint": "생각의 사슬(CoT)을 활용한 지도 학습(SFT).",
    "trap_points": [
      "모델의 추론 성능이 극대화되는 기반이 됨"
    ],
    "difficulty": "hard",
    "id": "0643"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝된 모델이 특정 주제에 대해 답변을 거부하게 만들거나, 특정 정치색을 띠지 않게 정렬하는 최종적이고 미세한 단계를 무엇이라 하나요?",
    "answer": "Safety Alignment (안전 정렬)",
    "why": "윤리 규정, 안전 지침 등을 모델에 내면화시키는 과정입니다.",
    "hint": "안전(Safety) + 정렬(Alignment).",
    "trap_points": [
      "사용자의 부적절한 질문에 대해 단호하게 거절하는 능력을 학습함"
    ],
    "difficulty": "medium",
    "id": "0644"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "모델 공유 사이트(Hugging Face 등)에서 모델 이름에 'GGUF'가 붙어 있다면 이는 무엇을 뜻하나요?",
    "options": [
      "가장 성능이 좋은 원본 모델이다.",
      "로컬 PC(Llama.cpp 등)나 모바일에서 돌리기 좋게 양자화된 전용 포맷이다.",
      "영어로만 된 모델이다.",
      "학습이 덜 된 모델이다.",
      "비싼 GPU가 있어야만 도는 모델이다."
    ],
    "answer": "로컬 PC(Llama.cpp 등)나 모바일에서 돌리기 좋게 양자화된 전용 포맷이다.",
    "why": "CP 추론 최적화와 메모리 점유율을 극도로 낮춘 파일 형식으로 개인 개발자들에게 가장 인기가 높습니다.",
    "hint": "GG로 시작하는 가벼운 포맷입니다.",
    "trap_points": [
      "파일 하나로 실행 가능하며 설저이 매우 간편함"
    ],
    "difficulty": "easy",
    "id": "0645"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "DPO(Direct Preference Optimization) 알고리즘이 기존 RLHF보다 혁신적인 이유는?",
    "options": [
      "돈이 더 많이 들어서",
      "별도의 보상 모델(Reward Model) 학습 없이, 선호도 답변 쌍만으로 직접 모델을 최적화할 수 있기 때문",
      "영어로만 학습해서",
      "파일 용량이 커서",
      "속도가 늦어져서"
    ],
    "answer": "별도의 보상 모델(Reward Model) 학습 없이, 선호도 답변 쌍만으로 직접 모델을 최적화할 수 있기 때문",
    "why": "리워드 모델링 단계의 복잡성과 할루시네이션 위험을 제거한 최신 정렬 기법입니다.",
    "hint": "직접(Direct) 선호도(Preference)를 최적화합니다.",
    "trap_points": [
      "현대 오픈 모델 정렬의 대세 기술임"
    ],
    "difficulty": "hard",
    "id": "0646"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 데이터셋 구축 시 ‘고품질’의 기준에 해당하지 않는 것은?",
    "options": [
      "내용의 정확성",
      "표현의 다양성",
      "형식의 일관성",
      "데이터의 단순 무작위 대량 복사",
      "중복 제거 여부"
    ],
    "answer": "데이터의 단순 무작위 대량 복사",
    "why": "단순 복사는 모델의 편향만 강화할 뿐 실제 지능 향상에는 도움이 되지 않습니다.",
    "hint": "질보다 양을 추구하는 행위를 찾으세요.",
    "trap_points": [
      "무조건 많은 게 아니라 정제된(Curated) 것이 최고임"
    ],
    "difficulty": "easy",
    "id": "0647"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "모델의 가중치를 4비트나 8비트 정수로 변환하여 용량을 획기적으로 줄이는 기술의 이름은?",
    "options": [
      "Quantization (양자화)",
      "Distillation",
      "Pruning",
      "Backprop",
      "Gradient Descent"
    ],
    "answer": "Quantization (양자화)",
    "why": "정보의 정밀도를 아주 조금 희생하는 대신 메모리 사용량을 1/4~1/8로 줄여 일반 컴퓨터에서도 LLM 구동을 가능하게 합니다.",
    "hint": "양자(Quantum) 단위로 쪼개 수치화합니다.",
    "trap_points": [
      "압축 과정에서 다소의 성능 하락이 발생할 수 있음"
    ],
    "difficulty": "medium",
    "id": "0648"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 학습 시 'Epoch (에포크)'가 의미하는 것은?",
    "options": [
      "모델의 크기",
      "전체 학습 데이터를 한 바퀴 모두 훑는 주기",
      "파일이 저장되는 경로",
      "인터넷 연결 상태",
      "학습 모델의 이름"
    ],
    "answer": "전체 학습 데이터를 한 바퀴 모두 훑는 주기",
    "why": "데이터셋 전체를 몇 번 반복해서 학습(학습 횟수)할지의 기준이 됩니다.",
    "hint": "시대를 뜻하는 단어이지만 학습에서는 한 주기를 뜻합니다.",
    "trap_points": [
      "너무 많이 돌리면 과적합(Overfitting)이 발생함"
    ],
    "difficulty": "easy",
    "id": "0649"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "LoRA 학습에서 원본 모델의 가중치를 전혀 건드리지 않고 ‘얼려두는’ 것을 무엇이라 하나요?",
    "answer": "Freezing (동결)",
    "why": "메모리를 아끼고 기존 지식을 보존하기 위해 베이스 모델의 파라미터 업데이트를 막습니다.",
    "hint": "얼리다라는 뜻입니다.",
    "trap_points": [
      "이 덕분에 아주 적은 양의 GPU로도 학습이 가능해짐"
    ],
    "difficulty": "medium",
    "id": "0650"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "고해상도 이미지와 텍스트를 동시에 이해하고 생성하는 모델을 만들기 위한 파인튜닝은?",
    "options": [
      "Multi-modal Tuning",
      "Direct Tuning",
      "Style Tuning",
      "Negative Tuning",
      "Single Tuning"
    ],
    "answer": "Multi-modal Tuning",
    "why": "여러 양식(Mode)의 데이터를 하나로 엮어 통합 지능을 기르는 과정입니다.",
    "hint": "여러 개(Multi)의 양식(Modal)입니다.",
    "trap_points": [
      "최신 파인튜닝 트렌드 중 하나임"
    ],
    "difficulty": "medium",
    "id": "0651"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 후 모델의 'Perplexity' 값이 비정상적으로 높아졌다면 이는 무엇을 의미하나요?",
    "options": [
      "모델이 천재가 되었다.",
      "모델이 다음 단어를 전혀 예측하지 못하고 매우 혼란스러워하며 성능이 망가졌다.",
      "파일 속도가 무지 빠르다.",
      "영어로만 답한다.",
      "성능이 최고로 좋아졌다."
    ],
    "answer": "모델이 다음 단어를 전혀 예측하지 못하고 매우 혼란스러워하며 성능이 망가졌다.",
    "why": "퍼플렉서티는 당혹감을 뜻하며, 낮을수록 안정적인 언어 모델임을 뜻합니다.",
    "hint": "수치가 낮을수록 좋은 지표입니다.",
    "trap_points": [
      "학습률이 너무 높을 때 발생하기 쉬운 증상임"
    ],
    "difficulty": "hard",
    "id": "0652"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝된 모델 배포 도구 중 하나로, 로컬 환경에서 명령어 한 줄로 모델을 실행하는 가장 유명한 도구는?",
    "options": [
      "Ollama",
      "vLLM",
      "Docker",
      "S3",
      "Kubernetes"
    ],
    "answer": "Ollama",
    "why": "맥/윈도우/리눅스에서 매우 간편하게 오픈 소스 모델을 구동할 수 있어 인기가 높습니다.",
    "hint": "요리용 '기름(Oil)'과 '라마(Llama)'의 합성어 같은 이름입니다.",
    "trap_points": [
      "로컬 개발자들에게는 사실상의 표준임"
    ],
    "difficulty": "easy",
    "id": "0653"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 시 모델이 학습 데이터 속에 숨겨진 개인정보(이메일, 주소 등)를 암기해버리는 문제를 무엇이라 하나요?",
    "answer": "Data Leakage (데이터 유출) 또는 Privacy Memorization",
    "why": "민감 정보가 모델 파라미터에 각인되어 대화 중 외부에 노출될 위험이 있습니다.",
    "hint": "데이터가 샌다(Leak)는 뜻입니다.",
    "trap_points": [
      "이를 막기 위해 비식별화 처리가 필수적임"
    ],
    "difficulty": "medium",
    "id": "0654"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "모델 학습 시 ‘가중치 손실(Loss)’을 최소화하기 위해 경사면을 따라 내려가는 기본 알고리즘은?",
    "options": [
      "Gradient Descent (경사 하강법)",
      "Random Selection",
      "Quick Sort",
      "Binary Search",
      "Hash Logic"
    ],
    "answer": "Gradient Descent (경사 하강법)",
    "why": "손실 함수의 기울기(Gradient) 반대 방향으로 가중치를 조금씩 이동시켜 오차를 줄여나가는 딥러닝의 심장입니다.",
    "hint": "경사(Gradient)를 내려간다(Descent)는 뜻입니다.",
    "trap_points": [
      "최신 모델은 이의 발전형인 Adam, AdamW 등을 주로 사용함"
    ],
    "difficulty": "medium",
    "id": "0655"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "DeepSeek-R1-Zero처럼 별도의 SFT 없이 규칙 기반 보상만으로 추론 능력을 학습시키는 강화학습 알고리즘은?",
    "options": [
      "PPO",
      "DPO",
      "GRPO",
      "SFT",
      "RAFT"
    ],
    "answer": "GRPO",
    "why": "GRPO는 그룹 내 상대적 보상을 통해 학습하며, DeepSeek R1의 추론 성능 극대화에 핵심적인 역할을 했습니다.",
    "hint": "G로 시작하는 4글자 알고리즘입니다.",
    "trap_points": [
      "PPO와 달리 별도의 가치 모델(Value Model)이 필요 없어 메모리가 절약됨"
    ],
    "difficulty": "hard",
    "id": "0656"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "모델의 가중치 비트 수를 낮추어(예: 16bit -> 4bit) 모델 크기를 줄이는 기술은?",
    "answer": "Quantization (양자화)",
    "why": "양자화는 메모리와 연산량을 획기적으로 줄여 일반 PC나 모바일에서도 거대 모델을 실행할 수 있게 합니다.",
    "hint": "Q로 시작하는 전문 용어입니다.",
    "trap_points": [
      "파인튜닝과는 별개의 최적화 단계임"
    ],
    "difficulty": "easy",
    "id": "0657"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "LoRA 파인튜닝 시 학습 데이터에 과하게 적응하여 범용 능력이 떨어지는 현상은?",
    "options": [
      "Regularization",
      "Overfitting (과적합)",
      "Underfitting",
      "Normalization",
      "Quantization"
    ],
    "answer": "Overfitting (과적합)",
    "why": "특정 데이터셋의 패턴만 완벽히 외워버려 새로운 질문이나 다른 도메인에 대한 대응력이 상상히 저하되는 현상입니다.",
    "hint": "너무(Over) 딱 맞게(Fitting) 된 상황입니다.",
    "trap_points": [
      "망각(Forgetting)과는 구분되는 개념임"
    ],
    "difficulty": "medium",
    "id": "0658"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "오픈 모델(Llama 등)을 CPU만 있는 환경이나 메모리가 부족한 Mac에서 효율적으로 실행하기 위해 주로 사용하는 포맷은?",
    "options": [
      "PyTorch (.pt)",
      "TensorFlow (.pb)",
      "GGUF",
      "ONNX",
      "Safetensors"
    ],
    "answer": "GGUF",
    "why": "GGUF(llama.cpp 계열)는 양자화된 가중치를 담은 단일 파일 포맷으로, CPU 추론 최적화와 함께 상용 LLM 도구(Ollama 등)에서 널리 쓰입니다.",
    "hint": "GG로 시작하는 4글자 포맷입니다.",
    "trap_points": [
      "Safetensors는 보안에 안전한 가중치 저장 방식이지만 CPU 최적화 포맷은 아님"
    ],
    "difficulty": "medium",
    "id": "0659"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "사전 학습된 베이스 모델의 능력을 그대로 유지하면서, 특정 '도메인(예: 법률, 의료)'의 지식만을 얇게 덧씌우는 PEFT의 이점은?",
    "options": [
      "학습 속도가 느려진다.",
      "모델 파라미터 전체를 학습할 때보다 GPU 메모리를 획기적으로 아끼고 빠른 튜닝이 가능하다.",
      "성능이 무조건 떨어진다.",
      "데이터가 많이 필요하다.",
      "인터넷 연결이 필수다."
    ],
    "answer": "모델 파라미터 전체를 학습할 때보다 GPU 메모리를 획기적으로 아끼고 빠른 튜닝이 가능하다.",
    "why": "가중치의 아주 일부(<1%)만 업데이트하므로 하드웨어 진입 장벽이 낮습니다.",
    "hint": "메모리와 비용의 효율성을 생각하세요.",
    "trap_points": [
      "LoRA가 대표적인 예시임"
    ],
    "difficulty": "easy",
    "id": "0660"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "RLHF 방식에서 아첨(Sycophancy) 부작용이란 무엇을 의미하나요?",
    "options": [
      "모델이 화를 내는 현상",
      "모델이 사용자에게 무조건 동조하거나 비위를 맞추는 답변만 내놓아 객관성이 떨어지는 현상",
      "답변의 글자 수가 많아지는 현상",
      "영어로만 답하는 현상",
      "인터넷 데이터를 지우는 현상"
    ],
    "answer": "모델이 사용자에게 무조건 동조하거나 비위를 맞추는 답변만 내놓아 객관성이 떨어지는 현상",
    "why": "사람이 매긴 선호도(Reward)가 '듣기 좋은 말'에 편향되어 있을 경우 모델이 이를 학습하게 됩니다.",
    "hint": "남의 비위를 맞춘다는 뜻의 어려운 단어입니다.",
    "trap_points": [
      "모델의 비판적 사고가 저해될 수 있음"
    ],
    "difficulty": "hard",
    "id": "0661"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "DeepSeek-R1의 성공 요인 중 하나인 'GRPO'는 무엇의 약자인가요?",
    "options": [
      "Grand Reward Policy Optimization",
      "Group Relative Policy Optimization",
      "General Reason Policy Output",
      "Grid Relation Point Object",
      "Gradient Reset Policy Option"
    ],
    "answer": "Group Relative Policy Optimization",
    "why": "그룹 내의 상대적인 보상을 비교하여 학습하는 강화학습 알고리즘으로 별도의 가치 모델이 필요 없습니다.",
    "hint": "그룹(Group) 상대적(Relative)인 정책 최적화입니다.",
    "trap_points": [
      "DeepSeek에서 제안하여 화제가 됨"
    ],
    "difficulty": "hard",
    "id": "0662"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "양자화 기법 중 0을 기준으로 대칭적인 분포를 사용하여 4비트 정밀도에서도 성능을 잘 유지하는 QLoRA의 핵심 포맷은?",
    "options": [
      "float32",
      "int8",
      "NF4 (Normal Float 4)",
      "BF16",
      "FP8"
    ],
    "answer": "NF4 (Normal Float 4)",
    "why": "정보의 손실을 방지하기 위해 가중치의 통계적 분포에 최적화된 비트 할당 방식입니다.",
    "hint": "Normal(정상 분포) + Float 4.",
    "trap_points": [
      "비트 수는 같아도 일반 int4보다 보존력이 높음"
    ],
    "difficulty": "hard",
    "id": "0663"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "고성능 교사 모델(Teacher)의 출력을 학생 모델(Student)이 따라 하게 하여 지식을 전수하는 기법은?",
    "answer": "Knowledge Distillation (지식 증류)",
    "why": "큰 모델의 확률 분포를 작은 모델이 학습하여 '가벼운 고성능 모델'을 만듭니다.",
    "hint": "지식을 증류(Distillation)함.",
    "trap_points": [
      "sLLM들의 비약적 발전의 숨은 공신임"
    ],
    "difficulty": "medium",
    "id": "0664"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 데이터셋 구축 시 '품질이 나쁜 데이터'가 섞여 있을 때 발생하는 가장 고질적인 문제는?",
    "options": [
      "모델의 크기가 커진다.",
      "모델의 출력 품질이 오염되어 횡설수설하거나 틀린 답을 확신 있게 말하게 된다.",
      "학습 속도가 빨라진다.",
      "영어로만 답한다.",
      "UI가 깨진다."
    ],
    "answer": "모델의 출력 품질이 오염되어 횡설수설하거나 틀린 답을 확신 있게 말하게 된다.",
    "why": "쓰레기가 들어가면 쓰레기가 나온다는 GIGO(Garbage In Garbage Out) 원칙은 LLM에서도 매우 강력합니다.",
    "hint": "입력 데이터의 수준이 출력 수준을 결정합니다.",
    "trap_points": [
      "무조건 양이 많은 것보다 소수의 고품질 데이터가 훨씬 나음"
    ],
    "difficulty": "easy",
    "id": "0665"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "강화학습에서 모델이 지나치게 변형되는 것을 막기 위해 '원본 모델의 확률 분포'와의 차이를 계산하는 손실 함수는?",
    "options": [
      "MSE Loss",
      "Cross Entropy",
      "KL Divergence (KL 발산)",
      "Huber Loss",
      "L1 Loss"
    ],
    "answer": "KL Divergence (KL 발산)",
    "why": "모델이 보상만 쫓다가 원래의 언어적 기초를 망가뜨리는 것을 방지하는 제약(Constraint) 장치입니다.",
    "hint": "통계적 거리 차이를 재는 용어입니다.",
    "trap_points": [
      "KL 값이 너무 크면 모델이 붕괴(Collapse)될 수 있음"
    ],
    "difficulty": "hard",
    "id": "0666"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 시 학습 데이터에 'Thinking(추론 과정)'을 명시적으로 포함해 교수하는 방식은?",
    "options": [
      "Zero-shot Tuning",
      "Reasoning SFT",
      "Direct Tuning",
      "Style Tuning",
      "Format Tuning"
    ],
    "answer": "Reasoning SFT",
    "why": "단순히 '질문-정답'만 주지 않고 '질문-생각과정-정답'을 함께 학습시켜 논리 구조를 각인시킵니다.",
    "hint": "추론(Reasoning) 과정을 담은 지도 학습(SFT)입니다.",
    "trap_points": [
      "DeepSeek R1-Distill 모델들이 이 방식으로 만들어짐"
    ],
    "difficulty": "hard",
    "id": "0667"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 시 특정 데이터를 완전히 잊어버리게 하거나 개인정보를 지우는 기술적 과정은?",
    "answer": "Machine Unlearning (기계 언러닝)",
    "why": "이미 학습된 모델의 가중치에서 특정 지식의 영향력을 제거하는 고도로 어려운 작업입니다.",
    "hint": "배운 것을 잊게 만드는 과정입니다.",
    "trap_points": [
      "단순히 거절 프롬프트를 넣는 것과는 차원이 다른 기술임"
    ],
    "difficulty": "hard",
    "id": "0668"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "모델 배포 효율을 위해 중복되거나 중요도가 낮은 뉴런(가중치)을 아예 제거해 버리는 최적화 기술은?",
    "options": [
      "Quantization",
      "Pruning (가지치기)",
      "Distillation",
      "Merging",
      "Clipping"
    ],
    "answer": "Pruning (가지치기)",
    "why": "연산 그래프에서 불필요한 연결을 끊어내어 속도를 높이고 용량을 줄이는 기법입니다.",
    "hint": "나뭇가지를 친다는 뜻입니다.",
    "trap_points": [
      "과도하게 하면 성능이 급락할 수 있음"
    ],
    "difficulty": "hard",
    "id": "0669"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "모델의 지능 자체를 높이기보다 '특정한 말투나 업무 포맷(예: 마크다운 보고서 형식)'을 우선적으로 각인시키기 위해 하는 튜닝은?",
    "options": [
      "Pre-training",
      "Style/Instruction Fine-tuning",
      "Quantization",
      "Normalization",
      "Distillation"
    ],
    "answer": "Style/Instruction Fine-tuning",
    "why": "SFT를 통해 답변의 외형적 피처(Feature)를 사용자의 입맛에 맞게 고정합니다.",
    "hint": "스타일과 형식에 집중하는 튜닝입니다.",
    "trap_points": [
      "지식 주입보다 '행동 교정'에 더 큰 효과가 있음"
    ],
    "difficulty": "medium",
    "id": "0670"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "DPO 파인튜닝 시 필요한 데이터의 가장 기본적인 최소 단위(Row) 구성은?",
    "options": [
      "질문 하나와 답변 하나",
      "질문 하나와 (좋은 답변, 나쁜 답변) 한 쌍",
      "문서 덩어리 하나",
      "단어 리스트",
      "유저 평점"
    ],
    "answer": "질문 하나와 (좋은 답변, 나쁜 답변) 한 쌍",
    "why": "DPO는 두 가지 답변 중 어느 것을 더 선호하는지 직접 비교하며 확률 분포를 조정하기 때문입니다.",
    "hint": "선호도(Preference)를 알려주기 위해 필요한 최소 비교 대상 수는?",
    "trap_points": [
      "좋은 것만 가르치는 SFT보다 오답의 확률을 직접 낮출 수 있어 강력함"
    ],
    "difficulty": "medium",
    "id": "0671"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "추론 모델(DeepSeek R1 등) 학습 시, '풀이 과정이 틀려도 최종 정답만 맞으면 보상을 주는' 강화학습 방식이 가능한 이유는?",
    "options": [
      "모델이 천재라서",
      "정답이 명확한(수학, 코딩 등) 문제의 경우 Rule-based로 자동 검증이 가능하기 때문",
      "사람이 24시간 감시해서",
      "데이터가 윈도우 기반이라서",
      "모델이 스스로 정답을 알기 때문"
    ],
    "answer": "정답이 명확한(수학, 코딩 등) 문제의 경우 Rule-based로 자동 검증이 가능하기 때문",
    "why": "검증 가능한 보상(Verifiable Reward)이 있으면 보상 모델의 주관성 없이도 강력하게 모델을 몰아붙일 수 있습니다.",
    "hint": "채점하기 쉬운 과목(수학/코딩)을 생각하세요.",
    "trap_points": [
      "인문학적 질문에는 이 방식을 적용하기 어려움"
    ],
    "difficulty": "hard",
    "id": "0672"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝된 모델을 배포할 때, 메모리 용량을 줄이면서도 성능 저하를 최소화하기 위해 '중요한 파라미터는 덜 자르고 안 중요한 것만 많이 자르는' 방식은?",
    "options": [
      "Uniform Quantization",
      "Importance-based Quantization (예: GPTQ, AWQ)",
      "Linear Pruning",
      "Dropout",
      "Early Stopping"
    ],
    "answer": "Importance-based Quantization (예: GPTQ, AWQ)",
    "why": "각 파라미터의 레이어별 중요도를 고려하여 전략적으로 양자화 비트를 할당하는 방식입니다.",
    "hint": "중요도(Importance)에 따른 차등 적용입니다.",
    "trap_points": [
      "그냥 일괄적으로 자르는 것보다 성능 보존력이 훨씬 뛰어남"
    ],
    "difficulty": "hard",
    "id": "0673"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "거대 모델의 학습을 작은 GPU 여러 대에서 나누어 수행하는 분산 학습 기술 중 하나는?",
    "answer": "DeepSpeed (또는 FSDP)",
    "why": "메모리 파편화를 줄이고 여러 GPU의 자원을 효율적으로 엮어주는 라이브러리/기술입니다.",
    "hint": "깊은 속도(Speed)라는 이름의 라이브러리입니다.",
    "trap_points": [
      "마이크로소프트에서 개발한 기술임"
    ],
    "difficulty": "hard",
    "id": "0674"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 시 'Overfitting'을 막기 위한 가장 좋은 데이터 구축 전략은?",
    "options": [
      "같은 데이터를 수만 번 반복해서 보여준다.",
      "문맥과 말투가 다양한 고품질의 데이터를 확보하고 적절한 에포크(Epoch) 수로 학습한다.",
      "데이터를 최대한 적게 넣는다.",
      "암기만 하도록 유도한다.",
      "모델의 층을 다 없앤다."
    ],
    "answer": "문맥과 말투가 다양한 고품질의 데이터를 확보하고 적절한 에포크(Epoch) 수로 학습한다.",
    "why": "다양성은 일반화 성능을 높여주고, 적절한 학습 횟수는 특정 데이터에 매몰되는 것을 막습니다.",
    "hint": "품질(Quality)과 다양성(Diversity)의 조화입니다.",
    "trap_points": [
      "데이터 양이 많다고 무조건 과적합이 안 생기는 것은 아님"
    ],
    "difficulty": "medium",
    "id": "0675"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "강화학습 기반 파인튜닝(RLHF 등)의 부작용 중 하나로, 모델이 지나치게 공손하고 방어적으로 변해 정보를 제공하지 않는 현상은?",
    "options": [
      "Helpfulness bias",
      "Safety Over-alignment",
      "Context Leaking",
      "Forgetting",
      "Token Collapse"
    ],
    "answer": "Safety Over-alignment",
    "why": "안전에 너무 치중하여 유익한 답변까지 거절해버리는 정렬의 부작용입니다.",
    "hint": "안전(Safety)이 과하게 맞춰진 상태입니다.",
    "trap_points": [
      "가동성과 안전성의 트레이드오프 관계를 보여줌"
    ],
    "difficulty": "medium",
    "id": "0676"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "데이터셋 구축 시 사용자의 '수정 전 텍스트'와 AI의 '수정 후 텍스트' 쌍을 학습시켜 교정 능력을 기르는 방식은?",
    "options": [
      "Continuous Pre-training",
      "Reconstruction Tuning",
      "Edit-based Fine-tuning",
      "Zero-shot",
      "Augmentation"
    ],
    "answer": "Edit-based Fine-tuning",
    "why": "문서 교정, 코드 디버깅 등 변환 작업에 특화된 능력을 키워주는 방식입니다.",
    "hint": "수정(Edit) 기반의 튜닝입니다.",
    "trap_points": [
      "단순 Q&A보다 정확한 목표 지점이 있는 튜닝임"
    ],
    "difficulty": "medium",
    "id": "0677"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "모델의 특정 지층(Layer)만 선택적으로 파인튜닝하여 효율을 높이는 기법은?",
    "answer": "Selective Fine-tuning",
    "why": "전체 층이 아닌, 주로 성능 변화가 큰 뒷부분의 층이나 특정 층만 선택해 가중치를 업데이트합니다.",
    "hint": "선택적(Selective)인 학습입니다.",
    "trap_points": [
      "성능은 Full FT에 가깝게 유지하면서 비용을 줄일 수 있음"
    ],
    "difficulty": "medium",
    "id": "0678"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 과정에서 모델 성능의 병목(Bottleneck)인 '역전파(Backpropagation)' 연산량을 줄이기 위해 LoRA가 채택한 방식은?",
    "options": [
      "그냥 연산을 안 한다.",
      "가중치 행렬을 두 개의 매우 작은(Low-rank) 행렬로 분해하여 해당 행렬들에 대해서만 전파한다.",
      "데이터 개수를 줄인다.",
      "모델 층을 삭제한다.",
      "영어로만 연산한다."
    ],
    "answer": "가중치 행렬을 두 개의 매우 작은(Low-rank) 행렬로 분해하여 해당 행렬들에 대해서만 전파한다.",
    "why": "큰 행렬 대신 작은 행렬 두 개(r 차원)만 학습하면 되므로 메모리와 연산량이 급감합니다.",
    "hint": "저차원(Low-rank) 분해를 생각하세요.",
    "trap_points": [
      "이 방식 덕분에 일반 사용자들도 집에서 튜닝이 가능해짐"
    ],
    "difficulty": "hard",
    "id": "0679"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "LoRA 파인튜닝 시 전체 모델 파라미터를 건드리지 않고 일부만 학습시킴으로써 얻는 이득은?",
    "options": [
      "성능이 100배 좋아진다.",
      "GPU 메모리 사용량을 획기적으로 줄여 일반 컴퓨팅 환경에서도 학습이 가능해진다.",
      "영어로만 답한다.",
      "글자 수가 늘어난다.",
      "파일이 깨진다."
    ],
    "answer": "GPU 메모리 사용량을 획기적으로 줄여 일반 컴퓨팅 환경에서도 학습이 가능해진다.",
    "why": "가중치의 아주 일부(<1%)만 업데이트하여 효율을 극대화하는 방식입니다.",
    "hint": "비용과 하드웨어 효율성입니다.",
    "trap_points": [
      "최신 기업용 서드파티 엔진(vLLM 등)에서 LoRA 어댑터를 실시간으로 교체 가능함"
    ],
    "difficulty": "medium",
    "id": "0680"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 학습 중 데이터셋의 답변 형식이 일정하지 않(예: JSON이었다가 텍스트였다가)을 때 발생하는 부작용은?",
    "options": [
      "모델이 화를 낸다.",
      "모델이 정해진 형식을 지키지 못하고 답변이 일관성을 잃게 된다.",
      "속도가 빨라진다.",
      "영어로만 답한다.",
      "비용이 줄어든다."
    ],
    "answer": "모델이 정해진 형식을 지키지 못하고 답변이 일관성을 잃게 된다.",
    "why": "모델은 데이터의 '형식적 패턴'도 학습하기 때문에, 오염된 형식 데이터는 결과 품질을 망칩니다.",
    "hint": "일관성(Consistency)의 상실입니다.",
    "trap_points": [
      "데이터 정제(Cleaning) 단계에서 형식을 통일하는 것이 필수임"
    ],
    "difficulty": "easy",
    "id": "0681"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 시 모델의 '안전(Safety)'을 위해 부적절한 답변을 거부하도록 훈련하는 기법은?",
    "options": [
      "Safety Alignment (안전 정렬)",
      "Speed Alignment",
      "Color Alignment",
      "File Alignment",
      "Random Alignment"
    ],
    "answer": "Safety Alignment (안전 정렬)",
    "why": "인간의 윤리적 기준에 맞춰 범죄나 차별적 발언을 하지 않도록 모델을 길들이는 과정입니다.",
    "hint": "안전(Safety)을 위한 조율입니다.",
    "trap_points": [
      "RLHF 단계에서 주로 수행됨"
    ],
    "difficulty": "easy",
    "id": "0682"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 시 'Overfitting'을 방지하기 위한 기술 중 하나로, 학습 데이터의 일부를 따로 떼어 '실력 검증'용으로만 쓰는 것은?",
    "options": [
      "Train set",
      "Test set",
      "Validation set (검증셋)",
      "Label set",
      "Index set"
    ],
    "answer": "Validation set (검증셋)",
    "why": "학습에 직접 쓰지 않고 중간중간 평가해봄으로써 모델이 암기 중인지 이해 중인지 확인합니다.",
    "hint": "검증(Validation)용 데이터입니다.",
    "trap_points": [
      "검증 손실이 올라가기 시작하면 학습을 멈춰야 함"
    ],
    "difficulty": "easy",
    "id": "0683"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝할 때 베이스 모델의 능력을 보존하면서 특정 지식만 덧씌울 수 있도록, 고정된 모델 가중치에 덧붙여지는 조각을 무엇이라 하나요?",
    "answer": "Adapter (어댑터)",
    "why": "LoRA 등에서 생성되어 기존 가중치에 병합(Merge)하거나 덧붙여 사용하는 작은 행렬입니다.",
    "hint": "연결 도구(Adapter)를 생각하세요.",
    "trap_points": [
      "이 어댑터 파일만 공유하면 누구나 똑같은 성능을 낼 수 있음"
    ],
    "difficulty": "medium",
    "id": "0684"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "DPO(Direct Preference Optimization) 모델 학습을 위해 필요한 데이터 형태는?",
    "options": [
      "질문 하나와 답변 하나",
      "질문 하나와 (좋은 답변, 싫은 답변) 한 쌍",
      "문서 뭉치 하나",
      "영단어 리스트",
      "이미지 데이터"
    ],
    "answer": "질문 하나와 (좋은 답변, 싫은 답변) 한 쌍",
    "why": "두 답변 중 더 나은 쪽을 선호(Preference)하도록 모델의 확률 분포를 직접 조정하기 때문입니다.",
    "hint": "선택지 한 쌍이 필요합니다.",
    "trap_points": [
      "최근 RLHF를 대체하는 강력한 파인튜닝 기법임"
    ],
    "difficulty": "hard",
    "id": "0685"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝된 모델을 배포할 때, 모델의 레이어 정규화 값을 학습 시 값으로 고정하여 추론 속도를 높이는 과정은?",
    "options": [
      "Quantization",
      "Model Merging",
      "Graph Optimization",
      "Layer Folding",
      "Cashing"
    ],
    "answer": "Graph Optimization",
    "why": "연산 그래프를 분석하여 중복되거나 불필요한 계산을 합치거나 상수화하여 속도를 높입니다.",
    "hint": "그래프 최적화입니다.",
    "trap_points": [
      "ONNX, TensorRT 등이 이 과정을 수행함"
    ],
    "difficulty": "hard",
    "id": "0686"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 시 ‘데이터 오염(Data Contamination)’이란 무엇을 의미하나요?",
    "options": [
      "데이터가 사라지는 것",
      "평가에 쓰일 문제들이 학습 데이터에 포함되어 있어 실제 실력보다 점수가 높게 나오는 것",
      "인터넷이 끊기는 것",
      "영어로만 코딩하는 것",
      "파일이 깨지는 것"
    ],
    "answer": "평가에 쓰일 문제들이 학습 데이터에 포함되어 있어 실제 실력보다 점수가 높게 나오는 것",
    "why": "모델이 규칙을 배운 게 아니라 정답을 암기해버린 상태이므로 실전 성능을 신뢰할 수 없게 만듭니다.",
    "hint": "시험 정답을 미리 보고 시험을 치는 것과 같습니다.",
    "trap_points": [
      "학습 전 평가 데이터와의 중복 검사(De-contamination)가 필수임"
    ],
    "difficulty": "hard",
    "id": "0687"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 시 'Overfitting'을 감지하는 가장 직접적인 지표는?",
    "options": [
      "학습 데이터의 손실(Loss)은 줄어드는데, 검증(Validation) 데이터의 손실은 정체되거나 올라갈 때",
      "전원이 꺼질 때",
      "데이터가 사라졌을 때",
      "영어로만 답할 때",
      "손실 값이 0이 될 때"
    ],
    "answer": "학습 데이터의 손실(Loss)은 줄어드는데, 검증(Validation) 데이터의 손실은 정체되거나 올라갈 때",
    "why": "학습 데이터에만 과하게 맞춰져 새로운 데이터에 대한 일반화 능력을 잃었음을 뜻합니다.",
    "hint": "학습셋과 검증셋의 손실 값 차이를 보세요.",
    "trap_points": [
      "이 시점이 학습을 중단해야 하는 'Early Stopping' 타이밍임"
    ],
    "difficulty": "medium",
    "id": "0688"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "LoRA 학습에서 학습되는 아주 작은 두 개의 행렬 조각을 통칭하는 용어는?",
    "answer": "Adapter (어댑터)",
    "why": "기존 모델에 수수료처럼 덧붙여서(Adapt) 성능을 조절하는 조각이라는 뜻입니다.",
    "hint": "끼우다, 적응시키다라는 단어입니다.",
    "trap_points": [
      "학습 후 베이스 모델과 합쳐서(Merge) 사용할 수 있음"
    ],
    "difficulty": "easy",
    "id": "0689"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "LoRA 파인튜닝 시 'Rank(r)' 파라미터가 가지는 의미는?",
    "options": [
      "모델의 순위",
      "추가되는 어댑터 행렬의 차원 크기 (작을수록 메모리 절약, 클수록 정교함)",
      "데이터의 개수",
      "학습 모델의 가격",
      "인터넷 속도"
    ],
    "answer": "추가되는 어댑터 행렬의 차원 크기 (작을수록 메모리 절약, 클수록 정교함)",
    "why": "랭크는 파인튜닝할 수 있는 '용량'을 결정하며, 보통 8, 16, 32 등이 널리 사용됩니다.",
    "hint": "행렬의 차원 수입니다.",
    "trap_points": [
      "너무 크면 전체 파라미터 학습과 차이가 없어 효율이 떨어짐"
    ],
    "difficulty": "hard",
    "id": "0690"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 학습 중 모델이 원본의 선한 의도나 안전 지침을 무시하고 사용자에게 욕설을 하거나 거칠게 답하는 것을 막는 과정을 무엇이라 하나요?",
    "options": [
      "Normalization",
      "Alignment (정렬)",
      "Scaling",
      "Cleaning",
      "Encryption"
    ],
    "answer": "Alignment (정렬)",
    "why": "모델의 가치관과 인간의 가치관을 일치(Align)시키는 RLHF, DPO 등의 기법이 여기에 포함됩니다.",
    "hint": "인간의 의도에 맞게 조정(Align)합니다.",
    "trap_points": [
      "정렬이 잘 안 된 모델은 자의적인 주장을 펼칠 수 있음"
    ],
    "difficulty": "medium",
    "id": "0691"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝된 모델 배포 시 가장 많이 쓰이는 최적화 라이브러리로, 특히 엔비디아 GPU 성능을 100% 끌어내는 것은?",
    "options": [
      "vLLM",
      "TensorRT-LLM",
      "Ollama",
      "Llama.cpp",
      "Auto-GPT"
    ],
    "answer": "TensorRT-LLM",
    "why": "엔비디아 하드웨어에 최적화된 추론 라이브러리로 배치 처리와 지연 시간을 획기적으로 줄여줍니다.",
    "hint": "엔비디아 공식 최적화 툴입니다.",
    "trap_points": [
      "설치는 어렵지만 성능은 압도적임"
    ],
    "difficulty": "hard",
    "id": "0692"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "사전 학습(Pre-training)과 파인튜닝(Fine-tuning)의 가장 큰 재료 차이는?",
    "options": [
      "재료가 똑같다.",
      "사전 학습은 레이블이 없는 방대한 인터넷 데이터(Next Token Prediction)를 쓰고, 파인튜닝은 질문-답변 쌍과 같은 레이블된 정제 데이터를 쓴다.",
      "사전 학습만 영어다.",
      "파인튜닝은 이미지를 안 쓴다.",
      "차이가 없다."
    ],
    "answer": "사전 학습은 레이블이 없는 방대한 인터넷 데이터(Next Token Prediction)를 쓰고, 파인튜닝은 질문-답변 쌍과 같은 레이블된 정제 데이터를 쓴다.",
    "why": "사전 학습은 세상의 일반적 패턴을 배우는 과정이고, 파인튜닝은 특정 목적으로 ‘길들이는’ 과정입니다.",
    "hint": "데이터의 덩어리(Bulk)와 정제(Curated)의 차이입니다.",
    "trap_points": [
      "최근에는 파인튜닝 단계의 데이터 질이 모델 품질을 결정함"
    ],
    "difficulty": "easy",
    "id": "0693"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "가중치 행렬 중 일부 중요한 값만 남기고 0으로 만들어 모델 용량을 줄이는 최적화 방식은?",
    "answer": "Pruning (가지치기)",
    "why": "신경망 연결망 중 불필요한 가지를 쳐내어 속도와 용량을 개선합니다.",
    "hint": "나뭇가지를 친다는 뜻입니다.",
    "trap_points": [
      "Sparse해진 행렬을 효율적으로 처리하는 전용 커널이 필요함"
    ],
    "difficulty": "medium",
    "id": "0694"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 시 'Learning Rate (학습률)'가 너무 높을 때 나타나는 일반적인 증상은?",
    "options": [
      "학습이 너무 완벽하게 된다.",
      "손실 값(Loss)이 발산하거나 요동치며 모델이 아무 기술도 배우지 못하고 망가진다.",
      "속도가 100배 빨라진다.",
      "영어로만 답한다.",
      "글자 수가 길어진다."
    ],
    "answer": "손실 값(Loss)이 발산하거나 요동치며 모델이 아무 기술도 배우지 못하고 망가진다.",
    "why": "보폭(Learning Rate)이 너무 크면 최적의 지점(Minimum)을 지나쳐버리기 때문입니다.",
    "hint": "너무 큰 보폭의 부작용을 생각하세요.",
    "trap_points": [
      "반대로 너무 작으면 학습 진행이 아예 안 될 수도 있음"
    ],
    "difficulty": "medium",
    "id": "0695"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 후 모델의 '할루시네이션(환각)'이 오히려 심해지는 주된 원인은?",
    "options": [
      "컴퓨터가 뜨거워서",
      "데이터셋에 잘못된 정보가 포함되어 있거나, 모델이 데이터를 암기하기 시작해서 (Overfitting)",
      "영어로만 학습해서",
      "파일이 너무 커서",
      "인터넷이 끊겨서"
    ],
    "answer": "데이터셋에 잘못된 정보가 포함되어 있거나, 모델이 데이터를 암기하기 시작해서 (Overfitting)",
    "why": "나쁜 데이터를 배우면 지능 자체가 오염되는 GIGO 법칙의 결과입니다.",
    "hint": "학습 데이터의 품질이 곧 출력의 품질입니다.",
    "trap_points": [
      "데이터 양보다 정제가 훨씬 중요함"
    ],
    "difficulty": "easy",
    "id": "0696"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "모델의 지능 전수 기법인 'Distillation'에서 교사 모델과 학생 모델의 관계는?",
    "options": [
      "둘은 쌍둥이다.",
      "거대하고 똑똑한 모델(Teacher)의 확률 분포를 작고 가벼운 모델(Student)이 모방하도록 학습한다.",
      "학생이 더 크다.",
      "둘은 싸우는 관계다.",
      "관계가 없다."
    ],
    "answer": "거대하고 똑똑한 모델(Teacher)의 확률 분포를 작고 가벼운 모델(Student)이 모방하도록 학습한다.",
    "why": "효율을 위해 큰 모델의 능력을 작은 모델로 압축하는 강력한 산업적 기술입니다.",
    "hint": "증류(Distillation)의 과정을 생각하세요.",
    "trap_points": [
      "최신 sLLM들이 성능을 비약적으로 올린 비결임"
    ],
    "difficulty": "medium",
    "id": "0697"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝 데이터를 만들 때, 사람이 직접 적는 대신 AI가 AI용 데이터를 자동으로 생성해 주는 기술은?",
    "answer": "Synthetic Data Generation (합성 데이터 생성)",
    "why": "고품질 데이터를 무한히 생성하여 학습 비용을 줄이고 다양성을 확보합니다.",
    "hint": "인공적으로 합성(Synthetic)한 데이터입니다.",
    "trap_points": [
      "최근에는 사람보다 AI가 만든 데이터로 학습한 성능이 더 높기도 함"
    ],
    "difficulty": "medium",
    "id": "0698"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝된 모델 공유 시 필수적으로 첨부해야 할 'Model Card'에 포함되지 않아도 되는 것은?",
    "options": [
      "모델의 용도와 제한 사항",
      "학습 데이터셋 정보",
      "평가 벤치마크 결과",
      "개발자의 어제 점심 메뉴",
      "저작권 및 라이선스 정보"
    ],
    "answer": "개발자의 어제 점심 메뉴",
    "why": "모델 카드는 기술적/윤리적 명세서이므로 사적인 정보는 필요 없습니다.",
    "hint": "모델의 '명세서'입니다.",
    "trap_points": [
      "투명한 AI 생태계를 위한 표준 약속임"
    ],
    "difficulty": "easy",
    "id": "0699"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "객관식",
    "question": "파인튜닝의 대표적인 방법인 LoRA에서 학습 대상이 되는 가중치는?",
    "options": [
      "모델의 전체 가중치",
      "모델에 추가된 두 개의 저차원 행렬 가중치 (A, B)",
      "바이어스 값만",
      "임베딩 레이어 가중치",
      "마지막 층의 가중치만"
    ],
    "answer": "모델에 추가된 두 개의 저차원 행렬 가중치 (A, B)",
    "why": "원본 파라미터는 동결하고, 옆에 붙인 가벼운 행렬만 업데이트하여 효율을 극대화합니다.",
    "hint": "옆에 덧붙인(Adapter) 작은 행렬들을 생각하세요.",
    "trap_points": [
      "이를 통해 수백 배 적은 메모리로 파인튜닝이 가능해짐"
    ],
    "difficulty": "hard",
    "id": "0700"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "LoRA 설정을 위한 PEFT 라이브러리의 클래스는? (문제 1)",
    "options": [],
    "answer": "LoraConfig",
    "why": "LoRA 학습을 위한 랭크, 알파 등의 하이퍼파라미터를 정의합니다.",
    "difficulty": "hard",
    "id": "0701"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "HuggingFace Trainer에서 학습 인자를 설정하는 클래스는? (문제 2)",
    "options": [],
    "answer": "TrainingArguments",
    "why": "배치 크기, 학습률, 에폭 수 등 학습 전반의 설정을 담당합니다.",
    "difficulty": "medium",
    "id": "0702"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "양자화된 모델을 로드하기 위한 설정 클래스는? (문제 3)",
    "options": [],
    "answer": "BitsAndBytesConfig",
    "why": "4bit, 8bit 등 양자화 설정을 통해 메모리 사용량을 줄입니다.",
    "difficulty": "hard",
    "id": "0703"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "LoRA 설정을 위한 PEFT 라이브러리의 클래스는? (문제 4)",
    "options": [],
    "answer": "LoraConfig",
    "why": "LoRA 학습을 위한 랭크, 알파 등의 하이퍼파라미터를 정의합니다.",
    "difficulty": "medium",
    "id": "0704"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "HuggingFace Trainer에서 학습 인자를 설정하는 클래스는? (문제 5)",
    "options": [],
    "answer": "TrainingArguments",
    "why": "배치 크기, 학습률, 에폭 수 등 학습 전반의 설정을 담당합니다.",
    "difficulty": "hard",
    "id": "0705"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "양자화된 모델을 로드하기 위한 설정 클래스는? (문제 6)",
    "options": [],
    "answer": "BitsAndBytesConfig",
    "why": "4bit, 8bit 등 양자화 설정을 통해 메모리 사용량을 줄입니다.",
    "difficulty": "medium",
    "id": "0706"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "LoRA 설정을 위한 PEFT 라이브러리의 클래스는? (문제 7)",
    "options": [],
    "answer": "LoraConfig",
    "why": "LoRA 학습을 위한 랭크, 알파 등의 하이퍼파라미터를 정의합니다.",
    "difficulty": "hard",
    "id": "0707"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "HuggingFace Trainer에서 학습 인자를 설정하는 클래스는? (문제 8)",
    "options": [],
    "answer": "TrainingArguments",
    "why": "배치 크기, 학습률, 에폭 수 등 학습 전반의 설정을 담당합니다.",
    "difficulty": "medium",
    "id": "0708"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "양자화된 모델을 로드하기 위한 설정 클래스는? (문제 9)",
    "options": [],
    "answer": "BitsAndBytesConfig",
    "why": "4bit, 8bit 등 양자화 설정을 통해 메모리 사용량을 줄입니다.",
    "difficulty": "hard",
    "id": "0709"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "LoRA 설정을 위한 PEFT 라이브러리의 클래스는? (문제 10)",
    "options": [],
    "answer": "LoraConfig",
    "why": "LoRA 학습을 위한 랭크, 알파 등의 하이퍼파라미터를 정의합니다.",
    "difficulty": "medium",
    "id": "0710"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "HuggingFace Trainer에서 학습 인자를 설정하는 클래스는? (문제 11)",
    "options": [],
    "answer": "TrainingArguments",
    "why": "배치 크기, 학습률, 에폭 수 등 학습 전반의 설정을 담당합니다.",
    "difficulty": "hard",
    "id": "0711"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "양자화된 모델을 로드하기 위한 설정 클래스는? (문제 12)",
    "options": [],
    "answer": "BitsAndBytesConfig",
    "why": "4bit, 8bit 등 양자화 설정을 통해 메모리 사용량을 줄입니다.",
    "difficulty": "medium",
    "id": "0712"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "LoRA 설정을 위한 PEFT 라이브러리의 클래스는? (문제 13)",
    "options": [],
    "answer": "LoraConfig",
    "why": "LoRA 학습을 위한 랭크, 알파 등의 하이퍼파라미터를 정의합니다.",
    "difficulty": "hard",
    "id": "0713"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "HuggingFace Trainer에서 학습 인자를 설정하는 클래스는? (문제 14)",
    "options": [],
    "answer": "TrainingArguments",
    "why": "배치 크기, 학습률, 에폭 수 등 학습 전반의 설정을 담당합니다.",
    "difficulty": "medium",
    "id": "0714"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "양자화된 모델을 로드하기 위한 설정 클래스는? (문제 15)",
    "options": [],
    "answer": "BitsAndBytesConfig",
    "why": "4bit, 8bit 등 양자화 설정을 통해 메모리 사용량을 줄입니다.",
    "difficulty": "hard",
    "id": "0715"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "LoRA 설정을 위한 PEFT 라이브러리의 클래스는? (문제 16)",
    "options": [],
    "answer": "LoraConfig",
    "why": "LoRA 학습을 위한 랭크, 알파 등의 하이퍼파라미터를 정의합니다.",
    "difficulty": "medium",
    "id": "0716"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "HuggingFace Trainer에서 학습 인자를 설정하는 클래스는? (문제 17)",
    "options": [],
    "answer": "TrainingArguments",
    "why": "배치 크기, 학습률, 에폭 수 등 학습 전반의 설정을 담당합니다.",
    "difficulty": "hard",
    "id": "0717"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "양자화된 모델을 로드하기 위한 설정 클래스는? (문제 18)",
    "options": [],
    "answer": "BitsAndBytesConfig",
    "why": "4bit, 8bit 등 양자화 설정을 통해 메모리 사용량을 줄입니다.",
    "difficulty": "medium",
    "id": "0718"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "LoRA 설정을 위한 PEFT 라이브러리의 클래스는? (문제 19)",
    "options": [],
    "answer": "LoraConfig",
    "why": "LoRA 학습을 위한 랭크, 알파 등의 하이퍼파라미터를 정의합니다.",
    "difficulty": "hard",
    "id": "0719"
  },
  {
    "chapter_name": "Fine Tuning",
    "type": "코드 완성형",
    "question": "HuggingFace Trainer에서 학습 인자를 설정하는 클래스는? (문제 20)",
    "options": [],
    "answer": "TrainingArguments",
    "why": "배치 크기, 학습률, 에폭 수 등 학습 전반의 설정을 담당합니다.",
    "difficulty": "medium",
    "id": "0720"
  }
]