# 📘 [학습 노트] 교재 3. LLM 기본 (PDF 기반 + 실무 보충)

이 교재는 원본 PDF **"LLM 기본"**의 내용을 중심으로, 트랜스포머의 핵심 개념과 주요 LLM의 특징을 실무 관점에서 정리했습니다.

---

## 1. LLM과 트랜스포머 (Transformer)
### 🧠 트랜스포머의 등장
- **기존 한계**: RNN/LSTM은 문장이 길어지면 앞 내용을 잊어버리는 문제(Long-term Dependency)가 있었습니다.
- **Attention is All You Need (2017)**: 구글이 발표한 트랜스포머 구조는 **"어떤 단어가 문맥상 중요한가?"(Attention)**에 집중하여 병렬 처리가 가능하고 긴 문맥을 잘 이해합니다.

### 🏗 핵심 구조: 인코더와 디코더
1.  **인코더 (Encoder)**: 문장을 이해하고 압축합니다. (예: BERT)
    - 문장의 빈칸 채우기, 감정 분석, 요약 등에 강합니다.
2.  **디코더 (Decoder)**: 다음에 올 단어를 예측하여 생성합니다. (예: GPT)
    - 문장 생성, 대화, 창작 등에 특화되어 있습니다. **현대 LLM은 대부분 디코더 전용 구조**를 따릅니다.

---

## 2. 토큰화 (Tokenization)
### 🔠 토큰이란?
LLM은 텍스트를 글자가 아닌 **토큰(Token)** 단위로 처리합니다.
- 토큰은 단어일 수도, 단어의 일부일 수도 있습니다.
- **영어**: 평균 1단어 ≈ 1.3토큰
- **한국어**: 교착어 특성상 토큰 소모가 더 많습니다. (약 2배)

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")
text = "Hello, world!"
tokens = tokenizer.encode(text)
print(tokens)  # [15496, 11, 995, 0] (숫자로 변환됨)
```

---

## 3. 주요 LLM 모델 계보
### 🌟 GPT 시리즈 (OpenAI)
- **GPT-1**: 트랜스포머 디코더를 처음으로 대규모 데이터에 적용.
- **GPT-2**: 파라미터 수를 늘리고 웹 텍스트를 대량 학습. 제로샷(Zero-shot) 가능성 확인.
- **GPT-3 (175B)**: 거대 모델의 시발점. 별도 학습 없이 프롬프트만으로 다양한 작업 수행(In-Context Learning).
- **GPT-4**: 멀티모달(이미지 인식) 능력과 압도적인 추론 성능.

### 🦙 LLaMA (Meta)
- 메타가 공개한 **오픈 웨이트(Open Weights)** 모델.
- 누구나 다운로드하여 연구하고 파이튜닝할 수 있어 LLM 민주화에 기여했습니다.
- 파생 모델(Alpaca, Vicuna 등)의 기반이 되었습니다.

---

## 4. LLM 활용 방식
### 🕹 API vs 오픈 모델
1.  **상용 API (OpenAI, Anthropic)**:
    - **장점**: 성능이 가장 좋고 인프라 관리가 필요 없습니다.
    - **단점**: 비용이 들고 데이터 보안 이슈가 있을 수 있습니다.
2.  **오픈 모델 (HuggingFace)**:
    - **장점**: 내 서버에 설치하여 보안이 강력하고 커스터마이징이 자유롭습니다.
    - **단점**: 고성능 GPU가 필요하고 직접 운영해야 합니다.

### 🌡 생성 제어 파라미터
API 호출 시 답변의 스타일을 바꿀 수 있습니다.
- **Temperature (온도)**:
    - 높을수록(`0.8~1.0`): 창의적이고 다양한 답변. (소설, 아이디어)
    - 낮을수록(`0.0~0.2`): 정확하고 일관된 답변. (요약, 코딩, 분류)
- **Max Tokens**: 생성할 최대 길이 제한.