
chapter_name = "RAG & Agent"

questions = []

# --- 100 MCQs ---
mcq_data = [
    # 1. RAG의 기본 개념 및 필요성 (1-20)
    ("RAG(Retrieval-Augmented Generation)의 가장 주된 도입 목적은?", ["모델의 파라미터를 실시간으로 갱신하기 위해", "LLM의 할루시네이션(환각)을 줄이고 최신/외부 정보를 참조하게 하기 위해", "모델의 생성 속도를 비약적으로 높이기 위해", "인터넷 연결이 필요 없는 모델을 만들기 위해", "모델의 크기를 획기적으로 줄이기 위해"], "LLM의 할루시네이션(환각)을 줄이고 최신/외부 정보를 참조하게 하기 위해", "RAG는 신뢰할 수 있는 외부 지식 베이스를 검색하여 답변의 근거로 활용함으로써 오답률을 낮춥니다. Fine-tuning과 달리 모델 재학습 없이 지식을 업데이트할 수 있어 유지보수 비용도 절감됩니다.", "RAG의 목적", "5001", "easy"),
    ("RAG 시스템의 3단계 흐름(Retrieval - Augmentation - Generation) 중 'Retrieval' 단계에서 하는 일은?", ["가장 적절한 답변을 생성한다.", "질문과 관련된 가장 유사한 문서 조각을 검색해온다.", "프롬프트를 보기 좋게 꾸민다.", "사용자의 지갑 주소를 확인한다.", "모델을 새로 학습시킨다."], "질문과 관련된 가장 유사한 문서 조각을 검색해온다.", "벡터 데이터베이스 등에서 질문의 의미와 부합하는 정보를 찾아내는 과정입니다. 질문을 임베딩 벡터로 변환한 뒤 코사인 유사도 등으로 가장 가까운 문서를 선별합니다.", "Retrieval", "5002", "easy"),
    ("RAG에서 데이터를 검색할 때 주로 쓰이는 '벡터 유사도' 방식 중 가장 대표적인 것은?", ["산술 평균", "코사인 유사도 (Cosine Similarity)", "랜덤 추출", "알파벳 순서 정렬", "파일 크기 비교"], "코사인 유사도 (Cosine Similarity)", "두 벡터 사이의 각도를 이용해 텍스트의 의미적 유사성을 측정하는 핵심 기법입니다. 값은 -1~1 사이이며 1에 가까울수록 의미가 유사합니다. 벡터 크기에 무관하게 방향만 비교하므로 텍스트 길이 차이의 영향을 받지 않습니다.", "코사인 유사도", "5003", "easy"),
    ("검색 증강 생성(RAG)이 Fine-tuning보다 유리한 상황은?", ["모델의 말투나 어조를 완전히 바꾸고 싶을 때", "정보가 수시로 업데이트되는 실시간 뉴스를 다뤄야 할 때", "모델의 내부 가중치를 영구적으로 고정하고 싶을 때", "데이터셋의 용량이 매우 작을 때", "인터넷이 전혀 안 되는 환경일 때"], "정보가 수시로 업데이트되는 실시간 뉴스를 다뤄야 할 때", "RAG는 데이터베이스만 업데이트하면 즉시 최신 정보를 반영할 수 있어 효율적입니다. Fine-tuning은 새 데이터로 재학습에 많은 시간·비용이 들며, 학습 완료 후에도 이미 지난 시점의 지식이 됩니다. 반면 RAG는 벡터 DB 갱신만으로 수분 내에 최신 정보를 적용할 수 있습니다.", "RAG vs Fine-tuning", "5004", "hard"),
    ("RAG 파이프라인 중 문서를 저장하기 위해 잘게 쪼개는 과정을 무엇이라 하는가?", ["Embedding", "Chunking (청킹)", "Scaling", "Masking", "Labeling"], "Chunking (청킹)", "모델의 인풋 제한(Context Window)에 맞춰 문서를 의미 있는 단위로 나누는 작업입니다. 청크 크기와 오버랩 설정이 RAG 품질에 크게 영향을 미칩니다.", "Chunking", "5005", "easy"),
    ("나눠진 텍스트 조각(Chunks)을 수치 형태의 벡터로 변환하는 모델을 무엇이라 부르는가?", ["LLM", "Embedding Model (임베딩 모델)", "Tokenizer", "Quantizer", "Compiler"], "Embedding Model (임베딩 모델)", "자연어의 의미를 고차원 공간상의 좌표(벡터)로 변환해주는 역할을 합니다. OpenAI의 text-embedding-3-small, HuggingFace의 sentence-transformers 등 다양한 임베딩 모델이 존재합니다.", "Embedding Model", "5006", "easy"),
    ("변환된 벡터들을 저장하고 의미 기반 검색을 지원하는 특수한 데이터베이스는?", ["MySQL", "Redis", "Vector Database (벡터 DB)", "MongoDB", "Oracle"], "Vector Database (벡터 DB)", "Pinecone, Chroma, Milvus 등 벡터 검색에 최적화된 DB를 활용합니다. 일반 관계형 DB와 달리 ANN(근사 최근접 이웃) 알고리즘으로 수백만 개의 벡터 중 유사한 것을 밀리초 단위로 찾습니다.", "Vector DB", "5007", "easy"),
    ("RAG에서 'Augmentation' 단계의 역할로 적절한 것은?", ["모델의 가중치를 키우는 행위", "검색된 결과물과 원래의 질문을 조합하여 풍부한 프롬프트를 만드는 것", "인터넷 속도를 높이는 작업", "모델의 성능 점수를 매기는 것", "데이터를 모두 지우는 것"], "검색된 결과물과 원래의 질문을 조합하여 풍부한 프롬프트를 만드는 것", "검색된 지식을 모델이 참고할 수 있도록 지시문과 함께 배치하는 단계입니다. 일반적으로 'Context: {검색결과}\\nQuestion: {질문}\\nAnswer:' 형식으로 프롬프트를 구성합니다. 이 구성 방식이 모델의 답변 품질을 크게 좌우합니다.", "Augmentation", "5008", "hard"),
    ("임베딩 벡터의 '차원(Dimension)'이 의미하는 바는?", ["데이터의 개수", "수치 데이터를 표현하는 화살표의 길이", "의미적 특징을 담고 있는 수치 리스트의 개수", "모델의 레이어 개수", "학습에 걸리는 시간"], "의미적 특징을 담고 있는 수치 리스트의 개수", "차원이 높을수록 더 정교한 의미를 담을 수 있지만 연산량도 늘어납니다. OpenAI text-embedding-3-small은 1536차원, text-embedding-3-large는 3072차원입니다. 쿼리 벡터와 문서 벡터는 반드시 동일한 차원이어야 유사도 비교가 가능합니다.", "차원의 의미", "5009", "hard"),
    ("RAG 시스템 성능 평가 시, 생성된 답변이 검색된 문서에 실제로 근거하고 있는지 측정하는 지표는?", ["Hit Rate", "Faithfulness (충실도)", "Context Precision", "Answer Relevance", "Latency"], "Faithfulness (충실도)", "모델이 지어내서 대답(할루시네이션)하지 않고 근거에 충실했는지를 봅니다. Ragas 프레임워크에서는 LLM-as-a-judge 방식으로 각 문장이 제공된 컨텍스트에서 추론 가능한지 자동 판별합니다. 0~1 사이의 점수로 산출됩니다.", "Faithfulness", "5010", "hard"),
    ("LLM이 단순히 답만 하는 게 아니라, 도구를 사용하거나 자율적으로 판단하여 동작하는 주체를 무엇이라 하는가?", ["Chatbot", "Scanner", "LLM Agent (에이전트)", "Parser", "Optimizer"], "LLM Agent (에이전트)", "주어진 목표를 달성하기 위해 '생각(Thought)'과 '행동(Action)'을 반복하는 시스템입니다. 단순 체인과 달리 외부 환경의 피드백(Observation)을 받아 다음 행동을 동적으로 결정합니다.", "Agent", "5011", "easy"),
    ("에이전트가 문제 해결을 위해 '생각 - 행동 - 관찰' 루프를 타는 대표적인 추론 방식은?", ["Chain of Thought (CoT)", "ReAct (Reasoning + Acting)", "Few-shot", "Top-P Sampling", "Fine-tuning"], "ReAct (Reasoning + Acting)", "사유와 행동, 외부 환경 관찰을 결합하여 복잡한 목표를 수행하는 방식입니다. 'Thought → Action → Observation'을 반복하며 목표 달성 시 'Final Answer'를 출력합니다.", "ReAct", "5012", "easy"),
    ("에이전트가 외부 세계와 상호작용하기 위해 갖추고 있는 기능(예: 검색, 계산기, API 호출)을 일컫는 말은?", ["Parameters", "Weights", "Tools (도구)", "Layers", "Biases"], "Tools (도구)", "LangChain 등 프레임워크에서 에이전트가 실행할 수 있는 함수들을 정의한 것입니다. @tool 데코레이터나 BaseTool 클래스로 정의하며, 함수의 독스트링이 에이전트의 도구 선택 판단 근거가 됩니다.", "Tools", "5013", "easy"),
    ("랭체인(LangChain) 프레임워크에서 선언적인 파이프라인 구성을 위해 사용하는 문법은?", ["HTML/CSS", "LCEL (LangChain Expression Language)", "SQL Query", "Direct API Call", "Regular Expression"], "LCEL (LangChain Expression Language)", "파이프 연산자(|)를 사용해 데이터의 흐름을 직관적으로 연결합니다. 예: chain = prompt | llm | parser. 각 컴포넌트는 Runnable 인터페이스를 구현하여 비동기·스트리밍을 자동 지원합니다.", "LCEL", "5014", "easy"),
    ("에이전트가 이전 대화 내역이나 실행 결과를 기억하고 활용하기 위해 필요한 구성 요소는?", ["CPU", "GPU", "Memory (메모리)", "Disk", "Monitor"], "Memory (메모리)", "ConversationBufferMemory 등을 통해 대화의 맥락을 유지합니다. 긴 대화에는 ConversationSummaryMemory로 요약하여 토큰 소비를 절감할 수 있습니다.", "Agent Memory", "5015", "easy"),
    ("RAG 시스템에서 청킹(Chunking)을 너무 크게 했을 때 발생할 수 있는 부작용은?", ["검색 적중률이 너무 높아진다.", "불필요한 정보(Noise)가 섞여 모델의 답변이 흐려진다.", "비용이 전혀 들지 않는다.", "모델의 지능이 낮아진다.", "한국어 답변이 안 나온다."], "불필요한 정보(Noise)가 섞여 모델의 답변이 흐려진다.", "너무 큰 덩어리는 질문과 핵심적인 관련이 없는 내용까지 포함하여 추론을 방해할 수 있습니다. Context Window의 상당 부분을 비관련 정보로 채워 모델이 핵심 내용에 집중하지 못하게 됩니다. 일반적으로 256~512 토큰 내외가 권장됩니다.", "과한 청킹", "5016", "hard"),
    ("반대로 청킹을 너무 작게(10글자 등) 했을 때의 문제점은?", ["문맥(Context)이 단절되어 문서의 의미를 파악하기 힘들다.", "검색 속도가 100배 빨라진다.", "모델이 모든 내용을 외워버린다.", "서버가 중단된다.", "답변이 너무 길어진다."], "문맥(Context)이 단절되어 문서의 의미를 파악하기 힘들다.", "파편화된 정보만으로는 질문에 대한 충분한 배경 지식을 전달하기 어렵습니다. 문장이 청크 경계에서 잘리면 의미가 완결되지 않은 텍스트 조각이 검색 결과로 제공되어 모델이 올바른 추론을 하지 못합니다.", "부족한 청킹", "5017", "hard"),
    ("벡터 DB 검색 성능 지표 중 'Hit Rate'의 의미로 옳은 것은?", ["서버가 다운된 횟수", "검색 결과 상위 K개 안에 실제 정답 문서가 포함된 비율", "화면을 클릭한 횟수", "데이터를 삭제한 개수", "비밀번호를 틀린 횟수"], "검색 결과 상위 K개 안에 실제 정답 문서가 포함된 비율", "검색 단계가 얼마나 질문과 연관된 문서를 잘 찾아오는지를 나타내는 기본 지표입니다. Hit Rate@3이 0.9라면 10번 중 9번은 상위 3개 결과 안에 정답 문서가 포함됨을 의미합니다.", "Hit Rate", "5018", "easy"),
    ("RAG 파이프라인 중 검색된 결과의 순위를 다시 매겨 정확도를 높이는 단계를 무엇이라 하나?", ["Pre-ranking", "Re-ranking (리랭킹)", "Decoding", "Encoding", "Flattening"], "Re-ranking (리랭킹)", "단순 벡터 유사도 계산 후, 더 정교한 모델로 실제 관련성을 재검증하는 과정입니다. 1차로 벡터 검색으로 Top-K 후보를 뽑고, Cross-Encoder 모델로 질문과의 관련성을 정밀 재평가하여 Top-N으로 압축합니다.", "Re-ranking", "5019", "hard"),
    ("에이전트 설계 시 '멀티 에이전트' 시스템의 장점은?", ["한 명의 에이전트가 모든 일을 다 하게 한다.", "역할별로 특화된 에이전트들이 협력하여 대규모 복잡한 문제를 효율적으로 푼다.", "비용을 무조건 줄여준다.", "AI 개발을 중단할 수 있다.", "컴퓨터 사양을 낮춰준다."], "역할별로 특화된 에이전트들이 협력하여 대규모 복잡한 문제를 효율적으로 푼다.", "기획 에이전트, 서칭 에이전트, 코딩 에이전트 등으로 분업하여 품질을 높입니다. 단일 에이전트로 처리하기 어려운 복잡한 작업을 병렬로 처리하거나 서로 검증하게 할 수 있어 오류율도 낮아집니다.", "Multi-Agent", "5020", "hard"),

    # 2. RAG 및 에이전트 상세 기술 (21-40)
    ("텍스트를 유의미한 단위로 나누기 위해 문장 구분자나 줄바꿈을 기준으로 재귀적으로 쪼개는 청커는?", ["CharacterTextSplitter", "RecursiveCharacterTextSplitter", "TokenTextSplitter", "RandomSplitter", "NoneSplitter"], "RecursiveCharacterTextSplitter", "의미적 단위를 최대한 보존하며 적절한 크기에 도달할 때까지 쪼개는 권장 방식입니다. 기본 구분자 목록은 ['\\n\\n', '\\n', ' ', '']으로 단락 → 줄 → 단어 순서로 재귀적으로 분리합니다.", "Recursive Splitter", "5021", "easy"),
    ("RAG 환경에서 'Top-K' 값을 높이면 어떤 일이 벌어지는가?", ["검색 결과에 더 많은 문서 조각을 포함시킨다.", "답변의 글자 수가 무조건 줄어든다.", "모델이 더 빨리 답한다.", "비용이 할인된다.", "한글이 영어로 변한다."], "검색 결과에 더 많은 문서 조각을 포함시킨다.", "다양한 참조 문서를 넣을 수 있지만, 너무 많으면 문맥 초과나 노이즈가 발생할 수 있습니다. 일반적으로 k=3~5가 권장되며, Context Window가 큰 모델에서는 더 높게 설정할 수 있습니다.", "Top-K 조절", "5022", "easy"),
    ("임베딩 모델을 선택할 때 고려해야 할 'MTEB' 벤치마크란?", ["모델의 크기 측정기", "다양한 텍스트 임베딩 성능을 평가하는 표준 리더보드", "서버의 발열 측정", "모니터의 주사율", "인터넷 속도"], "다양한 텍스트 임베딩 성능을 평가하는 표준 리더보드", "성능이 검증된 임베딩 모델을 선택하기 위한 공신력 있는 참조 지표입니다. MTEB(Massive Text Embedding Benchmark)는 56개의 다양한 태스크로 임베딩 모델을 평가하며 HuggingFace에서 관리합니다.", "MTEB", "5023", "easy"),
    ("벡터 DB의 인덱싱 기법 중 '고속 근사 최근접 이웃(ANN)' 검색을 위해 쓰이는 대표적 알고리즘은?", ["B-Tree", "HNSW (Hierarchical Navigable Small World)", "Hash Table", "Bubble Sort", "Quick Search"], "HNSW (Hierarchical Navigable Small World)", "방대한 벡터 데이터 속에서 가장 유사한 좌표를 빠르게 찾는 핵심 알고리즘입니다. 계층적 그래프 구조로 수백만 개의 벡터 중 최근접 이웃을 밀리초 단위에서 찾으며 Chroma, FAISS 등 주요 벡터 DB가 기본으로 사용합니다.", "HNSW", "5024", "hard"),
    ("랭체인에서 PDF 문서를 읽어오기 위해 사용하는 컴포넌트의 타입은?", ["Document Writer", "Document Loader", "Document Deleter", "Document Hider", "Document Copier"], "Document Loader", "비정형 파일에서 텍스트와 메타데이터를 추출하는 시작점입니다. PyPDFLoader, WebBaseLoader, CSVLoader 등 다양한 형식에 특화된 Loader가 제공되며, 모두 Document 객체 리스트를 반환합니다.", "Loader", "5025", "hard"),
    ("검색된 문서의 연관성이 떨어지는 경우 질문을 더 검색하기 좋은 형태로 다시 쓰는 기법은?", ["Query Rewriting (쿼리 재작성)", "Text Deleting", "Word Counting", "Grammar Error", "Stop"], "Query Rewriting (쿼리 재작성)", "사용자의 모호한 질문을 AI가 풍부한 키워드로 변환하여 검색 정확도를 높입니다. 예를 들어 '그거 어떻게 해?' 같은 모호한 질문을 문맥에 맞는 구체적 키워드로 변환합니다.", "Query Rewriting", "5026", "easy"),
    ("에이전트가 어떤 도구를 써야 할지 모델에게 알려주기 위해 제공하는 정보는?", ["도구의 소스 코드 전체", "도구의 이름, 기능 설명, 입력받을 인자(Parameter)의 스킴", "도구 제작자의 이름", "도구의 용량", "도구의 가격"], "도구의 이름, 기능 설명, 입력받을 인자(Parameter)의 스킴", "모델은 이 설명을 읽고 현재 상황에서 어떤 도구가 필요한지 논리적으로 판단합니다. @tool 데코레이터에서는 함수의 독스트링이 자동으로 도구 설명이 되며, Pydantic 모델로 입력 스키마를 정의할 수 있습니다.", "Tool description", "5027", "easy"),
    ("RAG 시스템 운영 시 'Ground Truth'의 역할은?", ["실제 답변 데이터", "성능 측정 시 비교 대상이 되는 정답(기준값) 데이터셋", "서버의 위치", "모델의 이름", "사용자의 개인정보"], "성능 측정 시 비교 대상이 되는 정답(기준값) 데이터셋", "시스템이 낸 답이 실제 정답과 얼마나 유사한지 점수를 매기기 위한 기준입니다. 고품질의 Ground Truth 데이터셋을 구축하는 것이 정확한 시스템 평가의 전제 조건입니다. Ragas에서는 question, answer, contexts, ground_truth 4개 필드로 구성됩니다.", "Ground Truth", "5028", "hard"),
    ("임베딩 벡터 사이의 거리가 '가까울수록' 텍스트의 의미는 어떠한가?", ["아무 상관 없다.", "의미가 매우 유사하다.", "의미가 정반대다.", "철자가 완전히 다르다.", "영어로 쓰여 있다."], "의미가 매우 유사하다.", "벡터 공간상의 가까운 거리는 자연어 처리에서 의미적 연관성이 깊음을 뜻합니다. 코사인 유사도 기준으로 1에 가까울수록, L2 유클리드 거리 기준으로 0에 가까울수록 의미가 유사합니다. 이것이 임베딩 기반 시맨틱 검색의 핵심 원리입니다.", "벡터 거리 의미", "5029", "hard"),
    ("검색 성능을 높이기 위해 벡터 유사도와 전통적인 키워드 매칭(BM25)을 섞어 쓰는 방식은?", ["Random Search", "Hybrid Search (하이브리드 검색)", "Linear Search", "Binary Search", "Manual Search"], "Hybrid Search (하이브리드 검색)", "의미적 유사성과 고유 대명사 매칭의 강점을 모두 활용하는 강력한 전략입니다. BM25는 정확한 키워드 매칭에, 벡터 검색은 의미 유사성에 강하므로 두 점수를 가중 합산하면 각각의 단점을 보완합니다.", "Hybrid Search", "5030", "hard"),
    ("에이전트가 루프에 빠져 무한히 도구를 실행하는 것을 방지하기 위한 안전장치는?", ["인터넷 끊기", "Max Iterations (최대 반복 횟수) 설정", "컴퓨터 끄기", "키보드 빼기", "질문 무시하기"], "Max Iterations (최대 반복 횟수) 설정", "일정 횟수 이상 도구를 돌려도 답이 안 나오면 중단하도록 하여 비용과 시간을 보호합니다. LangChain의 AgentExecutor에서 max_iterations 파라미터로 설정하며, 기본값은 15입니다.", "Max Iterations", "5031", "easy"),
    ("LCEL 문법에서 'prompt | model | parser' 구조일 때 'parser'의 역할은?", ["모델의 지능을 높임", "모델이 내놓은 텍스트 응답을 JSON이나 리스트 등 정형화된 데이터로 변환함", "프롬프트를 삭제함", "비용을 결제함", "오타를 냄"], "모델이 내놓은 텍스트 응답을 JSON이나 리스트 등 정형화된 데이터로 변환함", "AI의 답변을 소프트웨어 시스템에서 즉시 사용할 수 있는 데이터로 가공합니다. StrOutputParser는 문자열, JsonOutputParser는 JSON, PydanticOutputParser는 Pydantic 모델 객체로 변환합니다.", "Output Parser", "5032", "easy"),
    ("벡터 DB의 메타데이터 필터링(Metadata Filtering)이 유용한 시나리오는?", ["유사도만으로 충분할 때", "특정 날짜 이후의 문서나 특정 작성자의 글만 검색 범위로 한정하고 싶을 때", "데이터가 아예 없을 때", "모델을 튜닝할 때", "영어를 안 쓸 때"], "특정 날짜 이후의 문서나 특정 작성자의 글만 검색 범위로 한정하고 싶을 때", "의미 기반 검색에 '조건'을 추가해 결과의 정확도를 비약적으로 높입니다. 예를 들어 year >= 2024 AND category == 'finance' 조건을 함께 적용하면 수백만 문서 중 관련 항목만 빠르게 탐색합니다.", "Metadata Filtering", "5033", "hard"),
    ("에이전트가 문제를 해결하는 과정을 사용자가 실시간으로 보게 하는 기술은?", ["Stealing", "Streaming (스트리밍)", "Cracking", "Stopping", "Hiding"], "Streaming (스트리밍)", "사고 과정(Thought)과 결과가 나오는 즉시 화면에 노출하여 UX를 개선합니다. LangChain에서는 .stream() 메서드나 astream()으로 비동기 스트리밍을 지원하며, 첫 토큰까지의 지연(TTFT)을 줄여 사용자 체감 속도를 향상시킵니다.", "Streaming", "5034", "hard"),
    ("사내 RAG 시스템 구축 시 데이터 보안을 위해 가장 권장되는 방식은?", ["모든 데이터를 공용 챗봇에 입력한다.", "보안이 확보된 내부망에 벡터 DB와 임베딩 서버를 구축한다.", "데이터를 암호화하지 않고 보관한다.", "누구나 검색 가능하게 설정한다.", "모든 사내 문서를 인터넷에 공개한다."], "보안이 확보된 내부망에 벡터 DB와 임베딩 서버를 구축한다.", "민감한 기업 정보가 외부로 새나가지 않도록 폐쇄적인 파이프라인을 유지해야 합니다. 온프레미스 배포나 프라이빗 네트워크 구성이 대표적인 보안 전략입니다.", "RAG 보안", "5035", "medium"),
    ("RAG 시스템에서 '환각'을 유발하는 가장 흔한 원인은?", ["모델이 너무 똑똑해서", "질문과 전혀 상관없는 문서 조각이 검색 결과로 제공될 때", "인터넷이 너무 빨라서", "화면이 너무 커서", "키보드 타이핑이 빨라서"], "질문과 전혀 상관없는 문서 조각이 검색 결과로 제공될 때", "잘못된 정보를 근거로 주면 모델은 그 정보에 맞춰 엉뚱한 결론을 내리게 됩니다. Garbage In, Garbage Out(GIGO) 원칙으로, RAG에서 검색 품질이 결정적입니다.", "환각 원인", "5036", "medium"),
    ("청킹 시 앞뒤 조각의 내용을 일부 겹치게(Overlap) 설정하는 이유는?", ["데이터를 낭비하려고", "문장의 중간이 잘려 문맥의 의미가 훼손되는 것을 방지하기 위해", "똑같은 말을 반복하려고", "글자 수를 늘리려고", "비용을 높이려고"], "문장의 중간이 잘려 문맥의 의미가 훼손되는 것을 방지하기 위해", "조금씩 겹쳐야 문서의 전체적인 맥락이 끊기지 않고 벡터에 잘 반영됩니다. 오버랩이 너무 작으면 청크 경계에서 문장이 잘리고, 너무 크면 중복 정보가 저장 공간을 낭비합니다. 통상 청크 크기의 10~20%가 적절합니다.", "Overlap", "5037", "hard"),
    ("에이전트가 도구 사용을 거부하고 '직접 답'을 하려고만 한다면 고쳐야 할 부분은?", ["모델 가중치", "도구 사용법을 명확히 하고, 반드시 도구를 쓰도록 강조한 프롬프트(지침)", "마우스 마찰력", "방 안의 온도", "모니터 주사율"], "도구 사용법을 명확히 하고, 반드시 도구를 쓰도록 강조한 프롬프트(지침)", "시스템 프롬프트의 지시 강도를 높여 도구 사용의 당위성을 인지시켜야 합니다. '반드시 도구를 사용하여 답변하라. 도구 없이 스스로 답하지 말 것' 같은 명시적 지시가 효과적입니다.", "도구 사용 지시", "5038", "hard"),
    ("RAGAS 프레임워크가 평가에 사용하는 주된 동력은?", ["사람의 설문조사", "LLM(거대 언어 모델) 자체를 평가자로 활용 (LLM-as-a-judge)", "단어 개수 세기", "파일 크기 측정", "랜덤 점수 부여"], "LLM(거대 언어 모델) 자체를 평가자로 활용 (LLM-as-a-judge)", "사람보다 빠르고 객관적인 기준(수식)으로 RAG의 품질 점수를 자동 산출합니다. GPT-4 등 강력한 LLM이 평가자로 쓰이며, 인간 평가자와 높은 일치율을 보입니다. 대규모 평가 자동화가 가능한 것이 핵심 장점입니다.", "RAGAS", "5039", "hard"),
    ("교재 5장을 학습하며 우리가 만들 수 있는 최종적인 형태는?", ["단순한 말동무 챗봇", "특정 도메인의 전문 지식을 검색하고 직접 작업을 수행하는 인텔리전트 에이전트", "타이핑 연습 게임", "인터넷 검색 엔진", "컴퓨터 수리 도구"], "특정 도메인의 전문 지식을 검색하고 직접 작업을 수행하는 인텔리전트 에이전트", "AI의 지능과 외부 데이터, 외부 도구가 결합된 진정한 애플리케이션의 핵심입니다. RAG와 에이전트를 결합하면 최신 지식 기반으로 실제 작업까지 자율 수행하는 강력한 AI 어시스턴트를 구현할 수 있습니다.", "학습 결과", "5040", "hard"),

    # 3. 상황별 RAG/Agent 응용 시나리오 (41-70)
    ("복잡한 법률 판례를 RAG로 구축할 때 가장 중요한 청킹 전략은?", ["그냥 100글자씩 자르기", "법조항 섹션이나 판결 요지 단위로 의미를 보존하며 자르기", "아무렇게나 자르기", "자르지 않기", "영어로만 자르기"], "법조항 섹션이나 판결 요지 단위로 의미를 보존하며 자르기", "법률 문서는 구조가 중요하므로 의미적 완결성을 가진 단위로 나누어야 검색 정확도가 높습니다. 법조항 번호, 판결 요지 등 문서의 논리 구조를 기준으로 청킹해야 검색된 결과가 법적으로 유효한 맥락을 담습니다.", "법률 RAG", "5041", "easy"),
    ("에이전트가 '오늘 날씨'를 알려달라는 질문을 받았을 때 필요한 도구는?", ["계산기", "실시간 날씨 정보 API 연동 도구", "이미지 생성기", "소설 쓰기 도구", "파일 삭제기"], "실시간 날씨 정보 API 연동 도구", "학습 데이터에는 오늘 날씨가 없으므로 외부 API 호출이 필수적입니다. 에이전트가 날씨 API나 검색 엔진을 Tool로 등록해 실시간 정보를 가져오는 대표적인 사례입니다.", "날씨 에이전트", "5042", "medium"),
    ("검색 속도를 개선하기 위해 로컬 컴퓨터 메모리에 벡터를 올리는 라이브러리는?", ["FAISS (Facebook AI Similarity Search)", "Excel", "Notepad", "PowerPoint", "Calculator"], "FAISS (Facebook AI Similarity Search)", "고밀도 벡터 검색을 CPU/GPU에서 고속으로 수행해주는 오픈소스 라이브러리입니다. FAISS는 Facebook AI Research가 개발했으며, IVF·HNSW 등 다양한 인덱스를 지원합니다.", "FAISS", "5043", "medium"),
    ("RAG 시스템 구축 후 답변이 너무 느리다면 체크해야 할 단계는?", ["사용한 글꼴", "임베딩 생성 시간 및 검색 단계의 레이턴시(Latency)", "사용자의 의자 높이", "마우스 패드 재질", "방 안의 습도"], "임베딩 생성 시간 및 검색 단계의 레이턴시(Latency)", "문서 로딩이나 벡터 검색 과정에서 병목이 생기는지 확인해야 합니다. 각 단계의 실행 시간을 로깅하여 가장 느린 구간을 우선 개선해야 지연을 줄일 수 있습니다.", "속도 개선", "5044", "medium"),
    ("에이전트가 SQL 쿼리를 직접 짜서 DB를 조회하게 할 때의 위험성과 해법은?", ["위험성 없음", "잘못된 쿼리로 데이터가 삭제될 수 있으므로 전용 읽기 권한(Read-only) 계정을 부여한다.", "AI는 SQL을 모른다.", "데이터가 너무 많아진다.", "컴퓨터가 폭발한다."], "잘못된 쿼리로 데이터가 삭제될 수 있으므로 전용 읽기 권한(Read-only) 계정을 부여한다.", "보안과 데이터 무결성을 위해 실행 권한을 최소화하는 하드닝 작업이 필요합니다. 최소 권한 원칙(Least Privilege)을 적용해 DB 쓰기·삭제 권한을 제한해야 합니다.", "SQL 에이전트 보안", "5045", "medium"),
    ("고객 상담 RAG에서 '질문-답변 쌍'을 수만 개 저장했을 때 효과적인 필터링은?", ["무조건 다 읽기", "카테고리 메타데이터를 활용한 필터링 후 검색", "가나다 순 검색", "최근 저장 순 검색", "파일 이름 검색"], "카테고리 메타데이터를 활용한 필터링 후 검색", "검색 범위를 미리 좁히면 속도와 정확도가 동시에 향상됩니다. 날짜, 카테고리, 언어 등 메타데이터 기반 사전 필터링이 대규모 벡터 DB에서 필수적인 최적화 기법입니다.", "대량 데이터 검색", "5046", "medium"),
    ("사용자의 질문이 너무 짧아(예: '그거 알려줘') 검색이 안 될 때 에이전트의 대처는?", ["모른다고 화내기", "질문의 의도를 다시 물어보며 정보를 구체화해달라고 요청하기", "아무거나 알려주기", "인터넷 연결 끊기", "로그아웃 시키기"], "질문의 의도를 다시 물어보며 정보를 구체화해달라고 요청하기", "대화형 에이전트의 강점을 살려 부족한 정보를 사용자에게 되묻는 지능적 행동입니다. 이를 Clarification Request(명확화 요청)라 하며, 불완전한 입력으로 인한 오답을 방지합니다.", "질문 구체화", "5047", "medium"),
    ("임베딩 모델의 성능이 한국어에서 떨어진다면?", ["한국어를 포기한다.", "한국어 데이터로 학습된 특화 임베딩 모델(Ko-Embedding)을 검토한다.", "영어로만 모든 문서를 바꾼다.", "키보드를 한글용으로 바꾼다.", "파이썬 버전을 높인다."], "한국어 데이터로 학습된 특화 임베딩 모델(Ko-Embedding)을 검토한다.", "언어적 뉘앙스를 잘 파악하는 로컬 특화 모델이 RAG 품질을 좌우합니다. 한국어 특화 임베딩 모델(예: KLUE-BERT)이 영문 범용 모델보다 한국어 검색에서 더 높은 정확도를 보입니다.", "한국어 임베딩", "5048", "medium"),
    ("검색된 문서 내용이 상충할 때(A문서는 된다, B문서는 안 된다) 모델에게 줄 가이드는?", ["아무거나 믿어라", "최신 날짜의 문서를 우선시하거나, 상충하는 내용을 모두 보여주며 판단을 돕게 한다.", "답변을 하지 마라", "화내라", "둘을 합쳐서 제3의 답변을 지어내라"], "최신 날짜의 문서를 우선시하거나, 상충하는 내용을 모두 보여주며 판단을 돕게 한다.", "정보의 일관성을 관리하는 정책을 프롬프트나 로직에 반영해야 합니다. 메타데이터의 created_at 또는 updated_at 필드를 기준으로 최신 문서에 높은 가중치를 부여하는 방식이 실무에서 자주 쓰입니다.", "정보 상충 해결", "5049", "hard"),
    ("RAG 시스템을 웹 서비스로 배포할 때 사용하는 LangChain 호환 도구는?", ["LangServe", "HTML Edit", "Paint", "Excel Online", "Minesweeper"], "LangServe", "작성한 체인을 REST API 형태로 즉시 공개해주는 배포 특화 도구입니다. LangServe는 LangChain 공식 서빙 라이브러리로 FastAPI를 내부적으로 사용해 /invoke, /stream 엔드포인트를 자동 생성합니다.", "LangServe", "5050", "medium"),
    ("에이전트가 도구를 실행한 후 나온 '결과'를 ReAct에서 부르는 용어는?", ["Thought", "Action", "Observation (관찰)", "Final Answer", "Input"], "Observation (관찰)", "도구의 출력값을 통해 모델이 현 상황을 파악하는 단계를 의미합니다. ReAct 패턴의 Observation 단계가 대표적이며, 도구 결과를 바탕으로 다음 Action을 결정합니다.", "Observation", "5051", "medium"),
    ("RAG 개발 시 문서를 벡터화하는 작업을 미리 해두는 과정을 무엇이라 하나?", ["Online Ingestion", "Offline Indexing / Ingestion", "Real-time Chat", "Slow Reading", "Deleting"], "Offline Indexing / Ingestion", "사용자 질문 전에 데이터를 미리 준비해두는 배치 작업입니다. Offline Indexing이라고도 하며, 문서를 청킹·임베딩·저장해두어 실시간 응답 속도를 크게 향상시킵니다.", "Indexing", "5052", "medium"),
    ("모델이 답변 도중 '출처: 교재 123페이지'라고 적게 하려면?", ["모델이 알아서 한다.", "프롬프트에 '검색된 문서의 메타데이터 중 페이지 정보를 반드시 명시해'라고 지시한다.", "페이지를 다 외우게 한다.", "가짜 번호를 적는다.", "페이지 번호를 다 지운다."], "프롬프트에 '검색된 문서의 메타데이터 중 페이지 정보를 반드시 명시해'라고 지시한다.", "근거 제시(Citation)는 RAG 시스템의 신뢰도를 높여주는 강력한 장치입니다. Document 객체의 metadata 딕셔너리에 page, source 등 출처 정보를 저장해두면 프롬프트에서 이를 참조하여 인용 표시를 자동화할 수 있습니다.", "출처 명시", "5053", "hard"),
    ("여러 개의 질문을 한 번에 처리하는 에이전트 효율화 기법은?", ["한 개씩 한다.", "Async(비동기) 처리나 배치 처리를 활용한다.", "컴퓨터를 여러 대 산다.", "사용자를 기다리게 한다.", "질문을 지운다."], "Async(비동기) 처리나 배치 처리를 활용한다.", "동시에 여러 지식 소스를 검색하거나 도구를 돌려 응답 시간을 단축합니다. asyncio나 ThreadPoolExecutor를 이용한 병렬 호출로 순차 실행 대비 응답 지연을 크게 줄일 수 있습니다.", "비동기 처리", "5054", "medium"),
    ("RAG 시스템 성능 측정 도구 'Ragas'에서 Faithfulness가 0.1이라면?", ["아주 훌륭하다.", "모델이 검색된 근거와 무관한 소설을 쓰고 있다는 매우 위험한 신호다.", "컴퓨터가 고장 났다.", "점수가 원래 낮다.", "무시해도 된다."], "모델이 검색된 근거와 무관한 소설을 쓰고 있다는 매우 위험한 신호다.", "근거 충실도가 낮으므로 프롬프트를 고치거나 검색 품질을 점검해야 합니다. Faithfulness 지표가 낮으면 먼저 검색된 청크의 관련성을 확인하고, 그 다음 생성 프롬프트를 수정합니다.", "저점수 분석", "5055", "medium"),
    ("에이전트가 '반복 루프'에 빠졌을 때 터미널 로그에서 확인해야 할 것은?", ["프롬프트 색깔", "Thought와 Action이 동일한 내용으로 반복되는지 여부", "내 아이디", "오늘 날짜", "파이썬 로고"], "Thought와 Action이 동일한 내용으로 반복되는지 여부", "논리가 막혔거나 도구 설명이 모호할 때 발생하는 전형적인 에이전트 에러입니다. 이를 방지하려면 각 도구의 Description을 명확하게 작성하고 예시 입력을 함께 제공해야 합니다.", "루프 확인", "5056", "medium"),
    ("청킹 시 '의미 구조'를 파악하여 제목과 본문을 연결해두면 좋은 점은?", ["파일 이름이 예뻐진다.", "검색 시 본문만 나오는 게 아니라 제목이라는 문맥 정보도 함께 제공되어 정확도가 오른다.", "똑같은 지식이 두 번 저장된다.", "용량이 늘어난다.", "속도가 느려진다."], "검색 시 본문만 나오는 게 아니라 제목이라는 문맥 정보도 함께 제공되어 정확도가 오른다.", "상위 카테고리 정보가 포함된 청크는 모델이 정보를 파악하는 데 훨씬 유리합니다. 부모 문서나 섹션 헤더를 청크에 추가하는 Contextual Chunking 기법이 답변 품질을 높입니다.", "구조화 청킹", "5057", "medium"),
    ("RAG 시스템에서 '토큰 비용'을 가장 많이 잡아먹는 단계는?", ["모델의 1글자 답변", "다량의 검색 결과 조각을 프롬프트에 통째로 밀어넣는 Augmentation 단계", "마우스 클릭", "파일 저장", "윈도우 업데이트"], "다량의 검색 결과 조각을 프롬프트에 통째로 밀어넣는 Augmentation 단계", "검색 결과를 너무 많이 넣으면 입력 토큰량이 급증하여 비용이 상승합니다. 실무에서는 k=3~5개의 청크가 품질과 비용의 균형을 맞추는 적정 범위로 많이 사용됩니다.", "비용 병목", "5058", "medium"),
    ("에이전트에게 '전문가 페르소나'를 부여하는 것이 도구 사용과 관련이 있나?", ["상관없다.", "네, 전문가로서 어떤 상황에 어떤 도구를 쓰는 것이 논리적인지 더 잘 판단하게 돕는다.", "전혀 아니다.", "모델이 기분 나빠한다.", "돈이 더 든다."], "네, 전문가로서 어떤 상황에 어떤 도구를 쓰는 것이 논리적인지 더 잘 판단하게 돕는다.", "페르소나는 에이전트의 판단 로직 전반에 가이드라인 역할을 합니다. System Prompt에 '당신은 보안 전문가입니다'라고 명시하면 도구 선택과 답변 방향이 달라집니다.", "페르소나와 도구", "5059", "medium"),
    ("최종적으로 RAG와 에이전트를 결합했을 때의 모습은?", ["단순 텍스트 생성기", "외부 지식을 스스로 찾아 학습하고 실제 업무(API 호출 등)를 수행하는 인공지능 비서", "인터넷 게시판", "성적표 계산기", "게임 캐릭터"], "외부 지식을 스스로 찾아 학습하고 실제 업무(API 호출 등)를 수행하는 인공지능 비서", "생성 AI가 실질적인 비즈니스 가치를 창출하는 가장 강력한 워크플로우입니다. RAG로 지식을 검색하고 Agent로 행동을 수행하는 결합 패턴이 실무 자동화의 핵심입니다.", "에이전틱 RAG", "5060", "medium"),

    # 추가 40문제 (응용)
    ("LangChain의 'Memory' 옵션 중 'ConversationSummaryMemory'의 장점은?", ["모든 대화를 다 저장한다.", "긴 대화 내역을 요약해서 보관하므로 토큰 사용량을 효율적으로 관리할 수 있다.", "비밀번호를 외운다.", "사진을 저장한다.", "인터넷이 빨라진다."], "긴 대화 내역을 요약해서 보관하므로 토큰 사용량을 효율적으로 관리할 수 있다.", "대화가 길어져도 핵심 맥락을 유지하면서 비용을 절감하는 영리한 방법입니다. ConversationBufferMemory는 모든 내용을 그대로 저장하여 토큰이 빠르게 소진되지만, ConversationSummaryMemory는 LLM으로 요약하여 핵심만 유지합니다.", "Summary Memory", "5061", "easy"),
    ("검색된 문서가 너무 많아 모델의 Context Window를 초과할 때의 대처법은?", ["모델을 바꾼다.", "검색 결과를 요약해서 넣거나 리랭킹을 통해 상위 3개만 추려 넣는다.", "컴퓨터를 끈다.", "글자를 작게 적는다.", "영어로 번역한다."], "검색 결과를 요약해서 넣거나 리랭킹을 통해 상위 3개만 추려 넣는다.", "입력 제한을 지키면서 알짜 정보만 전달하는 엔지니어링이 필요합니다. MapReduceDocumentsChain이나 StuffDocumentsChain 등을 활용하면 다수의 문서를 요약 후 합치거나 필터링할 수 있습니다.", "컨텍스트 초과 대처", "5062", "easy"),
    ("RAG 시스템에서 'Semantic Search'가 'Keyword Search'보다 나은 점은?", ["오타가 나면 검색이 안 된다.", "단어가 일치하지 않아도 의미적으로 유사한 내용을 찾아낼 수 있다.", "속도가 훨씬 빠르다.", "가격이 무료다.", "모델이 안 필요하다."], "단어가 일치하지 않아도 의미적으로 유사한 내용을 찾아낼 수 있다.", "자연어의 맥락을 파악하므로 사용자 의도에 훨씬 부합하는 결과를 줍니다. '강아지 먹이'와 '반려견 사료'처럼 다른 단어라도 의미가 같으면 유사도가 높게 측정됩니다.", "의미 검색 장점", "5063", "easy"),
    ("에이전트 프롬프트에 'Thought:'라고 형식을 지정해주는 이유는?", ["모델을 놀리려고", "모델이 자신의 추론 과정을 명시적으로 적도록 강제하여 정답률을 높이기 위해", "서버 이름을 지으려고", "글자 수 채우려고", "내 이름 쓰려고"], "모델이 자신의 추론 과정을 명시적으로 적도록 강제하여 정답률을 높이기 위해", "CoT와 마찬가지로 중간 사고 단계를 거치게 함으로써 실수를 방지합니다. Tree-of-Thought(ToT)는 여러 사고 경로를 동시에 탐색하여 최적 경로를 선택하는 고급 추론 기법입니다.", "Thought 형식", "5064", "medium"),
    ("RAG 시스템 평가 지표 중 'Answer Relevance'가 낮다면 원인은?", ["검색은 잘 됐으나 모델이 질문과 무관한 엉뚱한 답변을 함", "인터넷이 끊김", "모델이 너무 똑똑함", "사용자가 질문을 안 함", "파일이 삭제됨"], "검색은 잘 됐으나 모델이 질문과 무관한 엉뚱한 답변을 함", "검색 품질보다는 모델의 생성 능력이나 가이드라인(프롬프트)에 문제가 있는 경우입니다. 정확한 청크가 컨텍스트에 있어도 프롬프트가 부실하면 엉뚱한 답변이 생성될 수 있습니다.", "Answer Relevance", "5065", "medium"),
    ("에이전트가 도구를 사용할 때 'Observation' 값을 읽지 못한다면?", ["모델을 때린다.", "도구의 반환 형식이 문자열(String) 등 모델이 읽기 쉬운 형태인지 확인한다.", "파일을 다 지운다.", "모니터를 닦는다.", "인터넷을 바꾼다."], "도구의 반환 형식이 문자열(String) 등 모델이 읽기 쉬운 형태인지 확인한다.", "데이터 파이프라인의 입출력 형식이 맞아야 에이전트가 다음 판단을 내릴 수 있습니다. Pydantic BaseModel이나 JSON Schema로 입출력 타입을 선언하면 파이프라인 안정성이 높아집니다.", "관찰값 확인", "5066", "medium"),
    ("사내 RAG 서버에서 'PDF 테이블'이 텍스트로만 읽혀 구조가 깨진다면?", ["표를 직접 그린다.", "표 구조를 인식하는 전용 Loader나 레이아웃 분석 모델을 활용한다.", "표를 무시한다.", "숫자만 다 지운다.", "PDF를 사진으로 찍는다."], "표 구조를 인식하는 전용 Loader나 레이아웃 분석 모델을 활용한다.", "데이터의 레이아웃을 보존하며 파싱하는 기술이 고도화된 RAG의 품질을 결정합니다. PyPDF2, Unstructured, pdfminer 같은 라이브러리로 표·이미지·헤더 구조를 유지해 파싱합니다.", "표 인식", "5067", "medium"),
    ("LangChain의 'RouterChain'을 사용하여 얻을 수 있는 효과는?", ["모든 명령을 한 곳으로 보낸다.", "질문의 주제에 따라 서로 다른 프롬프트나 DB 검색 경로로 자동 배정한다.", "인터넷 속도가 빨라진다.", "비용이 무조건 0원이다.", "컴퓨터가 알아서 꺼진다."], "질문의 주제에 따라 서로 다른 프롬프트나 DB 검색 경로로 자동 배정한다.", "효율적인 작업 분배를 통해 전문성 있는 답변을 가능하게 합니다. 예를 들어 금융 질문은 금융 전용 체인, 법률 질문은 법률 전용 체인으로 라우팅하여 각 도메인에 최적화된 응답을 얻을 수 있습니다.", "RouterChain", "5068", "hard"),
    ("RAG 개발 시 '임베딩 모델'과 '생성 모델'의 회사가 달라도 되나?", ["절대 안 된다.", "상관없지만, 임베딩 모델의 차원과 벡터 DB 설정은 일치해야 한다.", "회사 이름이 같아야 한다.", "모델을 섞으면 폭발한다.", "아무도 시도하지 않았다."], "상관없지만, 임베딩 모델의 차원과 벡터 DB 설정은 일치해야 한다.", "다양한 모델을 조합(Mix & Match)하여 최적의 가성비를 찾는 것이 실무입니다. 간단한 요약엔 경량 모델, 복잡한 추론엔 대형 모델을 라우팅하는 전략이 비용 효율을 높입니다.", "모델 조합", "5069", "medium"),
    ("에이전트가 '반복적인 질문'을 받을 때 성능을 높이는 메모리 기법은?", ["다 잊어버리기", "이전 답변을 캐싱(Caching)하여 동일한 질문엔 빠르게 답하기", "질문을 무시하기", "일부러 틀리기", "돈을 더 내기"], "이전 답변을 캐싱(Caching)하여 동일한 질문엔 빠르게 답하기", "반복적인 인프라 비용과 응답 지연을 방지하는 실용적인 방법입니다. 동일 쿼리의 임베딩·검색 결과를 Redis나 메모리에 캐싱하면 API 호출 비용과 응답 지연을 크게 줄입니다.", "캐싱", "5070", "medium"),
    ("RAG 파이프라인 중 'Semantic Chunking'이란?", ["글자 수로 자르기", "의미가 변하는 지점을 감지하여 논리적 단락 단위로 자르기", "아무렇게나 자르기", "영어로 자르기", "숫자만 자르기"], "의미가 변하는 지점을 감지하여 논리적 단락 단위로 자르기", "단순 글자 수보다 훨씬 정교하게 지식의 맥락을 보존하는 청킹 방식입니다. 문장·단락 경계를 기준으로 분할하므로 청크 내부 의미가 유지되어 검색 정확도가 향상됩니다.", "Semantic Chunking", "5071", "medium"),
    ("에이전트의 '자율성'을 제한하고 사람이 승인할 때만 실행하게 하는 설정은?", ["AI의 반란 방지", "Human-in-the-loop (사람의 개입)", "AI 정지", "사용자 차단", "서버 종료"], "Human-in-the-loop (사람의 개입)", "안전이 중요한 작업(예: 결제, 이메일 발송)에서 필수적인 설계 패턴입니다. Human-in-the-loop으로 위험 행동 전에 인간 승인 단계를 두어 자율 에이전트의 오작동을 방지합니다.", "Human-in-the-loop", "5072", "medium"),
    ("RAG 시스템 평가 시 'Context Precision'이란?", ["답변이 얼마나 긴가", "검색된 문서들 중 실제 질문과 관련된 문서가 상위에 잘 배치되었는가", "화질이 좋은가", "글꼴이 예쁜가", "오타가 없는가"], "검색된 문서들 중 실제 질문과 관련된 문서가 상위에 잘 배치되었는가", "검색 품질의 정교함을 나타내는 지표 중 하나입니다. MRR(Mean Reciprocal Rank)은 정답 문서가 검색 결과 상위 몇 번째에 위치하는지를 수치화하며, 높을수록 검색 성능이 좋습니다.", "Context Precision", "5073", "medium"),
    ("에이전트가 어떤 도구를 썼는지 사용자에게 보여주지 않는 'Private Agent'를 만드는 법은?", ["그냥 숨기기", "중간 과정(Intermediate Steps)을 사용자 응답 메시지에서 제외하도록 구현하기", "아무것도 안 하기", "파일 지우기", "비밀번호 걸기"], "중간 과정(Intermediate Steps)을 사용자 응답 메시지에서 제외하도록 구현하기", "사용자에게는 결과만 깔끔하게 보여주기 위한 UX 설계입니다. AgentExecutor의 return_intermediate_steps=False가 기본값이므로, 출력 처리 로직에서 'output' 키의 값만 추출하여 응답합니다.", "Private Agent", "5074", "hard"),
    ("벡터 DB 인덱싱 중 '브루트 포스(Brute-force)' 방식의 특징은?", ["가장 빠르다.", "모든 벡터와 하나하나 대조하므로 매우 정확하지만 데이터가 많으면 심각하게 느리다.", "아무도 안 쓴다.", "가장 싸다.", "예쁘다."], "모든 벡터와 하나하나 대조하므로 매우 정확하지만 데이터가 많으면 심각하게 느리다.", "데이터 양이 적을 때나 정확도 100%가 필요할 때만 제한적으로 쓰입니다. Brute Force 검색은 O(N) 비용이라 대규모에서 HNSW·IVF 같은 근사 최근접 이웃(ANN) 알고리즘으로 대체합니다.", "Brute-force", "5075", "medium"),
    ("에이전팅 시스템의 발열이나 리소스 낭비를 막기 위해 필요한 것은?", ["에어컨", "에이전트의 실행 시간을 제한하는 타임아웃(Timeout) 설정", "컴퓨터 끄기", "찬물 끼얹기", "질문 무시"], "에이전트의 실행 시간을 제한하는 타임아웃(Timeout) 설정", "응답 지연이 너무 길어지면 자원을 반납하게 하여 시스템 안정성을 지킵니다. Timeout 설정 없이 무한 대기하는 요청이 쌓이면 서버 스레드 풀이 고갈되어 전체 장애로 이어집니다.", "Timeout", "5076", "medium"),
    ("RAG에서 'Multimodal Retrieval'의 특징은?", ["글자만 찾기", "이미지, 오디오 등 텍스트 이외의 데이터도 벡터로 검색하기", "여러 번 찾기", "한 명만 찾기", "거짓말 찾기"], "이미지, 오디오 등 텍스트 이외의 데이터도 벡터로 검색하기", "다양한 매체의 정보를 의미 기반으로 통합 검색하는 고난도 기술입니다. CLIP 같은 멀티모달 임베딩 모델이 텍스트와 이미지를 동일 벡터 공간에 표현해 통합 검색을 가능하게 합니다.", "Multimodal RAG", "5077", "medium"),
    ("에이전트가 답변을 낼 때 '저는 AI라서 몰라요'라고만 한다면?", ["AI가 맞다.", "프롬프트의 페르소나와 작업 수행 의지를 보강하고 제약 사항을 완화한다.", "AI를 그만 쓴다.", "질문을 지운다.", "다른 사람에게 물어본다."], "프롬프트의 페르소나와 작업 수행 의지를 보강하고 제약 사항을 완화한다.", "모델의 방어적인 태도를 능동적인 문제 해결 모드로 전환시켜야 합니다. '당신은 전문가입니다, 최선의 답을 주세요'와 같은 긍정적 지시 프롬프트가 방어적 답변을 줄이는 데 효과적입니다.", "능동성 주입", "5078", "medium"),
    ("벡터 DB의 데이터를 주기적으로 동기화해야 하는 이유는?", ["용량을 채우려고", "원본 지식 베이스의 변경 사항이 RAG 시스템에 최신 상태로 반영되어야 하므로", "인터넷 속도 때문에", "컴퓨터가 심심해서", "비용을 내려고"], "원본 지식 베이스의 변경 사항이 RAG 시스템에 최신 상태로 반영되어야 하므로", "정보가 죽은 정보가 되지 않도록 지속적으로 신선함을 유지하는 파이프라인이 필수입니다. 정기 크롤링, 문서 갱신 감지, 델타 인덱싱을 조합해 벡터 DB를 최신 상태로 유지합니다.", "데이터 동기화", "5079", "medium"),
    ("성공적인 RAG/에이전트 개발자가 되기 위한 마음가짐은?", ["한 번에 완벽할 수 있다.", "데이터, 임베딩, 검색, 생성을 끊임없이 실험하고 측정하며 다듬어야 한다.", "남의 코드를 복사만 한다.", "운에 맡긴다.", "컴퓨터를 비싼 걸 산다."], "데이터, 임베딩, 검색, 생성을 끊임없이 실험하고 측정하며 다듬어야 한다.", "모든 단계가 유기적으로 얽혀 있으므로 전 과정을 세심하게 튜닝하는 끈기가 핵심입니다. 청킹 방식, 임베딩 모델, 프롬프트, LLM 선택 각각이 최종 답변 품질에 누적적으로 영향을 줍니다.", "엔지니어의 태도", "5080", "medium"),

    # 남은 20문제
    ("RAG 시스템에서 '질문(Query)'을 임베딩할 때와 '문서(Document)'를 임베딩할 때의 모델은?", ["서로 다른 회사 제품이어야 한다.", "반드시 동일한 임베딩 모델과 동일한 벡터 차원을 사용해야 한다.", "모델을 안 써도 된다.", "아무거나 써도 된다.", "영문 모델만 써야 한다."], "반드시 동일한 임베딩 모델과 동일한 벡터 차원을 사용해야 한다.", "같은 의미 공간(Vector Space) 상에 있어야 유사도 비교가 가능합니다. 쿼리와 문서에 서로 다른 임베딩 모델을 사용하면 코사인 유사도 계산이 무의미해져 검색 성능이 급락합니다.", "동일 모델 사용", "5081", "medium"),
    ("에이전트가 도구의 '파라미터' 형식을 자꾸 틀린다면?", ["모델을 비난한다.", "도구 정의 시 Pydantic 등을 사용해 데이터 형식을 명확히 정의하고 프롬프트로 가이드한다.", "타이핑을 대신 해준다.", "형식을 없앤다.", "인터넷을 바꾼다."], "도구 정의 시 Pydantic 등을 사용해 데이터 형식을 명확히 정의하고 프롬프트로 가이드한다.", "모델이 어떤 데이터 타입을 넣어야 하는지 엄격하게 인지시키는 것이 에이전트의 안정성입니다. Pydantic BaseModel이나 JSON Schema로 입출력 타입을 선언하면 잘못된 인자 전달을 방지합니다.", "파라미터 가이드", "5082", "medium"),
    ("RAG 파이프라인 성능을 시각적으로 모니터링해주는 랭체인 서비스는?", ["LangSmith", "LangPaint", "LangExcel", "LangWorld", "LangView"], "LangSmith", "복잡한 체인의 단계별 입출력을 트래킹하여 디버깅을 돕는 필수 서비스입니다. 각 체인 실행의 Trace를 기록하고 레이턴시, 토큰 사용량, 오류 등을 대시보드로 시각화합니다.", "LangSmith", "5083", "easy"),
    ("검색된 문서의 '날짜'가 틀리다면 어떤 정보를 업데이트해야 하나?", ["모델 이름", "벡터 데이터베이스 내 조각들의 메타데이터(Metadata)", "내 나이", "모니터 시계", "키보드 한영키"], "벡터 데이터베이스 내 조각들의 메타데이터(Metadata)", "메타데이터는 가공된 데이터의 상세 속성을 담고 있어 정확한 필터링과 출처 표시를 돕습니다. 파일명, 날짜, 챕터, 저자를 메타데이터로 저장하면 선택적 검색과 출처 인용이 가능합니다.", "메타데이터 수정", "5084", "medium"),
    ("에이전트가 도구 사용 도중 '에러 메시지'를 받으면 어떻게 대처하나?", ["즉시 중단한다.", "루프 체계에 따라 에러 메시지를 다시 '관찰'값으로 받아 스스로 수정을 시도하게 설계한다.", "사용자에게 욕한다.", "컴퓨터를 끈다.", "무시한다."], "루프 체계에 따라 에러 메시지를 다시 '관찰'값으로 받아 스스로 수정을 시도하게 설계한다.", "에러 자가 수정은 고도로 능동적인 에이전트의 특징입니다. 코드 실행 오류를 스스로 분석하고 재생성하는 Self-Healing Agent 패턴으로 운영 안정성을 높일 수 있습니다.", "에러 자가 수정", "5085", "medium"),
    ("RAG에서 'Context Relevance'가 0점이라면?", ["답변이 짧다.", "검색된 문서가 원래 질문과 아무런 상관이 없는 쓰레기 정보였다는 뜻이다.", "만점이다.", "컴퓨터가 꺼졌다.", "파일이 많다."], "검색된 문서가 원래 질문과 아무런 상관이 없는 쓰레기 정보였다는 뜻이다.", "검색 알고리즘이나 임베딩 모델의 품질을 원점에서 재검토해야 함을 시사합니다. Context Relevance가 낮으면 임베딩 모델 교체, 청킹 전략 변경, 또는 하이브리드 검색 도입을 고려해야 합니다.", "Context Relevance 중요성", "5086", "easy"),
    ("에이전트의 '최종 답변'에 도달했음을 알려주는 지시자는?", ["Stop", "Final Answer:", "The End", "Bye", "Logout"], "Final Answer:", "이 접두어(Prefix) 뒤의 텍스트가 사용자에게 전달될 최종 결과임을 모델에게 인지시킵니다. 'Final Answer:'처럼 특정 구분자 뒤를 파싱하는 패턴이 ReAct 에이전트에서 흔히 쓰입니다.", "Final Answer", "5087", "medium"),
    ("RAG에서 'Hybrid Search' 도입 시 조절하는 알파(Alpha) 값의 의미는?", ["모델의 지능", "벡터 검색(의미)과 키워드 검색(정확)의 비중을 가중하여 합계 점수를 내는 비율", "인터넷 속도", "비용 할인율", "글자 크기"], "벡터 검색(의미)과 키워드 검색(정확)의 비중을 가중하여 합계 점수를 내는 비율", "0.5는 반반, 1.0은 오직 벡터 검색만 하는 등 밸런스를 맞추는 값입니다. Hybrid Search의 alpha 파라미터는 BM25 키워드 검색과 벡터 시맨틱 검색의 가중치 비율을 결정합니다.", "Alpha 값", "5088", "medium"),
    ("에이전트 시스템에서 '라우팅(Routing)'이란?", ["네트워크 선 연결하기", "사용자의 의도에 따라 어떤 에이전트나 어떤 파이프라인으로 보낼지 정하는 교통정리", "길 찾기", "비행기 타기", "가장 빠른 길 검색"], "사용자의 의도에 따라 어떤 에이전트나 어떤 파이프라인으로 보낼지 정하는 교통정리", "효율적인 자원 배분과 요구사항 해결을 위한 분기 로직입니다. Query Router는 질문 유형을 분류해 FAQ 검색, 코드 실행, 웹 서칭 등 최적 도구로 자동 라우팅합니다.", "Routing", "5089", "medium"),
    ("RAG 시스템 운영 시 '토큰 절약'을 위해 문장의 중요 실질어만 남기는 처리는?", ["Text Deletion", "Stopword Removal (불용어 제거)", "Copy & Paste", "Bold Text", "Underline"], "Stopword Removal (불용어 제거)", "은, 는, 이, 가 같은 불필요한 단어를 걷어내어 토큰 효율을 높이는 기교입니다. 형태소 분석 후 불용어(Stop Words) 제거가 한국어 RAG 전처리의 핵심 단계 중 하나입니다.", "불용어 제거", "5090", "medium"),
    ("에이전트 도구로 '웹 검색'을 추가했을 때의 이점은?", ["전기가 아껴진다.", "모델 학습 데이터 이후의 최신 정보를 실시간으로 탐색할 수 있다.", "AI가 사람처럼 변한다.", "돈을 더 많이 번다.", "재미있다."], "모델 학습 데이터 이후의 최신 정보를 실시간으로 탐색할 수 있다.", "RAG와 결합된 웹 서칭은 에이전트의 지식 한계를 무한히 확장합니다. Tavily, Bing Search API 등을 LangChain Tool로 등록하면 모델 학습 이후 정보도 실시간으로 검색 가능합니다.", "웹 검색 도구", "5091", "medium"),
    ("임베딩 모델의 '차원'이 1536이라면 벡터는 어떤 모양인가?", ["1536개의 숫자가 담긴 리스트 주머니", "1536개의 단어", "1536미터 길이의 줄", "1536개의 그림", "1536층 건물"], "1536개의 숫자가 담긴 리스트 주머니", "고차원 공간상의 한 지점을 가리키는 1536개의 수치 좌표입니다. OpenAI text-embedding-3-small은 1536차원, text-embedding-3-large는 3072차원 벡터를 생성하며 차원이 클수록 표현력이 높습니다.", "차원 모양", "5092", "medium"),
    ("RAG 파이프라인 중 'Document Preprocessing' 단계에서 하는 것은?", ["데이터 지우기", "HTML 태그 제거, 노이즈 텍스트 필터링, 정규화 등 데이터 정제", "이름 바꾸기", "폴더 이동하기", "파일 압축하기"], "HTML 태그 제거, 노이즈 텍스트 필터링, 정규화 등 데이터 정제", "깨끗한 데이터가 들어가야 벡터 값도 명확하고 답변도 깔끔하게 나옵니다. HTML 태그 제거, 중복 제거, 인코딩 정규화가 벡터 품질을 보장하는 기본 전처리 단계입니다.", "전처리", "5093", "medium"),
    ("에이전트가 '생각(Thought)' 단계에서 자신의 한계를 인지하면?", ["포기한다.", "사용자에게 추가 정보를 요청하거나 작업 불가 상황임을 보고한다.", "허풍을 친다.", "울음을 터트린다.", "로그아웃한다."], "사용자에게 추가 정보를 요청하거나 작업 불가 상황임을 보고한다.", "자신의 능력 범위를 알고 정직하게 소통하는 것 또한 훌륭한 에이전트의 기능입니다. 모르는 것을 솔직히 인정하는 에이전트가 환각을 줄이고 사용자의 신뢰를 높입니다.", "한계 인지", "5094", "medium"),
    ("RAG 시스템 성능이 '임의의 질문'에 대해 들쭉날쭉하다면?", ["운이 없다.", "다양한 시나리오가 담긴 벤치마킹 데이터셋으로 전수 검사를 수행해 병목을 찾는다.", "컴퓨터를 바꾼다.", "질문을 줄인다.", "모델 가격을 낮춘다."], "다양한 시나리오가 담긴 벤치마킹 데이터셋으로 전수 검사를 수행해 병목을 찾는다.", "일관성 있는 품질을 위해 체계적인 테스트와 튜닝이 뒷받침되어야 합니다. RAGAs, TruLens 같은 평가 프레임워크로 Faithfulness·Relevance를 주기적으로 측정하며 지속 개선해야 합니다.", "품질 안정화", "5095", "medium"),
    ("에이전트의 '자율 행동' 도중 비용이 폭주하지 않게 하려면?", ["돈을 안 낸다.", "입출력 토큰 제한과 중간 단계 실행 횟수의 엄격한 한도(Budget)를 둔다.", "서버를 부순다.", "질문을 무시한다.", "천천히 타이핑한다."], "입출력 토큰 제한과 중간 단계 실행 횟수의 엄격한 한도(Budget)를 둔다.", "운영 안정성과 경제성을 위해 반드시 적용해야 할 관리 장치입니다. Rate Limiting으로 API 남용을 방지하고 Cost Cap으로 예기치 않은 과금을 차단하는 것이 운영 필수 설정입니다.", "비용 관리", "5096", "medium"),
    ("RAG에서 'Small-to-Big Retrieval'이란?", ["작은 AI가 찾고 큰 AI가 답하기", "검색은 작은 조각(Sentence)으로 하고, 답은 그 주변 문맥(Paragraph)까지 포함해 하기", "파일 크기를 키우기", "데이터를 늘리기", "비용을 비싸게 하기"], "검색은 작은 조각(Sentence)으로 하고, 답은 그 주변 문맥(Paragraph)까지 포함해 하기", "검색 적중률과 문맥 전달력이라는 두 마리 토끼를 다 잡는 기법입니다. 작은 청크로 정밀하게 검색하되, 실제 모델에 전달하는 문맥은 그 청크를 포함한 상위 단락이나 문서 전체로 확대합니다.", "Small-to-Big", "5097", "hard"),
    ("에이전트 도구가 '파일 생성' 기능을 가졌을 때의 보안 관리법은?", ["마음대로 쓰게 둔다.", "시스템 전용 샌드박스(Sandbox) 환경에서만 작동하게 격리하여 보안을 지킨다.", "파일을 못 만들게 한다.", "사용자 비번을 준다.", "파일 이름을 공란으로 한다."], "시스템 전용 샌드박스(Sandbox) 환경에서만 작동하게 격리하여 보안을 지킨다.", "외부 시스템에 영향을 주지 않도록 안전한 테두리 안에서 돌려야 합니다. Sandbox 환경에서 코드 실행, DB 쓰기 등 위험한 작업을 격리해 실제 시스템 손상 없이 테스트합니다.", "도구 보안", "5098", "medium"),
    ("RAG 시스템 구축 후 '만족도 설문' 결과가 나쁘다면?", ["사용자를 차단한다.", "사용자 피드백을 Context에 넣어 수동으로 튜닝하거나 검색 상위 노출 순서를 보정한다.", "서비스를 종료한다.", "모델을 욕한다.", "가격표를 올린다."], "사용자 피드백을 Context에 넣어 수동으로 튜닝하거나 검색 상위 노출 순서를 보정한다.", "서비스는 항상 사용자의 실질적인 만족을 향해 피드백 루프를 돌아야 합니다. 낮은 점수를 받은 질문-답변 쌍을 분석하여 어느 단계(검색/생성)에서 실패했는지 파악하는 것이 중요합니다.", "피드백 반영", "5099", "hard"),
    ("에이전트와 RAG 기술의 공통된 최종 목표는?", ["AI를 인간보다 똑똑하게 만들기", "LLM을 실제 비즈니스 도메인에 연결하여 실질적이고 정확한 가치를 창출하는 것", "인터넷 속도 경쟁", "전력 소비 늘리기", "글자 많이 쓰기"], "LLM을 실제 비즈니스 도메인에 연결하여 실질적이고 정확한 가치를 창출하는 것", "인간의 지적 활동을 돕고 자동화하는 현실적인 솔루션으로서의 가치입니다. RAG 기반 검색, 도구 호출, 자동화 워크플로우가 이 가치를 실현하는 세 가지 핵심 기술입니다.", "최종 목표", "5100", "medium")
]

for q, o, a, w, h, i, d in mcq_data:
    questions.append({"chapter_name": chapter_name, "type": "객관식", "difficulty": d, "id": i, "question": q, "options": o, "answer": a, "why": w, "hint": h})

# --- 20 Code Completion Questions ---
cc_data = [
    ("RecursiveCharacterTextSplitter로 문서 청킹",
     "from langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext = '인공지능은 컴퓨터 과학의 분야입니다. ' * 100\n\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,\n    _____=50\n)\nchunks = splitter.split_text(text)\nprint(f'청크 수: {len(chunks)}')\nprint(f'첫 번째 청크 길이: {len(chunks[0])}')",
     "chunk_overlap",
     "chunk_overlap은 청크 간의 겹치는 문자 수입니다. 50으로 설정하면 연속된 청크가 50자 겹쳐 문맥이 이어집니다. 겹침이 없으면 중요 문장이 청크 경계에서 잘릴 수 있습니다."),

    ("Chroma 벡터 DB 생성 및 저장",
     "from langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\ndocs = [\n    'LangChain은 LLM 앱 개발 프레임워크입니다.',\n    'RAG는 검색 증강 생성의 약자입니다.',\n    'Chroma는 오픈소스 벡터 데이터베이스입니다.'\n]\n\nvectorstore = Chroma.from_texts(\n    _____,\n    embedding=OpenAIEmbeddings()\n)\nresults = vectorstore.similarity_search('벡터 DB란?', k=2)\nprint(results[0].page_content)",
     "docs",
     "Chroma.from_texts()는 텍스트 리스트를 임베딩하여 벡터 DB에 저장합니다. embedding 파라미터에 임베딩 모델을 지정합니다. similarity_search()로 쿼리와 가장 유사한 k개 문서를 검색합니다."),

    ("LCEL로 RAG 파이프라인 구성",
     "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.prompts import ChatPromptTemplate\n\ntemplate = '''Context: {context}\\nQuestion: {question}\\nAnswer:'''\nprompt = ChatPromptTemplate.from_template(template)\nllm = ChatOpenAI(model='gpt-4o-mini')\n\nvectorstore = Chroma(embedding_function=OpenAIEmbeddings())\nretriever = vectorstore.as_retriever()\n\nchain = (\n    {'context': retriever, 'question': RunnablePassthrough()}\n    | _____\n    | llm\n    | StrOutputParser()\n)\n\nresult = chain.invoke('RAG란 무엇인가요?')\nprint(result)",
     "prompt",
     "LCEL(LangChain Expression Language)은 파이프 연산자(|)로 컴포넌트를 연결합니다. 딕셔너리로 컨텍스트와 질문을 매핑하고, prompt → llm → parser 순으로 데이터가 흐릅니다."),

    ("PyPDFLoader로 PDF 문서 로드",
     "from langchain_community.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nloader = PyPDFLoader('document.pdf')\ndocuments = loader._____()\n\nprint(f'총 페이지 수: {len(documents)}')\nprint(f'첫 페이지 내용: {documents[0].page_content[:100]}')\n\nsplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nchunks = splitter.split_documents(documents)\nprint(f'총 청크 수: {len(chunks)}')",
     "load",
     "PyPDFLoader.load()는 PDF를 페이지 단위로 Document 객체 리스트로 반환합니다. 각 Document는 page_content(텍스트)와 metadata(페이지 번호 등)를 가집니다. split_documents()는 Document 리스트를 청킹합니다."),

    ("FAISS 벡터 DB로 유사도 검색",
     "from langchain_community.vectorstores import FAISS\nfrom langchain_openai import OpenAIEmbeddings\n\ntexts = [\n    'Python은 인터프리터 언어입니다',\n    'Java는 컴파일 언어입니다',\n    'Python과 Java 모두 객체지향입니다'\n]\n\ndb = FAISS.from_texts(texts, OpenAIEmbeddings())\n\n# 유사도 점수와 함께 검색\nresults = db.similarity_search_with_score(_____, k=2)\nfor doc, score in results:\n    print(f'유사도: {score:.4f}, 내용: {doc.page_content}')",
     "'Python은 어떤 언어인가요'",
     "similarity_search_with_score()는 문서와 함께 유사도 점수를 반환합니다. FAISS는 Facebook AI가 만든 고속 벡터 검색 라이브러리로 메모리 내에서 빠른 검색을 지원합니다. 점수가 낮을수록 더 유사합니다(L2 거리)."),

    ("ConversationalRetrievalChain으로 대화형 RAG",
     "from langchain.chains import ConversationalRetrievalChain\nfrom langchain_openai import ChatOpenAI\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\nvectorstore = Chroma(embedding_function=OpenAIEmbeddings())\nretriever = vectorstore.as_retriever()\n\nqa_chain = ConversationalRetrievalChain.from_llm(\n    llm=ChatOpenAI(model='gpt-4o-mini'),\n    _____=retriever\n)\n\nchat_history = []\nresult = qa_chain({'question': 'LangChain이란?', 'chat_history': chat_history})\nprint(result['answer'])",
     "retriever",
     "ConversationalRetrievalChain은 대화 히스토리를 유지하면서 RAG를 수행합니다. retriever 파라미터에 벡터 DB retriever를 전달합니다. chat_history에 이전 대화를 누적하면 맥락을 유지한 질의응답이 가능합니다."),

    ("ReAct Agent 도구 정의",
     "from langchain.agents import AgentExecutor, create_react_agent\nfrom langchain_openai import ChatOpenAI\nfrom langchain.tools import tool\nfrom langchain import hub\n\n@_____ \ndef calculate(expression: str) -> str:\n    '''수학 표현식을 계산합니다'''\n    try:\n        return str(eval(expression))\n    except Exception as e:\n        return f'오류: {e}'\n\n@tool\ndef get_word_count(text: str) -> str:\n    '''텍스트의 단어 수를 반환합니다'''\n    return str(len(text.split()))\n\ntools = [calculate, get_word_count]",
     "tool",
     "@tool 데코레이터는 일반 파이썬 함수를 LangChain 에이전트가 사용할 수 있는 도구로 변환합니다. 함수의 독스트링(docstring)이 도구 설명이 되어 에이전트가 언제 사용할지 판단하는 근거가 됩니다."),

    ("벡터 검색 + 재랭킹 파이프라인",
     "from langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import CrossEncoderReranker\nfrom langchain_community.cross_encoders import HuggingFaceCrossEncoder\n\nvectorstore = Chroma(embedding_function=OpenAIEmbeddings())\nbase_retriever = vectorstore.as_retriever(search_kwargs={'k': 10})\n\nmodel = HuggingFaceCrossEncoder(model_name='cross-encoder/ms-marco-MiniLM-L-6-v2')\ncompressor = CrossEncoderReranker(model=model, top_n=3)\n\n_____ = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=base_retriever\n)\n\ndocs = reranking_retriever.invoke('Python 장점은?')\nprint(docs[0].page_content)",
     "reranking_retriever",
     "Re-ranking은 1차 벡터 검색으로 후보 10개를 뽑고, Cross-Encoder로 정확도 기반 3개를 선별하는 2단계 검색입니다. ContextualCompressionRetriever로 기본 검색기에 압축/재랭킹 레이어를 추가합니다."),

    ("Conversation Buffer Memory",
     "from langchain.memory import ConversationBufferMemory\nfrom langchain_openai import ChatOpenAI\nfrom langchain.chains import ConversationChain\n\nmemory = ConversationBufferMemory()\nllm = ChatOpenAI(model='gpt-4o-mini')\n\nconversation = ConversationChain(\n    llm=llm,\n    _____=memory,\n    verbose=True\n)\n\nconversation.predict(input='내 이름은 김철수입니다.')\nconversation.predict(input='내 직업은 개발자입니다.')\nresponse = conversation.predict(input='제 이름이 뭔가요?')\nprint(response)",
     "memory",
     "ConversationBufferMemory는 모든 대화 내역을 버퍼에 저장합니다. ConversationChain의 memory 파라미터에 전달하면 자동으로 히스토리가 관리됩니다. 대화가 길어지면 Context Window를 초과하므로 ConversationSummaryMemory 사용도 고려합니다."),

    ("임베딩 모델 직접 사용",
     "from langchain_openai import OpenAIEmbeddings\nimport numpy as np\n\nembeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n\ntexts = ['Python은 쉽다', '파이썬은 배우기 쉬운 언어이다', 'Java는 어렵다']\nvectors = embeddings._____(texts)\n\nprint(f'임베딩 차원: {len(vectors[0])}')\n\n# 코사인 유사도 계산\nv1, v2, v3 = np.array(vectors[0]), np.array(vectors[1]), np.array(vectors[2])\nsim_12 = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\nprint(f'문장 1-2 유사도: {sim_12:.4f}')",
     "embed_documents",
     "embed_documents()는 문자열 리스트를 받아 각각의 임베딩 벡터를 리스트로 반환합니다. embed_query()는 단일 쿼리용입니다. 유사한 의미의 문장은 높은 코사인 유사도(1에 가까운)를 가집니다."),

    ("멀티쿼리 Retriever",
     "from langchain.retrievers.multi_query import MultiQueryRetriever\nfrom langchain_openai import ChatOpenAI\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\nvectorstore = Chroma(embedding_function=OpenAIEmbeddings())\nretriever = vectorstore.as_retriever()\nllm = ChatOpenAI(model='gpt-4o-mini')\n\n# 하나의 질문을 여러 관점으로 변환하여 검색\nmulti_retriever = MultiQueryRetriever.from_llm(\n    retriever=_____,\n    llm=llm\n)\n\ndocs = multi_retriever.invoke('RAG 성능을 높이는 방법은?')\nprint(f'검색된 문서 수: {len(docs)}')",
     "retriever",
     "MultiQueryRetriever는 하나의 질문을 LLM이 여러 관점으로 재작성하여 각각 검색합니다. 단일 쿼리 검색보다 더 많은 관련 문서를 찾아 Recall을 높입니다. from_llm()에 기본 retriever와 쿼리 재작성용 llm을 전달합니다."),

    ("Ragas로 RAG 평가",
     "from ragas import evaluate\nfrom ragas.metrics import faithfulness, answer_relevancy\nfrom datasets import Dataset\n\ntest_data = {\n    'question': ['LangChain이란?'],\n    'answer': ['LangChain은 LLM 기반 앱을 만드는 프레임워크입니다.'],\n    'contexts': [['LangChain은 체인 방식으로 LLM을 연결하는 파이썬 라이브러리입니다.']],\n    'ground_truth': ['LangChain은 LLM 애플리케이션 개발 프레임워크입니다.']\n}\nds = Dataset.from_dict(test_data)\n\nresult = _____(ds, metrics=[faithfulness, answer_relevancy])\nprint(result)",
     "evaluate",
     "Ragas는 RAG 파이프라인을 자동으로 평가하는 라이브러리입니다. evaluate() 함수에 데이터셋과 평가 지표 리스트를 전달합니다. faithfulness는 답변이 컨텍스트에 근거한 정도, answer_relevancy는 질문과의 관련성을 측정합니다."),

    ("LangChain Tool with Wikipedia",
     "from langchain.tools import WikipediaQueryRun\nfrom langchain_community.utilities import WikipediaAPIWrapper\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import initialize_agent, AgentType\n\nwiki_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\ntools = [_____]\n\nllm = ChatOpenAI(model='gpt-4o-mini')\nagent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION\n)\n\nresult = agent.run('파이썬 프로그래밍 언어는 언제 만들어졌나요?')\nprint(result)",
     "wiki_tool",
     "WikipediaQueryRun은 Wikipedia API를 호출하는 LangChain 도구입니다. tools 리스트에 도구를 담아 에이전트에 전달합니다. ZERO_SHOT_REACT_DESCRIPTION 에이전트는 도구 설명을 읽고 어떤 도구를 사용할지 스스로 결정합니다."),

    ("문서 메타데이터 필터링",
     "from langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_core.documents import Document\n\ndocs = [\n    Document(page_content='Python은 데이터 분석에 강합니다', metadata={'source': 'python.txt', 'year': 2024}),\n    Document(page_content='Java는 엔터프라이즈에 강합니다', metadata={'source': 'java.txt', 'year': 2023}),\n]\n\nvectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())\n\n# 특정 메타데이터 필터로 검색\nresults = vectorstore.similarity_search(\n    '데이터 분석 언어',\n    k=1,\n    _____={'year': 2024}\n)\nprint(results[0].page_content)",
     "filter",
     "filter 파라미터로 메타데이터 기반 필터링을 수행합니다. 벡터 유사도 검색과 메타데이터 필터를 결합하면 특정 출처, 날짜, 카테고리의 문서만 검색할 수 있습니다. Hybrid 검색의 핵심 기능입니다."),

    ("HuggingFace 임베딩 로컬 사용",
     "from langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import Chroma\n\n# 로컬 HuggingFace 임베딩 모델 사용 (API 키 불필요)\nembeddings = HuggingFaceEmbeddings(\n    model_name=_____\n)\n\ntexts = ['AI는 미래다', '인공지능이 세상을 바꾼다']\ndb = Chroma.from_texts(texts, embeddings)\nresults = db.similarity_search('AI의 발전', k=1)\nprint(results[0].page_content)",
     "'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'",
     "HuggingFaceEmbeddings를 사용하면 OpenAI API 없이 로컬에서 임베딩이 가능합니다. 다국어 모델인 paraphrase-multilingual-mpnet-base-v2는 한국어를 포함한 50개 이상의 언어를 지원합니다."),

    ("BM25 + 벡터 하이브리드 검색",
     "from langchain_community.retrievers import BM25Retriever\nfrom langchain.retrievers import EnsembleRetriever\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\ntexts = ['Python 기초', 'Python 고급', 'Java 입문', '데이터 분석 with Python']\n\nbm25 = BM25Retriever.from_texts(texts, k=2)\nvectorstore = Chroma.from_texts(texts, OpenAIEmbeddings())\nvector_retriever = vectorstore.as_retriever(search_kwargs={'k': 2})\n\nhybrid = EnsembleRetriever(\n    retrievers=[bm25, _____],\n    weights=[0.5, 0.5]\n)\nprint(hybrid.invoke('Python'))",
     "vector_retriever",
     "EnsembleRetriever는 키워드 기반(BM25)과 의미 기반(벡터) 검색을 결합합니다. weights로 각 검색기의 가중치를 조정합니다. BM25는 키워드 매칭에 강하고 벡터는 의미 유사성에 강해 상호 보완적입니다."),

    ("Self-Query Retriever",
     "from langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain_openai import ChatOpenAI\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.chains.query_constructor.base import AttributeInfo\n\nmetadata_field_info = [\n    AttributeInfo(name='year', description='출판 연도', type='integer'),\n    AttributeInfo(name='genre', description='장르', type='string'),\n]\n\nvectorstore = Chroma(embedding_function=OpenAIEmbeddings())\nretriever = SelfQueryRetriever.from_llm(\n    llm=ChatOpenAI(),\n    vectorstore=vectorstore,\n    document_contents='책 정보',\n    _____=metadata_field_info\n)\nresult = retriever.invoke('2024년에 나온 SF 장르 책은?')\nprint(result)",
     "metadata_field_info",
     "SelfQueryRetriever는 자연어 질문에서 메타데이터 필터를 자동으로 추출합니다. '2024년 SF 책'이라는 질문에서 year=2024, genre='SF' 필터를 자동 생성합니다. metadata_field_info로 필터 가능한 필드를 정의합니다."),

    ("LangGraph 간단한 상태 그래프",
     "from langgraph.graph import StateGraph, END\nfrom typing import TypedDict\n\nclass State(TypedDict):\n    message: str\n    processed: bool\n\ndef process_node(state: State) -> State:\n    return {'message': state['message'].upper(), 'processed': True}\n\ndef should_end(state: State) -> str:\n    return END if state['processed'] else 'process'\n\nbuilder = _____(State)\nbuilder.add_node('process', process_node)\nbuilder.set_entry_point('process')\nbuilder.add_conditional_edges('process', should_end)\ngraph = builder.compile()\n\nresult = graph.invoke({'message': 'hello world', 'processed': False})\nprint(result)",
     "StateGraph",
     "LangGraph는 복잡한 에이전트 워크플로우를 방향 그래프로 정의합니다. StateGraph(타입)으로 상태 스키마를 정의하고, add_node로 처리 함수를 추가합니다. 조건부 엣지로 분기 로직을 구현할 수 있습니다."),

    ("벡터 DB 영속화 및 로드",
     "from langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\n# 1. 벡터 DB를 디스크에 저장\ntexts = ['RAG는 검색 증강 생성입니다', 'LangChain은 프레임워크입니다']\nvectorstore = Chroma.from_texts(\n    texts,\n    OpenAIEmbeddings(),\n    persist_directory=_____\n)\nvectorstore.persist()\nprint('저장 완료')\n\n# 2. 저장된 DB 다시 로드\nloaded_db = Chroma(\n    persist_directory='./chroma_db',\n    embedding_function=OpenAIEmbeddings()\n)\nprint(loaded_db.similarity_search('검색', k=1)[0].page_content)",
     "'./chroma_db'",
     "persist_directory를 지정하면 벡터 DB를 디스크에 저장합니다. vectorstore.persist()로 명시적 저장 후, 동일한 persist_directory로 Chroma()를 생성하면 이전에 저장한 데이터를 재사용합니다."),

    ("Custom Tool with Pydantic 검증",
     "from langchain.tools import BaseTool\nfrom pydantic import BaseModel, Field\nfrom typing import Type\n\nclass SearchInput(BaseModel):\n    query: str = Field(description='검색할 키워드')\n    max_results: int = Field(default=5, description='최대 결과 수')\n\nclass CustomSearchTool(_____):  \n    name = 'custom_search'\n    description = '제품 데이터베이스에서 검색합니다'\n    args_schema: Type[BaseModel] = SearchInput\n    \n    def _run(self, query: str, max_results: int = 5) -> str:\n        # 실제 검색 로직 대신 더미 반환\n        return f'{query} 검색 결과 {max_results}개'\n    \ntool = CustomSearchTool()\nprint(tool.invoke({'query': 'Python', 'max_results': 3}))",
     "BaseTool",
     "BaseTool을 상속하여 커스텀 LangChain 도구를 만듭니다. args_schema에 Pydantic 모델을 지정하면 입력 검증과 타입 안전성이 보장됩니다. name, description은 에이전트가 도구를 선택할 때 참고합니다.")
]

for i, (title, code, ans, explain) in enumerate(cc_data):
    questions.append({
        "chapter_name": chapter_name, "type": "코드 완성형", "difficulty": "medium", "id": str(5101 + i),
        "question": f"{title} 코드를 완성하세요.\n```python\n{code}\n```",
        "answer": ans,
        "why": explain,
        "hint": title,
    })

def get_questions():
    return questions
