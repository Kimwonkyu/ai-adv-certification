    "question": "Transformer 아키텍처에서 Self-Attention 연산이 RNN의 순차적(sequential) 계산 대비 병렬화 성능을 비약적으로 높일 수 있는 수리적 근거는?",
    "question": "LLM 추론 시 'KV Caching'이 생성 토큰당 레이턴시(Latency)를 줄이는 핵심 원리임에도 불구하고, 대규모 서빙 시 발생하는 주요 트레이드오프는?",
    "question": "Speculative Decoding에서 'Target Model'이 'Draft Model'의 출력을 검토할 때, 왜 전체 시퀀스를 한 번의 Forward Pass로 검증할 수 있는가?",
    "question": "LLM 의 'Context Window' 한계로 인해 발생하는 '잃어버린 중간(Lost in the Middle)' 현상의 기술적 정의는?",
    "question": "트랜스포머 레이어 중 'Multi-Query Attention (MQA)'과 'Grouped-Query Attention (GQA)'이 추론 최적화에서 해결하고자 하는 공통적인 수리적 병목은?",
    "question": "LLM이 특정 페르소나를 유지하거나 '나는 도움을 주는 AI'라는 정체성을 가지게 되는 것은 주로 어떤 데이터를 통한 훈련 결과인가?",
    "question": "LLM의 'Emergent Abilities (발현 능력)'가 특정 파라미터 규모 및 학습량(Compute) 이상에서 돌발적으로 나타나는 현상에 대한 가장 유력한 가설은?",
    "question": "현대 토크나이저가 '사전 외 단어(OOV)' 현상을 해결하기 위해 글자(Char)와 단어(Word)의 중간 지점에서 채택하는 전략은?",
    "question": "트랜스포머의 인코더와 디코더 아키텍처에서 '미래 정보 접근(Information Leakage)'을 차단하기 위해 디코더에만 존재하는 수리적 장치는?",
    "question": "단일 GPU 메모리를 초과하는 거대 모델 학습을 위해, 가중치와 그래디언트를 여러 GPU에 쪼개어 분산하는 DeepSpeed의 핵심 기술은?",
    "question": "현대 LLM의 긴 컨텍스트(Context Window) 지원을 가능하게 한 Flash Attention이 해결한 HBM(고대역폭 메모리) 차원의 결정적 병목은?",
    "question": "Mixture of Experts(MoE) 구조가 '거대한 조밀 모델(Dense Model)' 대비 가지는 결정적인 추론 성능 상의 이점은 무엇인가?",
    "question": "GPT 계열의 'Decoder-only' 모델이 BERT 계열의 'Encoder-only' 모델 대비 '생성(Generation)' 작업에 수리적으로 더 최적화된 근거는?",
    "question": "LLM의 'Temperature' 파라미터가 0에서 멀어질수록(높아질수록) 모델 출력의 확률 분포에는 어떤 기술적 변화가 일어나는가?",
    "question": "LLM을 4비트 양자화(Int4)하여 실행할 때, FP16 대비 VRAM 사용량을 극적으로 줄이면서도 연산의 높은 정밀도를 보존하기 위해 사용하는 기술적 우회법은?",
    "question": "모델이 스스로의 오류를 인지하고 수정하는 'Self-Correction' 능력을 극대화하기 위해 권장되는 프롬프트 설계 전략은?",
    "question": "LLM 학습 데이터에서 중복(Duplicate) 데이터를 완벽하게 제거(Deduplication)하지 않았을 때 발생하는 가장 치명적인 통계적 결함은?",
    "question": "BPE(Byte Pair Encoding) 토크나이저가 '희귀 단어(Rare words)'를 처리할 때 기존 단어 기반 처리 방식보다 기술적으로 우월한 이유는?",
    "question": "트랜스포머 디코더의 'Look-ahead Masking' 시, 소프트맥스 연산 전에 미래 시점의 어텐션 점수에 부여하는 수리적 값은?",
    "question": "RLHF(Reinforcement Learning from Human Feedback)의 PPO 알고리즘 학습 시, 'Catastrophic Forgetting'을 방지하기 위한 제약 조건은?",
    "question": "트랜스포머 아키텍처에서 인코더와 디코더 사이의 연결을 담당하며 인코더의 맥락 정보를 디코더로 전송하는 전용 어텐션은?",
    "question": "트랜스포머의 Self-Attention 연산 시, 입력 문장의 길이(L)가 늘어남에 따라 계산 복잡도와 필요한 메모리가 기하급수적으로 증가하는 수리적 관계는?",
    "question": "LLM이 실제 사실이 아닌 내용을 그럴듯하게 답하는 'Hallucination(환각)' 현상을 억제하기 위해, 외부 소스에서 정보를 검색하여 제공하는 기술적 명칭은?",
    "question": "BERT 모델이 GPT와 달리 문장 중간의 단어를 예측하는 'Masked Language Modeling (MLM)'을 통해 얻는 아키텍처적 결정적 강점은?",
    "question": "70억 파라미터(7B) 규모의 모델을 FP16(Half-precision) 정밀도로 GPU에 로드할 때, 순수 가중치용으로 필요한 최소 VRAM 용량은?",
    "question": "LLM의 'Pre-training' 단계와 'Fine-tuning' 단계의 가장 근본적인 수리적·목적적 차이는?",
    "question": "최신 추론형 모델들이 복잡한 질문에 대해 답변 전 거치는 논리적 사유 과정을 명시적으로 구분하기 위해 사용하는 태그는?",
    "question": "Llama 3와 같은 최신 LLM이 추론 효율을 극대화하기 위해 채택한 'Grouped-Query Attention (GQA)'의 핵심 메커니즘은?",
    "question": "임베딩 벡터 공간에서 두 단어 벡터 사이의 '코사인 유사도(Cosine Similarity)'가 1에 근접할 때의 통계학적 의미는?",
    "question": "텍스트, 이미지, 음성 등 하이브리드 입력을 처리하는 '멀티모달(Multimodal)' 모델이 이종 데이터를 동시에 이해하게 만드는 핵심 메커니즘은?",
    "question": "LLM의 'Context Window' 크기가 물리적으로 제한되는 가장 근본적인 수학적·하드웨어적 이유는?",
    "question": "Decoder-only 모델의 텍스트 생성 시, 미래 시점 정보가 현재 토큰 예측에 영향을 미치지 않도록 차단하는 'Causal Masking'의 수리적 원리는?",
    "question": "최신 LLM에서 사용하는 'RoPE (Rotary Positional Embedding)'가 상대적 위치 파악 및 문맥 외삽(Extrapolation)에 유리한 수학적 명분은?",
    "question": "BPE (Byte Pair Encoding) 토크나이징 과정에서 사전에 없는 단어(OOV) 문제를 해결하기 위해 조각들을 결합하는 결정적 기준은?",
    "question": "거대 모델의 전체 가중치를 수정하는 대신, 로우-랭크 행렬 분해를 통해 극소수의 파라미터만 학습시키는 효율적 파인튜닝 기법은?",
    "question": "LLM이 허위 사실을 진실처럼 출력하는 '환각(Hallucination)' 현상을 보완하기 위해 RAG 아키텍처가 동원하는 핵심 정보 처리 방식은?",
    "question": "Mixture of Experts (MoE) 구조가 '거대한 모델 규모'를 유지하면서도 추론 연산량(FLOPs)을 조밀 모델 대비 낮출 수 있는 결정적 비결은?",
    "question": "LLM의 'Temperature' 파라미터를 0.1 이하로 매우 낮게 설정했을 때, 모델의 답변 생성 양상에서 나타나는 결정적 변화는?",
    "question": "단순히 지식을 습득한 베이스 모델(Base Model)을 챗봇처럼 대화 형식에 정렬시키는 'Instruct Fine-tuning'의 데이터 핵심 구준은?",
    "question": "모델의 '지식 차단 시점(Knowledge Cut-off)'을 보완하기 위해 RAG 아키텍처를 도입했을 때 얻을 수 있는 이점과 가장 거리가 먼 것은?",
    "question": "트랜스포머의 'Self-Attention'이 RNN(순환 신경망)보다 긴 문장의 문맥을 압도적으로 잘 포착하는 수리적 배경은?",
    "question": "LLM 서비스에서 'Hallucination(환각)' 현상을 기술적으로 억제하기 위한 방법 중, 아키텍처나 워크플로우 관점에서 가장 부적절한 것은?",
    "question": "LLM 학습 데이터 전처리 중 '중복 제거(Deduplication)' 작업이 모델의 일반화 능력에 미치는 결정적 수리적 영향은?",
    "question": "트랜스포머의 'Self-Attention' 연산 시 모델이 특정 토큰 쌍의 관계를 결정짓는 세 가지 핵심 벡터 성분에 해당하지 않는 것은?",
    "question": "LLM의 'Reasoning(추론)' 능력을 극대화하기 위해 질문 끝에 '단계별로 생각해보자'라는 문구를 넣는 기법의 수리적 효과는?",
    "question": "LLM 학습의 RLHF 과정에서 사람의 주관적 선호도를 학습하여 답변의 품질 점수를 판독하는 핵심 모듈은?",
    "question": "트랜스포머의 'Positional Encoding'이나 'RoPE' 같은 위치 정보 주입 기술이 수리적으로 반드시 동반되어야 하는 아키텍처적 이유는?",
    "question": "LLM 보안 공격인 'Prompt Injection'을 방비하기 위해 지시 사항과 사용자 메시지의 경계를 명시적으로 정의하는 구조는?",
    "question": "최신 LLM 미세 조정 기법인 'DPO(Direct Preference Optimization)'가 고전적 PPO 기반 RLHF 대비 우월한 수리적 강점은?",
    "question": "트랜스포머 블록 내 'Dropout' 레이어가 FFN 혹은 Attention 직후에 위치하여 수행하는 결정적 수리 학습 보강 기능은?",
    "question": "모델의 지능 수준은 그대로 유지하면서 추론 시 답변 속도를 높이기 위해, 미리 계산된 '이후 답변들'을 활용하는 기술은?",
    "question": "트랜스포머 블록 내부의 'Residual Connection(잔차 연결)'이 거대 모델의 학습 안정성에 기여하는 근본적인 수리적 기능은?",
    "question": "LLM의 'Alignment' 단계 중 사람의 지시 사항을 따르도록 훈련시키는 첫 관문인 'SFT'의 정확한 정의와 역할은?",
    "question": "토크나이저 아키텍처 중 'SentencePiece'가 한국어 같은 언어에서 가지는 정보량적 강점은?",
    "question": "LLM의 'KV Cache' 기술이 추론 시 연산 효율을 높이는 수리적 원리로 가장 올바른 것은?",
    "question": "LLM 서비스에서 'Streaming(스트리밍)' 방식의 실시간 답변 출력이 기술적으로 가능한 근본적인 이유는?",
    "question": "LLM이 지능의 임계점을 넘기 위해 필요한 세 가지 핵심 수치 요소(Scaling Law)에 해당하지 않는 것은?",
    "question": "트랜스포머의 어텐션 연산을 수리적으로 최적화하여 연산 속도와 VRAM 효율을 혁신적으로 개선한 알고리즘은?",
    "question": "LLM이 스스로 자신의 오류를 검토하고 교정하는 'Self-Correction(자기 교정)' 능력을 높이기 위한 가장 효과적인 프롬프트 전략은?",
    "question": "GPT-4o 모델명에서 'o'가 의미하는 접미어 'Omni'의 기술적 함의로 가장 올바른 것은?",
    "question": "현대 LLM의 토크나이저에서 사용되는 'BPE(Byte Pair Encoding)' 기법이 기존 어절 단위 토크나이징 대비 가지는 수리적 강점은?",
    "question": "BERT(Encoder-only)의 'Masked Language Modeling(MLM)'이 GPT(Decoder-only)의 'Causal Language Modeling(CLM)'과 구별되는 수리적 결정타는?",
    "question": "트랜스포머의 'Positional Encoding'이 RNN의 재귀적 구조를 대체하여 문장의 순차적 관계를 처리하는 수리적 기법은?",
    "question": "LLM의 성능을 평가하는 벤치마크(예: HumanEval)에서 모델의 코드 생성 능력을 측정할 때 주로 사용되는 지표인 'Pass@k'의 의미는?",
    "question": "LLM 보안 공격인 'Prompt Injection'을 방어하기 위해 시스템 프롬프트의 권한을 유지하는 구조적 접근법으로 가장 부적절한 것은?",
    "question": "트랜스포머 연산 시 문장 길이(L)의 제곱(L^2)에 비례하는 연산 복잡도 문제를 해결하기 위해 'Sparse Attention'이 사용하는 수리적 전략은?",
    "question": "LLM의 'Scaling Law' 중 단순 자본 투입만으로 성능 향상이 정체되는 임계점을 극복하기 위한 'Chinchilla Scaling'의 핵심 제언은?",
    "question": "거대 파라미터 모델(예: 70B 이상)이 소형 모델 대비 실질적으로 더 뛰어난 추론 능력을 발휘하게 되는 수리적 원동력은?",
    "question": "LLM 아키텍처 중 'MoE(Mixture of Experts)'가 방대한 전체 파라미터를 보유하면서도 추론 시 연산 효율을 극대화하는 수리적 원리는?",
    "question": "트랜스포머의 'Self-Attention' 연산 시 내적(Dot-product) 값의 분산이 커져 소프트맥스 가중치가 한쪽으로 쏠리는 병목을 방지하기 위해 사용하는 수리적 장치는?",
    "question": "LLM이 사전 학습(Pre-training)이나 미세 조정 없이, 오직 지시문(Prompt) 내의 예시와 문맥만으로 새로운 작업을 수행하는 창발적 능력은?",
    "question": "최신 트랜스포머 아키텍처의 FFN 레이어에서 ReLU를 대체하여 주로 사용되는 'SwiGLU' 활성화 함수가 가지는 수리적 이점은?",
    "question": "모델의 파라미터가 70B(700억 개)일 때, 'FP16' 정밀도로 순수 가중치를 로드하기 위해 필요한 이론적 최소 VRAM 용량과 그 근거는?",
    "question": "LLM이 사용자의 질문을 '에이전틱(Agentic)'하게 처리하기 위해 수행하는 'Tool Use(Function Calling)'의 수리적 구동 메커니즘은?",
    "question": "LLM 데이터를 임베딩하여 'Vector Database'에 저장 시 사용하는 거리 지표 중, 벡터의 크기(절댓값)보다 '방향 유사성'을 측정하는 방식은?",
    "question": "멀티모달 모델에서 텍스트와 이미지 데이터를 단일 신경망으로 동시에 처리하는 'Native Multimodal' 방식의 핵심적 우위는?",
    "question": "LLM의 RLHF 과정 중 모델이 실제 지능을 기르기보다 높은 점수 패턴만을 편법으로 학습하는 'Reward Hacking' 현상의 정의는?",
    "question": "LLM의 출력 토큰 확률 분포에서 'Temperature' 값을 2.0 이상 극한으로 높게 설정했을 때 나타나는 결정적 변화는?",
    "question": "LLM을 소형 디바이스(온디바이스 AI)에 최적화하기 위해, 거대 모델의 지식을 소형 모델에 전수하는 '지식 증류(Distillation)'의 수리 원리는?",
    "question": "최신 추론 특화 LLM(예: OpenAI o1)이 정답 도출 직전 시간을 확보하며 지능을 높이는 'Test-Time Compute'의 수리적 원동력은?",
    "question": "지식을 저장하는 모델의 '파라미터'와 RAG(Retrieval-Augmented Generation) 시스템의 역할을 기술적으로 구분할 때 가장 정확한 것은?",
    "question": "LLM 학습의 'Scaling Law'가 제시하는 모델의 손실(Loss) 값을 결정하는 3대 핵심 물리적 자원이 아닌 것은?",
    "question": "트랜스포머의 'Self-Attention' 행렬 연산 시 Softmax를 취하기 직전, 'Causal Masking'을 수행하는 결정적인 수리적 이유는?",
    "question": "LLM의 복잡한 추론(Reasoning) 시, 초기 오답이 뒤따르는 모든 논리를 붕괴시키는 'Error Cascade' 현상을 방지하는 최적의 기법은?",
    "question": "거대 언어 모델 학습 시, Gradient와 Optimizer State 등을 모든 GPU에 분산 저장하여 메모리 중복을 없애는 최적화 기술은?",
    "question": "프롬프트 엔지니어링 기법 중, 생성된 답변의 사실 관계를 스스로 다시 질문하고 검증하는 'Chain-of-Verification (CoVe)'의 주된 목적은?",
    "question": "거대 언어 모델의 파레미터 전체를 학습시키는 대신, 소규모의 저차원 행렬만을 학습시켜 효율을 높이는 PEFT 기법은?",
    "question": "LLM 추론 서비스 환경에서 'KV Cache' 기술을 도입하여 얻을 수 있는 가장 결정적인 기술적 이점은?",
    "question": "트랜스포머 기반 언어 모델 중 'Encoder-only(BERT)'와 'Decoder-only(GPT)'의 수리적 차이점으로 옳지 않은 것은?",
    "question": "LLM 학습 데이터에서 'Data Contamination (데이터 오염)'이 심각한 기술적 이슈가 되는 실전적인 이유는?",
    "question": "LLM 아키텍처 중 'GQA (Grouped-Query Attention)'가 기존 Multi-Head Attention 대비 갖는 수리적 최적화 이점은?",
    "question": "LLM의 '컨텍스트 윈도우(Context Window)' 길이를 물리적으로 결정짓는 가장 핵심적인 하드웨어 제약 사항은?",
    "question": "모델이 자신의 이전 응답이나 사고 과정을 되돌아보고 논리적 모순을 스스로 교정하는 '성찰(Reflection)' 능력은 주로 어느 단계에서 강화되는가?",
    "question": "트랜스포머 기반 모델 중 '인코더 전용(BERT)' 아키텍처가 생성 전용(GPT) 모델 대비 압도적인 강점을 보이는 실무적 태스크는?",
    "question": "LLM이 인터넷에 공개되지 않았거나 지식 컷오프 이후에 발생한 최신 정보를 참조하여 답변하도록 돕는 RAG의 수리적 원리는?",
    "question": "임베딩 공간(Embedding Space)에서 두 문장 간의 '의미적 유사도'를 계산할 때 가장 표준적으로 사용되는 수리적 측정 지표는?",
    "question": "거대 모델의 규모(Parameters)가 임계점을 넘어섰을 때, 점진적 향상을 넘어 예상치 못한 고등 지능이 폭발적으로 나타나는 현상은?",
    "question": "LLM의 추론 성능을 크게 해치지 않으면서 GPU VRAM 비용을 절감하기 위해 도입하는 '양자화(Quantization)'의 필연적인 부작용은?",
    "question": "OpenAI o1 모델처럼 모델이 답변 생성 직전 스스로 여러 추론 경로를 시뮬레이션하고 검토하는 'Chain-of-Thought'의 심화 진화 방식은?",
    "question": "트랜스포머의 'Self-Attention' 연산 시, 입력 문장 길이(N)에 따라 발생하는 수리적 연산 복잡도와 그 병목 원인은?",
    "question": "HuggingFace의 `transformers` 라이브러리를 사용하여 사전 학습된 토크나이저를 로드할 때 사용하는 메서드를 작성하세요.\n\n```python\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.____(\"bert-base-uncased\")\n```",
    "question": "LLM의 출력에서 무작위성을 제어하기 위해 `generate` 메서드 내에서 사용하는 파라미터 명칭을 작성하세요.\n\n```python\n# 답변의 창의성 조절\noutputs = model.generate(inputs, ____=0.7, do_sample=True)\n```",
    "question": "로짓(Logits) 값을 확률 분포로 바꿀 때, 온도(T)를 적용하는 수식을 완성하세요.\n\n```python\n# logits를 T로 나눈 뒤 소프트맥스 계산\nprobs = torch.nn.functional.softmax(logits / ____, dim=-1)\n```",
    "question": "PEFT 라이브러리에서 LoRA 설정을 정의할 때, 저차원 행렬의 크기(Rank)를 지정하는 파라미터를 작성하세요.\n\n```python\nfrom peft import LoraConfig\nconfig = LoraConfig(____=8, lora_alpha=32)\n```",
    "question": "모델을 4비트로 양자화하여 메모리를 아껴 로드하기 위한 설정 클래스의 명칭을 완성하세요.\n\n```python\nfrom transformers import ____Config\nquant_config = ____Config(load_in_4bit=True)\n```",
    "question": "누적 확률이 특정 임계치를 넘는 토큰들만 샘플링 후보로 두는 기법의 파라미터 명칭을 작성하세요.\n\n```python\n# 상위 90% 확률 구간만 고려\noutputs = model.generate(inputs, ____=0.9, do_sample=True)\n```",
    "question": "어텐션 메커니즘에서 쿼리(Q)와 키(K)의 유사도를 구하기 위한 행렬 연산 기호를 작성하세요.\n\n```python\n# 어텐션 스코어 계산 (scaled dot-product)\nscores = torch.matmul(Q, K.____) / math.sqrt(d_k)\n```",
    "question": "파이토치에서 모델을 GPU(NVIDIA) 장치로 옮기기 위해 사용하는 문자열을 작성하세요.\n\n```python\ndevice = \"____\"\nmodel.to(device)\n```",
    "question": "토크나이저에서 문장 길이를 맞추기 위해 사용되는 특수 토큰의 ID를 확인하는 속성을 작성하세요.\n\n```python\nprint(tokenizer._____id)\n```",
    "question": "정수 형태의 토큰 ID를 연속적인 벡터 값으로 변환하는 파이토치 레이어의 명칭을 작성하세요.\n\n```python\nimport torch.nn as nn\nlayer = nn.____(vocab_size, hidden_dim)\n```",
    "question": "프롬프트 템플릿에서 동적으로 변수를 채워 넣기 위해 일반적으로 사용하는 괄호 형식을 작성하세요.\n\n```python\ntemplate = \"당신은 전문가입니다. 다음 {____}에 대해 답변하세요.\"\n```",
    "question": "두 임베딩 벡터 사이의 코사인 유사도를 계산하기 위해 사용하는 함수 명칭을 완성하세요.\n\n```python\nsim = torch.nn.functional.____(vec1, vec2)\n```",
    "question": "디코더에서 미래 단어를 가리기 위해 상삼각 행렬(Upper triangular matrix)을 생성하는 넘파이/파이토치 함수를 작성하세요.\n\n```python\n# 주대각선 위쪽을 True로 만드는 마스크\nmask = torch.____(torch.ones(L, L), diagonal=1)\n```",
    "question": "벡터 데이터를 인덱싱하고 빠르게 검색하기 위해 널리 쓰이는 라이브러리 `faiss`에서 검색을 실행하는 메서드를 작성하세요.\n\n```python\nimport faiss\nindex = faiss.IndexFlatL2(d)\ndists, ann = index.____(query_vectors, k=5)\n```",
    "question": "구성된 랭체인(LangChain) 객체를 실행하여 답변을 얻기 위해 호출하는 메서드를 작성하세요.\n\n```python\nchain = prompt | model\nresponse = chain.____({\"question\": \"LLM이란?\"})\n```",
    "question": "RLHF에서 정책 모델과 참조 모델 사이의 확률 분포 차이를 계산하는 지표 명칭을 완성하세요.\n\n```python\n# 분포의 소실(Divergence) 계산\nloss = torch.nn.functional._____loss(log_p, p)\n```",
    "question": "LoRA를 적용할 특정 신경망 레이어(예: q_proj, v_proj)를 리스트 형태로 지정하는 설정 파라미터를 작성하세요.\n\n```python\nconfig = LoraConfig(____=[\"q_proj\", \"v_proj\"])\n```",
    "question": "생성 시 빔 서치(Beam Search)를 활성화하고 탐색할 경로의 개수를 지정하는 파라미터를 작성하세요.\n\n```python\n# 5개의 경로를 동시 탐색\noutputs = model.generate(input, ____=5, early_stopping=True)\n```",
    "question": "모델의 언어 모델링 성능 지표인 퍼플렉서티(PPL)를 교차 엔트로피 손실(Loss)로부터 계산하는 수식을 완성하세요.\n\n```python\nimport math\nppl = math.____(loss)\n```",
    "question": "대화형 모델의 특수한 토큰 형식을 자동으로 입혀주는 토크나이저 메서드 명칭을 작성하세요.\n\n```python\n# 메시지 리스트를 모델 전용 텍스트로 변환\ninputs = tokenizer.____(messages, tokenize=False)\n```",
    "question": "프롬프트 엔지니어링에서 'few-shot' 예시를 줄 때, 예시의 '순서'가 답변에 미치는 영향은?",
