{
    "questions": [
        {
            "chapter_name": "Python 기초",
            "type": "multiple_choice",
            "question": "파이썬에서 리스트를 정렬할 때, 원본을 변경하지 않고 새로운 정렬된 리스트를 반환하는 함수는?",
            "options": [
                "list.sort()",
                "sorted()",
                "order()",
                "arrange()",
                "list.order()"
            ],
            "answer": "sorted()",
            "why": "sorted()는 내장 함수로 새로운 리스트를 반환하며, list.sort()는 리스트 자체를 정렬(In-place)합니다.",
            "hint": "동사형 형용사 형태의 내장 함수입니다.",
            "trap_points": [
                "원본 유지 여부에 따라 선택이 달라짐"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "Python 기초",
            "type": "multiple_choice",
            "question": "딕셔너리에서 키가 없을 때 에러를 내지 않고 기본값을 반환하는 메서드는?",
            "options": [
                "dict.find()",
                "dict.get()",
                "dict.search()",
                "dict.index()",
                "dict.pop()"
            ],
            "answer": "dict.get()",
            "why": "get(key, default)은 키가 존재하지 않을 경우 None이나 지정된 기본값을 안전하게 반환합니다.",
            "hint": "가져오다(Get)의 의미입니다.",
            "trap_points": [
                "대괄호 [] 접근은 KeyError가 발생함"
            ],
            "difficulty": "easy"
        },
        {
            "chapter_name": "Python 기초",
            "type": "multiple_choice",
            "question": "파이썬의 'Set' 자료형이 'List'와 차별화되는 가장 큰 특징은?",
            "options": [
                "순서가 보장된다.",
                "중복을 허용하지 않으며 순서가 없다.",
                "속도가 매우 느리다.",
                "인덱스로 접근 가능하다.",
                "문자열만 저장할 수 있다."
            ],
            "answer": "중복을 허용하지 않으며 순서가 없다.",
            "why": "집합(Set)은 수학적 집합과 동일하게 유일한 값들만 담으며 인덱싱이 불가능합니다.",
            "hint": "수학의 '집합' 개념을 떠올리세요.",
            "trap_points": [
                "중복 제거가 필요할 때 가장 효율적임"
            ],
            "difficulty": "easy"
        },
        {
            "chapter_name": "Python 기초",
            "type": "multiple_choice",
            "question": "부모 클래스의 생성자를 호출할 때 사용하는 올바른 문법은?",
            "options": [
                "parent.__init__(self)",
                "base.__init__(self)",
                "super().__init__()",
                "self.super().__init__()",
                "inherit.__init__()"
            ],
            "answer": "super().__init__()",
            "why": "super()는 MRO(Method Resolution Order)를 따라 부모 클래스에 접근하는 표준 방식입니다.",
            "hint": "이미 샘플에서도 다뤘던 핵심 키워드입니다.",
            "trap_points": [
                "파이썬 3에서는 super()에 인자를 생략 가능함"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "Python 기초",
            "type": "short_answer",
            "question": "문자열 '  Hello  '의 좌우 공백을 제거하는 메서드는?",
            "answer": "strip()",
            "why": "strip()은 문자열 양 끝의 공백이나 지정된 문자를 제거합니다.",
            "hint": "벗기다, 떼어내다의 뜻입니다.",
            "trap_points": [
                "lstrip()은 왼쪽만, rstrip()은 오른쪽만 제거함"
            ],
            "difficulty": "easy"
        },
        {
            "chapter_name": "Python 기초",
            "type": "multiple_choice",
            "question": "파이썬 클래스 내에서 특정 속성에 대한 접근 제어(Getter/Setter)를 위해 사용하는 데코레이터는?",
            "options": [
                "@staticmethod",
                "@classmethod",
                "@property",
                "@access",
                "@wrap"
            ],
            "answer": "@property",
            "why": "property 데코레이터를 쓰면 메서드를 변수처럼 호출할 수 있어 캡슐화에 유리합니다.",
            "hint": "속성을 뜻하는 영어 단어입니다.",
            "trap_points": [
                "@classmethod는 클래스 자체를 인자로 받는 메서드임"
            ],
            "difficulty": "hard"
        },
        {
            "chapter_name": "Python 기초",
            "type": "multiple_choice",
            "question": "이터러블 객체의 요소와 인덱스를 동시에 반환하는 반복문 도구는?",
            "options": [
                "zip()",
                "range()",
                "enumerate()",
                "slice()",
                "map()"
            ],
            "answer": "enumerate()",
            "why": "enumerate()는 리스트 등의 순회 시 (인덱스, 값) 튜플을 생성해 줍니다.",
            "hint": "열거하다라는 뜻입니다.",
            "trap_points": [
                "zip()은 여러 리스트를 묶을 때 사용함"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "Python 기초",
            "type": "multiple_choice",
            "question": "파이썬에서 두 리스트를 하나로 합치되, 원본 리스트를 확장(Extend)하는 메서드는?",
            "options": [
                "list.append()",
                "list.extend()",
                "list.merge()",
                "list.add()",
                "list.join()"
            ],
            "answer": "list.extend()",
            "why": "append()는 요소를 맨 뒤에 하나 추가하고, extend()는 다른 리스트의 모든 요소를 이어 붙입니다.",
            "hint": "연장하다라는 뜻입니다.",
            "trap_points": [
                "append()로 리스트를 넣으면 리스트 안에 리스트가 들어감"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "Python 기초",
            "type": "multiple_choice",
            "question": "전역 변수를 함수 내부에서 수정하려고 할 때 사용하는 키워드는?",
            "options": [
                "outer",
                "static",
                "global",
                "nonlocal",
                "public"
            ],
            "answer": "global",
            "why": "global 키워드를 선언해야 함수 외부의 전역 변수 메모리 주소에 접근하여 수정할 수 있습니다.",
            "hint": "전체적인, 전역의를 뜻합니다.",
            "trap_points": [
                "nonlocal은 중첩 함수 내부에서 한 단계 위 변수를 가리킴"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "Python 기초",
            "type": "short_answer",
            "question": "파이썬 딕셔너리에서 모든 '키(Key)'들만 모아서 반환하는 메서드는?",
            "answer": "keys()",
            "why": "keys() 메서드는 딕셔너리의 키들을 담은 뷰(view) 객체를 반환합니다.",
            "hint": "Key의 복수형입니다.",
            "trap_points": [
                "values()는 값들을, items()는 키-값 쌍을 반환함"
            ],
            "difficulty": "easy"
        },
        {
            "chapter_name": "데이터 분석",
            "type": "multiple_choice",
            "question": "Pandas에서 데이터프레임의 특정 열을 기준으로 데이터를 그룹화하여 통계를 내는 메서드는?",
            "options": [
                "gather()",
                "pivot()",
                "groupby()",
                "aggregate()",
                "bucket()"
            ],
            "answer": "groupby()",
            "why": "groupby()는 SQL의 GROUP BY와 동일하게 범주형 데이터를 기준으로 그룹별 연산을 수행하게 합니다.",
            "hint": "~별로 무리 짓기.",
            "trap_points": [
                "통계 함수(mean, sum)를 뒤에 붙여야 결과가 나옴"
            ],
            "difficulty": "easy"
        },
        {
            "chapter_name": "데이터 분석",
            "type": "multiple_choice",
            "question": "Numpy 배열의 차원을 변경하고자 할 때 사용하는 함수는?",
            "options": [
                "np.resize()",
                "np.reshape()",
                "np.convert()",
                "np.scale()",
                "np.flatten()"
            ],
            "answer": "np.reshape()",
            "why": "reshape()는 데이터의 총 개수는 유지하면서 모양(Shape)만 바꿀 때 사용합니다.",
            "hint": "모양을 다시 잡는다.",
            "trap_points": [
                "개수가 맞지 않으면 에러가 발생함"
            ],
            "difficulty": "easy"
        },
        {
            "chapter_name": "데이터 분석",
            "type": "multiple_choice",
            "question": "텍스트 전처리 중 단어의 기본 형태(어근)를 추출하는 기술인 'Lemmatization'과 'Stemming'의 차이는?",
            "options": [
                "Lemmatization이 훨씬 빠르다.",
                "Stemming은 문법적 맥락을 고려하지만 Lemmatization은 무조건 자른다.",
                "Lemmatization은 사전을 기반으로 단어의 의미상 원형을 찾고, Stemming은 단순 규칙으로 어미를 자른다.",
                "두 기술은 완전히 동일하다.",
                "Stemming은 영어에서만 사용 가능하다."
            ],
            "answer": "Lemmatization은 사전을 기반으로 단어의 의미상 원형을 찾고, Stemming은 단순 규칙으로 어미를 자른다.",
            "why": "표제어 추출(Lemmatization)은 품사 정보 등을 활용해 더 정교하지만 느리고, 어간 추출(Stemming)은 빠르지만 부정확할 수 있습니다.",
            "hint": "의미와 규칙의 차이를 생각하세요.",
            "trap_points": [
                "분석 목적에 따라 속도와 정확도를 트레이드오프해야 함"
            ],
            "difficulty": "hard"
        },
        {
            "chapter_name": "데이터 분석",
            "type": "multiple_choice",
            "question": "정규표현식에서 '문자열의 시작'을 의미하는 기호는?",
            "options": [
                "$",
                "^",
                ".",
                "*",
                "@"
            ],
            "answer": "^",
            "why": "^ 기호는 패턴이 문자열의 맨 앞에서 시작해야 함을 명시합니다.",
            "hint": "삿갓 모양의 기호입니다.",
            "trap_points": [
                "$는 문자열의 끝을 의미함"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "데이터 분석",
            "type": "multiple_choice",
            "question": "Pandas에서 데이터프레임의 모든 열에 대해 기술 통계량(count, mean, std 등)을 일괄적으로 보여주는 메서드는?",
            "options": [
                "df.info()",
                "df.describe()",
                "df.stats()",
                "df.summary()",
                "df.view()"
            ],
            "answer": "df.describe()",
            "why": "describe()는 숫자형 열들에 대해 주요 통계 지표를 한눈에 요약해 줍니다.",
            "hint": "설명하다, 묘사하다라는 뜻입니다.",
            "trap_points": [
                "info()는 데이터 타입과 결측치 여부를 보여줌"
            ],
            "difficulty": "easy"
        },
        {
            "chapter_name": "데이터 분석",
            "type": "short_answer",
            "question": "Numpy에서 행렬의 곱셈(행렬곱)을 수행하는 연산자 기호는?",
            "answer": "@",
            "why": "@ 연산자(또는 np.matmul)는 수학적인 행렬 곱셈을 수행합니다.",
            "hint": "이메일 주소에 쓰이는 '골뱅이' 기호입니다.",
            "trap_points": [
                "* 연산자는 요소별(element-wise) 곱셈임에 극히 주의"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "데이터 분석",
            "type": "multiple_choice",
            "question": "Pandas에서 행(Row)을 기준으로 데이터를 필터링할 때 주로 사용하는 속성은?",
            "options": [
                ".iloc",
                ".loc",
                ".at",
                ".filter",
                ".query"
            ],
            "answer": ".loc",
            "why": ".loc(Label-based)이나 .iloc(Integer-based)을 사용하여 데이터를 선택 및 필터링합니다.",
            "hint": "위치(Location)의 약자입니다.",
            "trap_points": [
                ".loc은 이름을, .iloc은 숫자를 사용함"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "데이터 분석",
            "type": "multiple_choice",
            "question": "텍스트 데이터에서 '단어의 순서 정보'를 완전히 무시하고 빈도만 계산하는 기법은?",
            "options": [
                "Word2Vec",
                "BERT",
                "Bag of Words (BOW)",
                "Transformer",
                "RNN"
            ],
            "answer": "Bag of Words (BOW)",
            "why": "BOW는 단어들을 순서 없이 가방(Bag)에 담아 둔 것과 같다고 하여 붙여진 이름입니다.",
            "hint": "단어들의 주머니라는 뜻입니다.",
            "trap_points": [
                "문맥 정보가 손실된다는 치명적 단점이 있음"
            ],
            "difficulty": "easy"
        },
        {
            "chapter_name": "데이터 분석",
            "type": "multiple_choice",
            "question": "Numpy 배열 연산 중 각 요소를 하나씩 순회하는 대신 한꺼번에 연산하는 방식을 무엇이라 하나요?",
            "options": [
                "Looping",
                "Vectorization (벡터화)",
                "Multiplexing",
                "Batching",
                "Streaming"
            ],
            "answer": "Vectorization (벡터화)",
            "why": "벡터화 연산은 내부적으로 최적화된 C 코드로 동작하여 파이썬 for 문보다 수백 배 빠릅니다.",
            "hint": "벡터 단위로 처리한다는 뜻입니다.",
            "trap_points": [
                "데이터 분석에서 속도 향상의 핵심 요소임"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "데이터 분석",
            "type": "short_answer",
            "question": "정규표현식에서 공백(Space, Tab, Newline 등)을 의미하는 특수 문자는?",
            "answer": "\\s",
            "why": "\\s는 space의 약자로 모든 종류의 공백 문자를 나타냅니다.",
            "hint": "역슬래시와 소문자 s의 조합입니다.",
            "trap_points": [
                "\\S(대문자)는 공백이 아닌 문자를 의미함"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "LLM 기본",
            "type": "multiple_choice",
            "question": "트랜스포머 아키텍처에서 인코더와 디코더 사이의 연결을 담당하며 인코더의 정보를 디코더로 넘겨주는 어텐션은?",
            "options": [
                "Self-Attention",
                "Masked Self-Attention",
                "Cross-Attention",
                "Multi-Head Attention",
                "Global Attention"
            ],
            "answer": "Cross-Attention",
            "why": "크로스 어텐션은 디코더가 생성 시 인코더가 뽑아낸 입력 문장의 맥락 벡터를 참조하게 해줍니다.",
            "hint": "서로 교차(Cross)한다는 뜻입니다.",
            "trap_points": [
                "디코더 전용 모델(GPT)에는 이 과정이 없음"
            ],
            "difficulty": "hard"
        },
        {
            "chapter_name": "LLM 기본",
            "type": "multiple_choice",
            "question": "BERT 모델이 문맥을 앞뒤 양방향으로 파악할 수 있는 근본적인 모델 구조는?",
            "options": [
                "Decoder-only",
                "Encoder-only",
                "Encoder-Decoder",
                "CNN",
                "RNN"
            ],
            "answer": "Encoder-only",
            "why": "BERT는 인코더 블록만 쌓아서 입력 문장 전체의 상관관계를 한꺼번에(양방향) 계산합니다.",
            "hint": "입력 처리에 특화된 구조입니다.",
            "trap_points": [
                "GPT는 디코더만 사용함"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "LLM 기본",
            "type": "multiple_choice",
            "question": "LLM의 성능을 평가하는 벤치마크 중 인간의 전문 지식(상식, 법률, 의학 등)을 묻는 가장 유명한 테스트는?",
            "options": [
                "HumanEval",
                "MMLU",
                "GSM8K",
                "Big-Bench",
                "Llama-Index"
            ],
            "answer": "MMLU",
            "why": "MMLU(Massive Multitask Language Understanding)는 57개 주제를 다루는 LLM 성능 측정의 표준 격인 벤치마크입니다.",
            "hint": "대규모 다중 작업 언어 이해의 약자입니다.",
            "trap_points": [
                "GSM8K는 수학 추론 능력을 평가함"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "LLM 기본",
            "type": "multiple_choice",
            "question": "토크나이저 아키텍처 중 '단어 빈도'를 기반으로 가장 자주 나오는 글자 쌍을 합쳐나가는 방식은?",
            "options": [
                "WordPiece",
                "BPE (Byte Pair Encoding)",
                "Unigram",
                "SentencePiece",
                "N-gram"
            ],
            "answer": "BPE (Byte Pair Encoding)",
            "why": "BPE는 가장 빈번한 바이트 쌍을 하나의 토큰으로 변환하는 과정을 반복하여 사전 크기를 관리합니다.",
            "hint": "바이트 쌍 인코딩의 약자입니다.",
            "trap_points": [
                "GPT 계열에서 주로 사용함"
            ],
            "difficulty": "hard"
        },
        {
            "chapter_name": "LLM 기본",
            "type": "multiple_choice",
            "question": "VRAM 16GB를 가진 GPU에서 7B 모델(FP16 정밀도)을 실행하기 위해 필요한 대략적인 최소 메모리는?",
            "options": [
                "약 7GB",
                "약 14GB",
                "약 28GB",
                "약 3.5GB",
                "약 1GB"
            ],
            "answer": "약 14GB",
            "why": "FP16은 파라미터당 2바이트를 사용하므로 70억 * 2 = 14GB가 순수 가중치 용량입니다.",
            "hint": "70억 개 가중치 * 2바이트(16비트)를 계산해 보세요.",
            "trap_points": [
                "실제 실행 시에는 KV 캐시 등으로 더 많은 메모리가 필요함"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "LLM 기본",
            "type": "short_answer",
            "question": "LLM 학습 시 방대한 양의 데이터셋으로부터 언어의 일반적인 패턴을 배우는 첫 번째 단계를 무엇이라 하나요?",
            "answer": "Pre-training (사전 학습)",
            "why": "사전 학습은 레이블 없는 원시 텍스트(Raw Text)를 통해 모델의 기초 지능을 구축하는 단계입니다.",
            "hint": "먼저(Pre) 학습시킨다는 뜻입니다.",
            "trap_points": [
                "이후에 하는 것이 파인튜닝임"
            ],
            "difficulty": "easy"
        },
        {
            "chapter_name": "LLM 기본",
            "type": "multiple_choice",
            "question": "모델이 긴 생각을 거친 후 정답을 내도록 유도하는 'Reasoning' 모델의 핵심 출력 토큰 구간은?",
            "options": [
                "<answer>",
                "<think>",
                "<reason>",
                "<output>",
                "<step>"
            ],
            "answer": "<think>",
            "why": "최신 모델(DeepSeek R1 등)은 생각하는 과정(Thinking Process)을 별도의 태그 안에 출력하여 논리력을 보강합니다.",
            "hint": "생각하다라는 영어 단어입니다.",
            "trap_points": [
                "사용자에게는 이 구간을 숨기고 최종 답변만 보여주기도 함"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "LLM 기본",
            "type": "multiple_choice",
            "question": "오픈 모델 중 Meta에서 공개하여 생태계를 주도하고 있는 모델의 이름은?",
            "options": [
                "Claude",
                "Llama",
                "Gemini",
                "Mistral",
                "Qwen"
            ],
            "answer": "Llama",
            "why": "Llama 시리즈는 고성능 오픈 가중치 모델로 AI 연구 및 서비스 개발의 표준이 되었습니다.",
            "hint": "남미의 동물 이름입니다.",
            "trap_points": [
                "Mistral은 프랑스의 경쟁 모델임"
            ],
            "difficulty": "easy"
        },
        {
            "chapter_name": "LLM 기본",
            "type": "multiple_choice",
            "question": "단어의 의미적 거리를 숫자로 표현한 벡터 공간에서 '가까울수록' 무엇이 유사한 것인가요?",
            "options": [
                "글자 수",
                "문법적 역할",
                "의미와 문맥",
                "알파벳 순서",
                "글자 크기"
            ],
            "answer": "의미와 문맥",
            "why": "임베딩 벡터 공간은 유사한 의미를 가진 단어들이 기하학적으로 가깝게 위치하도록 학습됩니다.",
            "hint": "벡터 유사도는 '의미'를 찾기 위한 도구입니다.",
            "trap_points": [
                "동음이의어라도 문맥에 따라 다른 벡터 값을 가질 수 있음(현대 LLM 기본)"
            ],
            "difficulty": "easy"
        },
        {
            "chapter_name": "LLM 기본",
            "type": "short_answer",
            "question": "하이브리드 처리 기능(텍스트+이미지+음성 등)을 가진 인공지능 모델을 일컫는 용어는?",
            "answer": "Multimodal (멀티모달)",
            "why": "여러 가지 양식(Modality)을 동시에 이해하고 생성할 수 있는 모델입니다.",
            "hint": "멀티(Multi) + 모달(Modal).",
            "trap_points": [
                "단순히 텍스트만 하는 모델과 구분됨"
            ],
            "difficulty": "easy"
        },
        {
            "chapter_name": "프롬프트 엔지니어링",
            "type": "multiple_choice",
            "question": "프롬프트의 4요소 중 '해결해야 할 구체적인 작업(요약, 분류 등)'에 해당하는 것은?",
            "options": [
                "Persona",
                "Context",
                "Format",
                "Task"
            ],
            "answer": "Task",
            "why": "Task는 모델에게 부여하는 명확한 미션입니다.",
            "hint": "일, 과업을 뜻하는 단어입니다.",
            "trap_points": [
                "Task가 불분명하면 모델이 엉뚱한 답을 줄 확률이 높음"
            ],
            "difficulty": "easy"
        },
        {
            "chapter_name": "프롬프트 엔지니어링",
            "type": "multiple_choice",
            "question": "LCEL 문법에서 입력을 가공한 뒤 정해진 스키마(JSON 등)에 맞춰 텍스트를 파싱해 주는 컴포넌트는?",
            "options": [
                "Template",
                "LLM",
                "Memory",
                "OutputParser",
                "Retriever"
            ],
            "answer": "OutputParser",
            "why": "OutputParser는 모델의 원시 텍스트 출력을 구조화된 데이터(JSON, Table 등)로 변환합니다.",
            "hint": "출력(Output)을 분석기(Parser)함.",
            "trap_points": [
                "Pydantic과 연동하여 강력한 타입 체크를 수행할 수 있음"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "프롬프트 엔지니어링",
            "type": "multiple_choice",
            "question": "사용자가 질문을 던졌을 때, 단순히 '단계적으로 생각하라'고 지시만 하는 기법은?",
            "options": [
                "Few-shot CoT",
                "Zero-shot CoT",
                "Active Prompting",
                "Plan-and-Execute",
                "ReAct"
            ],
            "answer": "Zero-shot CoT",
            "why": "예시 없이 'Step by Step으로 생각하라'는 문구만으로 추론 능력을 깨우는 방식입니다.",
            "hint": "예시가 0개(Zero)인 CoT입니다.",
            "trap_points": [
                "'Let's think step by step'이 가장 유명한 문구임"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "프롬프트 엔지니어링",
            "type": "multiple_choice",
            "question": "프롬프트 길이가 너무 길어져서 컨텍스트 한도를 초과할 때의 해결책이 아닌 것은?",
            "options": [
                "중요하지 않은 배경 설명 제거",
                "프롬프트 체이닝을 통한 단계적 처리",
                "모델의 맥락 한도(Window)를 늘리기 위해 모델 교체",
                "동일한 지시 사항을 여러 번 중복하여 작성하기",
                "RAG를 활용하여 필요한 정보만 선별 제공"
            ],
            "answer": "동일한 지시 사항을 여러 번 중복하여 작성하기",
            "why": "중복 작성은 토큰만 낭비할 뿐이며, 오히려 모델의 주의력을 분산시킬 수 있습니다.",
            "hint": "비효율적인 행동을 찾으세요.",
            "trap_points": [
                "토큰 수는 곧 비용과 직결됨"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "프롬프트 엔지니어링",
            "type": "multiple_choice",
            "question": "프롬프트 내에 '###' 이나 '---' 같은 기호를 사용하는 주된 이유는?",
            "options": [
                "단순히 예쁘게 보이기 위해",
                "구분자를 통해 모델이 각 섹션(지시, 배경, 질문)을 명확히 인지하게 하기 위해",
                "모델이 줄바꿈을 싫어하기 때문에",
                "특수 기호가 많을수록 속도가 빨라지기 때문에",
                "비밀 메시지를 숨기기 위해"
            ],
            "answer": "구분자를 통해 모델이 각 섹션(지시, 배경, 질문)을 명확히 인지하게 하기 위해",
            "why": "명확한 구조화는 모델의 주의가 흩어지는 것을 막고 정확한 응답을 유도합니다.",
            "hint": "경계선(Delimiter)의 역할입니다.",
            "trap_points": [
                "최신 모델일수록 구조화된 프롬프트를 더 잘 이해함"
            ],
            "difficulty": "easy"
        },
        {
            "chapter_name": "프롬프트 엔지니어링",
            "type": "short_answer",
            "question": "프롬프트에 입력 예제를 하나도 주지 않고 질문만 하는 방식을 무엇이라 하나요?",
            "answer": "Zero-shot Prompting",
            "why": "사전 지식 전이 없이 모델의 일반 지능만으로 해결을 시도하는 방식입니다.",
            "hint": "0개(Zero)의 샷(Shot)입니다.",
            "trap_points": [
                "모델의 성능이 좋을수록 Zero-shot만으로도 좋은 결과가 나옴"
            ],
            "difficulty": "easy"
        },
        {
            "chapter_name": "프롬프트 엔지니어링",
            "type": "multiple_choice",
            "question": "LangChain에서 입력 변수가 포함된 프롬프트 문자열을 생성하는 틀은?",
            "options": [
                "ChatModel",
                "PromptTemplate",
                "Chain",
                "OutputParser",
                "Callback"
            ],
            "answer": "PromptTemplate",
            "why": "프롬프트 템플릿은 {variable} 형태로 변수를 정의하고 나중에 값을 채워 넣는 구조입니다.",
            "hint": "프롬프트의 '틀'입니다.",
            "trap_points": [
                "StringPromptTemplate과 ChatPromptTemplate이 있음"
            ],
            "difficulty": "easy"
        },
        {
            "chapter_name": "프롬프트 엔지니어링",
            "type": "multiple_choice",
            "question": "프롬프트 엔지니어링 시 지시 사항을 '부정형(~하지 마)'보다 '긍정형(~해)'으로 주는 것이 좋은 이유는?",
            "options": [
                "모델이 긍정적인 성격이라서",
                "모델이 부정적인 지시보다 긍정적인 지시를 더 명확하게 이해하고 따르는 경향이 있기 때문에",
                "부정형 프롬프트는 비용이 더 비싸기 때문에",
                "영문법적으로 긍정형이 우수해서",
                "긍정형이 답변 길이를 늘려주기 때문"
            ],
            "answer": "모델이 부정적인 지시보다 긍정적인 지시를 더 명확하게 이해하고 따르는 경향이 있기 때문에",
            "why": "모델은 특정 행동을 하지 말라는 것보다 어떤 행동을 해야 하는지 명확히 알고 있을 때 더 잘 작동합니다.",
            "hint": "권장 사항(Do)과 금지 사항(Don't)의 효율 차이입니다.",
            "trap_points": [
                "단, 강력한 보안 지침 등에서는 부정 제약이 필수적임"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "프롬프트 엔지니어링",
            "type": "short_answer",
            "question": "사용자의 세션 정보를 유지하며 이전 대화를 프롬프트에 자동으로 포함시켜 주는 메모리 핵심 클래스는?",
            "answer": "ConversationBufferMemory",
            "why": "가장 기본적인 메모리 형태로, 대화 기록 전체를 버퍼에 담아 전달합니다.",
            "hint": "대화 버퍼 메모리입니다.",
            "trap_points": [
                "대화가 길어지면 토큰 소모가 급증하므로 요약 메모리 등을 고려해야 함"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "프롬프트 엔지니어링",
            "type": "multiple_choice",
            "question": "프롬프트 엔지니어링 지침 중 'Chain Of Thought'가 가장 효과적인 문제는?",
            "options": [
                "단순한 날씨 묻기",
                "수학 문제 풀이 및 복잡한 논리 추론",
                "오늘의 메뉴 추천",
                "인사말 나누기",
                "노래 가사 검색"
            ],
            "answer": "수학 문제 풀이 및 복잡한 논리 추론",
            "why": "CoT는 중간 계산 과정이 필요한 '추론' 작업에서 비약적인 성능 향상을 보입니다.",
            "hint": "머리를 많이 써야 하는 문제를 찾으세요.",
            "trap_points": [
                "단순 암기 문제에서는 오히려 성능 차이가 미미할 수 있음"
            ],
            "difficulty": "easy"
        },
        {
            "chapter_name": "RAG & Agent",
            "type": "multiple_choice",
            "question": "RAG 단계 중 검색된 정보(Context)를 사용자 질문과 결합하여 '풍부한 프롬프트'를 만드는 단계는?",
            "options": [
                "Indexing",
                "Searching",
                "Augmenting",
                "Generating",
                "Processing"
            ],
            "answer": "Augmenting",
            "why": "Augmenting은 검색된 지식을 덧붙여(증강하여) 모델이 답변하기 좋은 상태로 만드는 과정입니다.",
            "hint": "증강하다라는 뜻입니다.",
            "trap_points": [
                "Generating 직전의 프롬프트 조립 단계임"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "RAG & Agent",
            "type": "multiple_choice",
            "question": "벡터 DB에서 고차원 데이터를 빠르게 찾기 위해 사용하는 '근사 최근접 이웃' 검색 기술의 약자는?",
            "options": [
                "SQL",
                "ANN",
                "KNN",
                "API",
                "RAG"
            ],
            "answer": "ANN",
            "why": "ANN(Approximate Nearest Neighbor)은 정확도를 조금 희생하고 검색 속도를 획기적으로 올리는 방식입니다.",
            "hint": "가장 가까운 이웃(Nearest Neighbor)을 대략적으로(Approximate) 찾기.",
            "trap_points": [
                "정확히 찾는 KNN보다 대규모 데이터에서 압도적으로 빠름"
            ],
            "difficulty": "hard"
        },
        {
            "chapter_name": "RAG & Agent",
            "type": "multiple_choice",
            "question": "RAG의 성능을 평가할 때, '검색된 문서들이 사용자 질문과 실제로 얼마나 관련된 것인지'를 측정하는 지표는?",
            "options": [
                "Context Precision",
                "Context Recall",
                "Answer Relevance",
                "Groundedness",
                "L2 Norm"
            ],
            "answer": "Context Precision",
            "why": "컨텍스트 정밀도(Precision)는 정답과 관련된 정보가 검색 결과 상단에 얼마나 잘 포함되었는지를 봅니다.",
            "hint": "검색 결과의 '정밀도'를 생각하세요.",
            "trap_points": [
                "Context Recall은 정답에 필요한 모든 정보가 빠짐없이 검색되었는지를 봅니다"
            ],
            "difficulty": "hard"
        },
        {
            "chapter_name": "RAG & Agent",
            "type": "multiple_choice",
            "question": "RAG에서 사용자가 '최근 1주일간의 뉴스만 알려줘'라고 할 때 효율적으로 처리하는 방법은?",
            "options": [
                "그냥 전체 임베딩 검색을 한다.",
                "메타데이터 필터링(Metadata Filtering)을 적용한다.",
                "모델에게 직접 찾아보라고 한다.",
                "임베딩 모델을 바꾼다.",
                "검색된 결과를 하나씩 다 읽어본다."
            ],
            "answer": "메타데이터 필터링(Metadata Filtering)을 적용한다.",
            "why": "날짜, 작성자, 카테고리 등 속성을 미리 DB에 저장해 두고 하드웨어적으로 먼저 거른 뒤 유사도 검색을 수행합니다.",
            "hint": "데이터에 대한 데이터(Metadata)를 사용합니다.",
            "trap_points": [
                "의미 검색만으로는 정확한 시간대 제어가 어려울 수 있음"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "RAG & Agent",
            "type": "short_answer",
            "question": "검색된 다수의 문서 중 핵심적인 정보가 앞과 뒤에 있을 때보다 중간에 있을 때 모델이 잘 인지하지 못하는 현상은?",
            "answer": "Lost in the Middle",
            "why": "긴 컨텍스트에서 모델이 중간 부분을 덜 중요하게 여기는 바이어스 때문에 발생합니다.",
            "hint": "중간에서 길을 잃다(Lost).",
            "trap_points": [
                "를 해결하기 위해 Reordering 기법이 사용됨"
            ],
            "difficulty": "hard"
        },
        {
            "chapter_name": "RAG & Agent",
            "type": "multiple_choice",
            "question": "에이전트 구현 시 '반성(Reflection)' 기술이 필요한 주된 이유는?",
            "options": [
                "에이전트가 예의 바르게 답변하게 하기 위해",
                "모델의 첫 번째 답변에서 나타난 논리적 오류나 부족한 정보를 스스로 보완하기 위해",
                "답변의 속도를 빠르게 하기 위해",
                "비용을 절약하기 위해",
                "화려한 UI를 위해"
            ],
            "answer": "모델의 첫 번째 답변에서 나타난 논리적 오류나 부족한 정보를 스스로 보완하기 위해",
            "why": "한 번에 완벽한 답을 내기 어려운 복잡한 작업에서 '검증'과 '재시도' 루프를 통해 품질을 올립니다.",
            "hint": "스스로를 돌아보고 수정하는 능력입니다.",
            "trap_points": [
                "모델의 Reasoning 성능이 높을수록 반성 능력도 좋아짐"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "RAG & Agent",
            "type": "multiple_choice",
            "question": "전문가들(검색 에이전트, 작성 에이전트 등)이 협업하는 시스템 구조는?",
            "options": [
                "Single Agent",
                "Multi-Agent Systems (MAS)",
                "Linear Pipeline",
                "Manual Workflow",
                "Hardcoded Logic"
            ],
            "answer": "Multi-Agent Systems (MAS)",
            "why": "작업을 분담하고 서로 피드백을 주고받아 결과물의 수준을 높이는 고도화된 구조입니다.",
            "hint": "여러 에이전트가 함께합니다.",
            "trap_points": [
                "LangGraph 등이 이를 구현하기 위한 대표적인 라이브러리임"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "RAG & Agent",
            "type": "multiple_choice",
            "question": "벡터 DB에서 두 벡터의 직선 거리를 측정할 때 사용하는 가장 흔한 메트릭은?",
            "options": [
                "Cosine Similarity",
                "L2 Distance (Euclidean)",
                "Dot Product",
                "Hamming Distance",
                "Jaccard"
            ],
            "answer": "L2 Distance (Euclidean)",
            "why": "유클리드 거리는 다차원 공간에서의 직선의 절댓값을 계산합니다.",
            "hint": "피타고라스 정리의 확장형입니다.",
            "trap_points": [
                "거리이므로 작을수록 유사한 것임 (유사도는 클수록 유사)"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "RAG & Agent",
            "type": "short_answer",
            "question": "인터넷 웹사이트의 정보를 실시간으로 긁어와서 에이전트에게 제공하는 도구(Tool)를 보통 무엇이라 하나요?",
            "answer": "Search Tool (또는 Web Search/Browsing Tool)",
            "why": "최신 정보 반영(Grounding)을 위해 Tavily, SerpAPI 등을 연동하여 사용합니다.",
            "hint": "검색 도구입니다.",
            "trap_points": [
                "모델 자체가 인터넷을 돌아다니는 것이 아니라 도구를 '사용'하는 것임"
            ],
            "difficulty": "easy"
        },
        {
            "chapter_name": "RAG & Agent",
            "type": "multiple_choice",
            "question": "RAG 시스템에서 청크 오버랩(Overlap)의 크기를 결정할 때 고려해야 할 사항은?",
            "options": [
                "메모리가 많으면 무조건 최대한 크게 한다.",
                "청크 사이즈의 일정 비율(예: 10~20%)로 설정하여 의미 단절을 최소화한다.",
                "오버랩은 항상 0으로 두는 것이 깔끔하다.",
                "오버랩은 한글에서만 사용한다.",
                "오버랩을 크게 하면 검색이 무조건 정확해진다."
            ],
            "answer": "청크 사이즈의 일정 비율(예: 10~20%)로 설정하여 의미 단절을 최소화한다.",
            "why": "너무 크면 중복 정보가 많아 검색 효율이 떨어지고, 너무 작으면 문맥이 잘릴 위험이 있습니다.",
            "hint": "적절한(10~20%) 비율을 생각하세요.",
            "trap_points": [
                "문서의 특성에 따라 최적의 오버랩 값이 다름"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "Fine Tuning",
            "type": "multiple_choice",
            "question": "파인튜닝의 대표적인 방법인 LoRA에서 학습 대상이 되는 가중치는?",
            "options": [
                "모델의 전체 가중치",
                "모델에 추가된 두 개의 저차원 행렬 가중치 (A, B)",
                "바이어스 값만",
                "임베딩 레이어 가중치",
                "마지막 층의 가중치만"
            ],
            "answer": "모델에 추가된 두 개의 저차원 행렬 가중치 (A, B)",
            "why": "원본 파라미터는 동결하고, 옆에 붙인 가벼운 행렬만 업데이트하여 효율을 극대화합니다.",
            "hint": "옆에 덧붙인(Adapter) 작은 행렬들을 생각하세요.",
            "trap_points": [
                "이를 통해 수백 배 적은 메모리로 파인튜닝이 가능해짐"
            ],
            "difficulty": "hard"
        },
        {
            "chapter_name": "Fine Tuning",
            "type": "multiple_choice",
            "question": "DPO(Direct Preference Optimization) 알고리즘이 PPO(기존 RLHF)보다 구현이 쉬운 결정적인 이유는?",
            "options": [
                "데이터가 적어도 되기 때문에",
                "보상 모델(Reward Model) 학습 과정이 필요 없기 때문에",
                "한국어 전용이라서",
                "GPU 없이도 학습 가능해서",
                "학습 모델이 작아도 되기 때문에"
            ],
            "answer": "보상 모델(Reward Model) 학습 과정이 필요 없기 때문에",
            "why": "DPO는 선호도 데이터를 직접 정답 확률에 반영하여 복잡한 리워드 모델링 단계를 제거했습니다.",
            "hint": "샘플 문제에서도 다뤘던 핵심 비교 포인트입니다.",
            "trap_points": [
                "하지만 여전히 선호도 답변 쌍(A vs B) 데이터는 잘 구축해야 함"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "Fine Tuning",
            "type": "multiple_choice",
            "question": "파인튜닝 과정에서 모델의 가독성과 코드 형식이 혼재될 때(DeepSeek R1-Zero 사례), 이를 해결하기 위해 결합하는 방식은?",
            "options": [
                "다시 Pre-training 하기",
                "Cold-start SFT를 거친 후 강화학습 진행",
                "학습 데이터 다 지우기",
                "모델 파라미터 무작위 초기화",
                "영어로만 재학습"
            ],
            "answer": "Cold-start SFT를 거친 후 강화학습 진행",
            "why": "기초적인 답변 형식(SFT)을 먼저 가르친 뒤에 강화학습을 시켜야 가동성과 성능을 동시에 잡을 수 있습니다.",
            "hint": "입문(Cold-start) 과정을 생각하세요.",
            "trap_points": [
                "R1의 최종 성공 비결이 바로 이 단계적 학습임"
            ],
            "difficulty": "hard"
        },
        {
            "chapter_name": "Fine Tuning",
            "type": "multiple_choice",
            "question": "모델이 인간의 사회적 가치관이나 안전 지침을 위반하지 않도록 최종적으로 튜닝하는 것을 무엇이라 하나요?",
            "options": [
                "Alignment (정렬)",
                "Scaling",
                "Distillation",
                "Pruning",
                "Backpropagation"
            ],
            "answer": "Alignment (정렬)",
            "why": "모델의 지능과 인간의 의도/가치관을 일직선으로 맞춘다는 의미의 용어입니다.",
            "hint": "나란히 맞춘다는 뜻입니다.",
            "trap_points": [
                "RLHF가 이 정렬을 위한 가장 대표적인 도구임"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "Fine Tuning",
            "type": "short_answer",
            "question": "원본 모델의 지식을 유지하면서 4비트 양자화와 LoRA를 결합하여 VRAM 사용량을 극단적으로 낮춘 파인튜닝 기법은?",
            "answer": "QLoRA",
            "why": "Quantized LoRA의 약자로, 48GB VRAM이 필요한 모델을 16GB에서도 튜닝 가능하게 만든 혁명적 기술입니다.",
            "hint": "Q + LoRA.",
            "trap_points": [
                "NF4(Normal Float 4)라는 특수 양자화 분포를 사용함"
            ],
            "difficulty": "hard"
        },
        {
            "chapter_name": "Fine Tuning",
            "type": "multiple_choice",
            "question": "파인튜닝 데이터 구축 시, 모델이 같은 답변을 반복하지 않도록 다양성을 확보하는 것이 중요한 이유는?",
            "options": [
                "비용을 늘리기 위해",
                "모델이 한 가지 패턴에만 과적합(Overfitting)되는 것을 방지하기 위해",
                "답변 속도를 늦추기 위해",
                "사용자를 혼란스럽게 하기 위해",
                "글자 수를 늘리기 위해"
            ],
            "answer": "모델이 한 가지 패턴에만 과적합(Overfitting)되는 것을 방지하기 위해",
            "why": "데이터의 다양성이 부족하면 모델의 유연성이 떨어지고 '치명적 망각'이 심해질 수 있습니다.",
            "hint": "너무 한쪽으로 치우치는 현상을 막는 것입니다.",
            "trap_points": [
                "적은 양의 고품질 데이터라도 다양성이 담보되어야 함"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "Fine Tuning",
            "type": "multiple_choice",
            "question": "학습 과정에서 성능이 확인된 데이터만 선별하여 다시 학습 모델에 주입하는 기법은?",
            "options": [
                "Random Sampling",
                "Rejection Sampling (기각 샘플링)",
                "Data Scaling",
                "Model Merging",
                "Weight Averaging"
            ],
            "answer": "Rejection Sampling (기각 샘플링)",
            "why": "여러 답변 중 좋은 답만 골라내어(Sampling) 나머지 저품질 답변은 기각(Rejection)하는 반복 개선 방식입니다.",
            "hint": "거절, 기각의 의미입니다.",
            "trap_points": [
                "오픈 모델들의 성능을 끌어올릴 때 필수적인 단계임"
            ],
            "difficulty": "hard"
        },
        {
            "chapter_name": "Fine Tuning",
            "type": "multiple_choice",
            "question": "파인튜닝된 모델의 성능 평가 시, 훈련 데이터에 없는 새로운 질문에 대해 얼마나 잘 답하는지를 중점적으로 보는 이유는?",
            "options": [
                "훈련 데이터가 가짜라서",
                "모델의 '일반화(Generalization)' 능력을 검증하기 위해",
                "문법 오타를 찾기 위해",
                "답변의 미사여구를 평가하기 위해",
                "데이터 소유권을 확인하기 위해"
            ],
            "answer": "모델의 '일반화(Generalization)' 능력을 검증하기 위해",
            "why": "단순 암기가 아닌 실제 지적 능력을 습득했는지 확인하기 위함입니다.",
            "hint": "보편적으로 잘하는 능력을 생각하세요.",
            "trap_points": [
                "과적합된 모델은 이 단계에서 실패함"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "Fine Tuning",
            "type": "short_answer",
            "question": "고성능 모델의 출력값을 정답(Label)으로 삼아 더 작은 모델을 학습시키는 과정을 무엇이라 하나요?",
            "answer": "Knowledge Distillation (지식 증류)",
            "why": "거대 모델의 지능을 효과적으로 압축하여 효율적인 소형 모델을 만드는 전략입니다.",
            "hint": "지식을 증류(Distillation)함.",
            "trap_points": [
                "합성 데이터 학습도 이 범주에 포함됨"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "Fine Tuning",
            "type": "multiple_choice",
            "question": "파인튜닝 후 안전 필터링이 너무 강해져서 정상적인 질문에도 '답변할 수 없습니다'라고 거부하는 현상을 무엇이라 하나요?",
            "options": [
                "Under-refusal",
                "Over-refusal (과도한 거부)",
                "Correct Alignment",
                "Successful Tuning",
                "Model Collapse"
            ],
            "answer": "Over-refusal (과도한 거부)",
            "why": "안전 정렬(Safety Alignment)이 과하게 적용되어 모델이 무해한 질문까지 위험하다고 판단하는 부작용입니다.",
            "hint": "너무 많이(Over) 거절(Refusal)함.",
            "trap_points": [
                "가동성(Helpfulness)과 안전성(Safety)의 균형 잡기가 어려움"
            ],
            "difficulty": "medium"
        }
    ]
}