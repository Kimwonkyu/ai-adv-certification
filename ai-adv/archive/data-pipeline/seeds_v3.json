{
    "questions": [
        {
            "chapter_name": "Python 기초",
            "type": "short_answer",
            "question": "파이썬에서 여러 객체를 동시에 순회할 때 사용하는 내장 함수는?",
            "answer": "zip()",
            "why": "zip() 함수는 여러 개의 순회 가능한(iterable) 객체를 인자로 받아 각각의 요소를 튜플로 묶어서 반환합니다.",
            "hint": "지퍼처럼 두 쪽을 하나로 엮어줍니다.",
            "trap_points": [
                "enumerate()는 인덱와 값을 동시에 가져올 때 사용함"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "데이터 분석",
            "type": "short_answer",
            "question": "정규표현식에서 숫자를 나타내는 특수 문자는?",
            "answer": "\\d",
            "why": "\\d는 digit의 약자로 0~9 사이의 숫자를 의미하는 정규표현식 메타 문자입니다.",
            "hint": "역슬래시와 소문자 d의 조합입니다.",
            "trap_points": [
                "\\s는 공백을 의미함"
            ],
            "difficulty": "easy"
        },
        {
            "chapter_name": "LLM 기본",
            "type": "multiple_choice",
            "question": "GPT-4o에서 'o'가 의미하는 단어와 그 개념으로 올바른 것은?",
            "options": [
                "Open: 지식이 개방됨",
                "Omni: 텍스트, 이미지, 오디오 등을 통합 처리하는 멀티모달",
                "Online: 실시간 웹 검색 지원",
                "Optimized: 속도가 최적화됨",
                "original: 초기 모델로의 회귀"
            ],
            "answer": "Omni: 텍스트, 이미지, 오디오 등을 통합 처리하는 멀티모달",
            "why": "Omni는 모든(All)이라는 뜻으로, GPT-4o가 다양한 양식(Modality)을 동시에 입력받고 출력할 수 있음을 의미합니다.",
            "hint": "다양한 매체를 '모두' 아우른다는 뜻입니다.",
            "trap_points": [
                "단순히 성능이 좋은 것(Optimized)과는 의미가 다름"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "LLM 기본",
            "type": "multiple_choice",
            "question": "BERT 모델이 문장의 맥락을 파악할 때 사용하는 주요 사전학습 방식은?",
            "options": [
                "Next Token Prediction",
                "Masked Language Modeling (MLM)",
                "Adversarial Training",
                "Image Captioning",
                "Audio Filtering"
            ],
            "answer": "Masked Language Modeling (MLM)",
            "why": "BERT는 문장 내의 일부 단어를 [MASK]로 가리고 주변 단어들을 통해 이를 맞히는 양방향 학습 방식을 사용합니다.",
            "hint": "가면(Mask)을 씌우고 알아맞히기 놀이를 생각하세요.",
            "trap_points": [
                "GPT는 Next Token Prediction(다음 토큰 예측) 방식임"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "프롬프트 엔지니어링",
            "type": "multiple_choice",
            "question": "LLM이 스스로 답을 내기 전, 풀이 과정이나 중간 구조를 먼저 작성하게 하는 전략은?",
            "options": [
                "Zero-shot",
                "Chain of Thought (CoT)",
                "Negative Prompting",
                "Few-shot",
                "Reverse Prompting"
            ],
            "answer": "Chain of Thought (CoT)",
            "why": "CoT는 복잡한 추론이 필요한 작업에서 모델이 단계적으로 생각하도록 유도하여 정답률을 획기적으로 높입니다.",
            "hint": "생각의 사슬을 연결하세요.",
            "trap_points": [
                "토큰 사용량이 늘어 비용이 증가할 수 있음"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "프롬프트 엔지니어링",
            "type": "short_answer",
            "question": "LangChain에서 복잡한 로직을 순차적으로 처리하기 위해 단계별 프롬프트를 쪼개서 연결하는 기술을 무엇이라 합니까?",
            "answer": "Prompt Chaining",
            "why": "프롬프트 체이닝은 한 번의 커다란 요청 대신 여러 작은 단계로 나누어 정확도를 높이고 검증을 가능하게 합니다.",
            "hint": "프롬프트들을 사슬(Chain)처럼 엮는 것입니다.",
            "trap_points": [
                "단순한 RAG와는 구분되는 프롬프트 설계 기법임"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "RAG & Agent",
            "type": "multiple_choice",
            "question": "RAG에서 검색을 위해 질문에 대한 가상의 답변을 먼저 생성하고 이를 임베딩하여 문서를 찾는 방식은?",
            "options": [
                "BM25",
                "HyDE (Hypothetical Document Embedding)",
                "Multi-Query",
                "Contextual Retrieval",
                "Small2Big"
            ],
            "answer": "HyDE (Hypothetical Document Embedding)",
            "why": "HyDE는 실제 질문보다 LLM이 생성한 가상 답변이 문서 저장소의 내용과 유사도가 더 높을 수 있다는 점을 이용한 고급 검색 기법입니다.",
            "hint": "가상(Hypothetical)의 문서 임베딩입니다.",
            "trap_points": [
                "가상 답변에 할루시네이션이 섞여도 유사도 검색에는 도움이 될 수 있음"
            ],
            "difficulty": "hard"
        },
        {
            "chapter_name": "RAG & Agent",
            "type": "multiple_choice",
            "question": "문서를 작은 청크로 나누면서도 검색 시에는 상위의 더 큰 맥락을 가져오는 청킹 전략은?",
            "options": [
                "Recursive Chunking",
                "Sliding Window",
                "Parent-Child / Small2Big",
                "Token-based Chunking",
                "Sentence Chunking"
            ],
            "answer": "Parent-Child / Small2Big",
            "why": "검색은 작은 단위(Small)로 정밀하게 수행하고, 실제 LLM에 전달할 때는 그 작은 조각이 포함된 큰 맥락(Parent/Big)을 제공하여 성능을 높입니다.",
            "hint": "부모(Parent)와 자식(Child) 관계를 생각하세요.",
            "trap_points": [
                "단순히 겹치게 하는 Sliding Window와는 다름"
            ],
            "difficulty": "medium"
        },
        {
            "chapter_name": "Fine Tuning",
            "type": "multiple_choice",
            "question": "DeepSeek-R1-Zero처럼 별도의 SFT 없이 규칙 기반 보상만으로 추론 능력을 학습시키는 강화학습 알고리즘은?",
            "options": [
                "PPO",
                "DPO",
                "GRPO",
                "SFT",
                "RAFT"
            ],
            "answer": "GRPO",
            "why": "GRPO는 그룹 내 상대적 보상을 통해 학습하며, DeepSeek R1의 추론 성능 극대화에 핵심적인 역할을 했습니다.",
            "hint": "G로 시작하는 4글자 알고리즘입니다.",
            "trap_points": [
                "PPO와 달리 별도의 가치 모델(Value Model)이 필요 없어 메모리가 절약됨"
            ],
            "difficulty": "hard"
        },
        {
            "chapter_name": "Fine Tuning",
            "type": "short_answer",
            "question": "모델의 가중치 비트 수를 낮추어(예: 16bit -> 4bit) 모델 크기를 줄이는 기술은?",
            "answer": "Quantization (양자화)",
            "why": "양자화는 메모리와 연산량을 획기적으로 줄여 일반 PC나 모바일에서도 거대 모델을 실행할 수 있게 합니다.",
            "hint": "Q로 시작하는 전문 용어입니다.",
            "trap_points": [
                "파인튜닝과는 별개의 최적화 단계임"
            ],
            "difficulty": "easy"
        },
        {
            "chapter_name": "데이터 분석",
            "type": "multiple_choice",
            "question": "Pandas에서 결측치(NaN)를 특정 값으로 채우기 위해 사용하는 메서드는?",
            "options": [
                "dropna()",
                "fillna()",
                "isna()",
                "notna()",
                "replace()"
            ],
            "answer": "fillna()",
            "why": "fillna()는 데이터프레임 내의 결측값을 지정된 값이나 평균값 등으로 보충(fill)할 때 사용합니다.",
            "hint": "채우다(Fill)의 의미가 담겨 있습니다.",
            "trap_points": [
                "dropna()는 아예 지워버리는 것임"
            ],
            "difficulty": "easy"
        },
        {
            "chapter_name": "RAG & Agent",
            "type": "multiple_choice",
            "question": "하나의 질문을 여러 개의 다른 질문으로 변환하여 검색 범위를 넓히는 기법은?",
            "options": [
                "Query Reformulation",
                "Multi-Querying",
                "Step-back Prompting",
                "Self-RAG",
                "Correction Agent"
            ],
            "answer": "Multi-Querying",
            "why": "사용자의 단일 질문을 다양한 관점의 여러 쿼리로 확장하여 검색함으로써 유사도 검색의 한계를 극복하고 풍부한 컨텍스트를 확보합니다.",
            "hint": "여러 개(Multi)의 질문(Query)을 만듭니다.",
            "trap_points": [
                "단순한 결과 재배열(Re-ranking)과는 다름"
            ],
            "difficulty": "medium"
        }
    ]
}