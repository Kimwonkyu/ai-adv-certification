***

## 1. LLM 초기화 패턴

```python
from langchain_openapi import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.rate_limiters import InMemoryRateLimiter

rate_limiter = InMemoryRateLimiter(
    requests_per_second=0.167,
    check_every_n_seconds=0.1,
    max_bucket_size=10,
)

llm = ChatOpenAI(model="gpt-5-mini", temperature=0.3)
llm_gemini = ChatGoogleGenerativeAI(
    model = "gemini-2.5-flash",
    temperature=0.7,
    rate_limiter=rate_limiter,
)
```

빈칸 포인트: `ChatOpenAI`, `ChatGoogleGenerativeAI`, `model=...`, `temperature=...`, `rate_limiter=...`.

***

## 2. 기본 Prompt / LCEL 체인

### 2-1. ChatPromptTemplate + LLM

```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "당신은 LLM과 자연어 처리의 전문가입니다. ..."),
        ("human", "[단어]: {term}"),
    ]
)

chain = prompt | llm
result = chain.invoke({"term": "Transformer"})
```

빈칸 포인트: `ChatPromptTemplate.from_messages`, 튜플 `("system",...)`, `("human", ...)`, 체인 연산자 `|`.

### 2-2. RunnableSequence / Parser

```python
from langchain_core.output_parsers import StrOutputParser

chain = prompt | llm | StrOutputParser()
```

빈칸 포인트: `StrOutputParser()` 위치, `|`로 연결.

***

## 3. LangChain Prompt 템플릿들

### 3-1. 문자열 / 메시지 리스트

```python
from langchain_core.messages import SystemMessage, HumanMessage

messages = [
    SystemMessage("당신은 매우 창의력이 뛰어납니다."),
    HumanMessage("엔비디아와 XAI가 합병하면..."),
]

llm.invoke(message)
```

빈칸 포인트: `SystemMessage`, `HumanMessage`, 리스트 `[...]`로 전달.

### 3-2. ChatPromptTemplate 변수

```python
prompt = ChatPromptTemplate.from_template(
    "너는 {role}야. 아래 질문에 답해라. \n\n질문: {question}"
)

prompt.format(role="마케터", question="신제품 런청 전략?")
```

빈칸 포인트: `from_template`, 중괄호 `{role}`, `{question}`, `format(...)`.

***

## 4. Text Splitter / RAG 기본

### 4-1. 문서 청킹

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=10000,
    chunk_overlap=2000,
)

chunks = text_splitter.split_documents([result])
```

빈칸 포인트: `RecursiveCharacterTextSplitter`, `chunk_size`, `chunk_overlap` , `split_documents`.

### 4-2. 기본 Retriever → MultiQueryRetriever

```python
from langchain.vectorstores import FAISS
from langchain.retrievers.multi_query import MultiQueryRetriever

vectorstore = FAISS.from_documents(chunks, embedding)
base_retriever = vectorstore.as_retriever()

rewrite_prompt = ChatPromptTemplate.from_template(
    "사용자 질문을 3개의 유사하지만 다른 검색 쿼리로 바꿔줘.\n\n질문: {question}"
)

multi_query_retriver = MultiQueryRetriever.from_llm(
    retriever=base_retriever,
    prompt=rewite_prompt,
    llm=llm,
)
```

***

## 5. Tool / @tool 데코레이터

```python
from langchain_core.tools import tool
from langchain_experimental.utilities import PythonREPL
from typing import Annotated

repl = PythonREPL()

@tool
def python_repl_tool(
    code: Annotated[str, "The python code to execute."],
):
    """Use this to execute python code."""
    try:
        result = repl.run(code)
    except BaseException as e:
        return f"Failed to execute. Error: {repr(e)}"
    return f"Successfully executed:\n``````\nStdout: {result}"       
```

빈칸 포인트: `@tool`, `Annotated[...]`, `repl.run(code)`, `except BaseException as e`.

***

## 6. Tool Binding / ReAct Agent 뼈대

```python
def react_agent(llm, question, tools=[tavily_search]):
    tool_list = {x.name: x for x in tools}
    llm_with_tools = llm.bind_tools(tools)

    messages = [HumanMessage(question)]

    while True:
        response = llm_with_tools.invoke(message)
```

빈칸 포인트:
- 툴 dict: `{x.name: x for x in tools}`
- 도구 바인딩: `llm.bind_tools(tools)`
- `HumanMessage(question)`.

***

## 7. TavilySearch / Web Loader 툴

```python
from langchain_tavily import TavilySearch

@tool
def tavily_search(query, max_results=5):
    """Tavily API를 통해 검색 결과를 가져옵니다."""
    tavily_search = TavilySearch(
        max_results=max_results,
        include_raw_content="markdown",
    )
    search_result = tavily_search.invoke(query)["result"]

    context = ""
    for doc in search_results:
        if doc.get("raw_content"):
            doc_content = doc["raw_content"]
        else:
            doc_content = doc["content"]
        context += (
            "TITLE: " + doc.get("title", "N/A")
            + "\nURL:" + doc["url"]
            + "\nContent:" + doc_content + "\n\n---\n\n"
        )
    return context
```

빈칸 포인트: `TavilySearch`, `max_results` , `invoke`, `["results"]`, `doc.get(...)`.

***

## 8. 기타 자주 나올 수 있는 것들

- 환경 변수 로드
  ```python
  from dotenv import load_dotenv
  load_dotenv(override=True)
  ```

- LangChain Runnables 특수 기능
  ```python
  from langchain_core.runnables import RunnablPassthrough, RunnableParallel
  ```