# [ì‹¤ìŠµ] ë­ì²´ì¸ì˜ Document Loaderì™€ Splitter

ë­ì²´ì¸ì˜ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬, ë‹¤ì–‘í•œ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ê³  ì „ì²˜ë¦¬í•´ ë³´ê² ìŠµë‹ˆë‹¤.
í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.
!pip install langchain==0.3.27 langchain-community==0.3.27 langchain-experimental jq langchain-openai tiktoken pypdf beautifulsoup4 lxml python-docx pandas openpyxl -q
import os
from typing import List
import json
import pandas as pd
from pprint import pprint
from dotenv import load_dotenv

load_dotenv('.env', override=True)

# LangChain Document Loaders
from langchain.document_loaders import (
    TextLoader,
    PyPDFLoader,
    WebBaseLoader,
    CSVLoader,
    JSONLoader,
    UnstructuredWordDocumentLoader,
    UnstructuredExcelLoader,
    DirectoryLoader,
    NotebookLoader,
    GitLoader
)

from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    CharacterTextSplitter,
    TokenTextSplitter,
    MarkdownHeaderTextSplitter,
    HTMLHeaderTextSplitter,
    RecursiveJsonSplitter,
    Language
)
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings

print("í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ")
ì²­í‚¹ì€ RAG ì„±ëŠ¥ì— ë§¤ìš° ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. 

- **ì²­í¬ í¬ê¸°(Chunk Size)**: ë„ˆë¬´ í¬ë©´ ê²€ìƒ‰ ì •í™•ë„ ê°ì†Œ, ë„ˆë¬´ ì‘ìœ¼ë©´ ë¬¸ë§¥ ì†ì‹¤
- **ì˜¤ë²„ë©(Overlap)**: ì²­í¬ ê°„ ì—°ê²°ì„± ìœ ì§€
- **êµ¬ì¡° ë³´ì¡´**: ë¬¸ì„œì˜ ë…¼ë¦¬ì  êµ¬ì¡° ìœ ì§€
- **ì˜ë¯¸ì  ì¼ê´€ì„±**: ê´€ë ¨ ì •ë³´ë¥¼ ê°™ì€ ì²­í¬ì— ìœ ì§€
WebBaseLoaderëŠ” ì›¹ í˜ì´ì§€ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
# ì›¹ í˜ì´ì§€ ë¡œë”
from langchain.document_loaders import WebBaseLoader

# ì—¬ëŸ¬ ì›¹ í˜ì´ì§€ë¥¼ í•œë²ˆì— ë¡œë“œ
web_loader = WebBaseLoader([
    "https://python.langchain.com/docs/get_started/introduction",
    "https://python.langchain.com/docs/integrations/providers/openai/"
])

# ì›¹ í˜ì´ì§€ ë¡œë“œ
try:
    web_documents = web_loader.load()
    print(f"ë¡œë“œëœ ì›¹ í˜ì´ì§€ ìˆ˜: {len(web_documents)}")
    
    for i, doc in enumerate(web_documents[:2]):
        print(f"\nì›¹ í˜ì´ì§€ {i+1}:")
        print(f"URL: {doc.metadata.get('source', 'Unknown')}")
        print(f"ì œëª©: {doc.metadata.get('title', 'No title')}")
        print(f"ì „ì²´ ê¸¸ì´:" )
        print(f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {doc.page_content[:200]}...")
except Exception as e:
    print(f"ì›¹ í˜ì´ì§€ ë¡œë”© ì˜¤ë¥˜: {e}")
    print("ì˜¤í”„ë¼ì¸ í™˜ê²½ì´ê±°ë‚˜ ì›¹ì‚¬ì´íŠ¸ ì ‘ì†ì´ ì œí•œëœ ê²½ìš°ì…ë‹ˆë‹¤.")
web_documents[1]
CSVLoaderëŠ” CSV íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.    
ì„ì˜ì˜ Pandas DataFrameì„ ë§Œë“¤ê³  ì‹¤í–‰í•´ ë³´ê² ìŠµë‹ˆë‹¤.
# ìƒ˜í”Œ CSV ë°ì´í„° ìƒì„±
import pandas as pd

# ìƒ˜í”Œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
data = {
    'product_name': ['ë…¸íŠ¸ë¶ Pro', 'ë¬´ì„  ë§ˆìš°ìŠ¤', 'ê¸°ê³„ì‹ í‚¤ë³´ë“œ', 'USB-C í—ˆë¸Œ', 'ì›¹ìº  HD'],
    'category': ['ì»´í“¨í„°', 'ì•¡ì„¸ì„œë¦¬', 'ì•¡ì„¸ì„œë¦¬', 'ì•¡ì„¸ì„œë¦¬', 'ì•¡ì„¸ì„œë¦¬'],
    'price': [1500000, 35000, 120000, 45000, 80000],
    'description': [
        'ê³ ì„±ëŠ¥ ë…¸íŠ¸ë¶ìœ¼ë¡œ ê°œë°œìì™€ ë””ìì´ë„ˆì—ê²Œ ì í•©í•©ë‹ˆë‹¤.',
        'í¸ì•ˆí•œ ê·¸ë¦½ê°ê³¼ ì •í™•í•œ íŠ¸ë˜í‚¹ì„ ì œê³µí•˜ëŠ” ë¬´ì„  ë§ˆìš°ìŠ¤ì…ë‹ˆë‹¤.',
        'ì²´ë¦¬ MX ìŠ¤ìœ„ì¹˜ë¥¼ ì‚¬ìš©í•œ ê¸°ê³„ì‹ í‚¤ë³´ë“œë¡œ íƒ€ì´í•‘ ê°ê°ì´ ë›°ì–´ë‚©ë‹ˆë‹¤.',
        'ë‹¤ì–‘í•œ í¬íŠ¸ë¥¼ ì§€ì›í•˜ëŠ” USB-C í—ˆë¸Œì…ë‹ˆë‹¤.',
        '1080p í•´ìƒë„ë¥¼ ì§€ì›í•˜ëŠ” ê³ í™”ì§ˆ ì›¹ìº ì…ë‹ˆë‹¤.'
    ],
    'stock': [50, 200, 100, 150, 75]
}

df = pd.DataFrame(data)
df.to_csv('products.csv', index=False, encoding='utf-8')
print("CSV íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")


# CSV ë¡œë” ì‚¬ìš©
csv_loader = CSVLoader(
    file_path='products.csv',
    encoding='utf-8'
)

csv_documents = csv_loader.load()
print(f"\në¡œë“œëœ CSV í–‰ ìˆ˜: {len(csv_documents)}")

# ê° í–‰ì˜ ë‚´ìš© í™•ì¸
for i, doc in enumerate(csv_documents[:3]):
    print(f"\ní–‰ {i+1}:")
    print(f"ë‚´ìš©: {doc.page_content}")
    print(f"ë©”íƒ€ë°ì´í„°: {doc.metadata}")
JSON ë°ì´í„°ë„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ìƒ˜í”Œ JSON ë°ì´í„° ìƒì„±
json_data = {
    "courses": [
        {
            "id": 1,
            "title": "Python ê¸°ì´ˆ",
            "instructor": "ê¹€íŒŒì´ì¬",
            "duration": "4ì£¼",
            "level": "ì´ˆê¸‰",
            "topics": ["ë³€ìˆ˜", "ì¡°ê±´ë¬¸", "ë°˜ë³µë¬¸", "í•¨ìˆ˜"],
            "description": "íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë°ì˜ ê¸°ì´ˆë¥¼ ë°°ìš°ëŠ” ê³¼ì •ì…ë‹ˆë‹¤."
        },
        {
            "id": 2,
            "title": "ë¨¸ì‹ ëŸ¬ë‹ ì…ë¬¸",
            "instructor": "ì´AI",
            "duration": "8ì£¼",
            "level": "ì¤‘ê¸‰",
            "topics": ["ì§€ë„í•™ìŠµ", "ë¹„ì§€ë„í•™ìŠµ", "ì‹ ê²½ë§", "ë”¥ëŸ¬ë‹ ê¸°ì´ˆ"],
            "description": "ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ë³¸ ê°œë…ê³¼ ì•Œê³ ë¦¬ì¦˜ì„ í•™ìŠµí•©ë‹ˆë‹¤."
        },
        {
            "id": 3,
            "title": "LangChain ë§ˆìŠ¤í„°",
            "instructor": "ë°•ì²´ì¸",
            "duration": "6ì£¼",
            "level": "ê³ ê¸‰",
            "topics": ["Document Loader", "Text Splitter", "Embeddings", "Vector Store", "Chains", "Agents"],
            "description": "LangChainì„ í™œìš©í•œ AI ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤."
        }
    ]
}

# JSON íŒŒì¼ë¡œ ì €ì¥
with open('courses.json', 'w', encoding='utf-8') as f:
    json.dump(json_data, f, ensure_ascii=False, indent=2)
print("JSON íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

# JSONLoader ì‚¬ìš© (jq ìŠ¤íƒ€ì¼ í•„í„°ë§)
from langchain.document_loaders import JSONLoader

def metadata_func(record: dict, metadata: dict) -> dict:
    """ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜"""
    metadata["instructor"] = record.get("instructor", "Unknown")
    metadata["level"] = record.get("level", "Unknown")
    return metadata

# JSON ë¡œë” ì„¤ì •
json_loader = JSONLoader(
    file_path='courses.json',
    jq_schema='.courses[]',  
    metadata_func=metadata_func,
    content_key='description',
)

json_documents = json_loader.load()
print(f"\në¡œë“œëœ JSON ë¬¸ì„œ ìˆ˜: {len(json_documents)}")

# ê° ë¬¸ì„œ í™•ì¸
for i, doc in enumerate(json_documents):
    print(f"\në¬¸ì„œ {i+1}:")
    print(f"ë‚´ìš©: {doc.page_content[:200]}...")
    print(f"ë©”íƒ€ë°ì´í„°: {doc.metadata}")
ì´ì™¸ì—ë„ ë‹¤ì–‘í•œ Document Loaderë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **TextLoader**: ì¼ë°˜ í…ìŠ¤íŠ¸ íŒŒì¼ (.txt, .log)
- **PyPDFLoader**: PDF ë¬¸ì„œ (í˜ì´ì§€ë³„ ë©”íƒ€ë°ì´í„° ë³´ì¡´)
- **WebBaseLoader**: ì›¹ í˜ì´ì§€ í¬ë¡¤ë§
- **CSVLoader**: êµ¬ì¡°í™”ëœ í‘œ í˜•ì‹ ë°ì´í„°
- **JSONLoader**: API ì‘ë‹µ, ì„¤ì • íŒŒì¼
- **DirectoryLoader**: ëŒ€ëŸ‰ íŒŒì¼ ì¼ê´„ ì²˜ë¦¬
- **GitLoader**: ë²„ì „ ê´€ë¦¬ëœ ì½”ë“œë² ì´ìŠ¤
ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ëŠ” Text Splitterë¥¼ í†µí•´ ë¶„í• í•©ë‹ˆë‹¤.   
# ìƒ˜í”Œ í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
sample_text = """
RAG(Retrieval-Augmented Generation)ëŠ” 2020ë…„ Facebook AI Research íŒ€ì´ ì²˜ìŒ ì œì•ˆí•œ ê°œë…ìœ¼ë¡œ, ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ê³ ì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.
ê¸°ì¡´ì˜ ì–¸ì–´ ëª¨ë¸ë“¤ì´ í•™ìŠµ ì‹œì ì˜ ì •ë³´ë§Œì„ ë‚´ì¬í™”í•˜ì—¬ ìµœì‹  ì •ë³´ë‚˜ íŠ¹ì • ë„ë©”ì¸ ì§€ì‹ì— ëŒ€í•œ ì •í™•ì„±ì´ ë–¨ì–´ì§€ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì í–ˆìŠµë‹ˆë‹¤.
ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì–¸ì–´ ëª¨ë¸ì˜ ìƒì„± ê³¼ì •ì— í†µí•©í•¨ìœ¼ë¡œì¨, ë³´ë‹¤ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.
ì´ˆê¸° RAG ì‹œìŠ¤í…œì€ DPR(Dense Passage Retrieval)ê³¼ ê°™ì€ ë°€ì§‘ ë²¡í„° ê²€ìƒ‰ ê¸°ë²•ê³¼ BARTë‚˜ T5 ê°™ì€ ìƒì„± ëª¨ë¸ì„ ê²°í•©í•œ í˜•íƒœì˜€ìŠµë‹ˆë‹¤.
ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ê³ , ì´ì™€ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰í•œ í›„, ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” íŒŒì´í”„ë¼ì¸ êµ¬ì¡°ë¥¼ ê°€ì¡ŒìŠµë‹ˆë‹¤.
ì´ ì‹œê¸°ì˜ RAGëŠ” ì£¼ë¡œ ì˜¤í”ˆ ë„ë©”ì¸ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œì— ì ìš©ë˜ì—ˆìœ¼ë©°, Wikipediaì™€ ê°™ì€ ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤ë¥¼ ì§€ì‹ ì†ŒìŠ¤ë¡œ í™œìš©í–ˆìŠµë‹ˆë‹¤.
í•˜ì§€ë§Œ ê²€ìƒ‰ê³¼ ìƒì„±ì´ ë…ë¦½ì ìœ¼ë¡œ ìµœì í™”ë˜ì–´ í†µí•©ì ì¸ ì„±ëŠ¥ í–¥ìƒì— í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤.
2021ë…„ë¶€í„° 2022ë…„ê¹Œì§€ RAG ì‹œìŠ¤í…œì€ ì—¬ëŸ¬ ë°©í–¥ìœ¼ë¡œ ë°œì „í–ˆìŠµë‹ˆë‹¤.
Fusion-in-Decoderì™€ ê°™ì€ ëª¨ë¸ì€ ì—¬ëŸ¬ ê°œì˜ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ì—¬ ë³´ë‹¤ íš¨ê³¼ì ìœ¼ë¡œ ì •ë³´ë¥¼ í†µí•©í•  ìˆ˜ ìˆê²Œ í–ˆìŠµë‹ˆë‹¤.
RETRO(Retrieval-Enhanced Transformer)ëŠ” ì‚¬ì „ í•™ìŠµ ë‹¨ê³„ë¶€í„° ê²€ìƒ‰ì„ í†µí•©í•˜ì—¬ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±ì„ í¬ê²Œ ê°œì„ í–ˆìŠµë‹ˆë‹¤.
ë˜í•œ ê²€ìƒ‰ ë‹¨ê³„ì—ì„œë„ BM25ì™€ ê°™ì€ ì „í†µì ì¸ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ê³¼ ë²¡í„° ê²€ìƒ‰ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë°©ë²•ì´ ë“±ì¥í–ˆê³ , ì¬ìˆœìœ„í™”(re-ranking) ê¸°ë²•ì„ í†µí•´ ê²€ìƒ‰ í’ˆì§ˆì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.
ì´ ì‹œê¸°ì—ëŠ” ë„ë©”ì¸ íŠ¹í™” RAG ì‹œìŠ¤í…œë„ ë“±ì¥í•˜ì—¬ ì˜ë£Œ, ë²•ë¥ , ê¸ˆìœµ ë“± ì „ë¬¸ ë¶„ì•¼ì—ì„œì˜ í™œìš©ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.
2023ë…„ ì´í›„ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ê¸‰ì†í•œ ë°œì „ê³¼ í•¨ê»˜ RAG ê¸°ìˆ ë„ í¬ê²Œ ì§„í™”í–ˆìŠµë‹ˆë‹¤.
ChatGPTì™€ ê°™ì€ ëŒ€í™”í˜• AIì˜ ë“±ì¥ìœ¼ë¡œ RAGëŠ” ë‹¨ìˆœí•œ ì§ˆì˜ì‘ë‹µì„ ë„˜ì–´ ë³µì¡í•œ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•˜ë©´ì„œë„ ì™¸ë¶€ ì§€ì‹ì„ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.
íŠ¹íˆ LangChain, LlamaIndexì™€ ê°™ì€ í”„ë ˆì„ì›Œí¬ì˜ ë“±ì¥ìœ¼ë¡œ RAG ì‹œìŠ¤í…œ êµ¬ì¶•ì´ í‘œì¤€í™”ë˜ê³  ì ‘ê·¼ì„±ì´ ë†’ì•„ì¡ŒìŠµë‹ˆë‹¤.
ë˜í•œ Self-RAG, CRAG(Corrective RAG)ì™€ ê°™ì€ ê¸°ë²•ë“¤ì´ ì œì•ˆë˜ì–´ ê²€ìƒ‰ëœ ì •ë³´ì˜ ê´€ë ¨ì„±ì„ ìì²´ì ìœ¼ë¡œ í‰ê°€í•˜ê³  í•„ìš”ì‹œ ì¬ê²€ìƒ‰í•˜ëŠ” ë“±ì˜ ìê¸° ìˆ˜ì • ëŠ¥ë ¥ì„ ê°–ì¶”ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.
ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê¸°ìˆ ë„ í¬ê²Œ ë°œì „í•˜ì—¬ Pinecone, Weaviate, Qdrant ë“± ì „ë¬¸í™”ëœ ì†”ë£¨ì…˜ë“¤ì´ ë“±ì¥í–ˆìŠµë‹ˆë‹¤.
í˜„ì¬ RAGëŠ” ê¸°ì—…ì˜ ì§€ì‹ ê´€ë¦¬ ì‹œìŠ¤í…œ, ê³ ê° ì§€ì› ì±—ë´‡, ì½”ë“œ ìƒì„± ë„êµ¬, ì—°êµ¬ ë³´ì¡° ì‹œìŠ¤í…œ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œë°œíˆ ì ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.
íŠ¹íˆ ê¸°ì—… ë‚´ë¶€ ë¬¸ì„œì™€ ë°ì´í„°ë¥¼ í™œìš©í•œ ì—”í„°í”„ë¼ì´ì¦ˆ RAG ì†”ë£¨ì…˜ì´ ì£¼ëª©ë°›ê³  ìˆìœ¼ë©°, í”„ë¼ì´ë²„ì‹œì™€ ë³´ì•ˆì„ ê³ ë ¤í•œ ì˜¨í”„ë ˆë¯¸ìŠ¤ RAG ì‹œìŠ¤í…œë„ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤.
ë©€í‹°ëª¨ë‹¬ RAGì˜ ë°œì „ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¿ë§Œ ì•„ë‹ˆë¼ ì´ë¯¸ì§€, í‘œ, ê·¸ë˜í”„ ë“± ë‹¤ì–‘í•œ í˜•íƒœì˜ ì •ë³´ë¥¼ í†µí•©ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.
ë˜í•œ GraphRAGì™€ ê°™ì´ ì§€ì‹ ê·¸ë˜í”„ë¥¼ í™œìš©í•œ êµ¬ì¡°í™”ëœ ì •ë³´ ê²€ìƒ‰ê³¼ ì¶”ë¡ ì„ ê²°í•©í•œ ê³ ê¸‰ ê¸°ë²•ë“¤ë„ ë“±ì¥í–ˆìŠµë‹ˆë‹¤.
í–¥í›„ RAG ê¸°ìˆ ì€ ë”ìš± ì •êµí•˜ê³  íš¨ìœ¨ì ì¸ ë°©í–¥ìœ¼ë¡œ ë°œì „í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.
ì—ì´ì „í‹± RAGëŠ” ë‹¨ìˆœ ê²€ìƒ‰ì„ ë„˜ì–´ ëŠ¥ë™ì ìœ¼ë¡œ ì •ë³´ë¥¼ íƒìƒ‰í•˜ê³  ì¶”ë¡ í•˜ëŠ” ëŠ¥ë ¥ì„ ê°–ì¶”ê²Œ ë  ê²ƒì…ë‹ˆë‹¤.
ë˜í•œ ê°œì¸í™”ëœ RAG ì‹œìŠ¤í…œì„ í†µí•´ ì‚¬ìš©ìì˜ ì„ í˜¸ë„ì™€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•œ ë§ì¶¤í˜• ì •ë³´ ì œê³µì´ ê°€ëŠ¥í•´ì§ˆ ê²ƒì…ë‹ˆë‹¤.
ê²€ìƒ‰ íš¨ìœ¨ì„± ì¸¡ë©´ì—ì„œëŠ” ì¦ë¶„ ì¸ë±ì‹±ê³¼ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ê¸°ëŠ¥ì´ ê°•í™”ë˜ì–´ ë™ì ìœ¼ë¡œ ë³€í™”í•˜ëŠ” ì§€ì‹ ë² ì´ìŠ¤ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.
ë¬´ì—‡ë³´ë‹¤ RAGëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ í™˜ê° í˜„ìƒì„ ì¤„ì´ê³  ì‹ ë¢°ì„±ì„ ë†’ì´ëŠ” í•µì‹¬ ê¸°ìˆ ë¡œì„œ, AIì˜ ì‹¤ìš©ì  ì ìš©ì„ ìœ„í•œ í•„ìˆ˜ ìš”ì†Œë¡œ ìë¦¬ì¡ì„ ê²ƒìœ¼ë¡œ ì „ë§ë©ë‹ˆë‹¤."""

# í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
with open('sample_ai.txt', 'w', encoding='utf-8') as f:
    f.write(sample_text)

print("ìƒ˜í”Œ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
### RecursiveCharacterTextSplitter - ê¸°ë³¸ ë¶„í•  ì „ëµ
# í…ìŠ¤íŠ¸ ë¡œë”ë¡œ íŒŒì¼ ë¡œë“œ
loader = TextLoader('sample_ai.txt', encoding='utf-8')
documents = loader.load()

len(documents[0].page_content)

# RecursiveCharacterTextSplitter ì„¤ì •
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,  # ì²­í¬ í¬ê¸°
    chunk_overlap=30,  # ì²­í¬ ê°„ ì¤‘ë³µ
    length_function=len,
    separators=["\n\n", "\n", ".", " ", ""]  # ë¶„í•  ìš°ì„ ìˆœìœ„
)

# ë¬¸ì„œ ë¶„í• 
splits = text_splitter.split_documents(documents)

print(f"ì›ë³¸ ë¬¸ì„œ ìˆ˜: {len(documents)}")
print(f"ë¶„í• ëœ ì²­í¬ ìˆ˜: {len(splits)}\n")

# ì²˜ìŒ 3ê°œ ì²­í¬ í™•ì¸
for i, split in enumerate(splits[:3]):
    print(f"ì²­í¬ {i+1}:")
    print(f"ë‚´ìš©: {split.page_content[:80]}...")
    print(f"ê¸¸ì´: {len(split.page_content)}\n")
í† í° ê¸°ë°˜ì˜ ë¶„í• ì€ ì•„ë˜ì™€ ê°™ì´ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# RecursiveCharacterTextSplitter ì„¤ì •
token_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    model_name = 'gpt-5-mini',
    chunk_size=300,  # ì²­í¬ í¬ê¸°
    chunk_overlap=30,  # ì²­í¬ ê°„ ì¤‘ë³µ
    separators=["\n\n", "\n", ".", " ", ""]  # ë¶„í•  ìš°ì„ ìˆœìœ„
)

# ë¬¸ì„œ ë¶„í• 
splits = token_splitter.split_documents(documents)

print(f"ì›ë³¸ ë¬¸ì„œ ìˆ˜: {len(documents)}")
print(f"ë¶„í• ëœ ì²­í¬ ìˆ˜: {len(splits)}\n")

# ì²˜ìŒ 3ê°œ ì²­í¬ í™•ì¸
for i, split in enumerate(splits[:3]):
    print(f"ì²­í¬ {i+1}:")
    print(f"ë‚´ìš©: {split.page_content[:80]}...")
    print(f"ê¸¸ì´: {len(split.page_content)}\n")
**ğŸ’¡ ì‹¤ë¬´ íŒ**
- í•œê¸€ ë¬¸ì„œëŠ” í† í° ìˆ˜ê°€ ì˜ì–´ë³´ë‹¤ ë§ìœ¼ë¯€ë¡œ chunk_size ì¡°ì • í•„ìš”
- overlapì€ ë³´í†µ chunk_sizeì˜ 10-20% ê¶Œì¥


**ğŸ’¡ í† í° ê¸°ë°˜ ë¶„í• ì˜ ì¥ì **
- ëª¨ë¸ì˜ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œì— ì •í™•íˆ ë§ì¶¤
- ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ ë” ì¼ê´€ëœ ë¶„í• 
- API ë¹„ìš© ì˜ˆì¸¡ ê°€ëŠ¥
ë‹¨ìˆœíˆ ê¸€ì/í† í° ê¸°ë°˜ì˜ ë¶„í•  ì´ì™¸ì—ë„, ë‹¤ì–‘í•œ ê¸°ì¤€ì— ë§ì¶˜ ë¶„í• ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
# ìƒ˜í”Œ Markdown ë¬¸ì„œ ìƒì„±
markdown_document = """
# LangChain ì†Œê°œ

LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

## ì£¼ìš” êµ¬ì„±ìš”ì†Œ

### 1. Models

LangChainì€ ë‹¤ì–‘í•œ LLM ì œê³µìë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
- OpenAI
- Anthropic Claude
- Google Generative AI
- Ollama

### 2. Prompts

í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ í†µí•´ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

- ChatPromptTemplate
- PromptTemplate
- FewShotPromptTemplate

## ê³ ê¸‰ ê¸°ëŠ¥

### Chains

ì—¬ëŸ¬ ì»´í¬ë„ŒíŠ¸ë¥¼ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
ì²´ì¸ì„ ì—°ê²°í•˜ëŠ” ë¬¸ë²•ì€ LCEL(LangChain Expression Language)ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.

### Agents

bind_tools() ê³¼ ToolMessageë¥¼ í™œìš©í•˜ë©´
ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ììœ¨ì ì¸ ì—ì´ì „íŠ¸ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

# Markdown í—¤ë” ê¸°ë°˜ ë¶„í•  ì„¤ì •
headers_to_split_on = [
    ("#", "ì œëª©1"),
    ("##", "ì œëª©2"),
    ("###", "ì œëª©3"),
]

markdown_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=headers_to_split_on,
    strip_headers=False  # í—¤ë”ë¥¼ ì½˜í…ì¸ ì— í¬í•¨
)

# Markdown ë¬¸ì„œ ë¶„í• 
md_header_splits = markdown_splitter.split_text(markdown_document)

print(f"ë¶„í• ëœ ì„¹ì…˜ ìˆ˜: {len(md_header_splits)}\n")

# ê° ì„¹ì…˜ì˜ ë‚´ìš©ê³¼ ë©”íƒ€ë°ì´í„° í™•ì¸
for i, doc in enumerate(md_header_splits[:3]):
    print(f"ì„¹ì…˜ {i+1}:")
    print(f"ë©”íƒ€ë°ì´í„°: {doc.metadata}")
    print(f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {doc.page_content[:100]}...\n")
**ğŸ” í•µì‹¬ í¬ì¸íŠ¸**
- ë¬¸ì„œì˜ ê³„ì¸µ êµ¬ì¡°ê°€ ë©”íƒ€ë°ì´í„°ë¡œ ë³´ì¡´ë¨
- ê²€ìƒ‰ ì‹œ íŠ¹ì • ì„¹ì…˜ì„ íƒ€ê²ŸíŒ…í•˜ê¸° ìš©ì´
- ê¸´ ë¬¸ì„œì˜ êµ¬ì¡°ì  íƒìƒ‰ ê°€ëŠ¥
HTMLê³¼ ê°™ì´ ë³µì¡í•œ êµ¬ì¡°ëŠ” ë©”íƒ€ë°ì´í„°ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ìƒ˜í”Œ HTML ë¬¸ì„œ
html_string = """
<!DOCTYPE html>
<html>
<body>
    <h1>ì›¹ ê°œë°œ ê¸°ì´ˆ</h1>
    <p>ì›¹ ê°œë°œì€ í”„ë¡ íŠ¸ì—”ë“œì™€ ë°±ì—”ë“œë¡œ êµ¬ë¶„ë©ë‹ˆë‹¤.</p>
    
    <h2>í”„ë¡ íŠ¸ì—”ë“œ ê¸°ìˆ </h2>
    <p>ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.</p>
    
    <h3>HTML</h3>
    <p>ì›¹ í˜ì´ì§€ì˜ êµ¬ì¡°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.</p>
    <ul>
        <li>íƒœê·¸ë¥¼ ì‚¬ìš©í•œ ë§ˆí¬ì—…</li>
        <li>ì‹œë§¨í‹± HTML5 ìš”ì†Œ</li>
    </ul>
    
    <h3>CSS</h3>
    <p>ìŠ¤íƒ€ì¼ê³¼ ë ˆì´ì•„ì›ƒì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.</p>
    <table>
        <tr>
            <th>ì†ì„±</th>
            <th>ì„¤ëª…</th>
        </tr>
        <tr>
            <td>color</td>
            <td>í…ìŠ¤íŠ¸ ìƒ‰ìƒ</td>
        </tr>
        <tr>
            <td>margin</td>
            <td>ì™¸ë¶€ ì—¬ë°±</td>
        </tr>
    </table>
    
    <h2>ë°±ì—”ë“œ ê¸°ìˆ </h2>
    <p>ì„œë²„ ì¸¡ ë¡œì§ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.</p>
</body>
</html>
"""

# HTML í—¤ë” ê¸°ë°˜ ë¶„í• 
headers_to_split_on = [
    ("h1", "ì œëª©1"),
    ("h2", "ì œëª©2"),
    ("h3", "ì œëª©3"),
]

html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
html_header_splits = html_splitter.split_text(html_string)

print(f"HTML ë¶„í•  ê²°ê³¼: {len(html_header_splits)}ê°œ ì„¹ì…˜\n")

# êµ¬ì¡°í™”ëœ ìš”ì†Œ(í…Œì´ë¸”, ë¦¬ìŠ¤íŠ¸) ë³´ì¡´ í™•ì¸
for i, split in enumerate(html_header_splits):
    print(f"ì„¹ì…˜ {i+1}:")
    print(f"ë©”íƒ€ë°ì´í„°: {split.metadata}")
    if 'table' in split.page_content or 'ul' in split.page_content:
        print("âœ… êµ¬ì¡°í™”ëœ ìš”ì†Œ í¬í•¨")
    print(f"ë‚´ìš© ê¸¸ì´: {len(split.page_content)} ê¸€ì")
    print(f"ë‚´ìš©: {split.page_content[0:10]}\n")
ì†ŒìŠ¤ ì½”ë“œì˜ ì¢…ë¥˜ì— ë”°ë¥¸ ë¶„í• ë„ ì§€ì›í•©ë‹ˆë‹¤.
# ìƒ˜í”Œ Python ì½”ë“œ
python_code = """
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

class DataProcessor:
    
    def __init__(self, data_path):
        self.data_path = data_path
        self.data = None
        self.processed_data = None
    
    def load_data(self):
        try:
            self.data = pd.read_csv(self.data_path)
            print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.data)} í–‰")
            return self.data
        except Exception as e:
            print(f"ì—ëŸ¬ ë°œìƒ: {e}")
            return None
    
    def preprocess(self):
        if self.data is None:
            raise ValueError("ë°ì´í„°ë¥¼ ë¨¼ì € ë¡œë“œí•˜ì„¸ìš”")
        
        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        self.processed_data = self.data.fillna(0)
        
        # ì •ê·œí™”
        numeric_columns = self.processed_data.select_dtypes(include=[np.number]).columns
        self.processed_data[numeric_columns] = (self.processed_data[numeric_columns] - 
                                                self.processed_data[numeric_columns].mean()) / \
                                               self.processed_data[numeric_columns].std()
        return self.processed_data

def train_model(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    # ëª¨ë¸ í•™ìŠµ ì½”ë“œ
    return X_train, X_test, y_train, y_test

# ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    processor = DataProcessor("data.csv")
    processor.load_data()
    processor.preprocess()
"""

# Python ì½”ë“œ ì „ìš© ë¶„í• ê¸°
python_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.PYTHON,
    chunk_size=200,
    chunk_overlap=50
)

python_docs = python_splitter.create_documents([python_code])

print(f"Python ì½”ë“œ ë¶„í•  ê²°ê³¼: {len(python_docs)}ê°œ ì²­í¬\n")

# ê° ì²­í¬ í™•ì¸
for i, doc in enumerate(python_docs[:3]):
    print(f"ì²­í¬ {i+1}:")
    print(doc.page_content)
    print("-" * 50)
# JavaScript ì½”ë“œ ì˜ˆì‹œ
js_code = """
// React ì»´í¬ë„ŒíŠ¸ ì˜ˆì‹œ
import React, { useState, useEffect } from 'react';
import axios from 'axios';

const UserProfile = ({ userId }) => {
    const [user, setUser] = useState(null);
    const [loading, setLoading] = useState(true);
    
    useEffect(() => {
        fetchUserData();
    }, [userId]);
    
    const fetchUserData = async () => {
        try {
            const response = await axios.get(`/api/users/${userId}`);
            setUser(response.data);
        } catch (error) {
            console.error('Error fetching user:', error);
        } finally {
            setLoading(false);
        }
    };
    
    if (loading) return <div>Loading...</div>;
    if (!user) return <div>User not found</div>;
    
    return (
        <div className="user-profile">
            <h2>{user.name}</h2>
            <p>{user.email}</p>
        </div>
    );
};

export default UserProfile;
"""

# JavaScript ì½”ë“œ ë¶„í• 
js_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.JS,
    chunk_size=150,
    chunk_overlap=30
)

js_docs = js_splitter.create_documents([js_code])
print(f"JavaScript ì½”ë“œ ë¶„í• : {len(js_docs)}ê°œ ì²­í¬")

# ê° ì²­í¬ í™•ì¸
for i, doc in enumerate(js_docs[:3]):
    print(f"ì²­í¬ {i+1}:")
    print(doc.page_content[:150])
    print("-" * 50)
**ğŸ’¡ ì½”ë“œ ë¶„í•  íŒ**
- í•¨ìˆ˜/í´ë˜ìŠ¤ ê²½ê³„ë¥¼ ê³ ë ¤í•œ ë¶„í• 
- ì–¸ì–´ë³„ êµ¬ë¬¸ ê·œì¹™ ë°˜ì˜
- import ë¬¸ê³¼ í•¨ìˆ˜ ì •ì˜ ë¶„ë¦¬ ê³ ë ¤
### JSON ë°ì´í„° ë¶„í• 
# ë³µì¡í•œ JSON ë°ì´í„° ì˜ˆì‹œ
json_data = {
    "company": "TechCorp",
    "founded": 2010,
    "departments": [
        {
            "name": "Engineering",
            "employees": 150,
            "teams": [
                {
                    "name": "Backend",
                    "members": 30,
                    "technologies": ["Python", "Java", "Go"],
                    "projects": [
                        {"name": "API Gateway", "status": "active"},
                        {"name": "Data Pipeline", "status": "planning"}
                    ]
                },
                {
                    "name": "Frontend",
                    "members": 25,
                    "technologies": ["React", "Vue", "TypeScript"],
                    "projects": [
                        {"name": "Admin Dashboard", "status": "active"},
                        {"name": "Mobile App", "status": "development"}
                    ]
                },
                {
                    "name": "DevOps",
                    "members": 15,
                    "technologies": ["Docker", "Kubernetes", "Terraform"],
                    "infrastructure": {
                        "cloud_providers": ["AWS", "GCP"],
                        "monitoring_tools": ["Prometheus", "Grafana"],
                        "ci_cd": "Jenkins"
                    }
                }
            ]
        },
        {
            "name": "Marketing",
            "employees": 45,
            "campaigns": [
                {
                    "name": "Summer Launch",
                    "budget": 50000,
                    "channels": ["Social Media", "Email", "Content Marketing"]
                }
            ]
        }
    ],
    "products": [
        {
            "name": "Product A",
            "version": "2.5.0",
            "features": ["Feature 1", "Feature 2", "Feature 3"]
        },
        {
            "name": "Product B",
            "version": "1.2.0",
            "features": ["Feature A", "Feature B"]
        }
    ]
}

# RecursiveJsonSplitter ì‚¬ìš©
json_splitter = RecursiveJsonSplitter(max_chunk_size=300)

# JSONì„ ì²­í¬ë¡œ ë¶„í• 
json_chunks = json_splitter.split_json(json_data=json_data)

print(f"JSON ë°ì´í„° ë¶„í•  ê²°ê³¼: {len(json_chunks)}ê°œ ì²­í¬\n")

# ê° ì²­í¬ í™•ì¸
for i, chunk in enumerate(json_chunks[:3]):
    print(f"ì²­í¬ {i+1}:")
    print(json.dumps(chunk, indent=2, ensure_ascii=False)[:300])
    print(f"ì²­í¬ í¬ê¸°: {len(json.dumps(chunk))} ë¬¸ì")
    print("-" * 50)
**ğŸ” JSON ë¶„í•  íŠ¹ì§•**
- ì¤‘ì²©ëœ êµ¬ì¡° ìœ ì§€
- ë…¼ë¦¬ì  ë‹¨ìœ„ë¡œ ë¶„í• 
- API ì‘ë‹µ ë°ì´í„° ì²˜ë¦¬ì— ìœ ìš©
## Semantic Chunking

ì‹œë§¨í‹± ì²­í‚¹ì€ ì˜ë¯¸ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œë¥¼ ë¶„í• í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.   
ê°€ì¥ ê³ ë„í™”ëœ ì²­í‚¹ ë°©ë²•ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ì˜ë¯¸ì ìœ¼ë¡œ êµ¬ë¶„ëœ ê¸´ í…ìŠ¤íŠ¸ ì˜ˆì‹œ
long_text = """
ì¸ê³µì§€ëŠ¥ì˜ ì—­ì‚¬ëŠ” 1950ë…„ëŒ€ë¡œ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°‘ë‹ˆë‹¤. ì•¨ëŸ° íŠœë§ì€ ê¸°ê³„ê°€ ìƒê°í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ 
íŒë‹¨í•˜ëŠ” íŠœë§ í…ŒìŠ¤íŠ¸ë¥¼ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. ì´ëŠ” AI ì—°êµ¬ì˜ ì‹œì‘ì ì´ ë˜ì—ˆìŠµë‹ˆë‹¤.

1960ë…„ëŒ€ì™€ 1970ë…„ëŒ€ëŠ” AIì˜ í™©ê¸ˆê¸°ë¡œ ë¶ˆë¦½ë‹ˆë‹¤. ì „ë¬¸ê°€ ì‹œìŠ¤í…œì´ ê°œë°œë˜ì—ˆê³ , 
LISPì™€ ê°™ì€ AI ì „ìš© í”„ë¡œê·¸ë˜ë° ì–¸ì–´ê°€ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤.

1980ë…„ëŒ€ í›„ë°˜ë¶€í„° 1990ë…„ëŒ€ ì´ˆë°˜ê¹Œì§€ AI ê²¨ìš¸ì´ë¼ ë¶ˆë¦¬ëŠ” ì¹¨ì²´ê¸°ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. 
ê³¼ë„í•œ ê¸°ëŒ€ì™€ ì‹¤ë§ìœ¼ë¡œ ì¸í•´ ì—°êµ¬ ìê¸ˆì´ ì¤„ì–´ë“¤ì—ˆìŠµë‹ˆë‹¤.

2000ë…„ëŒ€ ë“¤ì–´ ë¹…ë°ì´í„°ì™€ ì»´í“¨íŒ… íŒŒì›Œì˜ ë°œì „ìœ¼ë¡œ ë¨¸ì‹ ëŸ¬ë‹ì´ ë¶€í™œí–ˆìŠµë‹ˆë‹¤. 
íŠ¹íˆ ë”¥ëŸ¬ë‹ì˜ ë“±ì¥ì€ AI ë¶„ì•¼ì— í˜ëª…ì„ ì¼ìœ¼ì¼°ìŠµë‹ˆë‹¤.

ìµœê·¼ì—ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ê°€ ë“±ì¥í–ˆìŠµë‹ˆë‹¤. BERT, GPT ì‹œë¦¬ì¦ˆ ê°™ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì´ 
ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ë¥¼ ì™„ì „íˆ ë°”ê¾¸ì–´ ë†“ì•˜ìŠµë‹ˆë‹¤. ì´ë“¤ì€ ë²ˆì—­, ìš”ì•½, ì§ˆì˜ì‘ë‹µ ë“± 
ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ì¸ê°„ ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤.

ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ë„ ê¸‰ì†íˆ ë°œì „í–ˆìŠµë‹ˆë‹¤. CNN(Convolutional Neural Network)ì„ í†µí•´ 
ì´ë¯¸ì§€ ì¸ì‹ì˜ ì •í™•ë„ê°€ í¬ê²Œ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ììœ¨ì£¼í–‰ì°¨, ì˜ë£Œ ì˜ìƒ ì§„ë‹¨ ë“±ì— í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.

ê°•í™”í•™ìŠµì€ ê²Œì„ê³¼ ë¡œë´‡ê³µí•™ì—ì„œ í° ì„±ê³¼ë¥¼ ê±°ë‘ì—ˆìŠµë‹ˆë‹¤. AlphaGoê°€ ë°”ë‘‘ì—ì„œ ì¸ê°„ì„ ì´ê¸´ ê²ƒì€ 
ì—­ì‚¬ì ì¸ ìˆœê°„ì´ì—ˆìŠµë‹ˆë‹¤. ì´ì œ AIëŠ” ë³µì¡í•œ ì „ëµì  ì˜ì‚¬ê²°ì •ë„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

embeddings = OpenAIEmbeddings()

# Semantic Chunker ìƒì„± - percentile ë°©ì‹
semantic_chunker_percentile = SemanticChunker(
    embeddings,
    breakpoint_threshold_type="percentile",
    breakpoint_threshold_amount=50  # ìƒìœ„ 50% ì´ìƒì˜ ì°¨ì´ê°€ ë‚˜ë©´ ë¶„í• 
)

# ì˜ë¯¸ ê¸°ë°˜ ë¶„í•  ìˆ˜í–‰
semantic_docs = semantic_chunker_percentile.create_documents([long_text])

print(f"ì˜ë¯¸ ê¸°ë°˜ ë¶„í•  ê²°ê³¼: {len(semantic_docs)}ê°œ ì²­í¬\n")

for i, doc in enumerate(semantic_docs):
    print(f"ì²­í¬ {i+1} (ì˜ë¯¸ì ìœ¼ë¡œ ì—°ê´€ëœ ë‚´ìš©):")
    print(doc.page_content[:200])
    print(f"ì²­í¬ ê¸¸ì´: {len(doc.page_content)} ë¬¸ì\n")
    print("-" * 50)

ë­ì²´ì¸ì˜ Semantic Chunkingì€ ë‹¤ì–‘í•œ Breakpoint ì „ëµì„ ì§€ì›í•©ë‹ˆë‹¤.  

ì „ì²´ ë¬¸ì¥ë“¤ ê°„ì˜ ê±°ë¦¬ë¥¼ ëª¨ë‘ ê³„ì‚°í•œ ë’¤, Breakpointë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.


ğŸ’¡ Breakpoint ì „ëµ ì„ íƒ ê°€ì´ë“œ:
- Percentile: ì¼ë°˜ì ì¸ ë¬¸ì„œì— ì í•©
- Standard Deviation: ì¼ê´€ëœ ìŠ¤íƒ€ì¼ì˜ ë¬¸ì„œ
- Interquartile: ì´ìƒì¹˜ê°€ ë§ì€ ë¬¸ì„œ
- Gradient: ê¸‰ê²©í•œ ì£¼ì œ ë³€í™”ê°€ ìˆëŠ” ë¬¸ì„œ
# Standard Deviation ë°©ì‹
semantic_chunker_std = SemanticChunker(
    embeddings,
    breakpoint_threshold_type="standard_deviation",
    breakpoint_threshold_amount=3  # 3 í‘œì¤€í¸ì°¨ ì´ìƒ ì°¨ì´
)

# Interquartile ë°©ì‹
semantic_chunker_iqr = SemanticChunker(
    embeddings,
    breakpoint_threshold_type="interquartile"
)

# Gradient ë°©ì‹
semantic_chunker_gradient = SemanticChunker(
    embeddings,
    breakpoint_threshold_type="gradient",
    breakpoint_threshold_amount=95
)

# ê° ë°©ì‹ìœ¼ë¡œ ë¶„í•  ìˆ˜í–‰
strategies = {
    "Percentile": semantic_chunker_percentile,
    "Standard Deviation": semantic_chunker_std,
    "Interquartile": semantic_chunker_iqr,
    "Gradient": semantic_chunker_gradient
}

print("ê° ì „ëµë³„ ë¶„í•  ê²°ê³¼ ë¹„êµ:\n")
for name, chunker in strategies.items():
    docs = chunker.create_documents([long_text])
    print(f"{name}: {len(docs)}ê°œ ì²­í¬")
    avg_length = sum(len(doc.page_content) for doc in docs) / len(docs)
    print(f"  í‰ê·  ì²­í¬ ê¸¸ì´: {avg_length:.0f} ë¬¸ì\n")



ğŸ“‹ ë¬¸ì„œ íƒ€ì…ë³„ ì¶”ì²œ Splitter:

ğŸ“„ MARKDOWN    
   ì¶”ì²œ: MarkdownHeaderTextSplitter   
   ì´ìœ : í—¤ë” ê¸°ë°˜ êµ¬ì¡° ë³´ì¡´   
   ì„¤ì •: `{'headers_to_split_on': ['#', '##', '###']}`

ğŸ“„ HTML   
   ì¶”ì²œ: HTMLSemanticPreservingSplitter   
   ì´ìœ : í…Œì´ë¸”, ë¦¬ìŠ¤íŠ¸ ë“± êµ¬ì¡° ë³´ì¡´   
   ì„¤ì •: `{'elements_to_preserve': ['table', 'ul', 'ol']}`   

ğŸ“„ CODE   
   ì¶”ì²œ: RecursiveCharacterTextSplitter.from_language   
   ì´ìœ : ì–¸ì–´ë³„ êµ¬ë¬¸ ê³ ë ¤   
   ì„¤ì •: `{'chunk_size': 200, 'chunk_overlap': 50}`   

ğŸ“„ JSON   
   ì¶”ì²œ: RecursiveJsonSplitter   
   ì´ìœ : ì¤‘ì²© êµ¬ì¡° ìœ ì§€   
   ì„¤ì •: `{'max_chunk_size': 500}`   

ğŸ“„ RESEARCH_PAPER   
   ì¶”ì²œ: SemanticChunker   
   ì´ìœ : ì˜ë¯¸ì  ì—°ê´€ì„± ê¸°ë°˜ ë¶„í•    
   ì„¤ì •: `{'breakpoint_threshold_type': 'percentile'}`   
## ğŸ¯ í•µì‹¬ ì •ë¦¬ ë° Best Practices

### Document Loader ì„ íƒ ê°€ì´ë“œ

| íŒŒì¼ íƒ€ì… | ì¶”ì²œ Loader | íŠ¹ì§• |
|---------|------------|------|
| `.txt` | TextLoader | ë‹¨ìˆœ í…ìŠ¤íŠ¸, ì¸ì½”ë”© ì§€ì • ê°€ëŠ¥ |
| `.pdf` | PyPDFLoader | í˜ì´ì§€ë³„ ë©”íƒ€ë°ì´í„° ë³´ì¡´ |
| `.csv` | CSVLoader | í–‰ ë‹¨ìœ„ ìë™ ë¶„í•  |
| `.json` | JSONLoader | jq ìŠ¤íƒ€ì¼ í•„í„°ë§ ì§€ì› |
| `.html` | WebBaseLoader | CSS ì„ íƒìë¡œ íŠ¹ì • ìš”ì†Œ ì¶”ì¶œ |
| ë””ë ‰í† ë¦¬ | DirectoryLoader | ëŒ€ëŸ‰ íŒŒì¼ ì¼ê´„ ì²˜ë¦¬ |
| ì›¹í˜ì´ì§€ | WebBaseLoader | ì‹¤ì‹œê°„ ì›¹ í¬ë¡¤ë§ |

### Text Splitter ì„ íƒ ê°€ì´ë“œ

| ë¬¸ì„œ íƒ€ì… | ì¶”ì²œ Splitter | í•µì‹¬ ê³ ë ¤ì‚¬í•­ |
|---------|-------------|------------|
| ì¼ë°˜ í…ìŠ¤íŠ¸ | RecursiveCharacterTextSplitter | ì²­í¬ í¬ê¸°ì™€ ì˜¤ë²„ë© ì¡°ì • |
| í† í° ì œí•œ | TokenTextSplitter | API í† í° ì œí•œ ì¤€ìˆ˜ |
| Markdown/Docs | MarkdownHeaderTextSplitter | í—¤ë” ë ˆë²¨ ê¸°ë°˜ êµ¬ì¡° ë³´ì¡´ |
| HTML/Web | HTMLHeaderTextSplitter | í…Œì´ë¸”, ë¦¬ìŠ¤íŠ¸ êµ¬ì¡° ìœ ì§€ |
| ì†ŒìŠ¤ ì½”ë“œ | Language-specific Splitter | êµ¬ë¬¸ ë‹¨ìœ„ ë¶„í•  |
| JSON/API | RecursiveJsonSplitter | ì¤‘ì²© êµ¬ì¡° ë³´ì¡´ |
| ì—°êµ¬ ë…¼ë¬¸ | SemanticChunker | ì˜ë¯¸ì  ì—°ê´€ì„± ê¸°ë°˜ |

### ğŸš€ í”„ë¡œë•ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

1. **ë¡œë” ìµœì í™”**
   - ëŒ€ëŸ‰ íŒŒì¼ ì²˜ë¦¬ ì‹œ DirectoryLoader + ë©€í‹°ìŠ¤ë ˆë”©
   - ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•œ lazy loading ê³ ë ¤
   - ì˜¤ë¥˜ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§ êµ¬í˜„

2. **ì²­í‚¹ ìµœì í™”**
   - ì„ë² ë”© ëª¨ë¸ì˜ í† í° ì œí•œ ê³ ë ¤
   - ì˜¤ë²„ë©ì€ chunk_sizeì˜ 10-20% ê¶Œì¥
   - ë¬¸ì„œ íƒ€ì…ë³„ ë§ì¶¤ ì „ëµ ì ìš©

3. **ë©”íƒ€ë°ì´í„° ê´€ë¦¬**
   - ì¶œì²˜, í˜ì´ì§€, ì„¹ì…˜ ì •ë³´ ë³´ì¡´
   - ê²€ìƒ‰ ì‹œ í•„í„°ë§ì„ ìœ„í•œ ë©”íƒ€ë°ì´í„° í™œìš©
   - ë²„ì „ ê´€ë¦¬ ë° íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€

4. **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**
   - ì²­í¬ í¬ê¸° ë¶„í¬ ë¶„ì„
   - ê²€ìƒ‰ ì •í™•ë„ ì¸¡ì •
   - ì‘ë‹µ í’ˆì§ˆ í‰ê°€

### ğŸ’¡ ì‹¤ë¬´ íŒ

- **í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼**: êµ¬ì¡° ê¸°ë°˜ 1ì°¨ ë¶„í•  â†’ í¬ê¸° ê¸°ë°˜ 2ì°¨ ë¶„í• 
- **ë™ì  ì²­í‚¹**: ë¬¸ì„œ íƒ€ì… ìë™ ê°ì§€ ë° ì ì‘ì  ì²˜ë¦¬
- **ìºì‹± ì „ëµ**: ìì£¼ ì‚¬ìš©ë˜ëŠ” ë¬¸ì„œ ì‚¬ì „ ì²˜ë¦¬ ë° ì €ì¥
- **A/B í…ŒìŠ¤íŠ¸**: ë‹¤ì–‘í•œ ë¶„í•  ì „ëµì˜ ì„±ëŠ¥ ë¹„êµ ì¸¡ì •
- **ë‹¤êµ­ì–´ ì§€ì›**: í•œê¸€ ë“± non-ASCII ë¬¸ì ì²˜ë¦¬ ì‹œ í† í° ê¸°ë°˜ ë¶„í•  ê¶Œì¥