# [프로젝트] 문서 요약 시스템 만들기

LLM을 사용하여, PDF 문서의 요약을 생성하는 어플리케이션을 구현해 보겠습니다.

1. Arxiv API에 검색어를 입력하여, 논문 정보를 가져옵니다.
2. Abstract와 Full Text를 불러옵니다. Full Text는 작은 크기로 청킹(Chunking)합니다.   
3. Step Back Prompting을 이용하여, Abstract의 내용을 바탕으로 요약의 포인트를 생성합니다. 
4. 요약 포인트를 이용하여 청크별 요약을 생성합니다.
5. 최종 요약본을 생성합니다.
!pip install openai arxiv langchain==0.3.27 langchain_community==0.3.27 langchain-openai langchain-google-genai langchain-community tiktoken pymupdf
## 기본 LLM 불러오기
import os
from dotenv import load_dotenv
# OPENAI_API_KEY, GOOGLE_API_KEY
load_dotenv(override=True)
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.rate_limiters import InMemoryRateLimiter

# Gemini: 무료 API 사용량 존재
# 안정적 서빙을 위해 분당 10개 설정
# 즉, 초당 약 0.167개 요청 (10/60)
# `https://aistudio.google.com/`에서 모델별 사용량 확인

rate_limiter = InMemoryRateLimiter(
    requests_per_second=0.167,  # 분당 10개 요청
    check_every_n_seconds=0.1,  # 100ms마다 체크
    max_bucket_size=10,  # 최대 버스트 크기
)


# LLM 초기화
try:
    llm = ChatOpenAI(model='gpt-5-mini', temperature=1.0)
    print("✅ GPT API 사용 가능!")
except:
    print("❌ GPT API 사용 불가- API 키를 확인하세요!")
try:
    llm_gemini = ChatGoogleGenerativeAI(model='gemini-2.5-flash', temperature=0.7, 
                                rate_limiter=rate_limiter)
    print("✅ Gemini API 사용 가능!")
except:
    print("❌ Gemini API 사용 불가- API 키를 확인하세요!")
필수 라이브러리를 불러옵니다.
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

print("필수 모듈 임포트 완료")
# Preliminary: LangChain의 Document Loader와 Text Splitter

LangChain은 외부 데이터와의 연결을 위한 다양한 도구를 제공합니다.

### 1) 랭체인 Document Loader

Document Loader는 파일 경로를 입력받아 데이터를 불러옵니다.

`langchain_community.document_loaders` (https://python.langchain.com/docs/integrations/document_loaders/)    

- CSVLoader, PyMuPDFLoader, WebBaseLoader 등의 다양한 로더가 존재합니다.
from langchain_community.document_loaders import PyPDFLoader, TextLoader, WebBaseLoader, CSVLoader
from langchain_community.document_loaders import ArxivLoader

로더는 load() 또는 비동기 방식으로 실행할 수 있으며, `Document` 클래스로 저장됩니다.
- metadata: Dict 형식으로 값이 저장됩니다.
- page_content: string 형태의 실제 문서 내용이 저장됩니다.

Data Loader와 Text Splitter   
이번 실습에서는 ArxivLoader를 사용합니다.
def load_arxiv_docs(query, max_chars=100000):
    loader = ArxivLoader(
        query=query, # 검색어
        load_max_docs=1, # 최대 문서 수 (여기서는 1개만 사용)
        doc_content_chars_max=max_chars, # 최대 문서 길이
        load_all_available_meta=True, # 모든 메타데이터 로드
    )
    return loader.load()


result = load_arxiv_docs("Korean Large Language Model Benchmark")[0]


print('전체 글자 수:', len(result.page_content))
print(result.metadata)
result
### 2) 랭체인 Text Splitter

불러온 데이터는 랭체인의 `TextSplitter`를 통해 작은 단위로 분리합니다.

이를 청킹(Chunking)이라고 하는데, 이후의 RAG 파트에서 더 자세히 다룰 예정입니다.
from langchain_text_splitters import RecursiveCharacterTextSplitter

# RecursiveCharacterTextSplitter: 문자나 토큰 단위로 분리하고, Overlap을 통해 내용 중첩
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=10000,
    chunk_overlap=2000,
) # 0- 10000, 8000- 18000, 16000 -26000
# 구분자(delimiter, separator 기준으로) 청킹 (줄바꿈 기호, 공백, 마침표, 쉼표) 


chunks = text_splitter.split_documents([result])
print('Chunk 개수:', len(chunks))
print(chunks[0].page_content)
이제, 텍스트 로드 결과를 활용하여 요약 시스템을 만들어 보세요!
# 1. Stuff - 1번

# 2. Map-Reduce - Chunk 개수 + 1 - 병렬 처리가 가능하기 때문에 빠름
# 3. Refine - Chunk 개수 - 토큰 소모는 제일 큼

# 4. Hybrid - 

## 2. Step-Back Prompting을 이용한 요약 가이드라인 생성 체인   

Step Back Prompting은 전체 답변을 작성하기 전 전체 구획이나 사전 정보를 준비하는 과정입니다.   
논문의 메타데이터를 입력하여 요약 가이드라인을 생성하는 체인을 구성하세요.    

-  LLM을 이용한 메타 프롬프팅을 추천합니다.
chunks[0].metadata
from langchain_core.prompts import ChatPromptTemplate


guideline_prompt = ChatPromptTemplate(
    [
        ('system','''
당신은 학술 논문 분석 전문가입니다.
         
논문의 제목과 Abstract을 분석하여, 논문을 요약할 때 중점적으로 다루어야 하는 핵심 기준을 도출해 주세요.
핵심 가이드라인을 개조식으로 작성하고.가이드라인만 출력하세요.
         
아래의 문제를 중점적으로 고려하세요.
         
1) 핵심 연구 질문(무슨 문제에 답하려고 하는가?)
2) 방법론 (어떤 접근법이 중요한가)
3) 주요 기여도(학술적으로 어떤 가치가 있는가?)
4) 평가 방식 (결과의 의미와 한계)
5) 기술적 세부사항

최종 가이드라인은 2000자 이내로 작성하세요.
'''),
        ('human','''
논문의 제목: {Title}
         
논문의 Abstract: {Summary}  
''')

    ]
)

guideliner = guideline_prompt | llm | StrOutputParser()

guide = guideliner.invoke(result.metadata)
# 매개변수에 해당하는 것만 전달

print(guide)

## 3. Map-Reduce 요약 프레임워크   

Map-Reduce 프레임워크는 Map-Reduce 과정으로 긴 문서를 요약하는 방식입니다.   
- Map: 전체 문서를 전체 문서를 부분으로 분리한 후, 각각 요약합니다.
- Reduce: 요약본을 모두 모아서, 최종 요약본을 작성합니다.
생성한 가이드라인을 이용해, 개별 문서를 요약하는 체인을 만들고 실행하세요.   

결과는 summaries에 리스트 형태로 저장하세요.
# Map 과정 : 각 문서에 대해 요약을 생성합니다.
from tqdm import tqdm
# 청크에 대한 요약 생성 (가이드라인 참고)

# 최종 요약만 한국어로 수행한다면 어떨까요?
map_prompt = ChatPromptTemplate(
    [
        ('system', '''주어진 논문의 내용을 요약하세요.
아래의 논문 일부와, 요약 가이드라인의 내용을 참고하여 상세한 요약문을 영어로 작성하세요.
요약문의 길이는 2-4개의 문단과 문단별 10문장 내외로 작성하세요.'''),
        ('human', '''
논문의 내용: {text}
         
---
         
요약 참고 가이드라인: {guide}''')
    ]
).partial(guide=guide)

map_chain  = map_prompt | llm | StrOutputParser()

summaries = await map_chain.abatch(chunks)
# 청크 개수만큼 LLM을 병렬 실행해서 요약문 생성
print(summaries)
summaries = map_chain.batch(chunks)
for summary in summaries:
    print(summary[0:100])
    print('-------------------')
summaries의 내용을 하나로 결합한 뒤, 전체 요약을 생성하는 Reduce 체인을 구성하세요.   

최종 결과는 summary에 저장하세요.
# Reduce 과정 : 각 문서의 요약을 하나로 합칩니다.
reduce_prompt = ChatPromptTemplate([
    ('system', '''논문 요약문의 리스트가 주어집니다.
이를 읽고, 전체 주제를 포함하는 최종 요약을 한국어로 작성하세요.
요약은 8000자 이내로 작성하세요.
     
형식은 크게 2단 목차로 구성된 마크다운으로 작성하세요.
     
답변은 한국어로 작성하세요.'''),
    ('human', '''

논문 요약문의 리스트: {summaries}

''')
])

summaries_str = '\n\n---\n\n'.join(summaries)
# (요약1 엔터 엔터 --- 엔터 엔터 요약2 ---)


reduce_chain = reduce_prompt | llm | StrOutputParser()

summary = reduce_chain.invoke(summaries_str)
print(summary)
생성한 요약은 파일로 저장해도 좋습니다.
{result.metadata['Title']}
with open(f'summary_{result.metadata['Title'][:4]}.md', 'w', encoding='utf-8') as f:
    f.write(summary)
<br><br><br><br><br><br>

### 성능 비교하기 (Future Work)
Stuff(전체를 입력하는 방식)과 비교하면 어떨까요?
prompt = ChatPromptTemplate([
    ('system', '''주어진 논문의 요약을 한국어로 작성하세요.
요약은 8000자 이내로 작성하세요.
답변은 한국어로 작성하세요.
'''),
    ('user', '''{text}
''')])

chain = prompt | llm | StrOutputParser()

original_summary = chain.invoke(result.page_content)

print(original_summary)
Stuff와 Step-Back만 수행한 경우와도 비교해 보겠습니다.
prompt = ChatPromptTemplate([
    ('system', '''주어진 논문의 요약을 한국어로 작성하세요.
요약은 8000자 이내로 작성하세요.
답변은 한국어로 작성하세요.
'''),
    ('user', '''{text}
---

다음은 요약 가이드라인입니다.

{guide}''')]).partial(guide=guide)

chain = prompt | llm | StrOutputParser()

stepback_summary = chain.invoke(result.page_content)

print(stepback_summary)
과거에는 요약 성능을 평가하기 위해 레퍼런스 요약문과 실제 요약을 키워드로 비교하는 ROUGE 등의 Score를 사용했으나,   

최근에는 LLM을 통해 평가하는 경우도 많습니다.
# 다양한 평가 기준을 세우고, Structured Output을 통해 평가하는 체인을 구성해도 좋습니다!

evaluation_prompt = ChatPromptTemplate(
    [
        ('system', '''논문의 원본 내용과, 이를 3가지 방법으로 요약한 요약문이 주어집니다.
세 요약문의 순위를 매기고, 그 이유를 설명하세요.

평가 기준은 다음과 같습니다.

- 논문에서 풀고자 하는 문제에 대해 정확히 파악했는가?
- 원문의 핵심 내용과 시사점을 잘 포함하고 있는가?
- 연구의 제약점과 향후 발전 방향에 대해 잘 기술하고 있는가?

'''),
        ('user', '''
원본 내용: 
{original_text}


---

1번 요약문: 
{summary1}

---

2번 요약문: 
{summary2}

---

3번 요약문: 
{summary3}
''')
])

evaluation_chain = evaluation_prompt | llm | StrOutputParser()

evaluation_result = evaluation_chain.invoke({
    'original_text': result.page_content,
    'summary1': summary,
    'summary2': stepback_summary,
    'summary3': original_summary
})

print(evaluation_result)

