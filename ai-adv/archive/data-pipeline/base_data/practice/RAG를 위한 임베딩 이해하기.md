# ğŸš€ RAGë¥¼ ìœ„í•œ ì„ë² ë”©(Embedding) ì´í•´í•˜ê¸°

## ğŸ“– í•™ìŠµ ëª©í‘œ
- ì„ë² ë”©ì˜ ê°œë…ê³¼ ì›ë¦¬ ì´í•´
- Qwen3-Embedding ëª¨ë¸ì„ ì‚¬ìš©í•œ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì„ë² ë”©
- ì„ë² ë”© ë²¡í„°ì˜ ì‹œê°í™”ì™€ ìœ ì‚¬ë„ ê³„ì‚°
- RAG ì‹œìŠ¤í…œì—ì„œì˜ ì„ë² ë”© í™œìš©ë²• ì´í•´
## 1. í™˜ê²½ ì„¤ì •

### 1.1 í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
ë¨¼ì € ì‹¤ìŠµì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.
!pip install -q transformers torch scikit-learn matplotlib seaborn numpy pandas accelerate setuptools koreanize-matplotlib
### 1.2 ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import torch
from transformers import AutoTokenizer, AutoModel
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

print("âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
print(f"PyTorch ë²„ì „: {torch.__version__}")
print(f"GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
## 2. ì„ë² ë”©ì´ë€?

**ì„ë² ë”©(Embedding)**ì€ í…ìŠ¤íŠ¸ë¥¼ ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.

### ğŸ’¡ í•µì‹¬ ê°œë…
- **í…ìŠ¤íŠ¸ â†’ ë²¡í„°**: "ì•ˆë…•í•˜ì„¸ìš”" â†’ [0.1, -0.5, 0.3, ...]
- **ì˜ë¯¸ ë³´ì¡´**: ë¹„ìŠ·í•œ ì˜ë¯¸ì˜ í…ìŠ¤íŠ¸ëŠ” ë¹„ìŠ·í•œ ë²¡í„°ë¡œ í‘œí˜„
- **ì°¨ì›**: ë³´í†µ 384~1536 ì°¨ì›ì˜ ë²¡í„° ì‚¬ìš©

### ğŸ¯ RAGì—ì„œì˜ ì—­í• 
1. **ë¬¸ì„œ ì¸ë±ì‹±**: ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ë²¡í„° DBì— ì €ì¥
2. **ìœ ì‚¬ë„ ê²€ìƒ‰**: ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°
3. **ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰**: ë‹¨ìˆœ í‚¤ì›Œë“œê°€ ì•„ë‹Œ ì˜ë¯¸ ê¸°ë°˜ ë§¤ì¹­
## 3. Qwen3-Embedding ëª¨ë¸ ë¡œë“œ

### ğŸ“¦ ëª¨ë¸ ì†Œê°œ
- **ëª¨ë¸ëª…**: Qwen3-Embedding-0.6B (https://huggingface.co/Qwen/Qwen3-Embedding-0.6B)
- **íŒŒë¼ë¯¸í„°**: 6ì–µ ê°œ (ê²½ëŸ‰ ëª¨ë¸)
- **íŠ¹ì§•**: BFloat16ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì , ë‹¤êµ­ì–´ ì§€ì›
- **ì°¨ì›**: 1024ì°¨ì› ë²¡í„° ì¶œë ¥
# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
print("ğŸ”„ Qwen3-Embedding ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...")

model_name = "Qwen/Qwen3-Embedding-0.6B"

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

# ëª¨ë¸ ë¡œë“œ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = AutoModel.from_pretrained(
    model_name,
    torch_dtype=torch.float16, # bfloat16ì´ ë§ì§€ë§Œ T4 GPUë¥¼ ê³ ë ¤í•´ì„œ ìˆ˜ì •ì •
    trust_remote_code=True
).to(device)

model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •

print(f"âœ… ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
print(f"ğŸ“ ë””ë°”ì´ìŠ¤: {device}")
print(f"ğŸ“ ì„ë² ë”© ì°¨ì›: {model.config.hidden_size}")
## 4. ì„ë² ë”© ìƒì„± í•¨ìˆ˜ êµ¬í˜„

í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
def get_embedding(text, model, tokenizer, device):
    """
    í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜

    Args:
        text: ì…ë ¥ í…ìŠ¤íŠ¸ (str ë˜ëŠ” list)
        model: ì„ë² ë”© ëª¨ë¸
        tokenizer: í† í¬ë‚˜ì´ì €
        device: ì—°ì‚° ë””ë°”ì´ìŠ¤

    Returns:
        numpy array: ì„ë² ë”© ë²¡í„°
    """
    # í…ìŠ¤íŠ¸ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    if isinstance(text, str):
        text = [text]

    # í† í¬ë‚˜ì´ì§•
    inputs = tokenizer(text, padding=True, truncation=True,
                      max_length=512, return_tensors="pt").to(device)

    # ì„ë² ë”© ìƒì„±
    with torch.no_grad():
        outputs = model(**inputs)
        # Mean pooling: í† í° ì„ë² ë”©ì˜ í‰ê· ì„ êµ¬í•¨
        embeddings = outputs.last_hidden_state.mean(dim=1)

    # CPUë¡œ ì´ë™ í›„ numpy ë³€í™˜
    return embeddings.cpu().numpy()

# í…ŒìŠ¤íŠ¸
test_text = "ì•ˆë…•í•˜ì„¸ìš”, ì„ë² ë”© í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."
test_embedding = get_embedding(test_text, model, tokenizer, device)
print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
print(f"ğŸ“Š ì„ë² ë”© shape: {test_embedding.shape}")
print(f"ğŸ“ˆ ë²¡í„° ì˜ˆì‹œ (ì²˜ìŒ 5ê°œ ì°¨ì›): {test_embedding[0][:5]}")
## 5. í•œêµ­ì–´ ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì¤€ë¹„

ë‹¤ì–‘í•œ ì£¼ì œì˜ í•œêµ­ì–´ ë¬¸ì¥ì„ ì¤€ë¹„í•˜ì—¬ ì„ë² ë”©ì˜ íŠ¹ì„±ì„ ê´€ì°°í•©ë‹ˆë‹¤.
# ì¹´í…Œê³ ë¦¬ë³„ ìƒ˜í”Œ í…ìŠ¤íŠ¸
sample_texts = {
    "ê¸°ìˆ ": [
        "ì¸ê³µì§€ëŠ¥ì€ ë¯¸ë˜ ê¸°ìˆ ì˜ í•µì‹¬ì…ë‹ˆë‹¤.",
        "ë¨¸ì‹ ëŸ¬ë‹ì„ í†µí•´ ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.",
        "ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ì´ë¯¸ì§€ë¥¼ ì¸ì‹í•©ë‹ˆë‹¤.",
        "ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ ì´ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
        "ë¹…ë°ì´í„° ë¶„ì„ì´ ì¤‘ìš”í•´ì§€ê³  ìˆìŠµë‹ˆë‹¤.",
        "í´ë¼ìš°ë“œ ì»´í“¨íŒ…ì€ IT ì¸í”„ë¼ì˜ íŒ¨ëŸ¬ë‹¤ì„ì„ ë°”ê¾¸ê³  ìˆìŠµë‹ˆë‹¤.",
        "5G ë„¤íŠ¸ì›Œí¬ëŠ” ì´ˆê³ ì† í†µì‹ ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.",
        "Quantum computing will redefine problem-solving in the future.",
        "Cybersecurity is becoming increasingly important in the digital era."
    ],
    "ìŒì‹": [
        "ê¹€ì¹˜ëŠ” í•œêµ­ì˜ ëŒ€í‘œì ì¸ ë°œíš¨ ìŒì‹ì…ë‹ˆë‹¤.",
        "ë¶ˆê³ ê¸°ëŠ” ë‹¬ì½¤í•œ ê°„ì¥ ì–‘ë…ì´ íŠ¹ì§•ì…ë‹ˆë‹¤.",
        "ë¹„ë¹”ë°¥ì€ ì—¬ëŸ¬ ì±„ì†Œì™€ ê³ ê¸°ë¥¼ ì„ì–´ ë¨¹ìŠµë‹ˆë‹¤.",
        "ëœì¥ì°Œê°œëŠ” êµ¬ìˆ˜í•œ ë§›ì´ ì¼í’ˆì…ë‹ˆë‹¤.",
        "ì‚¼ê²¹ì‚´ì„ êµ¬ì›Œì„œ ìŒˆì„ ì‹¸ë¨¹ìŠµë‹ˆë‹¤.",
        "ëƒ‰ë©´ì€ ì—¬ë¦„ì²  ì¸ê¸° ìˆëŠ” ì‹œì›í•œ ìŒì‹ì…ë‹ˆë‹¤.",
        "ë–¡ë³¶ì´ëŠ” ë§¤ì½¤ë‹¬ì½¤í•œ ë§›ìœ¼ë¡œ ì‚¬ë‘ë°›ìŠµë‹ˆë‹¤.",
        "ì¡ì±„ëŠ” ì”ì¹˜ì—ì„œ ë¹ ì§ˆ ìˆ˜ ì—†ëŠ” ìŒì‹ì…ë‹ˆë‹¤.",
        "í˜¸ë–¡ì€ ê²¨ìš¸ ê¸¸ê±°ë¦¬ ê°„ì‹ì˜ ëŒ€í‘œì£¼ìì…ë‹ˆë‹¤.",
        "Pizza is one of the most popular foods worldwide.",
        "Sushi combines fresh fish with delicate rice seasoning."
    ],
    "ë‚ ì”¨": [
        "ì˜¤ëŠ˜ì€ ë§‘ê³  í™”ì°½í•œ ë‚ ì”¨ì…ë‹ˆë‹¤.",
        "ë‚´ì¼ì€ ë¹„ê°€ ì˜¬ ì˜ˆì •ì…ë‹ˆë‹¤.",
        "ê²¨ìš¸ì—ëŠ” ëˆˆì´ ë§ì´ ë‚´ë¦½ë‹ˆë‹¤.",
        "ë´„ì—ëŠ” ê½ƒì´ ì•„ë¦„ë‹µê²Œ í•ë‹ˆë‹¤.",
        "ì—¬ë¦„ì€ ë¥ê³  ìŠµí•œ ë‚ ì”¨ê°€ ê³„ì†ë©ë‹ˆë‹¤.",
        "ê°€ì„ì—ëŠ” ë‹¨í’ì´ ë¬¼ë“¤ì–´ ê²½ì¹˜ê°€ ì•„ë¦„ë‹µìŠµë‹ˆë‹¤.",
        "íƒœí’ì´ ë¶ìƒí•˜ë©´ ê°•í’ê³¼ í­ìš°ê°€ ë™ë°˜ë©ë‹ˆë‹¤.",
        "ë¬´ë”ìœ„ ì†ì—ì„œëŠ” ì—´ì‚¬ë³‘ì— ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.",
        "The weather is unpredictable during the change of seasons.",
        "It often rains in April, making the flowers bloom beautifully."
    ],
    "ìš´ë™": [
        "ì¶•êµ¬ëŠ” ì „ ì„¸ê³„ì ìœ¼ë¡œ ì¸ê¸° ìˆëŠ” ìŠ¤í¬ì¸ ì…ë‹ˆë‹¤.",
        "ì•¼êµ¬ëŠ” í•œêµ­ì—ì„œ ë§ì€ ì‚¬ë‘ì„ ë°›ìŠµë‹ˆë‹¤.",
        "ë†êµ¬ëŠ” ë¹ ë¥¸ í…œí¬ê°€ ë§¤ë ¥ì ì…ë‹ˆë‹¤.",
        "ìˆ˜ì˜ì€ ì „ì‹  ìš´ë™ìœ¼ë¡œ ì¢‹ìŠµë‹ˆë‹¤.",
        "ìš”ê°€ëŠ” ëª¸ê³¼ ë§ˆìŒì˜ ê· í˜•ì„ ì¡ì•„ì¤ë‹ˆë‹¤.",
        "ë“±ì‚°ì€ ìì—° ì†ì—ì„œ ì²´ë ¥ì„ ê¸°ë¥¼ ìˆ˜ ìˆëŠ” ì¢‹ì€ í™œë™ì…ë‹ˆë‹¤.",
        "í…Œë‹ˆìŠ¤ëŠ” ë¯¼ì²©ì„±ê³¼ ì§‘ì¤‘ë ¥ì„ ìš”êµ¬í•©ë‹ˆë‹¤.",
        "ê³¨í”„ëŠ” ì „ëµê³¼ ì¸ë‚´ì‹¬ì´ í•„ìš”í•œ ìš´ë™ì…ë‹ˆë‹¤.",
        "ë‹¬ë¦¬ê¸°ëŠ” ì²´ë ¥ì„ ë‹¨ë ¨í•˜ê³  ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ í•´ì†Œí•©ë‹ˆë‹¤.",
        "íƒêµ¬ëŠ” ë¹ ë¥¸ ë°˜ì‚¬ ì‹ ê²½ì„ í•„ìš”ë¡œ í•©ë‹ˆë‹¤.",
        "Running a marathon requires months of preparation.",
        "Basketball brings people together as a team sport."
    ]
}


# ëª¨ë“  í…ìŠ¤íŠ¸ì™€ ë¼ë²¨ ì¤€ë¹„
all_texts = []
all_labels = []
all_categories = []

for category, texts in sample_texts.items():
    all_texts.extend(texts)
    all_labels.extend([category] * len(texts))
    all_categories.extend([list(sample_texts.keys()).index(category)] * len(texts))

print(f"ğŸ“ ì´ {len(all_texts)}ê°œì˜ ìƒ˜í”Œ í…ìŠ¤íŠ¸ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
print(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬: {list(sample_texts.keys())}")

# ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì •ë¦¬
df_samples = pd.DataFrame({
    'text': all_texts,
    'category': all_labels
})
print("\nìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
df_samples.head()
## 6. ëª¨ë“  í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìƒì„±

ì¤€ë¹„í•œ ìƒ˜í”Œ í…ìŠ¤íŠ¸ë“¤ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
print("ğŸ”„ ì„ë² ë”© ë²¡í„°ë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")

# ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„ë² ë”© ìƒì„± (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
batch_size = 5
embeddings = []

for i in range(0, len(all_texts), batch_size):
    batch_texts = all_texts[i:i+batch_size]
    batch_embeddings = get_embedding(batch_texts, model, tokenizer, device)
    embeddings.append(batch_embeddings)
    print(f"  ì²˜ë¦¬ ì¤‘: {i+len(batch_texts)}/{len(all_texts)}")

# ëª¨ë“  ì„ë² ë”©ì„ í•˜ë‚˜ì˜ ë°°ì—´ë¡œ ê²°í•©
embeddings = np.vstack(embeddings)

print(f"\nâœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
## 7. T-SNEë¥¼ ì´ìš©í•œ ì„ë² ë”© ì‹œê°í™”

ê³ ì°¨ì› ì„ë² ë”© ë²¡í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•˜ì—¬ ì‹œê°í™”í•©ë‹ˆë‹¤.
import koreanize_matplotlib
print("ğŸ”„ T-SNEë¥¼ ì‚¬ìš©í•˜ì—¬ ì°¨ì›ì„ ì¶•ì†Œí•˜ê³  ìˆìŠµë‹ˆë‹¤...")

# T-SNE ìˆ˜í–‰
tsne = TSNE(n_components=2, random_state=42, perplexity=5, n_iter=1000)
embeddings_2d = tsne.fit_transform(embeddings)

print("âœ… ì°¨ì› ì¶•ì†Œ ì™„ë£Œ!")

# ì‹œê°í™”
plt.figure(figsize=(10, 6))

# ì¹´í…Œê³ ë¦¬ë³„ ìƒ‰ìƒ ì„¤ì •
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']
markers = ['o', 's', '^', 'D', 'v']

# ì ê³¼ ë ˆì´ë¸” ê·¸ë¦¬ê¸°
for i, category in enumerate(sample_texts.keys()):
    mask = np.array(all_labels) == category
    plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],
               c=colors[i], label=category, s=100, alpha=0.7,
               marker=markers[i], edgecolors='black', linewidth=0.5)

# ê° ì  ì˜†ì— í…ìŠ¤íŠ¸ ë ˆì´ë¸” ì¶”ê°€ (ì²˜ìŒ 10ê¸€ìë§Œ)
for idx, (x, y) in enumerate(embeddings_2d):
    # í…ìŠ¤íŠ¸ ì²˜ìŒ 10ê¸€ìë§Œ í‘œì‹œ
    label_text = all_texts[idx][:10]
    if len(all_texts[idx]) > 10:
        label_text += "..."

    # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìƒ‰ìƒ ë§¤ì¹­
    category_idx = all_categories[idx]
    text_color = colors[category_idx]

    # í…ìŠ¤íŠ¸ ì¶”ê°€ (ì•½ê°„ ì˜¤í”„ì…‹ ì ìš©)
    plt.annotate(label_text,
                xy=(x, y),
                xytext=(3, 3),  # ì ìœ¼ë¡œë¶€í„°ì˜ ì˜¤í”„ì…‹
                textcoords='offset points',
                fontsize=8,
                color='black',
                alpha=0.8,
                ha='left')

plt.xlabel('T-SNE ì°¨ì› 1', fontsize=12)
plt.ylabel('T-SNE ì°¨ì› 2', fontsize=12)
plt.title('í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì„ë² ë”©ì˜ T-SNE ì‹œê°í™”', fontsize=14, fontweight='bold')
plt.legend(loc='best', fontsize=10)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
## 8. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°

ì„ë² ë”© ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
# ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
similarity_matrix = cosine_similarity(embeddings)

# íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”
plt.figure(figsize=(14, 12))

# ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì •ë ¬ëœ ì¸ë±ìŠ¤ ìƒì„±
sorted_indices = np.argsort(all_categories)
sorted_similarity = similarity_matrix[sorted_indices][:, sorted_indices]
sorted_labels = [all_labels[i] for i in sorted_indices]

# íˆíŠ¸ë§µ ê·¸ë¦¬ê¸°
ax = sns.heatmap(sorted_similarity,
                cmap='RdYlBu_r',
                vmin=-0.2, vmax=1,
                square=True,
                cbar_kws={'label': 'ì½”ì‚¬ì¸ ìœ ì‚¬ë„'})

# ì¹´í…Œê³ ë¦¬ ê²½ê³„ì„  ì¶”ê°€
category_counts = [5, 5, 5, 5]  # ê° ì¹´í…Œê³ ë¦¬ë³„ ìƒ˜í”Œ ìˆ˜
cumsum = np.cumsum([0] + category_counts)
for i in cumsum[1:-1]:
    ax.axhline(i, color='white', linewidth=2)
    ax.axvline(i, color='white', linewidth=2)

plt.title('í…ìŠ¤íŠ¸ ì„ë² ë”© ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ íˆíŠ¸ë§µ', fontsize=14, fontweight='bold')
plt.xlabel('í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤', fontsize=12)
plt.ylabel('í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤', fontsize=12)

# ì¹´í…Œê³ ë¦¬ ë ˆì´ë¸” ì¶”ê°€
category_positions = [(cumsum[i] + cumsum[i+1]) / 2 for i in range(len(category_counts))]
ax.set_xticks(category_positions)
ax.set_xticklabels(list(sample_texts.keys()), rotation=45)
ax.set_yticks(category_positions)
ax.set_yticklabels(list(sample_texts.keys()), rotation=0)

plt.tight_layout()
plt.show()
## 9. ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ì‹¤ìŠµ

ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ëŠ” ì‹¤ìŠµì…ë‹ˆë‹¤.
def semantic_search(query, documents, embeddings, model, tokenizer, device, top_k=5):
    """
    ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ í•¨ìˆ˜

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        embeddings: ë¬¸ì„œ ì„ë² ë”© ë²¡í„°
        top_k: ë°˜í™˜í•  ìƒìœ„ ë¬¸ì„œ ê°œìˆ˜

    Returns:
        ê²€ìƒ‰ ê²°ê³¼ (ë¬¸ì„œ, ìœ ì‚¬ë„ ì ìˆ˜)
    """
    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_embedding = get_embedding(query, model, tokenizer, device)

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarities = cosine_similarity(query_embedding, embeddings)[0]

    # ìƒìœ„ kê°œ ì¸ë±ìŠ¤
    top_indices = np.argsort(similarities)[::-1][:top_k]

    # ê²°ê³¼ ë°˜í™˜
    results = []
    for idx in top_indices:
        results.append({
            'text': documents[idx],
            'category': all_labels[idx],
            'similarity': similarities[idx]
        })

    return results

# ê²€ìƒ‰ ì¿¼ë¦¬ ì˜ˆì‹œ
queries = [
    "AIì™€ ì»´í“¨í„° ë¹„ì „ì— ëŒ€í•´ ì•Œë ¤ì¤˜",
    "í•œêµ­ ì „í†µ ìš”ë¦¬ëŠ” ë­ê°€ ìˆë‚˜ìš”?",
    "ì£¼ë§ ë‚ ì”¨ê°€ ì–´ë–¨ê¹Œìš”?",
    "ê±´ê°•ì„ ìœ„í•œ í™œë™ ì¶”ì²œí•´ì¤˜"
]

print("ğŸ” ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ì‹¤ìŠµ\n")
print("="*60)

for query in queries:
    print(f"\nğŸ“ ì¿¼ë¦¬: '{query}'")
    print("-"*40)

    results = semantic_search(query, all_texts, embeddings, model, tokenizer, device, top_k=3)

    for i, result in enumerate(results, 1):
        print(f"{i}. [{result['category']}] {result['text'][:50]}...")
        print(f"   ìœ ì‚¬ë„: {result['similarity']:.4f}")

    print()
## 10. ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ì„ë² ë”© ë¶„ì„

ê° ì¹´í…Œê³ ë¦¬ì˜ ì¤‘ì‹¬ ë²¡í„°(centroid)ë¥¼ ê³„ì‚°í•˜ê³  ì¹´í…Œê³ ë¦¬ ê°„ ê±°ë¦¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
# ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ì„ë² ë”© ê³„ì‚°
category_embeddings = {}
for category in sample_texts.keys():
    mask = np.array(all_labels) == category
    category_embeddings[category] = embeddings[mask].mean(axis=0)

# ì¹´í…Œê³ ë¦¬ ê°„ ìœ ì‚¬ë„ í–‰ë ¬
categories = list(category_embeddings.keys())
n_categories = len(categories)
category_similarity = np.zeros((n_categories, n_categories))

for i, cat1 in enumerate(categories):
    for j, cat2 in enumerate(categories):
        sim = cosine_similarity(
            category_embeddings[cat1].reshape(1, -1),
            category_embeddings[cat2].reshape(1, -1)
        )[0, 0]
        category_similarity[i, j] = sim

# ì‹œê°í™”
plt.figure(figsize=(10, 8))
sns.heatmap(category_similarity,
            annot=True,
            fmt='.3f',
            xticklabels=categories,
            yticklabels=categories,
            cmap='YlOrRd',
            vmin=0, vmax=1,
            square=True,
            cbar_kws={'label': 'ì½”ì‚¬ì¸ ìœ ì‚¬ë„'})

plt.title('ì¹´í…Œê³ ë¦¬ ê°„ í‰ê·  ì„ë² ë”© ìœ ì‚¬ë„', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# ê°€ì¥ ìœ ì‚¬í•œ ì¹´í…Œê³ ë¦¬ ìŒ ì°¾ê¸°
similarity_pairs = []
for i in range(n_categories):
    for j in range(i+1, n_categories):
        similarity_pairs.append((
            categories[i],
            categories[j],
            category_similarity[i, j]
        ))

similarity_pairs.sort(key=lambda x: x[2], reverse=True)

print("\nğŸ“Š ì¹´í…Œê³ ë¦¬ ê°„ ìœ ì‚¬ë„ ìˆœìœ„:")
print("="*40)
for i, (cat1, cat2, sim) in enumerate(similarity_pairs, 1):
    print(f"{i}. {cat1} â†” {cat2}: {sim:.4f}")
## 11. RAG ì‹œìŠ¤í…œ ì‹œë®¬ë ˆì´ì…˜

ê°„ë‹¨í•œ RAG íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•˜ì—¬ ì‹¤ì œ í™œìš© ì˜ˆì‹œë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
class SimpleRAG:
    def __init__(self, documents, model, tokenizer, device):
        """
        ê°„ë‹¨í•œ RAG ì‹œìŠ¤í…œ
        """
        self.documents = documents
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

        # ë¬¸ì„œ ì„ë² ë”© ìƒì„±
        print("ğŸ“š ë¬¸ì„œ ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        self.embeddings = get_embedding(documents, model, tokenizer, device)
        print(f"âœ… {len(documents)}ê°œ ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ!\n")

    def retrieve(self, query, top_k=3):
        """
        ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        """
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = get_embedding(query, self.model, self.tokenizer, self.device)

        # ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity(query_embedding, self.embeddings)[0]

        # ìƒìœ„ kê°œ ì„ íƒ
        top_indices = np.argsort(similarities)[::-1][:top_k]

        retrieved_docs = []
        for idx in top_indices:
            retrieved_docs.append({
                'document': self.documents[idx],
                'similarity': similarities[idx]
            })

        return retrieved_docs

    def answer(self, query):
        """
        RAG ê¸°ë°˜ ë‹µë³€ ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)
        """
        # 1. Retrieval: ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        retrieved = self.retrieve(query, top_k=3)

        # 2. Augmentation: ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n".join([doc['document'] for doc in retrieved])

        # 3. Generation: ë‹µë³€ ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)
        print(f"ğŸ’¬ ì¿¼ë¦¬: {query}\n")
        print("ğŸ“„ ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ:")
        for i, doc in enumerate(retrieved, 1):
            print(f"  {i}. {doc['document'][:50]}...")
            print(f"     (ìœ ì‚¬ë„: {doc['similarity']:.4f})")

        print(f"\nğŸ¤– ìƒì„±ëœ ë‹µë³€ (ì‹œë®¬ë ˆì´ì…˜):")
        print(f"  '{query}'ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.")
        print(f"  ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
        print(f"  {retrieved[0]['document']}")

        return context

# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
rag_system = SimpleRAG(all_texts, model, tokenizer, device)

# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
test_queries = [
    "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜",
    "ë§›ìˆëŠ” í•œêµ­ ìŒì‹ ì¶”ì²œí•´ì¤˜",
    "ì˜¤ëŠ˜ ìš´ë™í•˜ê¸° ì¢‹ì€ ë‚ ì”¨ì¼ê¹Œ?"
]

print("ğŸš€ RAG ì‹œìŠ¤í…œ ë°ëª¨\n")
print("="*60)

for query in test_queries:
    rag_system.answer(query)
    print("\n" + "="*60 + "\n")