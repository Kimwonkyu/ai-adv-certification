# [ì‹¤ìŠµ] í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•µì‹¬ ê¸°ë²•

## í•™ìŠµëª©í‘œ
- ì •ê·œí‘œí˜„ì‹(Regular Expression)ì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤
- í…ìŠ¤íŠ¸ ì •ì œ, í† í°í™”, ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤
- ë¶ˆìš©ì–´ ì œê±°ì™€ í˜•íƒœì†Œ ë¶„ì„ì„ ì ìš©í•  ìˆ˜ ìˆë‹¤
- ì‹¤ì œ ë‰´ìŠ¤ ê¸°ì‚¬ì— ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤
## ì •ê·œí‘œí˜„ì‹(Regular Expression) ê¸°ì´ˆ

ì •ê·œí‘œí˜„ì‹ì€ í…ìŠ¤íŠ¸ íŒ¨í„´ì„ ì°¾ê³  ì²˜ë¦¬í•˜ëŠ” ê°•ë ¥í•œ ë„êµ¬ì…ë‹ˆë‹¤.  
NLP ì „ì²˜ë¦¬ì—ì„œ í•„ìˆ˜ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
import re
import pandas as pd
import json
from collections import Counter

# ì •ê·œí‘œí˜„ì‹ ê¸°ë³¸ íŒ¨í„´
text = "ì—°ë½ì²˜: 010-1234-5678, ì´ë©”ì¼: user@example.com, ë‚ ì§œ: 2024-01-15"

# ì „í™”ë²ˆí˜¸ ì°¾ê¸°
phone_pattern = r'\d{3}-\d{4}-\d{4}'
phones = re.findall(phone_pattern, text)
print(f"ì „í™”ë²ˆí˜¸: {phones}")

# ì´ë©”ì¼ ì°¾ê¸°
email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
emails = re.findall(email_pattern, text)
print(f"ì´ë©”ì¼: {emails}")

# ë‚ ì§œ ì°¾ê¸°
date_pattern = r'\d{4}-\d{2}-\d{2}'
dates = re.findall(date_pattern, text)
print(f"ë‚ ì§œ: {dates}")
# ì£¼ìš” ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ ì‹¤ìŠµ
sample_text = """Python3.13 ë²„ì „ì´ ì¶œì‹œë˜ì—ˆìŠµë‹ˆë‹¤! 
ìƒˆë¡œìš´ ê¸°ëŠ¥ë“¤: 1) ì„±ëŠ¥ í–¥ìƒ 2) íƒ€ì… íŒíŠ¸ ê°œì„  3) ì—ëŸ¬ ë©”ì‹œì§€ ê°œì„ 
ë¬¸ì˜: support@python.org ë˜ëŠ” 02-123-4567"""

patterns = {
    'ìˆ«ì': r'\d+',
    'ì˜ë¬¸ ë‹¨ì–´': r'[a-zA-Z]+',
    'í•œê¸€ ë‹¨ì–´': r'[ê°€-í£]+',
    'ê´„í˜¸ ì•ˆ ë‚´ìš©': r'\([^)]*\)',
    'ë¬¸ì¥ ë¶€í˜¸': r'[!?.,;:]',
    'ë²„ì „ ë²ˆí˜¸': r'\d+\.\d+'
}

print("=== ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ ë§¤ì¹­ ê²°ê³¼ ===")
for name, pattern in patterns.items():
    matches = re.findall(pattern, sample_text)
    print(f"{name}: {matches[:5]}...")  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
## í…ìŠ¤íŠ¸ ì •ì œ(Cleaning)

ë¶ˆí•„ìš”í•œ ë¬¸ìì™€ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜"""
    # HTML íƒœê·¸ ì œê±°
    text = re.sub(r'<[^>]+>', '', text)
    
    # URL ì œê±°
    text = re.sub(r'http[s]?://\S+', '', text)
    
    # ì´ë©”ì¼ ì œê±°
    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '', text)
    
    # íŠ¹ìˆ˜ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜ (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ë§Œ ìœ ì§€)
    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s.,!?]', ' ', text)
    
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    text = re.sub(r'\s+', ' ', text)
    
    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    
    return text

# í…ŒìŠ¤íŠ¸
messy_text = """<p>ì•ˆë…•í•˜ì„¸ìš”!</p> ì›¹ì‚¬ì´íŠ¸ https://example.comì„ ë°©ë¬¸í•˜ì„¸ìš”.
ë¬¸ì˜: contact@example.com â˜ï¸ 02-1234-5678 #AI #ë¨¸ì‹ ëŸ¬ë‹"""

cleaned = clean_text(messy_text)
print("ì›ë³¸ í…ìŠ¤íŠ¸:")
print(messy_text)
print("\nì •ì œëœ í…ìŠ¤íŠ¸:")
print(cleaned)
# ë‹¤ì–‘í•œ ë…¸ì´ì¦ˆ ì²˜ë¦¬
def remove_noise(text):
    """ë‹¤ì–‘í•œ ë…¸ì´ì¦ˆ ì œê±°"""
    # ì´ëª¨ì§€ ì œê±°
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags
        "]+", flags=re.UNICODE)
    text = emoji_pattern.sub('', text)
    
    # í•´ì‹œíƒœê·¸ ì²˜ë¦¬ (íƒœê·¸ë§Œ ì¶”ì¶œ)
    hashtags = re.findall(r'#\w+', text)
    text = re.sub(r'#\w+', '', text)
    
    # ë©˜ì…˜ ì œê±°
    text = re.sub(r'@\w+', '', text)
    
    return text, hashtags

social_text = "ì˜¤ëŠ˜ ë‚ ì”¨ ì¢‹ë„¤ìš”! ğŸ˜Š #ë‚ ì”¨ #ë§‘ìŒ @friend1 ê³¼ í•¨ê»˜ ğŸŒ¸"
cleaned_social, tags = remove_noise(social_text)
print(f"ì›ë³¸: {social_text}")
print(f"ì •ì œ: {cleaned_social}")
print(f"í•´ì‹œíƒœê·¸: {tags}")
## í† í°í™”(Tokenization)

í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
def tokenize_korean(text):
    """í•œêµ­ì–´ í† í°í™” (ê°„ë‹¨í•œ ê³µë°± ê¸°ë°˜)"""
    # ë¬¸ì¥ ë¶„ë¦¬
    sentences = re.split(r'[.!?]+', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    # ë‹¨ì–´ ë¶„ë¦¬
    words = text.split()
    
    # í˜•íƒœì†Œ ë‹¨ìœ„ (ê°„ë‹¨í•œ ì¡°ì‚¬ ë¶„ë¦¬)
    morphemes = []
    josa_pattern = r'(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì—|ì—ì„œ|ë¡œ|ìœ¼ë¡œ|ì™€|ê³¼|ì˜|ë„|ë§Œ|ë¶€í„°|ê¹Œì§€)$'
    
    for word in words:
        match = re.search(josa_pattern, word)
        if match:
            stem = word[:match.start()]
            josa = match.group()
            if stem:
                morphemes.append(stem)
            morphemes.append(josa)
        else:
            morphemes.append(word)
    
    return {
        'sentences': sentences,
        'words': words,
        'morphemes': morphemes
    }

korean_text = "ìì—°ì–´ì²˜ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì²˜ë¦¬í•©ë‹ˆë‹¤."
tokens = tokenize_korean(korean_text)

print("=== í† í°í™” ê²°ê³¼ ===")
print(f"ë¬¸ì¥: {tokens['sentences']}")
print(f"ë‹¨ì–´: {tokens['words']}")
print(f"í˜•íƒœì†Œ: {tokens['morphemes']}")
# N-gram í† í°í™”
def get_ngrams(text, n=2):
    """N-gram í† í° ìƒì„±"""
    words = text.split()
    ngrams = []
    
    for i in range(len(words) - n + 1):
        ngram = ' '.join(words[i:i+n])
        ngrams.append(ngram)
    
    return ngrams

text = "ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤"

print("=== N-gram í† í°í™” ===")
for n in [1, 2, 3]:
    ngrams = get_ngrams(text, n)
    print(f"{n}-gram: {ngrams}")
## ì •ê·œí™”(Normalization)

í…ìŠ¤íŠ¸ë¥¼ í‘œì¤€ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
def normalize_text(text):
    """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
    # ëŒ€ì†Œë¬¸ì í†µì¼ (ì˜ë¬¸)
    text = text.lower()
    
    # ìˆ«ì ì •ê·œí™”
    text = re.sub(r'\d+', 'NUM', text)
    
    # ë°˜ë³µ ë¬¸ì ì¶•ì†Œ
    text = re.sub(r'(.)\1{2,}', r'\1\1', text)
    
    # ê³µë°± ì •ê·œí™”
    text = re.sub(r'\s+', ' ', text)
    
    return text.strip()

# í…ŒìŠ¤íŠ¸
texts = [
    "Python3ì™€ Python2ëŠ” ë‹¤ë¦…ë‹ˆë‹¤!!!!!!",
    "ì™€ì•„ì•„ì•„ì•„ì•„ ëŒ€ë°•!!!!   ë„ˆë¬´    ì¢‹ì•„ìš”",
    "ê°€ê²©ì€ 15,000ì› ì…ë‹ˆë‹¤. 2024ë…„ 1ì›” ì¶œì‹œ"
]

print("=== ì •ê·œí™” ê²°ê³¼ ===")
for text in texts:
    normalized = normalize_text(text)
    print(f"ì›ë³¸: {text}")
    print(f"ì •ê·œí™”: {normalized}\n")
# í•œê¸€ ìëª¨ ë¶„ë¦¬/ê²°í•© (ê°„ë‹¨í•œ ì˜ˆì‹œ)
def normalize_korean_chars(text):
    """í•œê¸€ ë¬¸ì ì •ê·œí™”"""
    # ã…‹ã…‹ã…‹, ã…ã…ã… ê°™ì€ ììŒ ë°˜ë³µ ì •ê·œí™”
    text = re.sub(r'[ã…‹]{2,}', 'ã…‹ã…‹', text)
    text = re.sub(r'[ã…]{2,}', 'ã…ã…', text)
    text = re.sub(r'[ã… ã…œ]{2,}', 'ã… ã… ', text)
    
    # ì´ìƒí•œ ë„ì–´ì“°ê¸° êµì • (ê°„ë‹¨í•œ ê·œì¹™)
    text = re.sub(r'\s+([.,!?])', r'\1', text)  # ë¬¸ì¥ë¶€í˜¸ ì• ê³µë°± ì œê±°
    
    return text

korean_samples = [
    "ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ ë„ˆë¬´ ì›ƒê²¨ìš”",
    "ã… ã… ã… ã… ã…  ìŠ¬í¼ìš”",
    "ì•ˆë…•í•˜ì„¸ìš” . ë°˜ê°‘ìŠµë‹ˆë‹¤ !"
]

for sample in korean_samples:
    normalized = normalize_korean_chars(sample)
    print(f"ì›ë³¸: {sample}")
    print(f"ì •ê·œí™”: {normalized}\n")
## ë¶ˆìš©ì–´(Stopwords) ì²˜ë¦¬

ë¶„ì„ì— ë¶ˆí•„ìš”í•œ ë‹¨ì–´ë“¤ì„ ì œê±°í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
# í•œêµ­ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
korean_stopwords = [
    'ì´', 'ìˆ', 'í•˜', 'ê²ƒ', 'ë¥¼', 'ë“¤', 'ê·¸', 'ë˜', 'ìˆ˜', 'ì´', 'ë³´', 'ì•Š', 'ì—†', 'ë‚˜', 'ì‚¬ëŒ',
    'ì£¼', 'ì•„ë‹ˆ', 'ë“±', 'ê°™', 'ìš°ë¦¬', 'ë•Œ', 'ë…„', 'ê°€', 'í•œ', 'ì§€', 'ëŒ€í•˜', 'ì˜¤', 'ë§',
    'ì¼', 'ê·¸ë ‡', 'ìœ„í•˜', 'ë•Œë¬¸', 'ê·¸ê²ƒ', 'ë‘', 'ë§í•˜', 'ì•Œ', 'ê·¸ëŸ¬ë‚˜', 'ë°›', 'ëª»í•˜',
    'ì¼', 'ê·¸ëŸ°', 'ë˜',  'ë”', 'ì‚¬íšŒ', 'ë§', 'ê·¸ë¦¬ê³ ', 'ì¢‹', 'í¬', 'ë”°ë¥´', 'ì¤‘'
]

def remove_stopwords(text, stopwords):
    """ë¶ˆìš©ì–´ ì œê±°"""
    words = text.split()
    filtered_words = [word for word in words if word not in stopwords]
    return ' '.join(filtered_words)

# í…ŒìŠ¤íŠ¸
text = "ë‹¤ìŒ ì€ ë§¤ìš° ì¤‘ìš”í•œ ë¬¸ì œ ì…ë‹ˆë‹¤ ìš°ë¦¬ í•¨ê»˜ ë¬¸ì œ ë¥¼ ê°™ì´ í•´ê²°í•´ì•¼ í•  ê²ƒ ì…ë‹ˆë‹¤"
filtered_text = remove_stopwords(text, korean_stopwords)

print(f"ì›ë³¸: {text}")
print(f"ë¶ˆìš©ì–´ ì œê±°: {filtered_text}")
print(f"ì œê±°ëœ ë‹¨ì–´: {set(text.split()) - set(filtered_text.split())}")
## ì‹¤ì „: ë‰´ìŠ¤ ê¸°ì‚¬ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

ì‹¤ì œ ë‰´ìŠ¤ ë°ì´í„°ì— ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•´ë³´ê² ìŠµë‹ˆë‹¤.
# ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„° ë¡œë“œ
try:
    with open('raw_articles.txt', 'r', encoding='utf-8') as f:
        articles = f.read().split('\n---\n')
    print(f"ì´ {len(articles)}ê°œ ê¸°ì‚¬ ë¡œë“œ")
except FileNotFoundError:
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    articles = [
        """[ì†ë³´] AI ê¸°ìˆ  í˜ì‹ ìœ¼ë¡œ ì‚°ì—… ì „ë°˜ ë³€í™” ì˜ˆìƒ
        ì „ë¬¸ê°€ë“¤ì€ ì¸ê³µì§€ëŠ¥(AI) ê¸°ìˆ ì´ í–¥í›„ 5ë…„ ë‚´ ëª¨ë“  ì‚°ì—…ì— í° ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ì „ë§í–ˆë‹¤.
        íŠ¹íˆ ì œì¡°ì—…ê³¼ ì„œë¹„ìŠ¤ì—…ì—ì„œì˜ ìë™í™”ê°€ ê°€ì†í™”ë  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.""",
        
        """íŒŒì´ì¬(Python), ê°œë°œìê°€ ê°€ì¥ ë°°ìš°ê³  ì‹¶ì€ ì–¸ì–´ 1ìœ„
        Stack Overflow ì¡°ì‚¬ì— ë”°ë¥´ë©´ Pythonì´ 5ë…„ ì—°ì† ê°€ì¥ ì¸ê¸° ìˆëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ ì„ ì •ëë‹¤.
        ë°ì´í„° ë¶„ì„ê³¼ AI ê°œë°œì—ì„œì˜ í™œìš©ë„ê°€ ë†’ì€ ê²ƒì´ ì£¼ìš” ìš”ì¸ìœ¼ë¡œ ë¶„ì„ëœë‹¤.""",
        
        """ChatGPT ì‚¬ìš©ì 2ì–µëª… ëŒíŒŒ... AI ëŒ€ì¤‘í™” ì‹œëŒ€ ì—´ë ¤
        OpenAIì˜ ChatGPTê°€ ì¶œì‹œ 1ë…„ ë§Œì— ì‚¬ìš©ì 2ì–µëª…ì„ ëŒíŒŒí–ˆë‹¤ê³  ë°œí‘œí–ˆë‹¤.
        ì¼ìƒìƒí™œì—ì„œ AIë¥¼ í™œìš©í•˜ëŠ” ì‚¬ë¡€ê°€ ê¸‰ì¦í•˜ê³  ìˆë‹¤."""
    ]
    print(f"ìƒ˜í”Œ ë°ì´í„° {len(articles)}ê°œ ìƒì„±")

print("\nì²« ë²ˆì§¸ ê¸°ì‚¬:")
print(articles[0][:200] + "...")
class TextPreprocessor:
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, stopwords=None):
        self.stopwords = stopwords or korean_stopwords
        self.stats = {}
    
    def preprocess(self, text):
        """ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        # ì›ë³¸ í†µê³„
        original_length = len(text)
        original_words = len(text.split())
        
        # 1. ì œëª©ê³¼ ë³¸ë¬¸ ë¶„ë¦¬
        lines = text.strip().split('\n')
        title = lines[0] if lines else ''
        body = ' '.join(lines[1:]) if len(lines) > 1 else ''
        
        # 2. ì •ì œ
        title = self._clean(title)
        body = self._clean(body)
        
        # 3. í† í°í™”
        title_tokens = self._tokenize(title)
        body_tokens = self._tokenize(body)
        
        # 4. ì •ê·œí™”
        title_normalized = self._normalize(title)
        body_normalized = self._normalize(body)
        
        # 5. ë¶ˆìš©ì–´ ì œê±°
        title_filtered = self._remove_stopwords(title_normalized)
        body_filtered = self._remove_stopwords(body_normalized)
        
        # ì²˜ë¦¬ í›„ í†µê³„
        processed_length = len(title_filtered) + len(body_filtered)
        processed_words = len(title_filtered.split()) + len(body_filtered.split())
        
        return {
            'title': title_filtered,
            'body': body_filtered,
            'title_tokens': title_tokens,
            'body_tokens': body_tokens,
            'stats': {
                'original_length': original_length,
                'processed_length': processed_length,
                'original_words': original_words,
                'processed_words': processed_words,
                'reduction_rate': 1 - (processed_words / original_words) if original_words > 0 else 0
            }
        }
    
    def _clean(self, text):
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        text = re.sub(r'\[.*?\]', '', text)  # ëŒ€ê´„í˜¸ ë‚´ìš© ì œê±°
        text = re.sub(r'\(.*?\)', '', text)  # ì†Œê´„í˜¸ ë‚´ìš© ì œê±°
        text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s.,!?%]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def _tokenize(self, text):
        """í† í°í™”"""
        return text.split()
    
    def _normalize(self, text):
        """ì •ê·œí™”"""
        text = text.lower()
        text = re.sub(r'\d+', 'NUM', text)
        return text
    
    def _remove_stopwords(self, text):
        """ë¶ˆìš©ì–´ ì œê±°"""
        words = text.split()
        filtered = [w for w in words if w not in self.stopwords]
        return ' '.join(filtered)

# ì „ì²˜ë¦¬ê¸° ìƒì„± ë° ì ìš©
preprocessor = TextPreprocessor()

print("=== ê¸°ì‚¬ ì „ì²˜ë¦¬ ê²°ê³¼ ===")
for i, article in enumerate(articles, 1):
    result = preprocessor.preprocess(article)
    print(f"\nê¸°ì‚¬ {i}:")
    print(f"ì œëª©: {result['title']}")
    print(f"ë³¸ë¬¸ (ì• 100ì): {result['body'][:100]}...")
    print(f"í†µê³„: ì›ë³¸ {result['stats']['original_words']}ë‹¨ì–´ â†’ "
          f"ì²˜ë¦¬ í›„ {result['stats']['processed_words']}ë‹¨ì–´ "
          f"(ê°ì†Œìœ¨: {result['stats']['reduction_rate']:.1%})")
## ë‹¨ì–´ ë¹ˆë„ ë¶„ì„

ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ìš” í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ë³´ê² ìŠµë‹ˆë‹¤.
def extract_keywords(texts, top_n=10):
    """í…ìŠ¤íŠ¸ì—ì„œ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    all_words = []
    
    for text in texts:
        # ì „ì²˜ë¦¬
        processed = preprocessor.preprocess(text)
        words = processed['body'].split() + processed['title'].split()
        all_words.extend(words)
    
    # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
    word_freq = Counter(all_words)
    
    # ì˜ë¬¸ê³¼ í•œê¸€ ë¶„ë¦¬
    korean_words = {}
    english_words = {}
    
    for word, freq in word_freq.items():
        if re.match(r'[ê°€-í£]+', word):
            korean_words[word] = freq
        elif re.match(r'[a-zA-Z]+', word):
            english_words[word] = freq
    
    return {
        'korean': sorted(korean_words.items(), key=lambda x: x[1], reverse=True)[:top_n],
        'english': sorted(english_words.items(), key=lambda x: x[1], reverse=True)[:top_n],
        'total': word_freq.most_common(top_n)
    }

keywords = extract_keywords(articles, top_n=5)

print("=== ì£¼ìš” í‚¤ì›Œë“œ ===")
print("\ní•œê¸€ í‚¤ì›Œë“œ:")
for word, freq in keywords['korean']:
    print(f"  {word}: {freq}íšŒ")

print("\nì˜ë¬¸ í‚¤ì›Œë“œ:")
for word, freq in keywords['english']:
    print(f"  {word}: {freq}íšŒ")
## ì „ì²˜ë¦¬ ì„±ëŠ¥ ë¹„êµ

ë‹¤ì–‘í•œ ì „ì²˜ë¦¬ ìˆ˜ì¤€ì— ë”°ë¥¸ ê²°ê³¼ë¥¼ ë¹„êµí•´ë³´ê² ìŠµë‹ˆë‹¤.
def compare_preprocessing_levels(text):
    """ì „ì²˜ë¦¬ ìˆ˜ì¤€ë³„ ë¹„êµ"""
    results = {}
    
    # Level 0: ì›ë³¸
    results['ì›ë³¸'] = {
        'text': text,
        'length': len(text),
        'words': len(text.split())
    }
    
    # Level 1: ì •ì œë§Œ
    cleaned = clean_text(text)
    results['ì •ì œ'] = {
        'text': cleaned,
        'length': len(cleaned),
        'words': len(cleaned.split())
    }
    
    # Level 2: ì •ì œ + ì •ê·œí™”
    normalized = normalize_text(cleaned)
    results['ì •ì œ+ì •ê·œí™”'] = {
        'text': normalized,
        'length': len(normalized),
        'words': len(normalized.split())
    }
    
    # Level 3: ì •ì œ + ì •ê·œí™” + ë¶ˆìš©ì–´
    filtered = remove_stopwords(normalized, korean_stopwords)
    results['ì „ì²´ì²˜ë¦¬'] = {
        'text': filtered,
        'length': len(filtered),
        'words': len(filtered.split())
    }
    
    return results

# í…ŒìŠ¤íŠ¸
test_article = articles[0]
comparison = compare_preprocessing_levels(test_article)

print("=== ì „ì²˜ë¦¬ ìˆ˜ì¤€ë³„ ë¹„êµ ===")
for level, result in comparison.items():
    print(f"\n[{level}]")
    print(f"ê¸¸ì´: {result['length']} ë¬¸ì, {result['words']} ë‹¨ì–´")
    print(f"í…ìŠ¤íŠ¸ (ì• 80ì): {result['text'][:80]}...")

# ê°ì†Œìœ¨ ê³„ì‚°
original_words = comparison['ì›ë³¸']['words']
final_words = comparison['ì „ì²´ì²˜ë¦¬']['words']
reduction = (1 - final_words/original_words) * 100
print(f"\nì´ ë‹¨ì–´ ê°ì†Œìœ¨: {reduction:.1f}%")
## ì‹¤ìŠµ í”„ë¡œì íŠ¸: ë‰´ìŠ¤ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬

ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ì „ì²˜ë¦¬ ë° íŠ¹ì§• ì¶”ì¶œì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
# ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ë°ì´í„°
categorized_news = {
    'AI': [
        "ì¸ê³µì§€ëŠ¥ì´ ì˜ë£Œ ì§„ë‹¨ì— í˜ëª…ì„ ì¼ìœ¼í‚¤ê³  ìˆë‹¤. ë”¥ëŸ¬ë‹ ê¸°ìˆ ë¡œ ì•” ì§„ë‹¨ ì •í™•ë„ 95% ë‹¬ì„±",
        "ChatGPT ê²½ìŸ ì‹¬í™”, êµ¬ê¸€ê³¼ ë©”íƒ€ë„ ëŒ€í™”í˜• AI ì¶œì‹œ ì¤€ë¹„"
    ],
    'ê°œë°œ': [
        "ìë°”ìŠ¤í¬ë¦½íŠ¸ í”„ë ˆì„ì›Œí¬ React 18 ì¶œì‹œ, ì„±ëŠ¥ ëŒ€í­ ê°œì„ ",
        "íŒŒì´ì¬ ì›¹ í”„ë ˆì„ì›Œí¬ Django 5.0 ë² íƒ€ ë²„ì „ ê³µê°œ"
    ],
    'ë³´ì•ˆ': [
        "ëŒ€ê·œëª¨ ëœì„¬ì›¨ì–´ ê³µê²© ì¦ê°€, ê¸°ì—…ë“¤ ë³´ì•ˆ ê°•í™” ë‚˜ì„œ",
        "ì œë¡œë°ì´ ì·¨ì•½ì  ë°œê²¬, ê¸´ê¸‰ ë³´ì•ˆ íŒ¨ì¹˜ ë°°í¬"
    ]
}

def analyze_category_features(news_dict):
    """ì¹´í…Œê³ ë¦¬ë³„ íŠ¹ì§• ë¶„ì„"""
    category_features = {}
    
    for category, articles in news_dict.items():
        all_text = ' '.join(articles)
        processed = preprocessor.preprocess(all_text)
        
        # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
        words = processed['body'].split() + processed['title'].split()
        word_freq = Counter(words)
        top_keywords = word_freq.most_common(5)
        
        # íŠ¹ìˆ˜ íŒ¨í„´ ì°¾ê¸°
        tech_terms = re.findall(r'[A-Z][a-z]+(?:[A-Z][a-z]+)*', all_text)  # CamelCase
        versions = re.findall(r'\d+\.\d+', all_text)  # ë²„ì „ ë²ˆí˜¸
        percentages = re.findall(r'\d+%', all_text)  # í¼ì„¼íŠ¸
        
        category_features[category] = {
            'keywords': top_keywords,
            'tech_terms': list(set(tech_terms)),
            'versions': versions,
            'percentages': percentages,
            'avg_length': sum(len(a) for a in articles) / len(articles)
        }
    
    return category_features

features = analyze_category_features(categorized_news)

print("=== ì¹´í…Œê³ ë¦¬ë³„ íŠ¹ì§• ë¶„ì„ ===")
for category, feature in features.items():
    print(f"\n[{category}]")
    print(f"ì£¼ìš” í‚¤ì›Œë“œ: {[w for w, f in feature['keywords'][:3]]}")
    print(f"ê¸°ìˆ  ìš©ì–´: {feature['tech_terms'][:3]}")
    print(f"ë²„ì „ ì •ë³´: {feature['versions']}")
    print(f"í‰ê·  ê¸¸ì´: {feature['avg_length']:.0f}ì")
## í•™ìŠµ ë‚´ìš© ì •ë¦¬

### í•µì‹¬ ì „ì²˜ë¦¬ ê¸°ë²•

| ê¸°ë²• | ëª©ì  | ì£¼ìš” ë°©ë²• |
|------|------|----------|
| ì •ê·œí‘œí˜„ì‹ | íŒ¨í„´ ë§¤ì¹­ | `re.findall()`, `re.sub()` |
| í…ìŠ¤íŠ¸ ì •ì œ | ë…¸ì´ì¦ˆ ì œê±° | HTML, URL, íŠ¹ìˆ˜ë¬¸ì ì œê±° |
| í† í°í™” | ë‹¨ìœ„ ë¶„í•  | ë¬¸ì¥, ë‹¨ì–´, í˜•íƒœì†Œ ë¶„ë¦¬ |
| ì •ê·œí™” | í‘œì¤€í™” | ëŒ€ì†Œë¬¸ì, ìˆ«ì, ë°˜ë³µ ì²˜ë¦¬ |
| ë¶ˆìš©ì–´ ì œê±° | ì¤‘ìš” ë‹¨ì–´ ì¶”ì¶œ | ë¹ˆë„ ë†’ì€ ë¬´ì˜ë¯¸ ë‹¨ì–´ ì œê±° |

### ì •ê·œí‘œí˜„ì‹ ì£¼ìš” íŒ¨í„´

| íŒ¨í„´ | ì„¤ëª… | ì˜ˆì‹œ |
|------|------|------|
| `\d+` | ìˆ«ì | 123, 456 |
| `[ê°€-í£]+` | í•œê¸€ ë‹¨ì–´ | ì•ˆë…•, ì„¸ê³„ |
| `[a-zA-Z]+` | ì˜ë¬¸ ë‹¨ì–´ | Hello, World |
| `\s+` | ê³µë°± | ê³µë°±, íƒ­, ì¤„ë°”ê¿ˆ |
| `.+?` | ìµœì†Œ ë§¤ì¹­ | ìš•ì‹¬ì—†ëŠ” ë§¤ì¹­ |

### ì‹¤ìŠµ ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

âœ“ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ íŒ¨í„´ ì¶”ì¶œ  
âœ“ í…ìŠ¤íŠ¸ ì •ì œ ë° ë…¸ì´ì¦ˆ ì œê±°  
âœ“ í† í°í™”ì™€ N-gram ìƒì„±  
âœ“ í…ìŠ¤íŠ¸ ì •ê·œí™” ì ìš©  
âœ“ ë¶ˆìš©ì–´ ì²˜ë¦¬ ë° í‚¤ì›Œë“œ ì¶”ì¶œ  
âœ“ ë‰´ìŠ¤ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
