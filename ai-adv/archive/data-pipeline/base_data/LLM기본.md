***

## 1. LLM이란? (정의·핵심 구조)

- 정의: 언어 패턴을 학습해서 "다음에 올 토큰"을 예측하며 문장을 생성하는 모델이다.
- 핵심: 대부분 LLM의 기본 골격은 Transformer 구조(Attention 기반)이며, 현재 LLM의 주된 역할은 자연스러운 언어 생성이다.

***

## 2. 자연어 처리 핵심 개념 (임배딩·토큰·맥락)

- 임베딩(Embeding): 단어/토큰을 고차원 벡터로 바꾸어 의미를 숫자로 표현하는 기술로, Word2Vec이 대표적인 초기 방식이다.
- Word2Vec 한계: 동음이의어·다의어 처리 불가, 오타·UNK 토근 문제, 단어 수 증가 시 확장성 한계가 있다.
- 토큰(Token): 단어보다 작고 문자보다 큰 단위로, LLM은 "토큰 단위"로 임베딩과 학습을 수행하며, 토큰 개수가 많을수록 컨텍스트 한도가 빨리 소진된다.
- 맥락(Context): 철자만이 아니라 주변 단어를 같이 고려해서 의미와 오타를 추론하는 정보이며, 현대 LLM은 "토큰 간 관계(맥락)"를 학습한다.

***

## 3. Transformer 구조 (Encoder·Decorder·Attention)

- 목적: RNN이 긴 문장에서 앞 내용을 잊는 문제를 해결해, 긴 문장도 병렬로 처리하면서 맥락을 유지하기 위해 제안되었다.
- 구성:
  - Encoder: 입력 문장의 토큰별 의미/맥락 벡터를 병렬로 계산한다.
  - Decoder: Encoder 결과 + 지금까지 출력된 토큰을 기반으로 "다음 토큰 확률 분포"를 순차적으로 생성한다(자기회귀, Auto-regressive).
- Self-Attention: 각 토큰을 Query/Key/Value로 바꾸어 "어디에 집중할지"를 계산해 맥락을 반영한 벡터를 만든다.
- Multi-Head Attention: 여러 개의 Self-Attention을 병렬로 돌려, 문법, 의미, 유사성 등 다양한 관점을 동시에 학습한다.
- FFNN(Feed-Forward NN): Attention 결과에 비선형성을 추가해 더 복잡한 의미를 학습하는 작은 딥러닝 블록이다.
- Positional Encoding: 순서를 모르는 병렬 구조의 한계를 보완하기 위해 토큰 위치 정보를 벡터에 더해 주는 장치이며, ROPE 등 회전 기반 방법도 사용된다.

***

## 4. BERT vs GPT, GPT 계열 발전

- BERT: Encoder 기반 "양방향 Transformer"로, 문장 의미 파악·빈칸 채우기·문장 관계 파악에 특화된 사전학습 모델이다.
- GPT: Decoder만 사용하는 "다음 토큰 예측" 모델로, 텍스트 생성에 특화되어 현재 대부분의 LLM이 따르는 구조다.
- Foundation Model: 언어 패턴을 대규모 데이터로 "사전 학습(Pretrain)"한 뒤, 소량 데이터로 "파인튜닝(Fine-tuning)"해서 여러 태스크에 쓰는 기본 모델의 의미한다.

- GPT 계보(시험용 키워드):
  - GPT-1: 0.1B 파라미터, 도메인별 추가 파인튜닝 필요.
  - GPT-2: 1.5B, 더 큰 모델·웹 텍스트 학습, TL;DR 등으로 요약 등 간단한 작업 가능.
  - GPT-3: 175B, 대규모 데이터로 In-Context Learning, Few-Shot Learning, 질의응답 능력 향상.
  - GPT-3.5: GPT-3 + SFT + RLHF로 "채팅형(ChatGPT)" 질의응답 포맷·인간 선호도 반영.
  - GPT-4: 1760B MoE, 여러 전문가 MLP를 Router가 선택해서 효율적으로 계산.
  - GPT-4o: 텍스트·이미지·오디오·비디오를 함께 처리하는 Omnimadal Early Fusion 모델.
  - GPT-5: 가장 최신 고성능 Reasoning 모델로, 긴 "Thinking" 과정을 거쳐 어려운 문제를 푼다.

***

## 5. 상용 vs 오픈 LLM, 주요 오픈 모델

- 상용 모델: GPT·Gemini·Claude·Grok 등, 가중치 비공개·API로만 사용 가능, 성능 최상위.
- 오픈 모델: Llama·Qwen·Mistral·Phi·Gemma 등, 가중치 공개·로컬 실행 및 파인튜닝 가능하지만 GPU·라이선스 제약이 있다.

### 대표 오픈 모델 특징

- Llama (Meta): 오픈 데이터 기반 고성능, 1 → 2 → 3.x → 4로 발전, 8B ~ 405B·SLM·멀티모달 라인까지 확장.
- Phi (Microsoft): 합성 데이터(다른 LLM이 만든 데이터)로 학습, 작은 파라미터 대비 성능 좋지만 한국어는 상대적 약점.
- Qwen (Alibaba): 다국어 처리 최강 수준, 언어별 토크나이저 최적화, 0.6B ~ 72B·MoE 구조까지 다양한 크기 제공.
- Mistral: 작은 크리로 높은 성능, Mixtral MoE(8x7B 등), Large/Small 시리즈, 한국어 지원은 점점 강화되는 중.
- Gemma (Google): Gemma 2, 3 등으로 발전, 한국어 성능 우수, 양자화 친화적 학습(QAT)·동적 파라미터 선택 등 효율성 강화.

***

## 6. 파리미터·VRAM·양자화(Quantization)

- 파라미터 수: 모델 크리를 나타내는 지표로, 예 7B 모델을 16bit로 쓰려면 약 14GB VRAM, 안정적 사용에는 약 1.5배(21GB) VRAM이 필요하다.
- 학습 시: 추론보다 훨씬 많은 메모리(최소 10배 수준)가 필요하다.

### 양자화 핵심 키워드

- 목적: Weight 비트를 줄여 모델 크기와 VRAM 사용량을 줄이되 성능 저하를 최소화하는 것.
- 고전적 INT8: 최소·최대값 기준으로 0~255 구간을 등간격 분할해서 저장 → 구현 단순하지만 값 왜곡·성능 저하가 발생한다.
- NF4/QLoRA: 분위수(Quantile) 기반 분포형 양자화로 원래 값 보존률을 높여 성능 저하를 줄인다.
- 중요도 기반 양자화: GPTQ·AWQ처럼 중요한 파리미터는 덜, 덜 중요한 파라미터는 더 강하게 양자화해 효율·성능을 동시에 노린다.
- GGUF/GGML: llama.cpp 계열 CPU 추론 최적화 포맷으로, 단일 파일 배포·Ollama 등에서 쉽게 서빙 가능하다.

***

## 7. Reasoning·Thinking LLM과 멀티모달

- Reasoning 모델: 답을 빨리 내기보다 "Thinkin 토큰 구간"에서 길게 사고한 뒤 답을 내도록 학습된 모델로, 수학·코딩 등 어려운 문제에서 성능이 높다.
- 특징: 첫 출력까지 시간이 길고, Thinking 구간에서 자기 분석·분할 정복·자체 수정(Self-Refinement)등을 텍스트로 수행한다.
- 멀티모달 LLM(LMM): 텍스트 + 이미지(더 나아가 음성·비디오)를 같이 처리·이해하는 모델로, GPT-4o·Qwne-VL·Llama 3.2 비전 버전 등이 있다.

***

