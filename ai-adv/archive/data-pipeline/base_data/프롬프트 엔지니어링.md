***

## 1. 프롬프트 엔지니어링 핵심 개념

- 프롬프트 엔지니어링 정의
  언어모델이 원하는 답을 내도록 "입력(맥락+지시)"을 설계하는 기술.

- 2023 → 2025 변화 포인트
  ① 프롬프트 단순화 (요청을 대충 해도 모델이 Reasoning으로 보정)
  ② 직업에서 "모든 직무의 기본 역량"으로 이동 (전문 직군 ↓, 실무 스킬 ↑)

- Context Engineering
  프롬프트만이 아니라 시스템프롬프트, 장•단기 메모리, RAG Tool, 출력 형식까지 전체 맥락을 설계하는 것.

***

## 2. 좋은 프롬프트의 4요소 (Persona-Task-Context-Format)

- Persona
  "너는 ~한 역할이야"로 말투•스타일•관점을 정하는 것. 최신 모델은 성능 향상 효과는 작지만 답변 형식 통제에는 유효.

- Task
  모델에게 시키는 "구체 작업"을 명시 (예: 요약, 번역, 코드리뷰, 분류 등).

- Context
  작업에 필요한 배경•자료•제약조건을 함께 제공 (예: 회사 상황, 데이터 일부, VOC 내용 등).

- Format
  출력 형식을 지정 (예: 마크다운 표, 불릿 리스트, JSON 필드 구조 등).

***

## 3. Persona 관련 최신 논문 포인트

- Role-Play / Role Prompting 효과
  "당신은 수학 선생님 입니다"처럼 역할 부여하면 산술•상식 등 Zero-shot 추론 성능이 올라가는 사례.

- System 프롬프트 Persona 무효 논문
  EMNLP 2024 "Personas in System Prompts Do Not Imporve Performances of LLMs":
  시스템 프롬프트의 다양한 역할 설정은 MMLU 지식 문제에서 의미 있는 향상이 거의 없음.

- 시사점
  ① 추론 태스크에서는 페르소나가 일부 도움,
  ② 지식 회상 위주에서는 효과 미미,
  ③ 지금은 "역할+형식" 정도로 축소•흡수.  

***

## 4. Prompt Framework / Chaining / 구조화

- 대표 Prompt Framework 공통 구조
  RACE / TRACE / ROSES / APE 모두 결국 "역할(Role/Persona) + 목표/작업(Task) + 맥락(Context) + 형식(Style/Format/Example)"을 채우는 틀.

- Prompt Chaining
  복잡한 일을 "여러 단계 프롬프트"로 쪼개서 순차 처리.
  장점: 컨텍스트 길이 감소, 토큰 절감, 단계별 검증 가능.

- 프롬프트 구조화
  #, ---, [], <> 등으로 Context / Question / Steps / Output Format 구역을 나눠서 모델이 어디에 집중해야 할지 명확하게 해주는 것.

- Few-shot Prompting
  프롬프트 안에 예시(입•출력 샘플)를 넣어 "이 패턴을 따라 해라"라고 가르치는 방식.
  단점: 예시의 톤•길이•주제에 과도하게 끌려가는 편향 가능.

***

## 5. LCEL과 구조화된 출력 (LangChain)

- LCEL(LangChain Expression Language)
  PromptTemplate, LLM, Parser, Tool 등을 "Runnable Chain"으로 파이프(|) 연결해서 구성하는 문법.

- 기본 패턴
  PromptTemplate | LLM | Parser
  - StrOutputParser: 문자열로 받기
  - JsonOutputParser / PysanticOutputParser: JSON / Pydatic 객체로 받기.

- 특수 Runnable
  - RunnablePassthrough: 입력을 그대로 다음 단계로 넘김.
  - RunnableParallel: 여러 Runnable을 병렬 실행하고 Dict로 결과 결함.

***

## 6. 프롬프트 엔지니어링 심화 기법

- Chain-of-Thought(CoT)
  "풀이 과정을 말하게 해서" 논리 추론을 유도하는 기법.
  CoT = 정답보다 "중간 Reasoning 토큰을 많이 쓰게 해서" 성능을 올리는 방법.

- Zero-shot CoT
  "Let's think step by step." 같은 한 줄로 CoT 모드를 키는 방식.

- Over-thinking 문제
  추론을 과하게 시키면 오히려 환각이 강화되거나 속도•비용이 늘어 성능이 떨어질 수 있음.

- Step-Back Prompting
  "바로 답" 말고, 먼저 문제 해결에 필요한 개념/구조/파일구성/ 목차를 뽑게 한 다음, 그 구조를 기반으로 답변•코드를 쓰게 하는 두 단계 전략.

- 프롬프트 길이 vs 비용
  출력 길이가 길수록 API 비용•GPU 사용량 증가 → CoT•장문 출력은 성능 이득과 비용을 같이 고려해야 함.

- 한국어/영어 프롬프트 팁
  지시는 영어로 줄수록 이해력•토큰 효율 ↑, 특히 오픈 LLM에서 유리.
  한국어 태스크에서는 "모델의 한국어 성능"이 전문지식보다 중요 → 한국어 리더보드 참고.

***

## 7. LangChain Tools & Function Calling

- Retrieval Augmented Generation (RAG)
  LLM + 검색(Retriever) 결합. 질문 → 검색 → 검색결과(Context)를 붙여 답변, 환각 감소.

- Tool Calling 개념
  LLM이 텍스트로 "함수 호출 스펙(JSON 스키마)"를 출력하며, 외부 시스템이 그 함수를 실행하고 다시 LLM에 넘기는 구조.

- Tool 예시
  웹검색(SerpAPI, Tvaily), Python 실행, Slack, 파일 쓰기 등.
  LLM이 "Taker(도구 사용)"이자 "Giver(도구 호출 포맷 생성)" 역할을 모두 수행 가능.

- Tool Calling 데이터셋 예: Glaive-function-calling-v2
  "SYSTEM+USER+ASSISTANT(Function Call)+FUNCTION RESPONSE+최종 답변" 포맷으로 훈련.

- Tool Calling 기대 효과
  텍스트 답변을 실제 행동(파일 저장, API 호출, 로봇 제어 등)으로 연결하는 Agent의 핵심.

- Memory
  Short-term: 세션 내 대화 히스토리.
  Long-term: 사용자/업무 별로 따로 저장하는 "스토리 형식"기억.

***

## 8. LangChain Agents & Agentic Work

- Tool Caliing 한계
  단순 작업: 한 번 호출로 끝.
  복잡 작업: 다단 Tool 사용, 오류 수정, 경로선택이 필요 → "Reasoning 능력"이 있는 Agent 필요.

- ReAct(2022)
  Reasoning + Acting = "생각(Thought) → 행동(Action) → 관찰(Observation)" 반복 프레임워크.

- Reflextion (2023)
  실패 경험에서 "반성(Reflection)"을 저장해 다음 시도에서 개선.

- Agent Q(2024)
  여러 실행 후보 중에서 Critic LLM이 "가장 유망한 액션"을 골라주는 구조.

- Tool Calling in LLM (2025)
  최신 Reasoning 모델: 별도 프롬프트 없이도 연속 Tool Calling 가능.
  하지만 작은/sLLM은 파싱 실패•불완전 사고•Context 붕괴 등 한계.

- Workflow vs Agentic Work
  - Workflow: 정해진 고정 플로우.
  - Agentic Work: Workflow 중 "판단이 필요한 지점"에만 LLM을 쓰는 것.

- Multi-Agent 구조
  - 역할 분리: 검색 Agent, 개발 Agent, 작성 Agent 등으로 나눠 토큰 낭비 줄이기.
  - Supervisor/Hierarchical/Multi-source RAG, LangGraph, STORM, Open Deep Research 등 다양한 패턴.

***
